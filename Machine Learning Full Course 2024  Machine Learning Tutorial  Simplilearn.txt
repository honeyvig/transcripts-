welcome to Simply learn's comprehensive
course on machine learning in today's
Tech driven World machine learning is a
game changer that's transforming
Industries and shaping the future so why
is it so powerful because it enables
machines to learn from data predict
outcomes and automate complex task
making our lives easier and businesses
more efficient so imagine having the
ability to create algorithms that can
detect patterns make decisions and even
solve problems without human
intervention the That's The Power of
machine learning it's a skill that's in
high demand and professionals with
machine learning expertise are highly
sought after in fact machine learning
specialist can earn top salaries and
enjoy exciting career opportunities but
machine learning isn't just for data
scientist it's a versatile skill that
can boost your value in almost any Tech
role so whether you are a developer
analyst engineer or entrepreneur
understanding machine learning can give
you a significant Edge and help you stay
ahead in your job market and in this
course we will cover what is machine
learning what is deep learning linear
algebra for machine learning how to
learn Ai and ml in 2024 and common
machine learning interview questions and
by the end of this course you will learn
how to use machine learning to boost
your productivity and open up new career
opportunities so are you ready to
supercharge your skills and stay ahead
in the software industry so let's dive
in and unlock the power of machine
learning together craving a career
upgrade subscribe like and comment
below dive into the link in the
description to FastTrack your Ambitions
whether you're making a switch or aiming
higher simply learn has your
back and if you are interested in making
a career in machine learning and AI
unlock your potential in Ai and machine
learning with simply learns professional
certificate course in generative Ai and
machine learning in collaboration with I
Cano this course is designed for
aspiring EI professionals and this
program offers hands-on experience with
machine learning algorithms deep
learning NLP and guided by the industry
experts and IIT kpur faculty you will
gain essential AI skills through
interactive live classes self-based
videos and practical projects you can
network with a community of Learners and
professionals and earn a prestigious
certificate upon completion and you can
find the course Link in the description
box and pin comment Sci-Fi movies and
books depict a future with sentinent
machines capable of interacting with
humans and Performing tasks just like
people would what if we told you that
this future is already a reality all of
this is already made possible by Machine
learning machine learning is the science
of programming machine machines to think
and act like humans without being
specifically programmed to we already
use machine learning in our daily life
without knowing it email spam
recognition spell check even the YouTube
video recommendation which brought you
here are implemented using machine
learning machine learning uses
algorithms to learn tasks these
algorithms are fed with data from which
they learn to perform these tasks this
means that over time as changes in data
occur we don't need to reprogram our
application
just let it find patterns and learn from
the new data machine learning is a
subset of artificial intelligence which
is a science concerned with imparting
humanlike intelligence onto machines and
creating machines which can sense reason
act and adapt deep learning is a
subbranch of machine learning which is
inspired by the working of the human
brain machine learning is leading us to
a future where machines can learn and
think and has opened us a whole new
plethora of job opportunities so what is
deep learning deep learning is a subset
of machine learning which itself is a
branch of artificial intelligence unlike
traditional machine learning models
which require manual feature extraction
deep learning models automatically
discovers representation from raw data
so this is made possible through neural
networks particularly deep neural
network which consist of multiple layers
of interconnected nodes so these neural
network are inspired by the structure
and the function of human brain layer in
the network transform the input data
into more abstract and composite
representation for instance in image
recognition the initial layer might
detect simple features like edges and
textures while the deeper layer
recognizes more complex structure like
shapes and objects so one of the key
advantage of deep learning is its
ability to handle large amount of
unstructured data such as images audios
and text making it extremely powerful
for various application so stay tuned as
we de deeper into how these neural
networks are trained the types of deep
learning models and some exciting
application that are shaping our future
types of deep learning deep learning AI
can be applied supervised unsupervised
and reinforcement machine learning using
various methods for each the first one
supervised machine learning in
supervised learning the neural network
learns to make prediction or classify
that data using label data sets both
input features and Target variables are
provided and the network learn by
minimizing the error between its
prediction and the actual targets a
process called B propagation CNN and RNN
are the common deep learning algorithms
used for tasks like image classification
sentiment analysis and language
translation the second one unsupervised
machine learning in unsupervised machine
learning the neural network discovers Ms
or cluster in unlabeled data sets
without Target variables it identifies
hidden pattern or relationship within
the data algorithms like Auto encoders
and generative models are used for tasks
such as clustering dimensionality
reduction and anomaly detection the
third one reinforcement machine learning
in this an agent learns to make decision
in an environment to maximize a reward
signal the agent takes action observes
the records and learns policies to
maximize cumulative rewards over time
deep reinforement learning algorithms
like deep Q networks and deep
deterministic poly gradient are used for
tasks such as Robotics and and game play
moving forward let's see what are the
artificial neural
networks artificial neural networks Ann
inspired by the structure and the
function of human neurons consist of
interconnected layers of artificial
neurals or units the input layer
receives data from the external
resources and it passes to one or more
hidden layers each neuron in these
layers computes a weighted sum of inputs
and transfer the result to the next
layer during training the weight of
these connection are adjusted to
optimize the Network's performance a
fully connected artificial neural
network includes an input layer or more
hidden layers and an output layer each
neuron in a hidden layer receives input
from the previous layer and sends its
output to the next layer so this process
continues until the final output layer
produced the network response so moving
forward let's see types of neural
networks so deep learning models can
automatically learn feature from data
making them ideal to T like image
recognition speech recognition and
natural language processing so the most
common architecture and deep learnings
are the first one feed foral neural
network fnn so these are the simplest
type of neural network where information
flows linearly from the input to the
output they are widely used for tasks
such as image classification speech
recognition and natural language
processing NLP the second one
convolutional neural network designed
specifically for image and video
recognition CNN automatically learn
feature from images making them ideal
for image classification object
detection and image segmentation the
third one recurrent neural networks RNN
are specialized for processing
sequential data time series and natural
language they maintain and internal
state to capture information from
previous input making them suitable for
task such as spee recognition and LP and
language translation so now let's move
forward and see some deep learning
application the first one is is
autonomous vehicle deep learning is
changing the development of self-driving
car algorithms like CNS process data
from sensors and cameras to detect
object recognize traffic signs and make
driving decision in real time enhancing
safety and efficiency on the road the
second one is Healthcare diagnostic deep
learning models are being used to
analyze medical images such as x-rays
MRIs and CT scans with high accuracy
they help in early detection and
diagnosis of diseases like cancer
improving treatment outcomes and saving
lives the third one is NLP recent
advancement in NLP powered by Deep
learning models like Transformer chat
GPT have led to more sophisticated and
humanik text generation translation and
sentiment analysis so application
include virtual assistant chat Bots and
automated customer service the fourth
one def technology so deep learning
techniques are used to create highly
realistic synthetic media known as defix
while this technology has entertainment
and creative application it also raises
ethical concern regarding misinformation
and digital manipulation the fifth one
predictive maintenance in Industries
like manufacturing and Aviation deep
learning models predict equipment
failers before they occur by analyzing
sensor data the proactive approach
reduces downtime lowers maintenance cost
and improves operational efficiency so
now let's move forward and see some
advantages and disadvantages of Deep
so first one is high computational
requirements so deep learning requires
significant data and computational
resources for training whereas Advantage
is high accuracy achieves a
state-of-the-art performance in tasks
like image recognition and natural
language processing whereas deep
learning needs large label data sets
often require extensive label data set
for training which can be costly and
time consuming together so second
advantage of deep learning is automated
feature engineering automatic
discovers and learn relevant features
from data without manual intervention
the third disadvantage is
overfitting so deep planning can overfit
to training data leading to poor
performance on new unseen data whereas
the third deep learning Advantage is
scalability so deep learning can handle
large complex data set and learn from
massive amount of data so in conclusion
deep learning is a transformative leap
in AI mimicking human neural networks it
has changed healthare Finance or
autonomous vase and NLP ever wondered
how computers make sense of massive
amounts of data today we're going to
peek behind the curtain and explore a
secret weapon used by data scientists
and ml Engineers linear algebra so in
this video we'll explore the fundamental
concepts of linear algebra that are
essential for understanding and solving
problems in statistics we'll start by
discussing what linear algebra is and
why it's important in data science next
we'll delve into solving linear systems
of equations which are equations
involving multiple variables you'll see
how we can find solutions to these
systems using simple methods then we'll
move on to matrices you'll learn what a
matrix is the different forms it can
take and how we can perform various
operations on matrices such as addition
subtraction and multiplication these
operations are crucial for many
statistical computations finally we'll
talk about vectors vectors are like
lists of numbers and they have specific
properties that make them useful in many
areas of data science by the end of this
video you'll have a solid grasp of the
key Concepts in linear algebra and be
ready to apply them to your data science
projects craving a career upgrade
subscribe like and comment
below dive into the link in the
description to FasTrack your Ambitions
whether you're making a switch or aiming
higher simply learn has your
back also also if you are interested in
becoming a machine learning engineer
then try simple learns postgraduate
program in Ai and machine learning from
Purdue University in collaboration with
IBM this course teaches in demand skills
such as machine learning deep learning
NLP computer vision reinforcement
learning generative AI prompt
engineering chat GPT and many more so
don't forget to check out the course
link from the description box and pin
comments All right so let's start with
the first topic
[Music]
learning objectives welcome to math
refresher linear
algebra in this lesson we are going to
explain the concepts of linear algebra
solve a linear system of equations
describe Matrix forms of Matrix and
Matrix operations and Define vectors and
list down its properties
introduction to linear
algebra linear algebra linear algebra
refers to study of linear combinations
for linear transformations to be carried
out a study of vector spaces lines and
planes as well as some mappings as
necessary it contains linear functions
vectors and matrices it is an
examination of characteristic of linear
set
Transformations linear equation
linear algebra's major goal is to
establish systematic techniques for
solving systems of linear
equations a linear equation with n
variables can be represented as A1 X1
plus A2 X2 plus A3 X3 and so on till a n
xn equals to B where X1 X2 X3 and so on
till xn are the unknown quantities that
are to be found
while A1 A2 A3 and so on until a in are
the
coefficients B here is a constant term a
linear equation with two variables X and
Y is a linear connection between X and Y
the value of one variable often Y is
determined by the value of the other
that is X the independent variable in
this situation is X while the term
dependent variable refers to Y since it
depends on X let's consider some
examples for linear
equations linear equations in one
variable can be written as 3x + 5 = 0 or
3x 2 of x + 7 = to 0 or 98x = to
49 linear equations in two variables
examples can include y + 7 x = 3 or 38 a
+ 2 B = 5 6 x + 9 y - 12 = 0 linear
equations can also be written with three
variables as x + y + z = to 0 Aus 3B =
to C 3x + 12 y = to half of Z
identifying linear and nonlinear
equations linear equations present
themselves as a straight line on a graph
however nonlinear equations represent
themselves as a non-straight line on the
graph C equations like y = to 8x -
9 or y + 3x - 1 = to 0 are examples of
linear
equations however y = to x^2 - 7 is a
nonlinear equation because X is raised
to the^
2 hence denoted a nonlinear relationship
or a non-straight line relationship
between X and
Y similarly square root of y + x = 6 or
Y raed to^ 2 - x = 9 are examples of
nonlinear equations as Y is raised to
the power half and two
respectively forms of linear
equation linear equations can be
represented in three forms a standard
form a slope intercept form or a point
slope
form linear equation in standard form a
linear equation in standard form for a
single variable can be represented as a
x + b equals to zero where A and B are
real integers and X is the variable
while a linear equation in standard form
for two variable can be represented as
ax + b y = 2 C where a B and C are real
integers while X and Y are the
variables to summarize a linear equation
for variables can be written as a x + B
yal to C where X and Y are the
variables C is the constant and a and b
are the coefficients of X and Y
respectively linear equation in the
slope intercept form a linear equation
slope can be calculated to see how one
variable varies in response to a unit
change in another variable it is
represented as y = to mx + b where m is
the slope B is The Intercept on the Y
AIS and X and Y are the lines distance
from the X and the Y AIS
respectively consider this example the
line's point x y represents the distance
from the X and the Y AIS
respectively while the line intercept on
the Y AIS is at 0 comma
B linear equation slope the slope
indicates how steep a line is with
respect to Y AIS when seen from left to
right it indicates whether the line goes
up or
down the slope describes how independent
variable has been changed while the
dependent variable is
changing types of slope there are four
types of slopes based on the
relationship between the two variables X
and Y the first is the positive slope
here as X
increases Y is also increasing in a
negative slope as X increases y
decreases if a line is parallel to xais
it is a zero slope
on the other hand if the line is
parallel to Y AIS then the slope is
undefined linear equation in point slope
form a straight line is represented in
point slope form by its slope and a
point on the
line it is represented as y - y1 = to M
of x -
X1 where X1 and y1 are the coordinates
of the point
linear equation forms some
examples consider solving the linear
equation given as 2x - 10 / 2 = to 3x -
1 first we will clear the
fraction in this example we can see only
left hand side we have 2x - 10 / 2 if
from the numerator we take two as common
we will be left with 2 * x - 5 / 2 hence
x - 5 on the left hand side on the right
hand side we still have 3 * x -1 now as
a part of next step we simplify both
sides of the equation that has open up
the brackets when we do that on the
right side we end up getting 3x - 3 by
multiplying 3 to x
-1 solving further we can write as X =
3x + 2 finally we try to clear the
fraction to solve for x we can rearrange
the equation as x - 3x =
2 solving it further we get - 2x = to
2 and then we solve for x as
minus1 system of linear equation a
system of linear equations is a finite
collection of linear equations equations
usually involving the same variables it
can be represented as a11 X1 plus a12 X2
and so on equals to B1 a21 X1 + A2 2x2
so on equals to B2 and so on here this
example is for n
variables a system of M
equations a linear system can have
unlimited number of solutions one
solution or no solution at all in case
of a single linear equation a consistent
linear system has a solution while an
inconsistent linear system has no
solution solving a linear
equation solving linear system of
equations various methods can be used to
solve linear system of equations which
include graphic method substitution
method linear combination or elimination
method or a matrix
method graphing method let's try and
solve these two linear equations using
the graphic method the first equation is
defined as y =
0.5x + 2 and the second equation is Y =
-2x - 3 the two equations as we can see
are in the slope intercept form the
first line has a slope of 0 5 and a y
intercept of pos2 while the second line
has a slope of -2 and a y intercept
of3 let's have a look at these lines
through a
plot here the blue line is the first
line represented by y = 2.5 x +
2 and the red line is represented by y =
to -2X -3
these two lines intersect at a point at
-2 comma
1 hence we can say that X = to -2 and Y
= to
1 solving systems of linear equation
using
substitution for substitution we start
by putting one of the equations in the
form of variable equals
2 we then sub subtitute that variable in
the other equation in its place take the
other equation to
solve if there are more than two
variables involved we repeat the process
from steps 1 through
three let's consider an
example let's consider the equation 3x +
2 y = to 19 and x + y = to 8 we could
begin with any equation and variable in
this case let's look at the second
equation with variable
y we start with subtracting X from both
sides of x + y =
8 and we end up getting y = 8 -
x the first equation still retains its
form now we take the value of y that is
8 - x and substitute in the first
equation in place of
Y we end up up getting the equation 3x +
2 * 8 - x =
19 we further simplify and solve this
equation using algebra method to get 3x
+ 16 - 2x =
290 we solve this equation further and
we end up getting x + 16 = to 19 and
finally X = to
3 now we take the value of x and
substitute in the new reformed second
equation as y = 8 -
x that gives us y = to 8 - 3 hence y =
to 5 The Final Answer remains X = to 3
and Y equals to 5 solving systems of
linear equation using elimination in
this case we start by multiplying an
equation by a constant anything other
than
zero we then then add or subtract an
equation onto the other
equation let's consider the same example
of 3x+ 2 y = 29 and x + y =
8 we first start by multiplying the
second equation by 2 and we get 2x + 2 y
=
216 now we subtract the second equation
from the first
equation that is 3x x -
2x 2 y - 2 Y and 19 -
60 on solving this we end up getting X =
to
3 Now using these steps of substitution
we put the value of x in the original
second equation that is x + y =
8 and achieve a value of y = to
5 hence the answer remains X = 3 and Y =
to
5 we can verify the solution by using
the
graph here again the Blue Line
represents the point at which 3x + 2 y =
to 19 is true while the red line
represents the point where X + Y = to 8
is
true the solution is found at the point
where X = to
3 and Y = to
5 this is the point where both lines
intersect introduction to
matrices
Matrix a matrix is rectangular array or
table with rows and Columns of numbers
symbols Expressions used to represent a
mathematical object or an
attribute for example here a is a matrix
of two rows and three columns as 2 1 3
minus one 2 and 4 well B is a squared
Matrix with two rows and two columns
given as 2 1 3 4 Matrix size The Matrix
size is expressed as M cross n where m
is the number of rows and N is the
number of columns a matrix with two rows
and three columns can be referred to as
2x3 Matrix or a 2 cross3 Matrix or a
matrix of Dimension 2 cross
3 notation of a matrix a matrix with M
rows and in columns can be presented as
this here each element will be
identified by its row number and column
number for example the element over here
is in the third row and the First
Column hence it is identified as a 31
forms of Matrix The Matrix is termed as
a squared Matrix if n equal to M that is
the number of rows and number of columns
are
equal an element's entry in The Matrix
of from a i i is located on the
diagonal he is called a diagonal matrix
if hey I J equals to
zero when I is not equals to
J that is in a square
Matrix apart from diagonal at all places
we have zero as a value Matrix
operations addition addition of two
matrices is adding their corresponding
elements as in a11 + b11 a12 + B12 a21 +
B21 a22 + B22 and so on
consider the example over here we have a
matrix a as 22 32 11 16 and Matrix b as
13 8 13 and 16 Matrix a is of the order
2 cross 2 and Matrix B is also of the
same order 2 cross 2 hence the
corresponding elements can be quickly
added as 22 +
13 32 + 8 11 + 13 and 16 + 16 giving us
the result in Matrix of 35 40 24 and
32 addition rules only matrices with the
same size can be
added that is the two matrices should
have same number of rows and same number
of columns Matrix addition follows
commutative property that means a + b is
equals to B+ a
subtraction Matrix subtraction is same
as Matrix
addition it is subtracting the
corresponding
values let's consider the same example
of Matrix A and B the result of a minus
B can be given as 22 -3 that is a11 -
b11 32 - 8 that is a12 - B12 11 - 13 a21
- B21 and 16 - 16 that is a22 minus
B22 giving us result as 9 24 -2 and
0 subtraction rules only matrices with
the same number of rows and columns can
be
subtracted Matrix subtraction does not
follow commutative
property that means a minus B is not
equals to B minus
a
multiplication matrix multiplication
results in what is called as a product
Matrix in matrix
multiplication each row of the first
Matrix is Multiplied to each column of
the second
Matrix to compute each of these element
in the Matrix the values are then added
for example consider the Matrix A and
B to get the first element we will
multiply the first row of Matrix a to
the First Column of Matrix
B and then add the individual elements
that is 22 multiply to 13 and 32
multiply to 13 and then add them up to
get this particular
element for the second element we
multiply the first row of a to the
second column of B hence getting 22
multiplied to 8 + 32 multili to
16 similarly we compute all the
remaining elements that is the second
row multiplied to the First Column and
added 11 multiplied to 13 + 16
multiplied to 13 and the second row now
multiplied to the second column that is
a 11 * to 8 + 16 * to
16 this gives us the resulting Matrix as
72
688
351 and
344 multiplication
rules let us say C equals to
AB here C's our product
Matrix which can be computed as
summation over AJ .
bjk for a matrix of size i j Matrix B of
size
JK to calculate the value of each
member in the I cross K Matrix of
C the matrix product a is defined only
when the number of columns in a equals
to the number of rows in
B the matrix product of ba is defined
only when the number of col colums in B
is equals to the number of rows in a a
do B is not always equals to b.
a
transpose a transpose is a matrix form
by turning all the rows of a given
Matrix into the columns and vice
versa the transpose of a matrix a is
denoted as a T for example if the Matrix
a is 22 32 11 and and 16 the transpose
of a would be 22 11 32 and
16 here we can see the row has been
converted to
column inverse if it is a non- singular
Square
Matrix there exists n cross n Matrix a
minus one known as A's
inverse that satisfies a. a inverse
equals to A inverse a equals to
I where I is the identity
Matrix and identity Matrix is
characterized as a diagonals one and all
other elements are
zero also a equals to b.
a equals to i n where I N denotes the N
cross n identity
Matrix special Matrix types symmetric
Matrix a matrix a is set to be symmetric
if a is equals to transpose of a
diagonal matrix a matrix D is diagonal
only if D equals to zero for all I not
equals to J identity
Matrix identity Matrix is denoted as I
in where a equal to I a equal to a
tensors tensors are arrays with more
than two
axes a tensor can have in dimensions a i
j k is the value in the tensor a at
coordinates J J and K the principal
component analysis
PCA which is a statistical technique
used to reduce the number of dimensions
in a given data set while we preserve
its
variability so there are different use
cases of PCA in realtime scenarios one
of the major use case is in the
healthcare domain so technically known
as the spike triggered covariance
analysis it uses the principle of PCA in
Neuroscience to identify the specific
properties of a stimulus that increase a
neuron's probability of generating an
action
potential so it is using reduction
techniques which is going to be really
helpful in detecting the coordinated
activities of of large neuronal
ensembles so it is basically used for
identifying a neuron from the shape of
its action potential then in terms of
quantitative Finance so PCA help us in
reducing the dimensionality of a complex
problem let's say a fund manager has 200
stocks in his portfolio now to analyze
these stocks quantitatively a stock
manager will require a correlational
metric of a size 200x 200 which makes
the problem very
complex however if he has to extract 10
principal components which best
represents the variance in the stock
price this will reduce the complexity of
the problem while still explaining the
movement of all 200 stocks so we use it
for analyzing the shape of the yield
curve in terms of forecasting the
portfolio returns developing the asset
allocation algorithms and so on and it
is all Al widely used in image
compression so PCA is also used for
image compression so if we go ahead and
use any of the pie charm IDE so here we
can work with Matt plot lib image P plot
and the numpy as NP and here we can also
import from sklearn and we can import
the PCA so basically we are trying to
print the shape of the image by reading
the image this is a sample logo.png
which has been placed in the same
directory we are trying to print the
image so if we run this particular
code then we will be able to see the
shape of the image return to us first
the current shape based on the image
which has been deducted which is
156 94 and 3 so what does this mean this
means image is in the form of
156 rows and each row containing 19 4
pixels and then having three channels
which is red green and blue so we had to
resize the image so that it is in a
format required for PCA input which is
196x 3 which is going to be
582 so here we can see we have performed
a simple operation as image R where we
are using the numpy library for
reshaping the image and then we are
printing the shape of the image again so
here we can see
156 and
582 has been returned as a response now
running the PCA with 32 principal
components we can use this fit for the
image that has been reshaped and then we
are trying to train the transform image
so with these 32 components we are able
to express
98.7% of the variance so inverse
transforming the PC output and reshaping
the visualization can also be done
forward and that's how PCA is widely
used as a part of the medical and the
other Sciences for example is used for
object detection model for the
healthcare domain for the transportation
Aviation and location based
Services introduction to
vectors objects with both magnitude and
direction are called as vectors the
magnitude of the vector determines its
size it is represented as a line with an
Arrow where the length of the line
indicates the vector's magnitude and the
arrow points in the desired Direction
other names for it include ukian Vector
geometric Vector spatial vector or
simply
Vector its length is indicated by the
symbol double bar vble bar and it begins
at the original 0 comma 0 notation of a
vector a vector can be represented in a
standard form as a equals to A I cap
plus b j cap plus c k cap where a B and
C are numeric values the unit factors
along the XY Z axis are i j and k cap
respectively as we know in 2024 Ai and
ml are evolving at lightning speed AI
tools like CH GPD Google Gemini and
apple Cutting Edge AI are transforming a
world chbt is changing customer service
l translation and even creative content
whereas Google Geminis is pushing the
boundaries of AI enhancing search
experience and making information access
smarter and more intuitive Apple CI with
Innovations like Ci and Apple's
intelligence is taking user experience
to the next level with a smarter
personal assistant enhanced device
performance and top-notch privacy
features with these advancement these
Technologies are driving a huge demand
for AI engineers and ml Engineers
companies across the globe are eager to
integrate these Advanced solution to
boost efficiency personalize customer
experience and innovate in the fields
like healthare logistic and in many
others so the opportunities in AI anml
field are endless and Incredibly
exciting so imagine being a part of this
revolutionary wave shaping the future
with your skills so in this video we
will show you exactly how to get there
we will cover the essential skills you
need to learn the best learning path
exciting Hands-On project and the
hottest opportunities whether you are
just starting out to looking to advance
your career this s will guide you to
successfully become Ai and ml engineer
so before moving forward let me give you
a quick information as a demand for
machine learning professionals keep
increasing it's a crucial for aspiring
individuals to have a clear road map to
navigate their Learning Journey craving
a career upgrade subscribe like and
comment
below dive into the link in the
description to FastTrack your Ambitions
whether you're making a switch or aiming
higher simply learn has your
back then try a simpl postgraduate
program in Ai and machine learning from
P University in collaboration with IBM
this core teaches in demand skills such
as machine learning deep learning NLP
compter Vision reinforcement learning
generative AI prompt engineering chat
GPT and many more so don't forget to
check out the course link from the
description box below and the pin
comment so without any further Ado let's
get started so how this machine learning
road map will help you this machine
learning road map offers a clear and
structured path to mastering machine
learning by following this road map you
will not only gain valuable knowledge
but also develop a mindset gear towards
Innovation and adaptability so what is
machine learning and AI imagine a
computer that learns from data like a
student that's machine learning and
machine learning is a subset of
artificial intelligence AI that enables
computer to learn from the data and make
prediction or decision without being
explicitly programmed it uses algorithm
and Stat models to improve performance
or a specific task through experience or
data input so Artificial Intelligence on
the other hand covers a broader scope
and involves developing computer system
that perform tasks typically requiring
human intelligence so machine learning
is a crucial component of AI providing
the capability to learn and adapt from
the data so now let's move forward and
see the steps for AIML road map so step
by-step machine learning road map so
this step-by-step machine learning road
map guides you through mastering ml a
vital branch of AI typically over
several months to a year so depending on
your background so start with prequest
like programming python R stat and
linear algebra progress through
understanding data processing learning
algorithms and model evaluation and
optimization so this structured approach
combined with Hands-On projects will
solfi you ml expertise while preparing
you for the advanced topics and machine
learning applications in the dynamic
field so now let's get started the step
one mastering mathematics so to excel in
machine learning a strong foundation in
mathematics is essential so focus on the
following areas the first one is linear
algebra and calculus so linear algebra
is the backbone of many machine learning
algorithms it helps you understand how
each algorithm Works whereas calculus is
crucial for optimizing algorithm used in
machine learning key concept include
like vectors metrics linear equations
ENT values differentiation integration
and gradient descent the second one is
probability and statistics so
probability and Stat are fundamentals
for analyzing data and making prediction
so here are some important topics like
probability distribution descriptive
stat hypothesis testing regression
analysis and basian Stat so moving
forward step two developing programming
skills Proficiency in programming is
essential focus on Python and R the top
language for machine learning the first
one python python is a popular due to it
Simplicity and extensive Library like
numai Panda psyched learn so it's great
for both beginners and expert the second
one is R programming R is excellent for
statical analysis and data visualization
platforms like simply offers specialized
sces in the r programming the third
important python libraries learn
libraries like numai for numerical
operations pandas for data manipulation
M plotly and cbon for data visualization
and psychic learn for machine learning
the third one exploring core machine
learning ml algorithms so once you have
got a good handle on math and
programming it's time to learn core
machine learning algorithm so
understanding these will help you solve
real world problems so here are some key
algorithms to explore the first one is
unsupervised learning algorithm explore
clustering that is K means clustering
and dimensionally reduction that is PCA
for understanding patterns in unlabeled
data the second one is supervised
learning algorithms learn regression for
continuous outcomes and classification
for discrete labels covering methods
like linear regression logistic
regression K neighbor and support Vector
machine svms and the third one is model
evaluation and validation understand
evaluation metrics like accuracy
precision recall and F1 score learn
cross validation and performance
performance metric to s's model
performance and you can also learn other
important machine learning algorithm
like reinforcement learning gradient
descent and algorithm for slope
understanding step four learn Advanced
Topic in machine learning so as you
advance in machine learning the journey
it's important to delve into more
advanced topics so these areas will
deepen your understanding and help you
tackle complex problems so here are some
key topics to focus on the first one is
deep learning and neural networks the
second is anible learning technique
third one is generative models and
adversarial learning the fourth one is
recommendation system and collaborative
filtering the last one is time series
analysis and forecasting so for step
five there is learn
deployment so you have to learn how to
deploy models using flask D Jango cloud
services like AWS aure gcp then Docker
and kubernetes so deployment skills are
crucial for making your models
accessible and usable in real world
applications so moving forward let's see
some machine learning projects so work
on real world projects to apply your
knowledge focus on data collection
preparation and Capstone project in
image recognition NLP and there is one
more predatory modeling and anomaly
deduction so practical experience is the
key to solidifying your skills step
seven continuous learning and
exploration so stay updated with the
latest development by the following
industry leaders engaging in online
communities and working on your personal
projects so pursue Advanced learning
through courses and certification to
keep your skills sharp so now moving
forward let's see machine learning
career opportunities with sell so the
job market for machine learning
professional is booming the average
annual salary for machine learning
Engineers can vary based on location
experience and Company so here are some
roles like machine learning engineer
data scientist NLP engineer computer
vision engineer and AIML researcher so
let's see how much they earn so the
first one machine learning
engineer in us they earn around
$153,000 and in India they earn around
11 lakhs per the second is data
scientist they earn in us around
$157,000 and in India they earn around
12 lakhs per the third one is NLP
engineer in us they earn around
$17,000 and in India they earn around 7
lakh per the fourth one div Vision
engineer they earn around
$126,000 in the US and in India they
earn around 6.5 lakh per the last one Ai
and ml Searcher they earn around
$130,000 in the US and 9 lakh perom in
India so note that these figures can
vary on the website to website and
changes frequently the machine learning
road map provides a structured guide to
help you navigate this Dynamic field by
following the step-by-step guide and
continuously honing your skills you can
embark on a successful career in machine
learning embrace the challenge stay
curious and equip yourself with the
necessary knowledge and expertise to
thrive in this ever evolving domain
hello everyone and Welcome to our video
on top 10 machine learning skills
machine learning has been the Talk of
the Town lately every organization has
realized the potential of machine
learning in improving their business
objectives and attending Enterprise
goals this expanding demand has led to
many people applying for machine
learning jobs and upskilling themselves
in the field of machine learning so you
can take up this growing opportunity in
the field of machine learning and
utilize it to land yourself a very
challenging fulfilling and high-paying
job so in this video we'll be breaking
down in complete detail every skill you
would need to crack the machine learning
engineer job interview the demand for
machine learning professionals is
skyrocketing across industries from
Healthcare and finance to entertainment
and transportation organizations are
actively seeking talented individuals
who can harness the power of artificial
intelligence to drive their business
forward but what skills does it take to
become a machine learning engineer how
can you embark on this thrilling Journey
we have answers to all your questions
also if you want to become an AI expert
and gain handsome salary packages look
at a wide range of AI and machine
learning courses by simply Lear in
collaboration with the top universities
across the globe by enrolling in any of
these certification programs you will
gain expertise in skills like generative
AI prompt engineering J GPT explainable
AI machine learning algorithm supervised
and unsupervised learning model training
and optimization and there's much more
on the list with hands-on experience and
tools like chat GPT Dal python open CV
and tensorflow you will catch the eye of
top recruiters so what are you waiting
for hurry up and enroll a year of
experience is preferred to enroll in
these courses so find the course link
mentioned in the description box below
so without any further delay let's get
started before moving on to machine
learning skills let's take a quick look
at who is a machine learning engine as
machine learning is intricately
connected to data science there are
overlaps in the roles of data scientist
data analyst and machine learning
engineer however data scientist and
analyst primarily concentrate on
extracting insights from data presenting
them for organized decision making with
some knowledge of machine learning
algorithms in contrast machine learning
Engineers are exclusively dedicated to
machine learning creating software
components for minimal human
intervention to derive insights from
provided data this underscores the
emphasis on computer fundamentals and
software development as their
specialized skills to become a machine
learning engineer you need a combination
of Technical and non-technical skills so
here are some essential skills required
to pursue a career as a machine learning
engineer skill number one is programming
languages strong programming skills are
essential you should be proficient in at
least one programming language such as
python Ora python is widely used in
machine Learning Community due to its
Rich libraries for example example numpy
Panda tensorflow pych that supports
machine learning tasks machine learning
algorithms and techniques you should
have a solid understanding of various
machine learning algorithms such as
linear regression logistic regression
decision trees random forests neural
network and deep learning so familiarize
yourself with these algorithms
principles pros and conses and when to
use them moving on to skill number three
we have data pre-processing machine
learning models require clean and
well-prepared data you you should know
how to handle missing data deal with
outliers normalize all standardized data
and perform feature engineering
understanding data pre-processing
techniques is crucial for Effective
machine learning model training moving
on to skill number four we have data
manipulation and Analysis data is the
foundation of machine learning models
you should be skilled in data
manipulation and Analysis using
libraries like numpy and pendas this
includes cleaning and transforming data
exploratory data analysis and
understanding the statistical properties
of the data the next skill we have in
our list is machine learning libraries
and Frameworks familiarity with popular
machine learning libraries and framework
is essential some commonly used ones
include numpy pandas tensorflow and pyou
these libraries provide pre-implemented
machine learning algorithms neural
network architectures and tools for
model training and evaluation the next
skill we have in our list is computer
vision computer vision and machine
learning are fundamental branches of
computer science that can independently
fuel highly Advanced systems relying on
CV and machine learning algorithms
however their combination has the
potential to unlock even greater
possibilities and achievements moving on
to the next skills we have neural
networks like the human brains neurons
neural network are pivotal for machine
learning Engineers they consist of
layers processing input valuable output
through parallel and sequential
computations various types exist like
feed forward recurrent convolutional and
while detail knowledge isn't mandatory
grasping core fundamentals is essential
for aspiring machine learning Engine
moving on to the next skill it's natural
language processing natural language
processing or NLP is a fundamental
aspect of Machining its objective is to
impart the complexities of human
language to computers enabling them to
comprehend and interpret human
communication more effectively numerous
libraries serve as the basis for natural
language processing offering functions
to help computers understand natural
language by passing text based on syntax
extracting key phrases eliminating
irrelevant words and one prominent
example is the natural language toolkit
a widely used platform for developing
NLP related applications now that we
have seen the Technical Machine learning
engineer skills let us have a look at
the non-technical skills machine
learning Engineers required the first is
industry knowledge machine learning
projects that effectively tackle genuine
challenges are likely to achieve great
success regardless of the industry you
are involved in it is crucial to
comprehensively understand its
operations and identify ways to optimize
business outcomes moving on to the last
scale for the video we have effective
communication effective communication
plays a crucial role in facilitating
these interactions companies seeking
skilled machine learning Engineers value
candidates who can effectively convey
technical discoveries to non-technical
teams like marketing or sales
demonstrating Clarity and fluency in
their explanations remember that the
field of machine learning is constantly
evolving so continuous learning and
staying updated with the latest de
velopments and research papers are
essential to be a practical machine
learning engineer the term generative AI
has emersed seemingly out of nowhere in
recent months with a notable search in
interest according to Google Trends even
within the past year the spike in
curiosity can be attributed to the
introduction of generative models such
as d 2 B and
chbt however what does generative Ai and
tail as a part of our introductory
series on generative AI this video will
provide a comprehensive overview of
subject starting from the basics the
explanation Will C to all levels of
familiarity ensuring that viewers gain a
better understanding of how this
technology operates and its growing
integration to our daily lives
generative AI is after all a tool that
is based on artificial intelligence a
professional who elit seats to switch
careers with AI by learning from the
experts then try giving a short simply
lar postgraduate program in Ai and
machine learning from per University in
collaboration with IBM the link in the
description box should navigate to the
homepage where you can find a complete
overview of the program being offered
take action up skill and get ahead what
is generative AI generative AI is a form
of artificial intelligence possesses the
capability of to generate a wide range
of content including text visual audio
and synthetic data the recent excitement
surrounding generative AI stems from the
userfriendly interfaces that allow users
to effortlessly create high quality text
graphics and video within a seconds now
moving forward let's see how does gener
generative AI Works generative AI begin
a prompt which can take form of text
image video design audio musical notes
or any input that AI system can process
various AI algorithm that generate new
content in response to the given prompt
this content can range from essay and
problem solution to realistic created
using images or audio of a person in the
early stages of generative AI utilizing
the technology involved submitting data
through an API or a complex process
developers need to acquaints with a
specialized tool and writing application
using programming language like python
some of the recent and fully operational
generative AIS are Google Bart D open AI
chgb Microsoft Bing and many more so now
let's discuss chat GPT Dal and B which
are the most popular generative AI
interfaces so first is DAL 2 which was
developed using open as GPT
implementation in 2021 exm simplify a
multimodel AI application it has been
trained on a v data set of images and
their corresponding textual description
Dal is capable of establishing
connection between various media forms
such as Vision text audio it
specifically links the meaning of wordss
to visual elements open a introduced an
enhanced version called d 2 in 2022
which empowers user to generate imagery
in multiple Styles based on their
prompts and the next one is Charity in
now November 2022 chat GPT and AI power
chatbot built on open AI GPD 3.5
implementation gained immense popularity
worldwide open AI enable user to
interact with and fine tune the chatbox
text response through a chat interface
with interactive feedback unlike earlier
version of GPT that was solely
accessible via an API CH GPT brought a
more interactive experience on March 14
2023 open a released GPT 4 chat GPT
integrate the conversion history with a
user making a genuine dialogue Microsoft
impressed by the success of new chgb
interface announced a substantial
investment in open Ai and integrated a
version of GPT into its Bing search
engine and the next one is Bard Google
bard Google was also an earlier Fortuner
in advancing Transformer AI techniques
for language processing protein analysis
and other content types it made some of
these model open source for researchers
but were not made available through a
public interface in response to
Microsoft integration of GPT into Bing
Google hardly launched a public facing
chat about name Google Bart bar deut was
met by an error when the language model
incorrectly claimed that the web
telescope was the first to discover a
planet in a foreign solar system as a
consequences Google stock price suffered
a significant decline meanwhile
Microsoft implementation of chat GPT and
GPT power system also face criticism for
producing inaccurate result and
displaying eratic behavior in their
early iritation so moving forward let's
see what are the use cases of generative
AI generative AI has broad applicability
and can be employed across a wide range
of use cases to generate diverse form of
content recent advancement like GPT have
made this technology more accessible and
customizable for various application
some notable use cases for generative AI
are as follows chatbot implementation
generative AI can be utilized to develop
chat for customer service and Technical
Support enhancing interaction with users
and providing efficient assistance the
second one is language dubbing
enhancement in the real in the realm of
movies and educational content
generative AI can contribute to
improving dubbing in different languages
ensuring accurate and high quality
translation and the third one is content
writing generative AI can assist in
writing email responses dating profiles
resums and term papers offering
valuable support and generating
customized content tailor to specific
requirement and the fourth one is Art
generation leveraging generative AI
artists can create photo realistic
artwork in various Styles enabling the
exploration of new artistic expression
and enhancing creativity the fifth one
is product demonstration videos
generative AI can hun to enhance protect
demonstration video making them more
engaging visually appealing and
effective in showcasing product features
and benefits so generative AI
versatility allow it to employ in in
many other application making it a
valuable tool for Content creation and
enhancing user experience across diverse
domains so after seeing use Cas of
generative AI let's see what are the
benefits of generative AI so generative
AI offers extensive application across
various business domains simplifying the
interpration and comprehension of
existing content while also enabling the
automated creation of a new content
developers are actively exploring ways
to leverage generative AI in order to
enhance the optimize existing workflows
and even to reshave workflows entirely
to harness the potential of Technology
fully implementing generative AI can
bring numerous benefits including
automated content creation generative AI
can automate the manual process of
writing content saving time and effort
by generating text or other form of
content the next one is efficient email
response responding to emails can be
made more efficient with generative a
reducing the effort required and
improving response time and the third
one is enhanced technical support
generative AI can improve responses to
specific technical queries providing
accurate and helpful information to
users or customers and the fourth one is
realistic person Generation by
leveraging generative AI it becomes
possible to create realistic
representation of people enabling
applications like virtual characters or
avatars and the fifth one is coherent
information summarization generative AI
can summarize complex information into
into a coherent narrative distilling key
points and making it easier to
understand and communicate complex
concept the implementation of generative
AI offers a range of potential benefits
steamingly process and enhancing content
Creation in various areas of business
operation so after seeing advantages of
generative AI let's move forward and see
what are the limitations of generative
AI early implementation of generative AI
serve as Vivid examples highlighting the
numerous limitation associated with this
technology sever challenges arise from
the specific approaches employed to
implement various use gifts for instance
while a summary of a complex topic May
more reader friendly than explanation
incorporating multiple supporting
sources the ease of readability comes at
the expense of transparent identifying
the information sources so the first one
is when implementing or utilizing a
generative AI application it is
important to consider the following
limitation I repeat the first one is
lack of source identification generative
AI does not always provide clear
identification of content Source making
it difficult to trace and verify origin
of the information the second one is
assessment of bias assessing the bias of
original sources used generative AI can
be challenging as it may be difficult to
determine the underlying perspective or
agenda of the data utiliz in the
training process the third one is
difficulty in identifying inaccurate
information generative AI can generate
realistic content making identifying in
accuracy or falsehoods within the
generated output harder and the fourth
one is adaptability to a new
circumstances understanding how to
fine-tune generative AI for a new
circumstances or specific context can be
complex requiring careful consideration
and expertise to achieve desired result
and the fifth one is glossing over bias
Prejudice and hatred generative AI
results May amplify or preate biases
prejudices or h ful content present in
the training data requiring Vigilant
scrutiny to prevent such issues so
awareness of these limitation is crucial
when the implementing of utilizing
generative AI as it helps users and
developers critically evaluate and
mitigate potential risk and challenges
associated with the technology so future
of generative AI furthermore advances in
AI development platforms will contribute
to the Exel progress of research and
development in the realm of generative
AI the development will encompass
various domains such as text images
videos 3D contact drugs Supply chains
logistic and business processes while
the current stand loan tools are
impressive the true transformative
impact generative AI will realize while
these capabilities are seemingly
integrated into the existing tools with
regular use imagine a world where
creativity knows no bounds where
machines can conjure of art music and
literature with the flick of a digital
switch this isn't the stuff of Science
Fiction it's the reality of generative
AI a Cutting Edge technology that's
reshaping our digital landscape picture
this according to a recent report by
Salesforce generative tools are already
in the hands of 27% of Millennials 28%
of genx and a staggering 29% of genzi
these aren't just numbers they are a
testand to the growing influence of
generative AI in our daily lives and as
organizations raise to harness its power
the demand for skilled generative AI
experts is skyting but what exactly is
generative AI it's more than just lines
of code it's a gateway to infinite
possibilities with generative AI
machines can create anything from images
to text to music all by learning from
existing data sets it's the technology
behind deep fakes virtual influencers
and even the next big hit song so why
should you care about generative AI in
2024 because it's not just the future
it's the present it's the key to
unlocking new Realms of creativity
Innovation and opportunity and in this
video we are going to show you how to
become a master of generative AI in 2024
so bugle up for the world of generative
AI because the future is here and it's
more exciting than ever before so
welcome to the road map of becoming a
generative AI expert in 2024 and before
we move on guys just a quick info for
you craving a career upgrade subscribe
like and comment
below dive into the link in the
description to FastTrack your Ambitions
whether you're making a switch or or
aiming higher simply learn has your
back if you're one of the aspiring Ai
and ml Enthusiast looking for online
training and graduating from the best
universities or a professional who
elicits to switch careers in Ai and ml
by learning from the experts then try
giving a short to Simply learns
postgraduate program in& ml in
collaboration with per University and
IBM and you can find the course Link in
the description box and pin comment so
let's get started guys so let's start
with why should you learn generative AI
in 2024 so learning generative AI in
2024 is crucial for several compelling
reasons as we will outline in this
diagram so the number one reason is
technological advancement generative a
represents a significant leap in the
evolution of Technology particularly in
its ability to generate complex outputs
like video audio text and images this
Innovation is set to expand
exponentially marking a new age of
technological innovation and the next
reason is wide ranging applications the
surge in interest and development in
generative AI is fueled by advancements
in machine learning models artificial
intelligence and platforms like chat jpd
and Bart these tools have broad
applications across various sectors
making knowledge in this area highly
valuable now moving to the next reason
that is solving complex problems
generative AI has the potential to
simplify problem solving processes
significantly its capabilities in
creating real estate models can be
applied to innovate and enhance
Solutions across across Industries and
the next reason is impact on major
Fields the integration of artificial
intelligence into major Fields is
undeniable and generative AI plays a
substantial role in this transformation
it not only presents a threat to certain
jobs but also opens up a plethora of new
opportunities in the tech industry and
Beyond and the next is dynamic and
unexplored field the field of generative
AI is filled with challenges and
unexplored territories offering an
exciting Frontier for those interested
in shaping the future of technology it
calls for creativity problem solving
skills and a willingness to delve into
the unknown so learning generative a in
20124 positions individuals at the
Forefront of technological innovation
equipping them with the skills and
knowledge to contribute to significant
advancements and explore new
possibilities in the digital world and
now we'll move to the major skills
required to learn generative AI in 2024
so to effectively learn and Excell in
generative a in 2024 individuals need to
possess a specific set of skills that
are foundational to understanding and
applying this technology so let's see
what those skills are so let's start
with the skills and the number one skill
is deep learning and fundamentals a
solid understanding of deep learning
Concepts is crucial this includes
familiarity with neural networks back
propagation and the various types of
deep learning models and architectures
and the next is machine learning
Concepts Proficiency in machine learning
is repeat Proficiency in machine
learning is necessary and compassing a
broad range of algorithms their
applications and an understanding of how
they can be used within generative a
Frameworks and then comes the Python
Programming Python Programming Remains
the dominant programming language in Ai
and machine learning Mastery over python
includes its syntax data structure
libraries such as tensorflow and py toch
and Frameworks is essential and the next
skill is generating models knowledge
specific knowledge of generative models
such as generative ADV verial networks
Gans and variational Auto encoders
repeat and variational auto encoders
vaes is required understanding how these
models function and are applied to key
to innovating within the generative VI
space and the next skill is image and
text processing skills in processing and
manipulating image and Text data are
necessary as many generative AI
applications involve creating or
modifying such content and the next on
the list is data processing and data
augmentation the ability to pre-process
and augment data efficiently can
significantly improve the performance of
generative models skills and data
cleaning augmentation techniques and
feature engineering are vital and then
comes ethical considerations with the
power of generative AI comes the
responsibility to use it ethically
understanding the ethical implications
of generative AI including issues of
bias fairness and piracy is crucial the
next is communication given the
interdisciplinary nature of generative
projects effective communication skills
are essential for collaborating with
teams explaining complex Concepts in
simple terms and and engaging with
stakeholders so developing these skills
will prepare individuals for the dynamic
and evolving field of generative
enabling them to contribute meaningfully
to advancements in technology and
address the challenges that come with it
so let's move to the road mapap to learn
generative AI in 2024 and the road map
is as follows so the first step is
understanding the basics of machine
learning then comes mastering
programming language that is mainly the
python then is the learning data science
and related Technologies and then we
have handson realtime projects and then
learning mathematical and statistics
fundamentals and then on the list is
developer skills and then we have the
important thing that is keep learning
and exploring so starting with the
number one point that is understanding
the basics of machine learning so let's
start by wrapping your head around the
core machine learning algorithms it's
like getting to know the tools in your
toolkit each has its unique use make
sure to understand the differences
between supervised unsupervised and
reinforcement learning think of them as
different paths to solving a puzzle some
are straightforward repeat some are
straightforward While others need you to
figure out the rules as you go get
comfortable with handling data after all
data is the fuel for your machine
learning engine learn how to clean split
and pre-process it to get to models
running smoothly learn how to evaluate
your models with metrics like accuracy
and precision it's like checking the
health of your model to ensure it's fit
for the real world and the next step for
your road map would be Master Python
Programming so focus on getting a strong
strong grip on python syntax and
structure python is the language of
choice in AI so this is the learning
repeat so this is like learning the
alphabet before writing stories dive
into libraries is essential for AI such
as pandas for data manipulation and
psychic learn for machine learning think
of these libraries as your shortcuts to
build powerful models practice writing
efficient code it's not just about
getting the right answer it's about
getting their faster and cleaner engage
with the python Community it's a
treasure Trove of knowledge and a great
way to to stay updated on the latest
trends and packages and then comes the
next step that is explore data science
and related Technologies sharpen your
skills in data visualization visuals can
reveal patterns and insights in data
that numbers alone might not show Master
feature engineering to transform row
data into a format that machines can
better understand and predict for get a
handle on building machine learning
pipelines these are like assembly lines
that take your row data and I on
emerging Technologies and Frameworks in
data science that complement generative
staying updated will give you an edge in
your projects so now moving to the next
step that is engage in Hands-On realtime
projects so choose projects that spark
on your interest and challenge you this
is where you get to apply what you have
learned and see your knowledge comes to
life work with different generative AI
models each project is a chance to
deepen your understanding and refine
your skills don't just build evaluate
and iterate on your projects every
iteration is a step closer to Mastery
document and present your work clearly
sharing your journey not not only helps
others learn but also solidifies your
own understanding now moving to the next
step for your road map is solidify your
math and statistics fundamentals dive
deep into linear algebra and calculus
these are the building blocks for
understanding how AI models learn and
make predictions understand probability
and statistics this is crucial for
modeling uncertainty and making informed
predictions learn about optimization
techniques these are the strategies your
models use to improve over time like a
person learning from the mistakes to get
better now moving to the next step that
is develop essential developer skills
get comfortable with AI development
tools these tools can make your work
faster more efficient and more
collaborative focus on debugging and
testing a model that works flawlessly in
theory might face unexpected challenges
in the real world Embrace ethical AI
development it's important to ensure
your AI Solutions are fair accountable
and transparent and then comes the keep
learning the field of AI is always
evolving and staying curious is the key
so this was for the development of
essential developer skills now coming to
the next step that is commit to
continuous learning and exploration so
participa in AI communities these are
great spaces for learning from others
experiences and sharing your own make
reading research papers blogs and books
a habit they are windows to the latest
advancements and theories in AI attend
workshops and conferences these events
can inspire you and expose you to the
new ideas and Technologies seek
mentorship or collaborate on projects
learning from others can accelerate your
growth and and open New Paths so this
was the road map for learning generative
AI in 2024 welcome to the world of
machine learning as we embrace the
Innovations of tomorrow here are the top
10 projects that will Skyrocket your
skills in the ever evolving landscape of
machine learning and artificial
intelligence the project number one is
image recognition with convolutional
neural network or CNN delve into the
domain of image recognition with a focus
on convolutional neural networks this
project serves as an introduction to the
icies of CNN architectures aiding an
image classification and feature
extraction offering a foundational
understanding of visual data processing
tools needed to work on this project
includes Python tensorflow and cars
skills that you will learn are image
processing CNN architecture tensorflow
usage and much more moving on to project
number two we have sentiment analysis
with natural language processing or NLP
engage in the analysis of sentiment
using natural language processing tools
this project aims to instill the skills
necessary for text pre-processing
feature extraction and sentiment
analysis contributing to a comprehensive
understanding of emotional contexts with
textual data tools needed to work on
this project are Python nltk and C the
skills you will acquire includes text
pre-processing feature extraction and
sentiment analysis moving on to project
number three we have reinforcement
learning with openig Master the
principles of reinforcement learning
through the utilization of the open AI
gym environment this project focuses on
Q learning policy iteration and the
Practical implementation of
reinforcement learning algorithms
facilitating a deeper comprehension of
decision- making systems tools needed to
work on this projects include python
open a skills you will acquire includes
Q learning deep Q networks and policy at
ratio moving on to project number four
we have generative adversarial networks
or G for image generation explore the
creative possibilities of generative
adversarial networks in crafting
synthetic images this Advanced project
delves into the architecture of G
offering insight into image generation
and adversarial Network Frameworks tools
needed to work on this project includes
python tens oflow and caral skills that
you'll acquire include G and
architecture image generation and
adversarial networks moving on to
project number five we have time series
forecasting with long short-term memory
or lstm networks enhance predictive
capabilities through the study of long
short-term memory networks this project
addresses time series data handling lstm
architectures and sequence prediction
enabling adeptness in forecasting
sequential data tools needed includes
python caras and PS the skills you will
acquire includes time series data
handling LM architecture sequence
prediction moving on to project number
six we have object detection using Yol
you only look words examine realtime
object detection techniques with you
only look once model this intermediate
level project Focus on object
recognition YOLO architecture and
computer vision culminating in the
ability to detect and categorize objects
swiftly tools needed includes python
open CV and darket skills you will learn
includes object detection YOLO
architecture and computer vision moving
on to the next project we have
Transformer models for language
translation embark on a linguistic
Journey exploring Transformer models for
language translation dive into the
mechanics of attention mechanisms
enabling a comprehensive understanding
of language translation and
transformative capabilities of those
models tools needed includes python
hugging face Transformers the skills you
will acquire includes transform
architectures attention mechanisms and
language translation moving on to
project number eight we have Federated
learning for privacy preserving machine
learning explore the Paradigm of privacy
preserving machine learning with a focus
on Federated learning this Advanced
project emphasizes secure Collaborative
Learning techniques facilitating the
enhancement of models while ensuring
data privacy tools needed to work on
this project are Python tensorflow and
pis skills that you will acquire
includes Federated learning secure and
privacy preserving ml moving on to
project number nine we have
reinforcement learning for realtime
strategy games develop strategic
decision making skills through
reinforcement learning with a realtime
strategy games this project focuses on
empowering AI agents with strategic Pros
thereby owning decision-making
capabilities in game environments the
tools needed includes Python and
PC2 moving on to the skills you will
learn out of this project includes
strategic decision making RL in game
environments moving on to the last
project for the video we have
explainable ai for model
interpretability unveil Integra of model
interpretability through explainable AI
techniques the SK will acquire include
model
interpretability Tech hi everyone and
welcome to this fantastic video on top
AI and ml certifications for 2024 by
simply learn artificial intelligence and
machine learning have emerged as a
transformative technology that are
revolutionizing various aspects of our
lives with its ability to simulate human
intelligence and perform complex tasks
Ai and ml have found applications in
fields ranging from health care and
finance to transportation and
entertainment let me share some
fascinating statistics with you that
illustrate the impact and potential
potential of this gamechanging
technology before we delve into the
fascinating world of artificial
intelligence according to glass door the
average salary of an AI engineer is
$150,000 in the United States and about
14 lakhs perom in India learning Ai and
ml offers numerous benefits ranging from
career opportunities and Innovation to
personal growth and ethical
considerations it equips you with
valuable skills and knowledge that can
have a profound impact on various
aspects of society and the World At
Large but choosing the right course to
excel in the field of AI and ml can be
challenging with so many options
available out there in the market how do
you know which one is right for you well
here we are to help you without any
further Ado let's discuss the top five
Ai and ml certifications for 2024 number
one on the list is postgraduate program
in Ai and ml this postgraduate program
in AI ml in collaboration with per
University and IBM offered by simply
learn is an online program designed to
provide students with a comprehensive
understanding of AI and machine learning
Concepts tools and techniques the
program covers various topics such as
data science statistics deep learning
computer vision and natural language
processing and helps you gain the right
skills on various tools such as kiras
matplot lip tensorflow D Jango and many
more for the the curriculum is
structured around interactive online
classes live sessions with industry
experts and handsome projects the
program also includes access to Simply
learns learning management system which
provides students with additional
resources such as practice exercises
quizzes and simulations overall the
Purdue postgraduate program in AI ml
offered by simply laon is a well
structured and comprehensive program
that can help students to gain expertise
in Ai and ml Concepts and enhance their
career prospects in the field upon
completing the program the students will
receive a post-graduate certificate from
Purdue University and IBM indicating
that they have acquired the skills and
knowledge necessary to apply Ai and ml
Concepts in real world situations
admission to this postgraduate program
in a ml course requires 2 plus years of
work experience bachelor's degree with
an average of 50% or higher and a basic
understanding of programming Concepts
and Mathematics so hurry up enroll now
in the Purdue postgraduate program in AI
ML and create your own successful career
the course link is in the description
box below number two on the list is
masters in artificial intelligence
program this Masters program in
artificial intelligence is one of the
most sought after courses offered by
simply learn this artificial
intelligence Masters program in
collaboration with IBM introduces
students to Blended learning and
prepares them to be specialists in Ai
and data science this program will teach
you the necessary skills you need to
become an AI expert and land in your
dream job this course covers the basic
fundamentals of Python programming
languages and its popular libraries like
numai and pandas from scratch to some
Advanced topics like machine learning
algorithms deep learning natural
language processing computer vision and
AI deployment by covering some important
tools like kasas tensorflow scipi and
many more so what else can you expect
from this program well this course also
includes three Capstone and 12 indust
indry relevant projects from the likes
of Amazon Walmart Mercedes-Benz Uber and
many more immersive learning experience
8 times higher live interaction in the
live AI online classes delivered by
season trainers and Industry experts and
the simply learn job assess program
sounds interesting right so what are you
waiting for don't miss out on this
chance to take your career to the next
level enroll in our AI Masters program
course today and start your journey to
become an AI expert the link is added in
the description box and the comment
section below number three on this list
is AI and machine learning boot camp
this Ai and machine learning boot camp
program May develop your career as an
expert in artificial intelligence and
machine learning boot camp in
collaboration with CTIC University the
AI nml boot camp offers master classes
by CTIC professors live classes taught
by industry experts interactive labs and
projects relevant to the industry this
boot camp offers calex academic
Excellence simply learns carer
assistance campus immersion and hands-on
experience this Ai and ml boot camp will
help you get noticed by top hiring
companies and help you in the industry
relevant Capstone projects the AI and
machine learning boot camp at Caltech is
a best option for anyone wishing to get
a Competitive Edge in the era of Cutting
Edge Technologies this boot camp offers
a complete understanding of AI based
Technologies like machine learning NLP
deep learning speech recognition
reinforcement learning and much more
this course covers tools and techniques
like python tlow kasas matplot lib and
many more along with industry projects
like social media e-commerce
manufacturing ettech Healthcare and many
more Amazing projects choosing the boot
camp can get you hired by Renown
companies like Microsoft Google IBM
Amazon Samsung and adob this boot camp
has benefited many aspiring Ai and emble
engineers and professionals candidates
applying for AI and ml boot camp should
have the following this boot camp is for
the United States only be at least 18
years old and have a high school diploma
or equivalent have prior knowledge or
experience in programming and
Mathematics preferably two plus years of
formal work experience so hurry up
enroll now in the AI and ml boot camp
and create your own successful career
the course link is in the description
box below number fourth on the list is
professional certificate program in
generative Ai and machine learning with
this comprehensive generative Ai and
machine learning program prog by ihub
IIT RI embark on a thrilling career with
a generative Ai and machine learning
program by ihab Divia Sark a Technology
Innovation Hub of IIT RI benefit from
live classes by experts and top faculty
IIT rooki campus immersion simpar career
services and certificates for IBM
courses with this course you will
achieve Mastery in your AI and machine
learning career through this rigorous
program this post-graduate program in
gen Ai and machine learning combines
both Theory and Hands-On training
offering live virtual sessions by
industry experts self-based videos
projects labs and master classes by
faculties from various eminent
institutes key features of this course
include earn a certificate of completion
from ihab Divia SAR IIT RI curriculum
delivered in live online classroom
sessions by seasoned industry experts
Master Class delivered by faculties from
Premier institutes like iits and nits
gain exposure to the latest AI
advancement such as generative AI prompt
engineering and chat GPT dedicated live
project Le trading for crucial Topics in
generative AI opportunity to attend a
two-day campus imersion program by ihab
Divya SAR at IIT RI work on 25 plus
handson industry relevant projects and
three industry oriented Capstone
projects build expertise in 20 plus
tools and techniques with seamless
access to integrated labs this course
covers tools and techniques like chat
GPT open CV Python kasas and along with
industry projects like create a virtual
assistance with generative AI predicting
employee iteration with machine learning
utilize deep learning to automate ship
detection and many more eligibility
criteria for admission to this program
in gen Ai and machine learning
candidates should have a bachelor's
degree with an average of 50% or higher
marks two plus years of work experience
preferred basic understanding of
mathematics and programming Concepts so
hurry up enroll now in in the
postgraduate program in generative Ai
and machine learning and create your own
successful career the course link is in
the description box below finally we
have CTIC postgraduate program in Ai and
machine learning today we have got an
exciting topic for all you aspiring
machine learning engineers and data
science Enthusiast out there we will be
diving into the world of algorithms
discussing the top 10 algorithms every
machine learning engineer should know
and whether you are a beginner or a
Season Pro this video is packed with a
ential knowledge that will elevate your
machine learning game so without any
further Ado let's get started so machine
learning algorithms are classified into
four types and that is supervised
unsupervised semi-supervised and
reinforcement learning so we'll just
have a brief on all these types so first
is supervised learning so supervised
learning is a machine learning approach
where algorithms learn from label data
the algorithm receives input data and
corresponding correct output labels in
the process and the objective is to
train the algorithm to predict accurate
labels for new unseen data an example of
supervised learning algorithms include
decision trees support Vector machines
random Forest na based algorithm and
these algorithms can be used for
classification regression and time
series forecasting task now coming to
unsupervised learning in this machine
learning approach algorithms analyze
unlabeled data without predefined output
labels and the objective is to discover
patterns relationships or structures
within the data and now so the common
unsupervised learning techniques include
clustering algorithms like K means
hierarchical clustering dimensionality
reduction methods like PCA and PSN and
now coming to semisupervised learning so
semi- supervisor learning is a hybrid
machine learning approach that combines
labeled and unlabeled data for training
it leverages The Limited label data and
larger set of unlabelled data to improve
the learning process the idea is that
the unlabel data provide additional
information and context to enhance the
model's understanding and performance so
now we'll come to semisupervised
learning techniques that can be applied
to various tasks such as classification
regression and anomaly detection that
allow models to make more accurate
predictions and generalize better in
real world scenarios now coming to the
fourth type that is reinforcement
learning reinforcement learning is a
machine learning algorithm inspired by
how humans learn from trial and error
here an agent interacts with an
environment and learns to make optimal
decisions to maximize cumulative rewards
the agent receives feedback through
Rewards or penalties based on its action
and the agent learns to take actions
that lead to the most favorable outcomes
over time so this Dynamic approach to
learning makes reinforcement learning a
powerful technique for tackling complex
decision making problems and now let's
kick things off by understanding why
these algorithms are so crucial machine
learning algorithms are the heart and
soul of AI they are the tools that
enable computers to learn from data and
make Intelligent Decisions so mastering
these top 10 algorithms will give you a
solid foundation in machine learning so
now we'll start with the machine
learning algorithms so we'll just get a
brief of all the algorithms that every
machine learning engineer should know so
number one on our list is linear
regression it's the goto algorithm for
understanding the relationship between
variables making it fundamental in
predictive model we'll dive deep into
workings and practical applications so
to understand the working functionality
of linear regression imagine how you
would arrange random logs of wood in
increasing order of their weight there
is a catch however you cannot weigh each
log you have to guess its weight just by
looking at the height and GTH of the log
and arranging them using a combination
of these visible parameters this is what
linear regression in machine learning is
like in this process a relationship is
established between independent and
dependent variables by fitting them to a
line this line is known as the
regression line and is represented by a
linear equation that is y = a into x + B
and in this equation Y is is dependent
variable a is slope X is independent
variable and B is intercept the
coefficient of A and B are derived by
minimizing the sum of the square
difference of distance between data
points and the regression line and now
coming to the second algorithm that is
logistic regression so logistic
regression is used to estimate discrete
values usually binary values like 0 or
one from a set of independent variables
it helps predict the probability of an
event by fitting data into a logit
function it is also called logit
regression these methods listed below
are often used to help improve logistic
regression models that is include
interaction terms eliminate features
regularize techniques and use a nonn
linear model so this was about logistic
regression now moving to the next
algorithm that is decision tree decision
tree algorithm in machine learning is
one of the most popular algorithm in
used today this is a supervised learning
algorithm that is used for classifying
problems it works well in classifying
both categorical and continuous
dependent variables this algorithm
divides the population into two or more
homogeneous sets based on the most
significant attributes or independent
variables so this was about the decision
3 algorithm now moving to the next
algorithm that is svm support Vector
machine algorithm so svm algorithm is a
method of a classification algorithm in
which you plot row data as points in an
N dimensional space where n is the
number of features you have the value of
each feature is then tied to a
particular coordinate making it easy to
classify the data lines called
classifiers can be used to split the
data and Float them on a graph this was
about support Vector machine algorithm
now moving to the next algorithm that is
Nave base algorithm a na Bas classifier
assumes that the presence of a
particular feature in a class is
unrelated to the presence of any other
feature even if these features are
related to each other a na based
classifier would consider all of these
properties independently when
calculating the probability of a
particular outcome a na basan model is
easy to build and useful for massive
data sets it's simple and is known to
outperform even highly sophisticated
classification methods so this was about
NBAs algorithm now moving to the next
algorithm that is KNN K nearest
neighbors algorithm so this algorithm
can be applied to both classification
and regression problems apparently
within the data science Industry it's
more widely used to solve classification
problems it's a simple algorithm that
stores all available cases and
classifies any new cases by taking a
majority vote of its Kors the case is
then assigned to the class with which it
has the most in common a distance
function performs this measurement pnn
can be easily understood by comparing it
to real life for example if you want
information about a person it makes
sense to talk to his or her friends and
colleagues things to consider before
selecting K nearest neighbors algorithm
number one K&N is computationally
expensive number two variabl should be
normalized or else higher range
variables in bias algorithm third data
still needs to be pre-processed so this
was about the K&N the K nearest
neighbors algorithm now moving to the
next algorithm that is K means algorithm
it is an unsupervised learning algorithm
that solves clustering problems data
sets are classified into a particular
number of clusters let's call that
number K in such a way that all the data
points within a cluster are homogeneous
and heterogeneous from the data in other
clusters how K means forms clusters the
K means algorithm picks K number of
points called Central for each cluster
and each data point forms a cluster with
the closest centroids that is K clusters
it now creates new centroids based on
the existing cluster members with these
new centroids the closest distance for
each data point is dater mind this
process is repeated until the centroids
do not change so this was about kein
algorithm now moving to the next
algorithm That Is Random Forest
algorithm Collective of decision trees
is called a random Forest to classify a
new object based on its attributes each
tree is classified and the tree boards
for that class the forest chooses the
classification having the most walls and
each tree is planted and grown as
follows if the number of cases in the
training set is n then a sample of n
cases is taken at random and this sample
will be the training set for growing the
tree and if there are M input variables
a number M less than m is specified such
that each node that is M variables are
selected at random out of the m and the
best split of this m is used to split
the node the value of M is held constant
during the process and each tree is
grown to the most substantial extent
possible there's no pruning this was
about the random Forest algorithm and
now moving to the next algorithm that is
dimensionality reduction algorithm in
today's world vast amounts of data are
being stored and analyzed by corporates
government agencies and research
organizations as a data scientist you
knowe that this low data contains a lot
of information that is the challenge is
to identify significant patterns and
variables and the dimensionality
reduction algorithms like decision Tre
factor analysis missing value ratio and
random Forest can help you find relevant
details so this was about dimensionality
reduction algorithms now moving to the
last algorithm that is gradient boosting
algorithm and add a boosting algorithm
so gradient boosting algorithm are
boosting algorithms used when massive
loads of data have to be handled to make
predictions with high accuracy boosting
is an ensemble learning algorithm that
combines the predictive power of several
base estimators to improve robustness in
short it combines multiple weak or
average predictors to build a strong
predictor these boosting algorithms
always work well in data science
competitions like kaggle a hecaton croud
analytics and these are the most
preferred machine learning algorithms
today use them along with python and R
codes to achieve accurate outcomes so
this was all about the top 10 machine
learning algorithms that every machine
learning engineer should know and
remember mastering these algorithms will
provide you with a solid foundation to
tackle various machine learning problems
probability and statistics are essential
tools for understanding and analyzing
data helping us make informed decisions
in various Fields probability measures
the likelihood of events occurring
providing a mathematical framework to
handle uncertainty and Randomness it
allows us to quantify risks make
predictions and understand the behavior
of random processes from simple events
like rolling a DI to complex phenomena
in finance and science statistics
involves systematic collection analysis
interpretation and presentation of data
by examining data statistics help us
draw meaningful conclusions as identify
Trends test hypothesis and make data
driven decisions this field is crucial
in areas like healthcare engineering
economics and social science where
analyzing data accurately can lead to
significant insights and
advancements craving a career upgrade
subscribe like and comment
below dive into the link in the
description to Fast Track Your Ambitions
whether you're making a switch or aiming
higher simply learn has your
back and if you want to make your career
in Ai and machine learning here is a
quick info for you try simply learns
postgraduate program in Ai and machine
learning from Peru University in
collaboration with IBM this course
teaches in demand skills such as machine
learning deep learning NLP computer
vision reinforcement learning generative
AI prompt engineering chat GP and many
more so don't forget to check out the
course link from the description box and
pin comments so without any further Ado
let's get
[Music]
started learning objectives welcome to
math refresher probability and
statistics in this lesson we are going
to explain the concepts of statistics
and
probability describe conditional
probability Define the chain rule of
probability discuss the measure of
variance identify the types of gsan
distribution basic of statistics and
probability probability and statistics
data science relies heavily on estimates
and predictions a significant portion of
data science is made up of evaluations
and
forecast statistical methods are used to
make estimates for further analysis
probability theory is help ful for
making predictions statistical methods
are highly dependent on probability
Theory and all probability and
statistics are dependent on data data is
information acquired for reference or
research via observations facts and
measurements data is a set of facts
structured in the form that computers
can interpret such as numbers words
estimations and Views importance of data
data AIDS in seeing more about the
information by identifying possible
connections between two features data
assists in the detection of distortion
by uncovering hidden patterns based on
prior information patterns data may be
utilized to anticipate the future or
predict the current state of affairs
also data AIDS in determining whether
two pieces of information have any
instance in common or not types of data
data might be quantitative that is data
that can be measured or counted in
numbers or it may be qualitative which
is data which is generally divided into
groups or in simpler Words which cannot
be counted or measured in numbers let's
consider an example a customer
information data of a bank may contain
quantitative and qualitative data
consider this snapshot where we have
customer ID surname geography gender age
balance has CR or card is active member
amongst these variables we can see
surname is mostly qualitative as it
cannot be counted and measured in
numbers geography and gender are also
qualitative as they cannot be counted in
numbers and are mostly groups has Seer
card that is has credit card and his
active member although are containing
numerical in form but these are
categorical that means these have been
divided into groups of one and zero that
represent yes and no as an answer hence
these two variables are also
qualitative customer ID is again
although a numerical data however the
significance or intuition behind
customer ID is
categorical hence it may be kept in the
qualitative data also however age and
balance these are numerical information
which have been measured or counted and
numerical operations can be performed on
them hence these are Under quantitative
data categories introduction to
descriptive statistics descriptive
statistics a descriptive measurement is
summary measure that quantitatively
portrays the most important features of
a set of data allowing for a better
comprehension of the information data
can be measured as different levels the
levels of measurement describe the
nature of information stored in the data
assigned to the variables qualitative
data can be measured as nominal or
ordinal quantitative data can be
measured in terms of interval and ratio
type nominal data the data is
categorized using names labels or
qualities for example brand name zip
code and gender ordinal data can be
arranged in order or ranked and can be
compared examples include grades star
reviews position and race and date
interval data is the data that is
ordered and has meaningful differences
between the data points example
temperature in celsius and year of birth
ratio data is similar to the interval
level with the added property of
inherent zero mathematical calculations
can be performed on both interval as
well as ratio data for example height
age and weight po population versus
sample before analyzing the data it's
important to figure out if it's from a
population or a sample population is a
collection of all available items as
well as each unit in our study sample is
a subset of the population that contains
only a few units of the
population population data is used for
study when the data pool is very small
and can give all the required
information samples are collected
randomly
and represent the entire population in
the best possible way measures of
central
tendency the central tendency is a
single value that aids in the
description of the data by determining
its Center position measures of central
tendency are sometimes known as summary
statistics or measures of central
location the most popular measurements
of central tendency are mean median and
mode the normal distribution is a
bell-shaped symmetrical distribution in
which mean median and mode all are equal
the curve over here shows the
bell-shaped curve or the normal
distribution of variable X the point
over here that is
X1 is the point which represents the
mean median and mode of this
distribution mean mean is calculated by
dividing the sum of all data values by
the total number of data values
it gets affected when there are unusual
or extreme values it is sensitive to the
outliers mean can be calculated as
summation over all the values of X in a
collection divided by the size of the
collection for example we have a
collection where we have values as 7 3 4
1 6 and
7 we find out the sum of these values
which is 28 and there are total of six
values so 28 / 6 gives us a mean value
of
4.66
median it is the middle value in the set
of the data that has been sorted in
ascending
order it is a better alternative to mean
since it is less impacted by outliers
and
skewness it is closer to the actual
Central
value median is calculated differently
for different sizes of data
differentiated as if the total number of
values is odd or if the total number of
values is even if the size of the data
is odd for example in this case we have
five
elements after sorting whatever middle
value we
get that means n + 1 by twoth term in
this
case 5 + 1 / 2
that is the third term which is four is
the median
value in case when the total number of
values is even like here there are six
values the average or the mean of the
two Central values is considered as the
median in this case the median is the
mean of 6 and four which is five
mode mode represents the most common
value in the data set it is not at all
effective Ed by extreme
observations it is the best measure of
central tendency for highly skewed or
nonnormal
distribution mode for categorical data
is determined by estimating the
frequencies for each
categories and then the category with
the highest frequency is considered to
be
mode like in this case 7 has the highest
frequency hence seven becomes the mode
value however in case of continuous data
or quantitative data the calculation of
mode is slightly different the first
step in calculation of mode is dividing
the data into classes which are equal
with then getting the frequency of data
points lying in within that range of
classes and finally selecting the class
with the highest
frequency using the range of that class
and the frequencies we can get the final
mode
value using the form formula
l+ fmus fub1 multiplied to h / fmus fub1
Plus FM minus
F2 here L is the lower limit or the
lower observation of the mode class H is
the size of the mode
class FM is the frequency of the mode
class F1 is the frequency of the class
proceeding to mode and F2 is the
frequency of the class succeeding to
mode this gives us the final mode
value mean versus
expectation now let's talk about mean
versus
expectation so in general we use the
expected value or expectation when we
want to calculate the mean of a
probability distribution that represents
the average value we expect to occur
before collecting any data and me on the
other hand mean is basically used when
we want to calculate the average value
of a given sample this represents the
average value of raw data that we may
have already
collected we can understand this by
using a simple
example now to calculate the expected
value of this probability distribution
we can use a specific formula from the
previous
discussion this is going to be the
expected value where X is going going to
be the date of value and this PX is the
probability of
value for example we could calculate the
expected value for this probability
distribution to be his
shown so here it will be 1.45
goals so this represents the expected
number of goals that the team will score
in any given
game and then if you talk about Cal
calculating mean so we typically
calculate the mean after we have
actually collected raw
data for example suppose we record the
number of goals that a soccer team will
score in 15 different
games now to calculate the mean number
of goals scored per
game we can use the following
formula where sum of X is basically the
sum of all the goals divided by n and
the number of Records or we can say the
sample
size it is as shown on the
screen so this represents the mean
number of goals scored per game by the
team measures of asymmetry
the difference between the three
distinct curves can be studied in this
image the central curve is the normal or
no skewness curve here mean median and
mode all lie on the same point this
normal curve is symmetrical about its
mean median and
mode that means the left hand side of
the curve is a mirror image of the right
hand side of the
curve however in case of negatively
skewed data the tail is elongated on the
left hand
side and the mean is smaller than the
mode and the median values or is on the
left hand side of the
mode hence indicating that the outliers
are in the negative
Direction on the other hand in case of
positively skewed the data is
concentrated on the left hand side of
the
curve while the tail is a oated or
longer on the right hand side of the
curve the mean is greater than the mode
and
median or is on the right hand side of
the mode and median indicating that the
outliers are in the positive
direction let's consider an
example the graph here shows the global
income distribution for the year 2003
2013 and a projection for
2035 if we see the global income
distribution statistics for 2003 it is
highly right
skewed we can observe in the previous
graph that in
2003 the mean of
$3,451 was higher than the median of
$190 the global income is definitely not
e evenly distributed the majority of
people make less than $2,000 each
year while only a small percentage of
the population earns more than
$114,000 measures of
variability measures of variability
dispersion the measure of central
Tendencies provide a single value that
addresses the full worth however the
central tendency cannot depict the
Viewpoint entirely the metric of
dispersion helps us focus on the
inconsistency in the data spread
measures of dispersion describe the
spread of the
data the range interqual range standard
deviation and variance are examples of
dispersion
measures
range the range of distribution is the
difference between the largest and the
smallest amount of
data the range for example does not
include all of a series positive
aspects it concentrates on the most
shocking aspects and ignores that aren't
considered critical for example for a
set 13 33 45 67
70 the range is
57 that is the maximum of this which is
70 minus the minimum over here
which is
13
variance variance is the average of all
square
deviations it is defined as the sum of
squar distance between each point and
the mean or the dispersion around the
mean the standard deviation is used as
variance suffers from a unit
difference variance can be computed as
Sigma Square summation over x - mu^
s divided
n where mu is the mean of the data X is
the individual data
point and N is the size of the
data this representation is for a
population
data for a sample data variance can be
computed as
xus xar whole Square
summation over it divided by n minus
one here xar is the mean of these sample
data and N is the sample
size the units of values and variance
are not
equal so another variability measure is
used standard
deviation standard deviation is a
statistical term used to measure the
amount of variability or dispersion
around a mean
the standard deviation is calculated as
the square root of variance it depicts
the concentration of the data around the
mean of the data
set standard deviation as indicated
previously can be computed as square
root of
variance for a population data standard
deviation Sigma can be computed as
square root of summation over X IUS mule
Square /
n where mu is the mean of the data XI
are the data points and N is the size
let's consider an
example let's find out the mean variance
and standard deviation for this data the
data values are 3 5 6 9 and 10 to find
out the mean we first find the sum of
all these data values
that is 33 and divide it by the count
which is
five we get the mean of 6.6 to compute
the variance we start by Computing the
deviation that is x minus the mean of X
here three is one of the values of the
data and 6.6 is the
mean so 3 - 6.6 squ and we do
that to find out sum of all the
deviations divided by the
count which is
five we end up getting an overall
variance of
6.64 standard deviation as we know is
measured at square root of variance that
is square root of
6.64 which amounts to
2576 measures of
relationship measures of relationship
covariance covariance is the measure of
joint variability of two
variables it measures the direction of
the relationship between the variables
it determines if one variable will cause
the other to alter in the same
way covariance between variable X and Y
can be computed as summation over the
product of x i -
xar and y i- y bar the whole divided by
n
minus1 here xar and Y Bar are the mean
of X and Y respectively the value of
covariance can range from minus infinity
to a plus
infinity
correlation correlation is normalized
covariance it measures the strength of
association between two variables the
most common measure for correlation is
the pure correlation
coefficient correlation between two
variables X and Y can be measured with
respect to covariance as covariance
between
X and Y divided by the standard
deviation of X and standard deviation of
Y the value of correlation ranges from a
negative 1 to positive
1 types of correlation
correlation can be either a positive
correlation zero correlation or a
negative
correlation the first picture over here
represents a perfect positive
correlation we in a straight line with a
positive
slope is representing the relationship
between the two
variables zero correlation means that
the line representing the relationship
between the two variables is horizontal
to the X
AIS perfect negative correlation can be
represented by a straight line with a
negative
slope correlation equals to 1 implies a
positive
relationship that is when one variable
increases the other variable also
increases a correlation value of NE 1
implies a negative relationship that is
when one variable increases the other
decreases the correlation coefficient of
zero shows that the variables are
completely independent of each
other let's consider an
example here we have two variables
height and
weight to compute the correlation
between height and
weight we use the correlation formula as
covariance of x
and Y divided by standard deviation of X
and standard deviation of
Y here height is the X variable and
weight is the Y
variable first to compute covariant we
compute the x - xar and Y - Y Bar values
and then the product of
them we then compute x - xar
s and Y - Y Bar Square values to compute
the standard deviations of height and
weight respectively correlation as we
know has been defined as covariant of X
and I and Y divided by standard
deviations of X and
Y this can also be represented as
summation over x - xar multiplied to y -
Y
Bar divided by square root of summation
over sum of squared deviations
that is x - xar s multiplied to square
root of summation over y - Y Bar square
that is sum of square deviations for
y now let's find out values to put into
this
formula first we find out the overall
sum of height to get the mean of height
which is
5.14 similarly we get the sum of weight
the mean of weight as 50 we now get the
summation over x - xar multiplied to y -
y to get the numerator for the
formula then we compute x - xar s
summation and Y - y bar squ that is sum
of squared deviation of X and Y
respectively now we put in the values in
this final correlation formula to get a
correlation value of 0.8
889 this indicates that height and
weight have a positive
relationship it is evident that as
height grows weight also
increases in this module we will be
talking about expectation and
variance so the expected value or we can
say mean of a given variable that we can
denote by X is a discrete random
variable where it is a weighted average
of the possible values that X can take
and each value is going to be according
to the probability of that specific
event
occurring so usually the expected value
of x is denoted by a simple formula
where we can Define the expectation
based on the X
parameter which is going to be the sum
of each possible outcome multiplied by
the probability of the outcome
occurring so in more concrete terms the
expectation is what we would expect the
outcome of an experiment to be on
average we can take an example for the
coin if a coin is being tossed 10 times
then one is most likely to get five
heads and five
tails same logic can be discussed if we
talk about another example of rolling a
die so there are six possible outcomes
when you roll a die 1 2 3 4 5 6 and each
of these has a probability of 1x six of
occurring so we can say that the
expectation is going to be 1 multiplied
by the probability of that happening
which is going to be 1X 6 + 2x 6 + 3x 6
+ 4x 6 + 5x 6 + 6x6 and that is going to
give us 3.5 as an output the expected
value is
3.5 so if you think about it 3.5 is
halfway between the possible values that
I can take and this is what we should
have
expected next we talk about the concept
of variance so variance of a random
variable allows us to know something
about the spread of the possible values
of the
variable so for a discrete random
variable X the variances of X is going
to be denoted by using a simple formula
that is going to be varx equal e x - M
the whole Square where m is basically
the expected value of the expectation of
X so this is more like a standard
deviation of X which can also be
represented by using this formula so the
variance does not behave in the same way
as expectation when we multiply and add
constants to random
variables so now there are two different
type of variance that we can have a fair
understanding on first of all we have
low variance and then we have high
variance so low variance simply means
that there is a small variation in the
production of the target function with
changes in the trading data set and at
the same time high variance as we can
see here High variance shows a large
variation in prediction of the target
function with changes in the trading
data set
so a model that shows High variance
learns a lot and perform well with the
trading data set and it does not
generalize well with the Unseen data set
and that's why as a result such a model
gives good results with training data
set but shows High error rates on the
test data set and since the high
variance a model learns Too Much from
the data set it leads to an overfitting
of the model so model with high variance
will be having couple of issues like it
may lead to overfitting or it may also
lead to increase in model
complexities next we have
skewness so skewness in simple terms is
basically a measure of asymmetry of a
distribution so distribution is
asymmetrical when it's left and right
sides are not the mirror
images right now this is a mirrored
image and a distribution can have right
positive or we can say say negative or
it can have zero
skewness so right skewed in this
scenario is basically the distribution
is longer on the right side of its
peak and a left skew distribution is
going to be we can say where it is
longer on the left
side so we can see we have this one as a
part of right side it is more elongated
towards the right side and this one is
more elongated towards the left side
side so we can think of skewness in
terms of Tails a tail is long tampering
and the end of a distribution so it
simply indicates that they are
observations at one end of the
distribution but that they are
relatively infrequent so a right skew
distribution has a long tail on the
right side as you can see here so the
number supports observed let's say we
have a data on a perear basis so again
we can have a more skewness towards the
right side where data is being dropping
as we continue to increase the number of
years for example we may have a high
sales towards the beginning of year
suppose in
2022 but again as we proceed to
2023 second half we are seeing the dip
in performance so that is rightly skewed
and same way let's suppose if we started
with the sales figure it was really Less
in suppose
2002 but again as as we proceeded to
2023 now our Sals have been gradually
increasing so it's more like skew
towards the left section as a part of
negative skew next we have
curtosis so curtosis is basically a
measure of the tailedness of a
distribution so tailedness is how often
the outliers occur and access curtes is
the tailedness of the distribution
related to a normal distribution
so a distribution with medium curtosis
is called as mesokurtic a distribution
with low curtosis like this one this is
called as the platic and then
distribution with high curtosis like
this one this is called as the lepto
kurtic so Tails here they are tapering
ends on either side of a distribution
like this so they represent the
probability or the frequency of values
that are extremely high or extremely low
to the
mean in other words tals here represents
how often the outliers
occur so there are three type of
curses we have platic kurtic which is
negative lepto kurtic which is a
positive towards the upper end and then
we have mesokurtic which is a normal
distribution so mesokurtic is the medium
tail so normal distributions they have a
curtois of three so any distribution
with a curtois of a prox value of three
is going to be mesokurtic and curtosis
is described in terms of excess curtosis
which is curtosis minus 3 and since
normal distribution they have a curtosis
of three axis curtes makes comparing a
distribution curtois to a normal
distribution even easier introduction to
probability probability Theory
probability is a measure of the
likelihood that an event will
occur let's consider an example of coin
toss where the chances of getting heads
on a coin are 1 by two or
50% the probability of each given event
is between zero and one both inclusive
sum of an events cumulative probability
cannot be greater than
one hence the probability of an event X
lies between 0o and 1
this means that the integral of
probability of distribution over xal to
1 conditional
probability conditional probability of
any event a is defined as the
probability of occurrence of a given
that event B has previously
occurred condition probability of event
a given B can be estimated as
probability of a intersection B that is
probability of both A and B happening
together divided by the probability of
B it is also written as that probability
of a intersection b equals to
probability of a given B multiplied to
probability of
B let's consider an
example in a coin we are doing a two
coin flip coin one gets heads Tails
heads and tails in subsequent
flips while coin 2 gets Tails heads
heads and tails in the subsequent flips
now the probability that coin one will
get aead is two out of four while the
probability that coin 2 will get heads
is again two out of
four the probability that both coin one
and coin 2 will have a heads is just one
out of the four
flips hence the probability that coin
one will get heads given that coin 2 is
already heads can be computed as
probability of coin 1 Edge intersection
coin 2
Edge that is 1x4 divided by probability
of coin 2
Edge that's a given that is 2x4 which is
going to be 0.5 or 50% based
base theorem base theorem calculates the
conditional probability of an event
based on its prior
probabilities basically base theorem
incorporates the prior probability
distribution to predict the posterior
probabilities base theorem for
conditional
probability can be expressed as
probability of a given b equals
probability of B given a divided by
probability of B multiplied to
probability of
a base theorem allows updating the
probability values by using new
information or evidence here probability
of a is known as prior probability that
is the probability of event before any
new data is collected probability of a
given B is known as the posterior
probability it is the revised
probability of an event occurring after
taking into consideration the new
information probability of B given a is
known as the likelihood and probability
of B is probability of observing in
evidence Baye model an example consider
an example for calculating the
likelihood of having diabetes based on
frequency of fast food consumption here
is the observed data let's say the fast
food audience is 20% diabetes prevalence
is 10% and 5% is fast fast food and
diabetes the chances of diabetes given
fast food that is the conditional
probability of D given B can be
calculated as probability of diabetes
and fast food together divided by
probability of fast food that means 5%
ided
20% that equals
25% Define an analysis can State eating
fast food increases the chance of having
diabetes by 25%
the multiplication rule of probability
if events A and B are statistically
independent and probability of a
intersection B can be given as
probability of a given B multiplied to
probability of B however probability of
a intersection B is also given as
probability of a multiply to probability
of B here probability of a given B
equals to probability of a when we
assume that probability of B is non zero
similarly probability of b equals
probability of B given a assuming
probability of a is non zero chain rule
of probability joint probability
distributions over many random variables
can be reduced into conditional
distributions over a single variable it
can be expressed as probability of X1 X2
so on until xn equals probability of X1
intersection probability of x i given
probability of X1 till x i minus
one for example The Joint probability of
a b and c can be given as probability of
a given b c multiplied to probability of
B given C multiply to probability of C
logistic sigmoid
the logistics function is a type of
sigmoid function that aims to predict
the class to which a particular sample
belongs its outcome is discrete binary
value a probability between 0 and 1 the
logistics sigmoid is a useful function
that follows the yes curve it saturates
when the input is very large or very
small logistic sigmoid is expressed as
Sigma of x = 1 upon 1 + e to the power
minus
X the logistic sigmoid can be expressed
as sigmoid function of X is given as 1
upon 1 + e to^ - x where e is the ool's
number gsan
distribution the gossing distribution is
a type of distribution in which data
tends to Cluster around a central value
with little or no bias to the left or
right it is often referred to as normal
distribution in absence of Prior
information the normal distribution is
frequently a fair assumption in machine
learning
equation the formula for calculating
gsan distribution is described as the
normal distribution of
X that is the function of x given mean
as Mu and variance is Sigma Square can
be calculated as 1 upon Sigma Square <
TK of 2 pi e to^ minus half x - mu /
Sigma whole
Square where mu is the mean or Peak
value which also is the expected value
of
x Sigma is the standard deviation Sigma
square is the
variance a standard normal distribution
has a mean of zero and a standard
deviation of
one gsh and distribution can be
univariate
which describes the distribution of a
single variable
X it can also be multivariate where it
can just used to describe the
distribution of several
variables it is represented in 3D of ND
formats law of large
numbers now let's talk about law of
large numbers the law of large numbers
states that an observed sample average
average from a large sample will be
close to the true population average and
that it will get closer in the larger
sample so the law of large number does
not guarantee that a given sample
spatially a small sample will reflect
the true population characteristics or
that a sample does not reflect the true
population will be balanced by a
subsequent sample this is for the law of
large numbers to express the
relationship between scale and growth
rate
so there are multiple examples through
which we can
understand and it is widely used in
statistical analysis in working with the
central limit theorem in terms of the
business growth so there are multiple
real times set up in which these are
going to be used so if you talk about
tossing a coin so tossing a coin in a
number of times will give us two
different type of
outcomes the result will spread even ly
between head and Tails and the expected
average value is going to be
half that means 50 * tails and 30 times
heads but again if you toss a coin 1,000
times then the result can be in
different manners because out of 1,000
let's say 850 times it has been head and
only 150 times it has been taals and so
on so that's why the possibility of one
event occurring is going to be be
changed in large sample sets as compared
to a small sample sets as in let's say
10 times so the number of heads and
tails unbalanced for lower number of
Trials so we can see it is
unbalanced but again as soon as we toss
more number of coins more leans towards
the balance value or we can see the
observed
averages next we have P
value so P value is basically a number
calculated from the statistical test
that describes How likely we are to have
found a particular set of observations
if the null hypothesis were true so P
values are used in hypothesis testing to
help decide whether to reject the null
hypothesis and the smaller the P value
the more likely we are to reject the
null
hypothesis so we have a term called as
null hypothesis so all statistical tests
they have null hyp hypothesis so for
most tests the null hypothesis is that
there is no relationship between our
variables of INF first or that there is
no difference among groups for example
in a two-tail T Test the non-hypothesis
is that the difference between two
groups is going to be
zero so P value is going to tell us how
likely it is that our data could have
occurred under the null
hypothesis it is done by calculating the
likely it of a test
statistic which is the number calculated
by a statistical test using our data so
P value tell us how often we would
expect to see a test statistic as
extreme or more
extreme than one calculated by a
statistical test if the null hypothesis
of the test was
true so there are multiple limitations
as well so first one is the results can
be significant but again they are they
Main not be practical as we have
compared it can be based on multiple
hypothesis for a game for the healthcare
test if the test is going to be positive
or not it may show even values of the
effect of a variable but not the
magnitude in real life what exactly is
going to be the application of a drug
test being failed in Pharma company
therefore it is recommended to use
confidence and levels in addition to the
P values to quantify or weaken say to
give a solid figure to the reserve which
we are going to get the P values they
are interpreted as supporting or we can
say refuting the alternative
hypothesis so P value can only tell you
whether or not the null hypothesis is
supported it cannot tell us whether our
alternative hypothesis is true or why so
the risk of rejecting the null
hypothesis is often higher than the P
value so especially when we are looking
looking at a single study or when using
small sample sizes so this is because
the smaller frame of reference the
greater are the chance that as we
stumble across a statistically
significant pattern completely by
accident key
takeaways key takeaways probability and
statistics structure the premise of the
data the data helps in anticipating the
future or gauging in view of the past
patterns of information
the central tendency is a single value
that helps to describe the data by
identifying these Central positions the
mean median and mode are the measures of
central
Tendencies the distribution where the
data tends to be around a central value
with a lack of bias or minimal bias
towards the left or right and this is
one of the things is very important with
linear aggression in any of these models
is to understand the error and so we can
calculate the error on all of our
different values and you can see over
here we plotted um X and Y and Y predict
and we drawn a little line so you can
sort of see what the error looks like
there between the different points so
our goal is to reduce this error we want
to minimize that error value on our
linear regression model minimizing the
distance there are lots of ways to
minimize the distance between the line
and the data points like sum of squared
errors sum of absolute errors root mean
square error Etc we keep moving this
line through the data points to make
sure the best fit line has the least
Square distance between the data points
and the regression line so to recap with
a very simple linear regression model we
first figure out the formula of our line
through the middle and then we slowly
adjust the line to minimize the error
keep in mind this is a very simple
formula the math gets even though the
math is very much the same it gets much
more complex as we add in different
dimensions so this is only two
Dimensions y = mx + C but you can take
that out to X Z ijq all the different
features in there and they can plot a
linear regression model on all of those
using the different formulas to minimize
the error let's go ahead and take a look
at decision trees a very different way
to solve problems in the linear
regression model decision tree is a
tree-shaped algorithm used to determine
a course of action each branch of a tree
represents a possible decision
occurrence or reaction we have data
which tells us if it is a good day to
play golf and if we were to open this
data up in a general spreadsheet you can
see we have the Outlook whether it's a
rainy overcast Sunny temperature hot
mild cool humidity windy and did I like
to play golf that day yes or no so we're
taking a census and certainly I wouldn't
want a computer telling me when I should
go play golf or not but you can imagine
if you got up in the night before you're
trying to plan your day and it comes up
and says tomorrow would be a good day
for golf for you in the morning and not
a good day in the afternoon or something
like that this becomes very beneficial
and we see this in a lot of applications
coming out now where it gives you
suggestions and lets you know what what
would be uh fit the match for you for
the next day or the next purchase or the
next uh whatever you know next mail out
in this case is tomorrow a good day for
playing golf based on the weather coming
in and so we come up and let's uh
determine if you should play golf when
the day is sunny and windy so we found
out the forecast tomorrow is going to be
sunny and windy and suppose we draw our
tree like this we're going to have our
humidity and then we have our normal
which is if it's if you have a normal
humidity you're going to go play golf
and if the humidity is really high then
we look at the Outlook and if the
Outlook is sunny overcast or rainy it's
going to change what you choose to do so
if you know that it's a very high
humidity and it's sunny you're probably
not going to play golf because you're
going to be out there miserable fighting
off the mosquitoes that are out joining
you to play golf with you maybe if it's
rainy you probably don't want to play in
the rain but if it's slightly overcast
and you get just the right Shadow that's
a good day to play golf and be outside
out on the green now in this example you
can probably make your own tree pretty
easily because it's a very simple set of
data going in but the question is how do
you know what to split where do you
split your data what if this is much
more complicated data where it's not
something that you would particularly
understand like studying cancer they
take about 36 measurements of the
cancerous cells and then each one of
those measurements represents how
bulbous it is how extended it is how
sharp the edges are something that as a
human we would have no understanding of
so how do we decide how to split that
data up and is that the right decision
tree but so that's a question is going
to come up is this the right decision
tree for that we should calculate
entropy and Information Gain two
important vocabulary words there are the
entropy and the Information Gain entropy
entropy is a measure of Randomness or
impurity in the data set entropy should
be low so we want the chaos to be as low
as possible we don't want to look at it
and be confused by the images or what's
going on there with mixed data and the
Information Gain it is the measure of
decrease in entropy after the data set
is split also known as entropy reduction
Information Gain should be high so we
want our information that we get out of
the split to be as high as possible
let's take a look at entropy from the
mathematical side in this case we're
going to denote entropy as I of P of and
N where p is the probability that you're
going to play a game of golf and N is
the probability where you're not going
to play the game of golf now you don't
really have to memorize these formulas
there's a few of them out there
depending on what you're working with
but it's important to note that this is
where this formula is coming from so
when you see it you're not lost when
you're running your programming unless
you're building your own decision tree
code in the back and we simply have a
log s of P Over p+ N minus n/ P plus n *
the log squ of n of p plus n but let's
break that down and see what actually
looks like when we're Computing that
from the computer script side entropy of
a target class of the data set is the
whole entropy so we have entropy play
golf and we look at this if we go back
to the data you can simply count how
many yeses and no in our complete data
set for playing golf days in our
complete set we find we have five days
we did play golf and nine days we did
not play golf and so our I equals if you
add those together 9 + 5 is 14 and so
our I equals 5 over 14 and 9 over 14
that's our p and N values that we plug
into that formula and you can go 5 over
14 = 36 9 over 14 = 64 and when you do
the whole equation you get the minus. 36
log tk^ 2 of 36 -64 log s < TK of 64 and
we get a set value we get .9 four so we
now have a full entropy value for the
whole set of data that we're working
with and we want to make that entropy go
down and just like we calculated the
entropy out for the whole set we can
also calculate entropy for playing golf
in the Outlook is it going to be
overcast or rainy or sunny and so we
look at the entropy we have P of Sunny
time e of 3 of two and that just comes
out how many sunny days yes and how many
sunny days no over the total which is
five don't forget the but the we'll
divide that five out later on equals P
overcast = 4 comma 0 plus rainy = 2A 3
and then when you do the whole setup we
have 5 over 14 remember I said there was
a total of five 5 over 14 * the I of 3
of 2 + 4 over 14 * the 4 comma 0 and 514
over I of 23 and so we can now compute
the entropy of just the part it has to
do with the forecast and we get
693 similarly we can calculate the
entropy of other predictors like
temperature humidity and wind and so we
look at the gain Outlook how much are we
going to gain from this entropy play
golf minus entropy play golf Outlook and
we can take the original 0.94 for the
whole set minus the entropy of just the
um rainy day in temperature and we end
up with a gain of
0247 so this is our Information Gain
remember we Define entropy and we Define
information gain the higher the
information gain the lower the entropy
the better the information gain of the
other three attributes can be calculated
in the same way so we have our gain for
temperature equals
0.029 we have our gain for humidity
equals 0.152 and our gain for a windy
day equals
048 and if you do a quick comparison
you'll see the. 247 is the greatest gain
of information so that's the split we
want now let's build the decision tree
so we have the Outlook look is it going
to be sunny overcast or rainy that's our
first split because that gives us the
most Information Gain and we can
continue to go down the tree using the
different information gains with the
largest information we can continue down
the nodes of the tree where we choose
the attribute with the largest
Information Gain as the root node and
then continue to split each sub node
with the largest Information Gain that
we can compute and although it's a
little bit of a tongue twister to say
all that you can see that it's a very
easy to view visual model we have our
Outlook we split it three different
directions if the Outlook is overcast
we're going to play and then we can
split those further down if we want so
if the over Outlook is sunny but then
it's also windy if it's uh windy we're
not going to play if it's uh not windy
we'll play so we can easily build a nice
decision tra to guess what we would like
to do tomorrow and give us a nice
recommendation for the day so we want to
know if it's a good day to play golf
when it's sunny and windy remember the
original question that came out
tomorrow's weather report is sunny and
windy you can see by going down the tree
we go outl sunny out look windy we're
not going to play golf tomorrow so our
little Smartwatch pops up and says I'm
sorry tomorrow is not a good day for
golf it's going to be sunny and windy
and if you're a huge golf fan you might
go uhoh it's not a good day to play golf
we can go in and watch a golf game at
home so we'll sit in front of the TV
instead of being out playing golf in the
wind now that we looked at our decision
tree let's look at the third one of our
algorithms we're investigating support
Vector machine so support Vector machine
is a widely used classification
algorithm the idea of support Vector
machine is simple the algorithm creates
a separation line which divides the
classes in the best possible manner for
example dog or cat disease or no disease
suppose we have a labeled sample data
which tells height and weight of males
and females a new data point arrives and
we want to know whether it's going to be
a male or a female so we start by
drawing a line we draw decision lines
but if we consider decision line one
then we will classify the individual as
a male and if we consider decision line
two then it will be a female so you can
see this person kind of lies in the
middle of the two groups so it's a
little confusing trying to figure out
which line they should be under we need
to know which line divides the classes
correctly but how the goal is to choose
a hyperplane and that is one of the key
words they use when we talk about
support Vector machines choose a
hyperplane with the greatest possible
margin between the decision line and the
nearest Point within the training set so
you can see here we have our support
Vector we have the two nearest points to
it and we draw a line between those two
points and the distance margin is the
distance between the hyperplane and the
nearest data point from either set so we
actually have a value and it should be
equally distant between the two um
points that we're comparing it to when
we draw the hyperplanes we observe that
line one has a maximum distance so we
observe that line one has a maximum
distance margin so we'll classify the
new data point Point correctly and our
result on this one is going to be that
the new data point is Mel one of the
reasons we call it a hyperplane versus a
line is that a lot of times we're not
looking at just weight and height we
might be looking at 36 different
features or dimensions and so when we
cut it with a hyper plane it's more of a
three-dimensional cut in the data
multi-dimensional it cuts the data a
certain way and each plane continues to
cut it down until we get the best fit or
match let's understand this with the
help of an example problem statement I
always start with a problem statement
when you're going to put some code
together we're going to do some coating
now classifying muffin and cupcake
recipes using support Vector machines so
the cupcake versus the muffin let's have
a look at our data set and we have the
different recipes here we have a muffin
recipe that has so much flour I'm not
sure what measurement 55 is in but it
has 55 maybe it's
ounces but uh it has a certain amount of
flour certain amount of milk sugar
butter egg baking powder vanilla and
salt and So based on these measurements
we want to guess whether we're making a
muffin or a cupcake and you can see in
this one we don't have just two features
we don't just have height and weight as
we did before between the male and
female in here we have a number of
features in fact in this we're looking
at eight different features to guess
whether it's a muffin or a cupcake
what's the difference between a muffin
and a cupcake turns out muffins have
more flour while cupcake have more
butter and sugar so basically the
cupcakes a little bit more of a dessert
where the muffins a little bit more of a
fancy bread but how do we do that in
Python how do we code that to go through
recipes and figure out what the recipe
is and I really just want to say
cupcakes versus muffins like some big
professional wrestling thing before we
start in our cupcakes versus muffins we
are going to be working in Python
there's many versions of python many
different editors that is one of the
strengths and weaknesses of python is it
just has so much stuff attached to it
it's one of the more popular data
science programming packages you can use
in this case we're going to go ahead and
use anaconda and Jupiter notebook the
Anaconda Navigator has all kinds of fun
tools once you're into the anacon
Navigator you can change environments I
actually have a number of environments
on here we'll be using python 36
environment so this is in Python version
36 although it doesn't matter too much
which version you use I usually try to
stay with the 3x because they're current
unless you have a project that's very
specifically in version 2x 27 I think is
usually what most people use in the
version two and then once we're in our
um Jupiter notebook editor I can go up
and create a new file and we'll just
jump in here in this case we're doing
spvm muffin versus Cupcake and then
let's start with our packages for data
analysis
and we almost always use a couple
there's a few very standard packages we
use we use import oops import
import
numpy that's for number python they
usually denote it as NP that's very
comma that's very common and then we're
going to import pandas as
PD and numpy deals with number arrays
there's a lot of cool things you can do
with the numpy uh setup as far as multi
M applying all the values in an array in
a numpy array data array pandas I can't
remember if we're using it actually in
this data set I think we do as an import
it makes a nice data frame and the
difference between a data frame and a
nump array is that a data frame is more
like your Excel spreadsheet you have
columns you have indexes you have
different ways of referencing it easily
viewing it and there's additional
features you can run on a data frame and
pandas kind of sits on numpy so they you
need them both in there
and then finally we're working with the
support Vector machine so from sklearn
we're going to use the SK learn model
import svm support Vector
machine and then as a data scientist you
should always try to visualize your data
some data obviously is too complicated
or doesn't make any sense to the human
but if it's possible it's good to take a
second look at it so you can actually
see what you're doing and for that we're
going to use two packages we're going to
import map plot library. pyplot as PLT
again very common and we're going to
import caborn as SNS and we'll go ahead
and set the font scale in the SNS right
in our import line that's with this um
semicolon followed by a line of data
we're going to set the SNS and these are
great because the the caborn sits on top
of map plot Library just like pandas
hits on numpy so it adds a lot more
features and use and control we're
obviously not going to get into matplot
library and caborn it' be its own
tutorial we're really just focusing on
the sbm the support Vector machine from
sklearn and since we're in Jupiter
notebook uh we have to add a special
line in here for our mat plot library
and that's your percentage sign or Amber
sign matap plot library in line now if
you're doing this in just a straight
code Project A lot of times I use like
notepad Plus plus and I'll run it from
there you don't have to have that line
in there because it'll just pop up as
its own window on your computer
depending on how your computer set up
because we're running this in the
Jupiter notebook as a browser setup this
tells it to display all of our Graphics
right below on the page so that's what
that line is for remember the first time
I ran this I didn't know that and I had
to go look that up years ago it's quite
a headache so map plot library in line
is just because we're running this on
the web setup and we can go ahead and
run this make sure all our modules are
in they're all imported which is great
if you don't have them import you'll
need to go ahead and pip use the PIP or
however you do it there's a lot of other
install packages out there although pip
is the most common and you have to make
sure these are all installed on your
python setup the next step of course is
we got to look at the data can't run a
model for predicting data if you don't
have actual data so to do that let me go
ahead and open this up and take a look
and we have our uh cupcakes versus
muffins and it's a CSV file or CSV
meaning that it's comma separated
variable and it's going to open it up in
a nice uh spreadsheet for me and you can
see up here we have the type we have
muffin muffin muffin cupcake cupcake
cupcake and then it's broken up into
flour milk sugar butter egg baking
powder vanilla and salt so we can do is
we can go ahead and look at this data
also in our
python let us create a variable recipes
equals we're going to use our pandas
module. read CSV remember was a comma
separated
variable and the file name happened to
be cupcakes versus muffins oops I got
double brackets
there do it this
way there we go cupcakes versus
muffins because the program I loaded or
the the place I saved this particular
Python program is in the same folder we
can get by with just the file name but
remember if you're storing it in a
different location you have to also put
down the full path on
there and then because we're in pandas
we're going to go ahead and you can
actually in line you can do this but let
me do the full print you can just type
in recipes. head in the Jupiter notebook
but if you're running in code in a
different script You' need to go ahead
and type out the whole print recipes.
head
and Panda's knows is that's going to do
the first five lines of data and if we
flip back on over to the spread sheet
where we opened up our CSV
file uh you can see where it starts on
line two this one calls it zero and then
2 3 4 5 six is going to match go and
close that out because we don't need
that anymore and it always starts at
zero and these are it automatically
indexes it since we didn't tell it to
use an index in here so that's the index
number for the left hand side and it
automatically took the top row as uh
labels so Panda's using it to read a CSV
is just really slick and fast one of the
reasons we love our pandas not just
because they're cute and cuddly teddy
bears and let's go ahead and plot our
data and I'm not going to plot all of it
I'm just going to plot the uh sugar and
flour now obviously you can see where
they get really complicated if we have
tons of different features and so you'll
break them up and maybe look at just two
of them at a time to see how they
connect and to plot them we're going to
go ahead and use caborn so that's our
SNS and the command for that is SNS dolm
plot and then the two different
variables I'm going to plot is flour and
sugar data equals recipes the Hue equals
type and this is a lot of fun because it
knows this is pandas coming in so this
is one of the powerful things about
pandas mixed with Seaborn and
doing
graphing and then we're going to use a
pallet set one there's a lot of
different sets in there you can go look
them up for Seaborn we're do a regular a
fit regular equals false so we're not
really trying to fit anything and it's a
scatter
kws a lot of these settings you can look
up in Seaborn half of these you could
probably leave off when you run them
somebody played with this and found out
the these were the best settings for
doing a Seaborn plot let's go ahead and
run that and because it does it in line
it just puts it right on the
page and you can see right here that
just based on sugar and flour alone
there's a definite split and we use
these models because you can actually
look at it and say hey if I drew a line
right between the middle of the blue
dots and the red dots we'd be able to do
an svm and and a hyperplane right there
in the
middle then the next step is to format
or
pre process our
data and we're going to break that up
into two
parts we need a type label and remember
we're going to decide whether it's a
muffin or a cupcake well a computer
doesn't know muffin or cupcake it knows
zero and one so what we're going to do
is we're going to create a type layer
and from this we'll create a nump array
in P where and this is where we can do
some logic we take our recipes from our
Panda and wherever type equals muffin
it's going to be zero and then if it
doesn't equal muffin which is cupcakes
it's going to be one so we create our
type label this is the answer so when
we're doing our training model remember
we have to have a a training data this
is what we're going to train it with is
that it's zero or one it's a muffin or
it's
not and then we're going to create our
recipe
features and if you remember correctly
from right up here the First Column is
type so we really don't need the type
column because that's our muffin or
cupcake and in pandas we can easily sort
that
out we take our value
recipes dot columns that's a pandas
function built into pandas
got values so converting them to values
so it's just the column titles going
across the top and we don't want the
first one so what we do is since it
always starts at zero we want
one colon till the
end and then we want to go ahead and
make this a list and this converts it to
a list of
strings and then we can go ahead and
just take a look and see what we're
looking at for the features make sure it
looks right
go ahead and run
that and I forgot the S on recipes so
we'll go ahead and add the s in there
and then run that and we can see we have
flour milk sugar butter egg baking
powder vanilla and salt and that matches
what we have up here right where we
printed out everything but the type so
we have our features and we have our
label Now the recipe features is just
the titles of the columns and we
actually need the ingredients
and at this point we have a couple
options one we could rent it over all
the
ingredients and when you're doing this
usually you do but for our example we
want to limit it so you can easily see
what's going on because if we did all
the ingredients we have you know that's
what um seven eight different
hyperplanes that would be built into it
we only want to look at one so you can
see what the svm is
doing and so we'll take our recipes and
we'll do just flour and sugar again you
can replace that with your recipe
features and do all of them but we're
going to do just flour and sugar and
we're going to convert that to values we
don't need to make a list out of it
because it's not string values these are
actual values on there and we can go
ahead and just
print ingredients and you can see what
that looks
like uh and so we have just the am of
flour and sugar just the two sets of
plots and just for fun let's go ahead
and take this over here and take our
recipe
features and so if we decided to use all
the recipe features you'll see that it
makes a nice column of different data so
it just strips out all the labels and
everything we just have just the values
but because we want to be able to view
this easily in a plot later on we'll go
ahead and take that and just do flour
and
sugar and we'll run that you'll see it's
just the two columns
so the next step is to go ahead and fit
our
model we'll go and just call it model
and it's a svm we're using a package
called
SVC in this case we're going to go ahead
and set the kernel equals linear so it's
using a specific setup on there and if
we go to the reference on their website
for the
svm you'll see that there's about
there's eight of them here three of them
are for regression three are for
classification the SVC support Vector
classification is probably one of the
most commonly
used and then there's also one for
detecting outliers and another one that
has to do with something a little bit
more specific on the model but SBC and
SV are the two most commonly used
standing for support vector classifier
and support Vector regression remember
regression is an actual value a float
value or whatever you're trying to work
on an BC is a classifier so it's a yes
no true
false but for this we want to know 01
muffin cupcake we go ahead and create
our model and once we have our model
created we're going to do model. fit and
this is very common especially in the
sklearn all their models are followed
with the fit
command and what we put into the fit
what we're training with it is we're
putting in the ingredients which in this
case we limited to just flour and sugar
and the type label is it a muffin or
cupcake now in more complicated data
science series you'd want to split into
we won't get into that today we split it
into uh training data and test data and
they even do something where they split
it into thirds where a third is used for
where you switch between which one's
training and test there's all kinds of
things go into that it gets very
complicated when you get to the higher
end not overly complicated just an extra
step which we're not going to do today
because this is a very simple set of
data and let's go ahead and run this and
now we have our model fit and I got a
error here so let me fix that real quick
it's Capital SBC it turns
out I did it
lowercase support
Vector classifier there we go let's go
ahead and run that and you'll see it
comes up with all this information that
it prints out automatically these are
the defaults of the model you notice
that we changed the kernel to linear and
there's our kernel linear on the print
out and there's other different settings
you can mess
with we're going to just leave that
alone for right now for this we don't
really need to mess with any of
those so next we're going to dig a
little bit into our newly trained model
and we're going to do this so we can
show you on a
graph and let's go ahead and get the
separating we're going to say we're
going to use a W for our variable on
here
we're going to do model.
coefficient 0 so what the heck is that
again we're digging into the model so
we've already got a prediction and a
train this is a math behind it that
we're looking at right now and so the W
is going to represent two different
coefficients and if you remember we had
y equals MX plus C so these coefficients
are connected to that but in
two-dimensional it's it's a
plane we don't want to spend too much
time on this because you can get lost in
the confusion of the math so if you're a
math wiiz this is great you can go
through here and you'll see that we have
AAL minus W of 0 over W of 1 remember
there's two different values there and
that's basically the slope that we're
generating and then we're going to build
an XX what is XX we're going to set it
up to a numpy array there's our NP
Lin space so we're creating a
line plus The Intercept well to make
this work we can do this as y y equals
the slope times each value in that array
that's the neat thing about numpy so
when I do a * XX which is a whole numpy
array of values it multiplies a across
all of them and then it takes those same
values and we subtract the model
intercept that's your uh
we had MX plus C so that'd be the C from
the formula yal MX plus
C and that's where all these numbers
come from a little bit confusing because
it's digging out of these different
arrays and then we want to do is we're
going to take this and we're going to go
ahead and plot it so plot the parallels
to separating hyperplane that pass
through the support vectors and so we're
going to create b equals a model support
vectors pulling our support vectors out
there here's our y y which we now know
is a set of data and we have uh we're
going to create y y down equals a * XX +
B1 minus a * B 0 and then model support
Vector B is going to be set that to a
new value the minus1 set up and y y up
equals a * XX + B1 - A * B 0 and we can
go ahead and just run this to load these
variables up if you want to know
understand a little bit more of what's
going on you can see if we
print y y we just run that you can see
it's an array it's this is a line it's
going to have in this case between 30
and 60 so it's going to be 30 variables
in here and the same thing with y y up y
y down and we'll we'll plot those in
just a minute on a graph so you can see
what those look
like just go ahead and delete that out
of here and run that so it loads up the
variables nice clean slate I'm just
going to copy this from before remember
this our SNS our caborn plot LM plot
flower
sugar and I'll just go and run that real
quick so you can see what remember what
that looks like it's just a straight
graph on there and then one of the new
things is because Seaborn sits on top of
pip plot we can do the PIP plot for the
line going through and that is simply
PLT
dolot and that's our xx and y y our two
corresponding values XY and then
somebody played with this to figure out
that the line width equals two and the
color black would look nice so let's go
ahead and run this whole thing with the
PIP plot on there and you can see when
we do this it's just doing flour and
sugar on
here corresponding line between the
sugar and the flour and the muffin
versus
Cupcake um and then we generated the um
support vectors the YY down and y y up
so let's take a look and see what that
looks
like so we'll do our PL
plot and again this is all against XX
the our x value but this time we have
YY
down and let's do something a little fun
with this we can put in a k dash dash
that just tells it to make it a dotted
line and if we're going to do the down
one we also want to do the up one
so here's our
YY up and when we run that it adds both
sets of line and so here's our support
and this is what you expect you expect
these two lines to go through the
nearest data point so the dash lines go
through the nearest muffin and the
nearest cupcake when it's plotting it
and then your svm goes right down the
middle so it gives it a nice split in
our data and you can see how easy it is
to see based just on sugar and flour
which one's a muffin or a
cupcake let's go ahead and create a
function to
predict muffin or
cupcake I've got my um recipes I've
pulled off the um internet and I want to
see the difference between a muffin or a
cupcake and so we need a function to
push that through and I create a
function with DEA and let's call it
muffin or cupcake and remember we're
just doing flour and sugar today not
doing all the ingredients and that
actually is a pretty good split you
really don't need all the ingredients to
know it's flour and sugar and let's go
ahead and do an IFL statement so if
model
predict is of flour and sugar equals
zero so we take our model and we do run
a predict it's very common in sklearn
where you have a DOT predict you put the
data in and it's going to return a value
in this case if it equals zero then
print you're looking at a muffin recipe
else if it's not zero that means it's
one then you're looking at a cupcake
recipe that's pretty straightforward
for function or def for definition DF is
how you do that in Python and of course
you're going to create a function you
should run something in it and so let's
run a cupcake and we're going to send it
values 50 and 20 a muffin or cupcake I
don't know what it is and let's run this
and just see what it gives us and it
says oh it's a muffin you're looking at
a muffin recipe so it very easily
predicts whether we're looking at a
muffin or a cupcake recipe let's plot
this there we go plot this on the graph
so we can see what that actually looks
like and I'm just going to copy it and
paste it from below we plotting all the
points in
there so this is nothing different than
what we did before if I run it you'll
see it has all the points in the lines
on there and what we want to do is we
want to add another point and we'll do
PLT
plot and if you remember correctly we
did for our test we did 50 and
20 and then somebody went in here and
decided we'll do yo for yellow or it's
kind of a orange is yellow color is
going to come out marker size nine those
are settings you can play with somebody
else played with them to come up with
the right setup so it looks good and you
can see there it is graphed um clearly a
muffin in this case in cupcakes versus
muffins the muffin has won and if you'd
like to do your own muffin cupcake
Contender series you certainly can send
a note down below and the team at simply
learn will send you over the data they
use for the muffin and cupcake and
that's true of any of the data um we
didn't actually run a plot on it earlier
we had men versus women you can also
request that information to run it on
your data setup so you can test that
out so to go back over our setup we went
ahead for our Port Vector machine code
we did a predict 40 Parts flour 20 Parts
sugar I think it was different than the
one we did whether it's a muffin or a
cupcake hence we have built a classifier
using spvm which is able to classify if
a recipe is of a cupcake or a muffin
which wraps up our cupcake versus muffin
today in our second tutorial we're going
to cover c means and linear regression
along with going over the quiz questions
we had during our first tutorial
what's in it for you we're going to
cover clustering what is clustering K
means clustering which is one of the
most common Ed clustering tools out
there including a flowchart to
understand K means clustering and how it
functions and then we'll do an actual
python live demo on clustering of cars
based on Brands then we're going to
cover logistic regression what is
logistic regression logistic regression
curve and sigmoid function and then
we'll do another python code demo to
classify a tumor malignant or benign
based on features and let's start with
clustering suppose we have a pile of
books of different genres now we divide
them into different groups like fiction
horror education and as we can see from
this young lady she definitely is into
heavy horror you can just tell by those
eyes in the maple Canadian leaf on her
shirt but we have fiction horror and
education and we wanted to go ahead and
divide our books up well organizing
objects into groups based on similarity
is clustering and in this case as we're
looking at the books we're talking about
clustering things with no one categories
but you can also use it to explore data
so you might not know the categories you
just know that you need to divide it up
in some way to conquer the data and to
organize it better but in this case
we're going to be looking at clustering
in specific categories and let's just
take a deeper look at that we're going
to use K means clustering K means
clustering is probably the most commonly
used clustering tool in the machine
learning library K means clustering is
an example of unsupervised learning if
you remember from our previous thing it
is used when you have unlabeled data so
we don't know the answer yet we have a
bunch of data that we want to Cluster
into different groups Define clusters in
the data based on feature similarity so
we've introduced a couple terms here
we've already talked about unsupervised
learning and unlabeled data so we don't
know the answer yet we're just going to
group stuff together and see if we can
find un answer of how things connect
we've also introduced feature similarity
features being different features of the
data now with books we can easily see
fiction and horror and history books but
a lot of times with data some of that
information isn't so easy to see right
when we first look at it and so K means
is one of those tools where we can start
finding things that connect that match
with each other suppose we have these
data points and want to assign them into
a cluster now when I look at these data
points I would probably group them into
two clusters just by looking at them I'd
say two of these group of data kind of
come together but in K means we pick K
clusters and assign random centroids to
clusters where the K clusters represents
two different clusters we pick K
clusters and S random centroids to the
Clusters then we compute distance from
objects to the centroids now we form new
clusters based on minimum distances and
calculate the centroids so we figure out
what the best distance is for the
centroid then we move the centroid and
recalculate those distances repeat
previous two steps iteratively till the
cluster centroids stop changing their
positions and become Static repeat
previous two steps iteratively till the
cluster centroid stop changing and the
positions become Static once the
Clusters become Static then K means
clustering algorithm is said to be
converged and there's another term we
see throughout machine learning is
converged that means whatever math we're
using to figure out the answer has come
to a solution or it's converged on an
answer shall we see the flowchart
understand make a little bit more sense
by putting it into a nice easy step by
step so we start we choose K we'll look
at the elbow method in just a moment we
assign random centroids to clusters and
sometimes you pick the centroids because
you might look at the data in a graph
and say oh these are probably the
central points then we compute the
distance from the objects to the
centroids we take that and we form new
clusters based on minimum distance and
calculate their centroids then we
compute the distance from objects to the
new centroids and then we go back and
repeat those last two steps we calculate
the distances so as we're doing it it
brings into the new centroid and then we
move the centroid around and we figure
out what the best which objects are
closest to each centroid so the objects
can switch from one centroid to the
other as the centroids are moved around
and we continue that until it is
converged let's see an example of this
suppose we have this data set of seven
individuals and their score on two
topics a and B so here's our subject in
this case referring to the person taking
the test and then we have subject a
where we see what they've scored on
their first subject and we have subject
B and we can see what they score on the
second subject now let's take two
farthest apart points as initial cluster
centroids now remember we talked about
selecting them randomly or we can also
just put them in different points and
pick the furthest one apart so they move
together either one works okay depending
on what kind of data you're working on
and what you know about it so we took
the two furthest points one and one and
five and seven and now let's take the
two farthest apart points as initial
cluster centroids each point is then
assigned to the closest cluster with
respect to the distance from the
centroids so we take each one of these
points in there we measure that distance
and you can see that if we measured each
of those distances and you use the
Pythagorean theorem for a triangle in
this case because you know the X and the
Y and you can figure out the diagonal
line from that or you just take a ruler
and put it on your monitor that'd be
kind of silly but it would work if
you're just eyeballing it you can see
how they naturally come together in
certain areas now we again calculate the
centroids of each cluster so cluster one
and then cluster two and we look at each
individual dot there's one two three
we're in one cluster uh the centroid
then moves over it becomes 1.8 comma 2.3
so remember it was at one in one well
the very center of the data we're
looking at would put it at the 1 roughly
22 but 1.8 and 2.3 and the second one if
we wanted to make the overall mean
Vector the average Vector of all the
different distances to that centroid we
come up with 4 comma 1 and 54 so we've
now moved the centroids We compare each
individual's distance to its own cluster
mean and to that of the opposite cluster
and we find can build a nice chart on
here that the as we move that centroid
around we now have a new different kind
of clustering of groups and using ukian
distance between the points and the mean
we get the same formula you see new
formulas coming up so we have our
individual dots distance to the mean
centr of the cluster and distance to the
mean centroid of the cluster only
individual three is nearer to the mean
of the opposite cluster cluster two than
its own cluster one and you can see here
in the diagram where we've kind of
circled that one in the middle so when
we've moved the clust the centroids of
the Clusters over one of the points
shifted to the other cluster because
it's closer to that group of individuals
thus individual 3 is relocated to
Cluster 2 resulting in a new Partition
and we regenerate all those numbers of
how close they are to the different
clusters for the new clusters we will
find the actual cluster centroids so now
we move the centroids over and you can
see that we've now formed two very
distinct clusters on here on comparing
the distance of each individual's
distance to its own cluster mean and to
that of the opposite cluster we find
that the data points are stable hence we
have our final clusters now if you
remember I brought up a concept earlier
k mean on the K means algorithm choosing
the right value of K will help in less
number of iterations and to find the
appropriate number of clusters in a data
set we use the elbow method and within
sum of squares WSS is defined as the sum
of the squared distance between each
member of the cluster and its centroid
and so you see what we've done here is
we have the number of clusters and as
you do the same K means algorithm over
the different clusters and you calculate
what that centroid looks like and you
find the optimal you can actually find
the optimal number of clusters using the
elbow the graph is called as the elbow
method and on this we guessed at two
just by looking at the data but as you
can see the slope you actually just look
for right there where the elbow is and
the slope and you have a clear answer
that we want two different to start with
k means equals 2 A lot of times people
end up Computing K means equals 2 3 four
five until they find the value which
fits on the elbow joint sometimes you
you can just look at the data and if
you're really good with that specific
domain remember domain I mentioned that
last time you'll know that that where to
pick those numbers and where to start
guessing at what that K value is so
let's take this and we're going to use a
use case using K means clustering to
Cluster cars into Brands using
parameters such as horsepower cubic
inches make year Etc so we're going to
use the data set cars data having
information about three brands of cars
Toyota Honda and Nissan we'll go back to
my favorite tool the Anaconda Navigator
with the Jupiter notebook and let's go
ahead and flip over to our Jupiter
notebook and in our Jupiter notebook I'm
going to go ahead and just paste the uh
basic code that we usually start a lot
of these off with we're not going to go
too much into this code because we've
already discussed numpy we've already
discussed matplot library and pandas npy
being the number array pandas being the
panda data frame and map plot for the
graphing and don't forget uh since if
you're using the Jupiter notebook you do
need the matap plot library in line so
that it plots everything on the screen
if you're using a different python
editor then you probably don't need that
because it'll have a popup window on
your computer and we'll go ahead and run
this just to load our libraries and our
setup into here the next step is of
course to look at our data which I've
already opened up in a spreadsheet and
you can see here we have the miles per
gallon cylinders cubic inches horsepower
weight pounds how you you know how heavy
it is time it takes to get to 60 my card
is probably on this one at about 80 or
90 what year it is so this is you can
actually see this is kind of older cars
and then the brand Toyota Honda Nissan
so the different cars are coming from
all the way from 1971 if we scroll down
to uh the 80s we have between the 70s
and 80s a number of cars that they've
put out and let's uh when we come back
here we're going to do importing the
data so we'll go ahead and do data set
equals and we'll use pandas to read this
in and it's uh from a CSV file remember
you can always post this in the comments
and request the data files for these
either in the comments here on the
YouTube video or go to Simply learn.com
and request that the car CSV I put it in
the same folder as the code that I've
stored so my python code is stored in
the same folder so I don't have to put
the full path if you store them in
different folders you do have to change
this and double check your name
variables and we'll go ahead and run
this and uh We've chosen data set
arbitrarily because you know it's a data
set we're importing and we've now
imported our car CSV into the data set
as you know you have to prep the data so
we're going to create the X data this is
the one that we're going to try to
figure out what's going on with and then
there is a number of ways to do this but
we'll do it in a simple Loop so you can
actually see what's going on so we'll do
for i n x. columns so we're going to go
through each of the columns and a lot of
times it's important I I'll make lists
of the columns and do this because I
might remove certain columns or there
might be columns that I want to be
processed differently but for this we
can go ahead and take X of I and we want
to go fill Na and that's a panda's
command but the question is when are we
going to fill the missing data with we
definitely don't want to just put in a
number that doesn't actually mean
something and so one of the tricks you
can do with this is we can take X of I
and in addition to that we want to go
ahead and turn this in into an integer
because a lot of these are integers so
we'll go ahead and keep it integers and
me add the bracket here and a lot of
editors will do this they'll think that
you're closing one bracket make sure you
get that second bracket in there if it's
a double bracket that's always something
that happens regularly so once we have
our integer of X ofi this is going to
fill in any missing data with the
average and I was so busy closing one
set of brackets I forgot that the mean
is also has brackets in there for the
pandas so we can see here we're going to
fill in all the data with the average
value for that column so if there's
missing data it's in the average of the
data it does have then once we've done
that we'll go ahead and loop through it
again and just check and see to make
sure everything is filled in correctly
and we'll print and then we take X is
null and this returns a set of the null
value or the how many lines are null and
we'll just sum that up to see what that
looks like and so when I run this and so
with the X what we want to do is we want
to remove the last column because that
had the model model that's what we're
trying to see if we can cluster these
things and figure out the models there
is so many different ways to sort the X
out for one we could take the X and we
could go data set our variable we're
using and use the iocation one of the
features that's in
pandas and we could take that and then
take all the rows and all but the last
column of the data set and at this time
we can do values we just convert it to
values so one way to do this and if I
let me just put this down here and print
X it's a capital x we chose and I run
this you can see it's just the values we
could also take out the values and it's
not going to return anything because
there's no values connected to it what I
like to do with this is instead of doing
the iocation which does integers more
common is to come in here and we have
our data set and we're going to do data
set dot or data set. columns and
remember that lists all the columns so
if I come in here let me just Mark that
as red and I print data set.
columns you can see that I have my index
here I have my MPG cylinders everything
including the brand which we don't want
so the way to get rid of the brand would
be to do data Columns of Everything But
the last one minus one so now if I print
this you'll see the brand disappears
and so I can actually just take data set
columns minus
one and I'll put it right in here for
the columns we're going to look
at and let's unmark this and unmark
this and now if I do an x. head I now
have a new data frame and you can see
right here we have all the different
columns except for the brand at the end
of the year and it turns out when you
start playing with the data set you're
going to get an error later on and it'll
say cannot convert string to uh float
value and that's because for some reason
these things the way they recorded them
must have been recorded as strings so we
have a neat feature in here on pandas to
convert and it is simply convert
objects and for this we're going to do
convert oops convert
underscore numeric numeric equals
true and yes I did have to go look that
up I don't have it memorized the convert
numeric in there if I'm working with a
lot of these things I remember them but
um depending on where I'm at what I'm
doing I usually have to look it up and
we run that oops I must have missed
something in here let me double check my
spelling and when I double check my
spelling you'll see I missed the first
underscore in the convert objects and
when I run this it now has everything
converted into a numeric value because
that's what we're going to be working
with is numeric values down here
and the next part is that we need to go
through the data and eliminate null
values most people when they're doing
small amounts working with small data
pools discover afterwards that they have
a null value and they have to go back
and do this so you know be aware
whenever we're formatting this data
things are going to pop up and sometimes
you go backwards to fix it and that's
fine that's just part of exploring the
data and understanding what you
have and I should have done this earlier
but let me go ahead and increase the
size of my window one
notch there we go easier to
see so we'll do 4 I in working with x.
columns we'll page through all the
columns and we want to take X of I we're
going to change that we're going to
alter it and so with this we want to go
ahead and fill in X of I pandis Has the
fill in a and that just fills in any
non-existent missing data I we'll put my
brackets up and there's a lot of
different ways to fill this data if you
have a really large data set some people
just void out that data because if and
then look at it later in a separate
exploration of data one of the tricks we
can do is we can take our column and we
can find the
means and the means is in other our
quotation marks so when we take the
columns we're going to fill in the the
non-existing one with with the means the
problem is that returns a decimal float
so some of these aren't decimals
certainly may to be a little careful of
doing this but for this example we're
just going to fill it in with the
integer version of this keeps it on par
with the other data that isn't a decimal
point and then what we also want to do
is we want to double check ways you can
do that is simply go in here and take
our X of I column so it's going to go
through the x of I column it says is
null so it's going to return any any
place there's a null value it actually
goes through all the rows of each column
is null and then we want to go ahead and
sum that so we take that we add the sum
value and these are all pandas so is
null is a panda command and so is sum
and if we go through that and we go
ahead and run
it and we go ahead and take and run that
you'll see that all the columns have
zero null values so we've now tested and
double checked and our data is nice and
clean we have no null values everything
is now a number value we turned it into
numeric and we've removed the last
column in our data and at this point
we're actually going to start using the
elbow method to find the optimal number
of clusters so we're now actually
getting into the SK learn part uh the K
means clustering on here I guess we'll
go ahead and assume it up one more notot
so you can see what I'm typing in
here and then from
sklearn going to or
sklearn cluster we're going to import K
means I always forget to capitalize the
K and the M when I do this so it's
capital K capital K
means and we'll go and create a um aray
wcss equals we'll make it an empty array
if you remember from the elbow method
from our
slide within the sums of squares WSS is
defined as the sum of square distance
between each member of the cluster and a
centroid so we're looking at that change
in differences as far as a square
distance and we're going to run this
over a number of K mean
values in fact let's go for I in range
we'll do 11 of
them range Zer of
11 and the first thing we're going to do
is we're going to create the actual
we'll do it all lower
case and so we're going to create this
object from the K means that we just
imported and the variable that we want
to put into this is in clusters and
we're going to set that equals to I
that's the most important one because
we're looking at how increasing the
number of clusters changes our answer
there are a lot of settings to the K
means
our guys in the back did a great job
just kind of playing with some of them
the most common ones that you see in a
lot of stuff is how you init your K
means so we have K means plus plus plus
this is just a tool to let the model
itself be smart how it picks it
centroids to start with its initial
centroids we only want to iterate no
more than 300 times we have a Max
iteration we put in there we have an the
infinite the random State equals zero
you really you don't need to worry too
much about these when you're first
learning this as you start digging in
deeper you start finding that these are
shortcuts that will speed up the
process as far as a setup but the big
one that we're working with is the in
clusters equals I so we're going to
literally train our K means 11 times
we're going to do this process 11 times
and if you're working with h Big Data
you know the first thing you do is you
run a small sample of the data so you
can test all your stuff on it and you
can already see the problem that if I'm
going to iterate through a terabyte of
data 11 times and then the K means
itself is iterating through the data
multiple times that's a heck of a
process so you got to be a little
careful with this a lot of times though
you can find your elbow using the elbow
method find your opal number on a sample
of data especially if you're working
with larger data sources so we want to
go ahead and take our K means and we're
just going to fit it if you're looking
at any of the SK learn very common you
fit your model and if you remember
correctly our our variable we're using
is the capital x and once we fit this
value we go back to the um array we made
and we want to go and just toin that
value on the
end and it's not the actual fit we're
pinning in there it's when it generates
it it generates the value you're looking
for is inertia so K means. inertia will
pull that specific value out that we
need and let's get a visual on this
we'll do our PLT plot and what we're
plotting PL in
here is first the xaxis which is range
01 so that will generate a nice little
plot there and the wcss for our Y
axis it's always nice to give our plot a
title and let's see we'll just give it
the elbow method for the title and let's
get some labels so let's go ahead and do
PLT X
label and what we'll do we'll do number
of cluster for that and PLT y
label and for that we can do oops there
we go wcss since that's what we're doing
on the plot on there and finally we want
to go ahead and display our graph which
is simply PLT do
oops. show there we go and because we
have it set to inline it'll appear
inline hopefully I didn't make a type
error on
there and you can see we get a very nice
graph you can see a very nice elbow
joint there at uh two and again right
around three and four and then after
that there's not very much now as a data
scientist if I was looking at this I
would do either three or four and I'd
actually try both of them to see what
the um output look like and they've
already tried this in the back so we're
just going to use three as a setup on
here and let's go ahead and see what
that looks like when we actually use
this to show the different kinds of
cars and let's go ahead and apply the K
means to the cars data set and basically
we're going to copy the code that we
looped through up above where K means
equals K means number of clusters and
we're just going to set that number of
clusters to three since that's what
we're going to look for and you could do
three and four on this and graph them
just to see how they come up
differently' be kind of curious to look
at that but for this we're just going to
set it to three go ahead and create our
own variable y k means for our answers
and we're going to set that equal to
whoops double equal there to K means but
we're not going to do a fit we're going
to do a fit predict is the setup you
want to use and when you're using
untrained models you'll see um a
slightly different usually you see fit
and then you see just the predict but we
want to both fit and predict the K means
on this and that's fitcore predict and
then our capital x is the data we're
working
with and before we plot this data we're
going we're going to do a little pandas
trick we're going to take our x value
and we're going to set X as Matrix so
we're converting this into a nice rows
and columns kind of set up but we want
the we're going to have columns equals
none so it's just going to be a matrix
of data in here and let's go ahead and
run
that a little warning you'll see These
Warnings pop up because things are
always being updated so there's like
minor changes in the versions and future
versions instead of Matrix now that it's
more common to set it values instead of
doing as Matrix but M Matrix works just
fine for right now and you'll want to
update that later on but let's go ahead
and dive in and plot this and see what
that looks like and before we dive into
plotting this data I always like to take
a look and see what I am plotting so
let's take a look at why K means I'm
just going to print that out down here
and we see we have an array of answers
we have 2 1 0 2 one two so it's
clustering these different rows of dat
based on the three different spaces it
thinks it's going to
be and then let's go ahead and print X
and see what we have for x and we'll see
that X is an array it's a matrix so we
have our different values in the array
and what we're going to do it's very
hard to plot all the different values in
the array so we're only going to be
looking at the first two or positions
zero and
one and if you were doing a full
presentation in front of the board
meeting you might actually do a little
different and and dig a little deeper
into the different aspects because this
is all the different columns we looked
at but we only look at columns one and
two for this to make it easy so let's go
ahead and clear this data out of here
and let's bring up our plot and we're
going to do a scatter plot here so PLT
scatter
and this looks a little complicated so
let's explain what's going on with this
we're going to take the X values
and we're only interested in y of K
means equals zero the first cluster okay
and then we're going to take value zero
for the x axis and then we're going to
do the same thing here we're only
interested in K means equals zero but
we're going to take the second column so
we're only looking at the first two
columns in our answer or in the data and
then the guys in the back played with
this a little bit to make it
pretty and they discovered that it looks
good with as a size equals 100 that's
the size of the dots we're going to use
red for this one and when they were
looking at the data and what came out it
was definitely the Toyota on this so
we're just going to go ahead and label
it Toyota again that's something you'd
really have to explore in here as far as
playing with those numbers and see what
looks good we'll go ahead and hit enter
in there and I'm just going to paste in
the next two lines which is the next two
cars and this is our Nissa and Honda and
you'll see with our scatter plot we're
now looking at where Yore K means equals
1 and we want the zero column and y k
means equals 2 again we're looking at
just the first two columns zero and one
and each of these rows then corresponds
to Nissan and
Honda and I'll go ahead and hit enter on
there and uh finally let's take a look
and put the centroids on there again
we're going to do a scatter
plot and on the centroids you can just
pull that from our c means the uh model
we created do cluster centers and we're
going to just do
um all of them in the first number and
all of them in the second number which
is 0 one because you always start with
zero and
one and then they were playing with the
size and everything to make it look good
we'll do a size of 300 we're going to
make the color yellow and we'll label
them it's always good to have some good
labels
centroids and then we do want to do a
title PLT
title and pop up there PLT title so you
always make want to make your graphs
look pretty we'll call it clusters of
Carm
make and one of the features of the plot
library is you can add a legend it'll
automatically bring in it since we've
already labeled the different aspects of
the legend with Toyota Nissan and
Honda and finally we want to go ahead
and show so we can actually see it and
remember it's in line uh so if you're
using a a different editor that's not
the Jupiter notebook you'll get a popup
of this and you should have a nice set
of clusters here so we can look at this
and we have a clusters of Honda and
green Toyota and red Nissan and purple
and you can see where they put the
centroids to separate
them now when we're looking at this we
can also plot a lot of other different
data on here as far because we only
looked at the first two columns this is
just column one and two or 01 as as you
label them in computer scripting but you
can see here we have a nice clusters of
car make and we've able to pull out the
data and you can see how just these two
columns form very distinct clusters of
data so if you were exploring new data
you might take a look and say well what
makes these different almost going in
reverse you start looking at the data
and pulling apart the columns to find
out why is the first group set up the
way it is maybe you're doing loans and
you want to go well why is this group
not defaulting on their loans and why is
the last group defaulting on their loans
and why is the middle group 50%
defaulting on their bank loans and you
start finding ways to manipulate the
data and pull out the answers you
want so now that you've seen how to use
K mean for clustering let's move on to
the next topic now let's look into
logistic regression the logistic
regression algorithm is the simplest
classification algorithm used for binary
or multiclassification problems and we
can see we have our little girl from
from Canada who's into horror books is
back that's actually really scary when
you think about that with those big eyes
in the previous tutorial we learned
about linear regression dependent and
independent variables so to brush up y =
mx + C very basic algebraic function of
uh Y and X the dependent variable is the
target class variable we are going to
predict the independent variables X1 all
the way up to xn are the features or
attributes we're going to use to predict
the target class we know what a linear
regression looks like but using the
graph we cannot divide the outcome into
categories it's really hard to
categorize 1.5 3.6 9.8 uh for example a
linear regression graph can tell us that
with increase in number of hours studied
the marks of a student will increase but
it will not tell us whether the student
will pass or not in such cases where we
need the output as categorical value we
will use logistic regression and for
that we're going to use the sigmoid
function so you can see here we have our
marks 0 to 100 number of hours studied
that's going to be what they're
comparing it to in this example and we
usually form a line that says y = mx + C
and when we use the sigmoid function we
have P = 1/ 1 + eus y it generates a
sigmoid curve and so you can see right
here when you take the Ln which is the
natural logarithm I always thought it
should be n l not Ln that's just the
inverse of uh e your e to the minus y
and so we do this we get Ln of p over
1us p = m * x + C that's the sigmoid
curve function we're looking for and we
can zoom in on the function and you'll
see that the function as it derives goes
to one or to zero depending on what your
x value is and the probability if it's
greater than 0.5 the value is
automatically rounded off to one
indicating that the student will pass so
if they're doing a certain amount of
studying they will probably pass then
you have a threshold value at the 0.5 it
automatically puts that right in the
middle usually and your probability if
it's less than 05 the value rendered off
to zero indicating the student will fail
so if they're not studying very hard
they're probably going to fail this of
course is ignoring the outliers of that
one student who's just a natural genius
and doesn't need any studying to
memorize everything that's not me
unfortunately have to study hard to
learn new stuff
problem statement to classify whether a
tumor is malignant or benign and this is
actually one of my favorite data sets to
play with because it has so many
features and when you look at them you
really are hard to understand you can't
just look at them and know the answer so
it gives you a chance to kind of dive
into what data looks like when you
aren't able to understand the specific
domain of the data but I also want you
to remind you that in the domain of
medicine if I told you that my
probability was really good it
classified things at say 90% or 95% and
I'm classifying whether you're going to
have a malignant or a B9 tumor I'm
guessing that you're going to go get it
tested anyways so you got to remember
the domain we're working with so why
would you want to do that if you know
you're just going to go get a biopsy
because you know it's that serious this
is like an all or nothing just
referencing the domain it's important it
might help the doctor know where to look
just by understanding what kind of tumor
it is so it might help them or Aid them
in something they missed from before so
let's go ahead and dive into the code
and I'll come back to the domain part of
it in just a minute so use case and
we're going to do our noral Imports here
we're importing numpy Panda Seaborn the
matplot library and we're going to do
mat plot library in line since I'm going
to switch over to Anaconda so let's go
ahead and flip over there and get this
started so I've opened up a new window
in my anaconda Jupiter
notebook and by the way jupyter notebook
uh you don't have to use Anaconda for
the jupyter notebook I just love the
interface and all the tools that
Anaconda brings so we got our import
numpy is in P for our numpy number array
we have our Panda's PD we're going to
bring in caborn to help us with our
graphs as SNS so many really nice Tools
in both caborn and matplot library and
we'll do our matplot library. pyplot as
PLT and then of course we want to let it
know to do it in line and let's go and
just run that so it's all set up
and we're just going to call our data
data not creative today uh equals PD and
this happens to be in a CSV file so
we'll use a pd. read CSV and I happen to
name the file or renamed it data for
p2.png
further and let's just see what it looks
like in a
spreadsheet so when I pop it open in a
local spreadsheet and this is just a CSV
file comma separate variables we have an
ID so I guess they um categorizes for
reference of what id which test was done
the diagnosis M for malignant B for B9
so there's two different options on
there and that's what we're going to try
to predict is the m and b and test it
and then we have like the radius mean or
average the texture average perimeter
mean area mean smoothness I don't know
about you but unless you're a doctor in
the field most of the stuff I mean you
can guess what concave means just by the
term concave but I really wouldn't know
what that means in the measurements
they're taking so they have all kinds of
stuff like how smooth it is uh the
Symmetry and these are all float values
we can just page through them real quick
and you'll see there's I believe 36 if I
remember correctly in this
one so there's a lot of different values
they take and all these majure ments
they take when they go in there and they
take a look at the different growth the
tumorous growth so back in our data and
I put this in the same folder as a code
so I saved this code in that folder
obviously if you have it in a different
location you want to put the full path
in there and we'll just do uh
Panda's first five lines of data with
the data. head and we run that we can
see that we have pretty much what we
just looked at we have an ID we have a
diagnosis
if we go all the way across you'll see
all the different columns coming across
displayed nicely for our
data and while we're exploring the data
our caborn which we referenced as
SNS makes it very easy to go in here and
do a joint plot you'll notice the very
similar to because it is sitting on top
of the U plot Library so the joint plot
does a lot of work for us and we're just
going to look at the first two columns
that we're interested in the radius mean
and the texture mean we'll just look at
those two columns and data equals data
so that tells it which two columns we're
plotting and that we're going to use the
data that we pulled in let's just run
that and it generates a really nice
graph on here and there's all kinds of
cool things on this graph to look at I
mean we have the texture mean and the
radius mean obviously the axes you can
also
see and one of the cool things on here
is you can also see the histogram they
show that for the rad radius mean where
is the most common radius mean come up
and where the most common texture is so
we're looking at the tech the on each
growth its average texture and on each
radius its average uh radius on there
gets a little confusing because we're
talking about the individual objects
average and then we can also look over
here and see the the histogram showing
us the median or how common each
measurement is and that's only two
columns so let's dig a little deeper
into Seaborn they also have a heat map
and if you're not familiar with heat
Maps a heat map just means it's in color
that's all that means heat map I guess
the original ones were plotting heat
density on something and so ever since
then it's just called a heat map and
we're going to take our data and get our
corresponding numbers to put that into
the heat map and that's simply data. C
RR for that that's a pandas expression
remember we're working in a pandas data
frame so that's one of the Cool Tools in
pandas for our data and let's just pull
that information into a heat map and see
what that looks like and you'll see that
we're now looking at all the different
features we have our ID we have our
texture we have our area our compactness
concave points and if you look down the
middle of this chart diagonal going from
the upper left to bottom right it's all
white that's because when you compare
texture to texture they're identical so
they're 100% or in this case perfect one
in their correspondence
and you'll see that when you look at say
area or right below it it has almost a
black on there when you compare it to
texture so these have almost no
corresponding data They Don't Really
form a linear graph or something that
you can look at and say how connected
they are they're very scattered data
this is really just a really nice graph
to get a quick look at your data doesn't
so much change what you do but it
changes verifying so when you get an
answer or something like that or you
start looking at some of these
individual pieces you might go hey that
doesn't match according to showing our
heat map this should not correlate with
each other and if it is you're going to
have to start asking well why what's
going on what else is coming in there
but it does show some really cool
information on here mean we can see from
the ID there's no real one feature that
just says if you go across the top line
that lights up there's no one feature
that says hey if the area is a certain
size then it's going to be B9 or
malignant it's says there's some that
sort of add up and that's a big hint in
the data that we're trying to ID this
whether it's malignant or B9 that's a
big hint to us as data scientists to go
okay we can't solve this with any one
feature it's going to be something that
includes all the features or many of the
different features to come up with the
solution for it and while we're
exploring the data let's explore one
more area and let's look at data do is
null we want to check for null values in
our data if you remember from earlier in
this tutorial we did it a little
differently where we added stuff up and
summed them up you can actually with
pandas do it really quickly data. isnull
and Summit and it's going to go across
all the columns so when I run
this you're going to see all the columns
come up with no null
data so we've just just to reash these
last few steps we've done a lot of
exploration we have looked at the first
two columns
and seeing how they plot with the caborn
with a joint plot which shows both the
histogram and the data plotted on the X
Y coordinates and obviously you can do
that more in detail with different
columns and see how they plot together
and then we took and did the Seaborn
heat map the SNS do heat map of the data
and you can see right here where it did
a nice job showing us some bright spots
where stuff correlates with each other
inform forms a very nice combination or
points of scattering points and you can
also see areas that
don't and then finally we went ahead and
checked the data is the data null value
do we have any missing data in there
very important step because it'll crash
later on if you forget to do this step
it will remind you when you get that
nice error code that says null values
okay so not a big deal if you miss it
but it it's no fun having to go back
when you're you're in a huge process and
you've missed this step and now you're
10 steps later and you got to go
remember where you were pulling the data
in so we need to go ahead and pull out
our X and our y so we just put that down
here and we'll set the x equal to and
there's a lot of different options here
certainly we could do x equals all the
columns except for the first two because
if you remember the first two is the ID
and the diagnosis so that certainly
would be an option but what we're going
to do is we're actually going to focus
on the worst the worst radius the worst
texture parameter area smoothness
compactness and so on one of the reasons
to start dividing your data up when
you're looking at this information is
sometimes the data will be the same data
coming in so if I have two measurements
coming into my model it might overweigh
them it might overpower the other
measurements because it's measure it's
basically taking that information in
twice that's a little bit past the scope
of this tutorial I want you to take away
from this though is that we are dividing
the data up into pieces and our team in
the back went ahead and said hey let's
just look at the worst so I'm going to
create a an array and you'll see this
array radius worst texture worst
perimeter worst we've just taken the
worst of the worst and I'm just going to
put that in my X so this x is still a
pandas data frame but it's just those
columns and our y if you remember
correctly is going to be oops hold on
one second it's not X it's data there we
go so x equal data and then it's a list
of the different columns the worst of
the worst and if we're going to take
that then we have to have our answer for
our Y for the stuff we know and if you
remember correctly we're just going to
be looking
at the diagnosis that's all we care
about is what is it diagnosed is it Bine
or malignant and since it's a single
column we can just do diagnosis oh I
forgot to put the brackets the there we
go okay so it's just diagnosis on there
and we can also real quickly do like x.
head if you want to see what that looks
like and Y do head and run this and
you'll see um it only does the last one
I forgot about that if you don't do
print you can see that the the Y do head
is just Mmm because the first ones are
all malignant and if I run this the x.
head is just the first five values of
radius worst texture worst parameter
worst area worst and so
on I'll go ahead and take that out so
moving down down to the next step we've
built our two data sets our answer and
then the features we want to look
at in data science it's very important
to test your model so we do that by
splitting the
data and from sklearn model selection
we're going to import train test split
so we're going to split it into two
groups there are so many ways to do this
I noticed in one of the more modern ways
they actually split it into three groups
and then you model each group and test
it against the other groups so you have
all kinds and there's reasons for that
which is past the scope of this and for
this particular example isn't necessary
for this we're just going to split it
into two groups one to train our data
and one to test our data and the sklearn
uh. model selection we have train test
split you could write your own quick
code to do this we just randomly divide
the data up into two groups but they do
it for us nicely
and we actually can almost we can
actually do it in one statement with
this where we're going to generate four
variables capital x train capital X test
so we have our training data we're going
to use to fit the model and then we need
something to test it and then we have
our y train so we're going to train the
answer and then we have our test so this
is the stuff we want to see how good it
did on our model and we'll go ahead and
take our train test split that we just
imported and we're going to do X and our
y our two different data that's going in
for our split and then the guys in the
back came up and wanted us to go ahead
and use a test size equals 3 that's
testore size random State it's always
nice to kind of switch a random State
around but not that important what this
means is that the test size is we're
going to take 30% of the data and we're
going to put that into our test
variables our y test and our X test and
we're going to do 70% into the X train
and the Y train so we're going to use
70% of the data to train our model and
30% to test it let's go ahead and run
that and load those up so now we have
all our stuff split up and all our data
ready to go now we get to the actual
Logistics part we're actually going to
do our create our model so let's go
ahead and bring that in from sklearn
we're going to bring in our linear model
and we're going to import logistic
regression that's the actual model we're
using and this's we'll call it log
model there model and let's just set
this equal to our logistic regression
that we just imported so now we have a
variable log model set to that class for
us to use and with most the uh models in
the SK learn we just need to go ahead
and fix it fit do a fit on there and we
use our X train that we separated out
with our y train and let's go ahead and
run this so once we've run this we'll
have a model that fits this data that
70% of our training data
uh and of course it prints this out that
tells us all the different variables
that you can set on there there's a lot
of different choices you can make but
for word do we're just going to let all
the defaults sit we don't really need to
mess with those on this particular
example and there's nothing in here that
really stands out as super important
until you start fine-tuning it but for
what we're doing the basics will work
just fine and then let's we need to go
ahead and test out our model is it
working so let's create a variable y
predict and this is going to be be equal
to our log model and we want to do a
predict again very standard uh format
for the SK learn library is taking your
model and doing a predict on it and
we're going to test y predict against
the Y test so we want to know what the
model thinks it's going to be that's
what our y predict is and with that we
want the capital XX test so we have our
train set and our test set and now we're
going to do our y predict and let's go
ahead and run that
and if we uh
print y predict let me go ahead and run
that you'll see it comes up and it PR a
prints a nice array of uh B and M for B9
and
malignant for all the different test
data we put in there so it does pretty
good we're not sure exactly how good it
does but we can see that it actually
works and was functional was very easy
to create you'll always discover with
our data science that as you explore
this you spend a significant amount of
time prepping your data and making sure
your data coming in is good uh there's a
saying good data in good answers out bad
data in bad answers out that's only half
the thing that's only half of it
selecting your models becomes the next
part as far as how good your models are
and then of course find tuning it
depending on what model you're using so
we come in here we want to know how good
this came out so we have our our y
predict here log model. predict X
test so for deciding how good our model
is we're going to go from the sklearn
metrics we're going to import
classification report and that just
reports how good our model is doing and
then we're going to feed it the uh model
data and let's just print this out and
we'll take our classification
report and we're going to put into
there our test our actual data so this
is what we actually know is true and our
prediction what our model predicted for
that data on the test side and let's run
that and see what that
does so we pull that up you'll see that
we have um a Precision for B9 and
malignant
B&M and we have a Precision of 93 and 91
a total of 92 so it's kind of the
average between these two of 92 there's
all kinds of different information here
your F1
score your recall your support coming
through on this and for this I'll go
ahead and just flip back to our slides
that they put together for describing it
and so here we're going to look at the
Precision using the classification
report and you'll see this is the same
print out I had up above some of the
numbers might be different because it
does randomly pick out which data we're
using so this model is able to predict
the type of tumor with
91% accuracy so we look back here that's
you will see where we have uh B9 Inland
it actually is 92 coming up here we're
looking about a 92 91% precision and
remember I reminded you about domain so
we're talking about the domain of a
medical domain with a very catastrophic
outcome you know at 91 or 92% Precision
you're still going to go in there and
have somebody do a biopsy on it very
different than if you're investing money
and there's a 92% chance you're going to
earn 10% and 8% chance you're going to
lose 8% you're probably going to bet the
money because at that odds it's pretty
good that you'll make some money and in
the long run you do that enough you
definitely will make money and also with
this domain I've actually seen them use
this to identify different forms of
cancer that's one of the things that
they're starting to use these models for
because then it helps the doctor know
what to investigate so that wraps up
this section we're finally we're going
to go in there and let's discuss the
anwers to the quiz asked in machine
learning tutorial part one can you tell
what's happening in the following cases
grouping documents into different
categories based on the topic and
content of each document this is an
example of clustering where K means
clustering can be used to group the
documents by topics using bag of words
approach so if you've gotten in there
that you're looking for clustering and
hopefully you had at least one or two
examples like K means that are used for
clustering different things then give
yourself a two thumbs up B identifying
handwritten digits and images correctly
this is an example of classification the
traditional approach to solving this
would be to extract digit dependent
features like curvature of different
digits Etc and then use a classifier
like svm to distinguish between images
again if you got the fact that it's a
classification example give yourself a
thumb up and if you're able to go hey
let's use svm or another model for this
give yourself those two thumbs up on it
C behavior of a website indicating that
the site is not working as design
designed this is an example of anomaly
detection in this case the algorithm
learns what is normal and what is not
normal usually by observing the logs of
the website give yourself a thumbs up if
you got that one and just for a bonus
can you think of another example of
anomaly detection one of the ones I use
for my own business is detecting
anomalies in stock markets stock markets
are very ficked and they behave very
radical so finding those erratic areas
and then finding ways to track down why
they're erratic was something released
in social media was something released
you can see where knowing where that
anomaly is can help you to figure out
what the answer is to it in another area
D predicting salary of an individual
based on his or her years of experience
this is an example of regression this
problem can be mathematically defined as
a function between independent years of
experience and dependent variables
salary of an individual and if you guess
that this was a regression model give
yourself a thumbs up and if you're able
to remember that it is between
independent and dependent variables and
that terms give yourself two thumbs up
we're going to cover mathematics for
machine learning so today's agenda is
going to cover data and its types then
we're going to dive into linear algebra
and its Concepts calculus statistics for
machine learning probability for machine
learning Hands-On demos and of course
throwing in there in the middle is going
to be your matrixes and a few other
things to go along with all this
data and its types data denotes the
individual pieces of factual information
collected from various sources it is
stored process and later used for
analysis and so we see here uh just a
huge grouping of information a lot of
text stuff money dollar signs
numbers uh and then you have your
performing analytics to drive insights
and hopefully you have a nice share your
shareholders gather it at the meeting
and you're able to explain it in
something they can understand so we talk
about data types of data we have in our
types of data we have a qualitative
categorical you think nominal or ordinal
and then you have your quantitative or
numerical which is discrete or
continuous and let's look a little
closer at those data type vocabulary
always people's favorite is the
vocabulary words okay not mine uh but
let's dive into this what we mean by
nominal nominal they are used to label
various uh label our variables without
providing any measurable value uh
country gender race hair color Etc it's
something that you either mark true or
false this is a label it's on or off
either they have a red hat on or they do
not uh so a lot of times when you're
thinking nominal data labels uh think of
it as a true false kind of setup and we
look at ordinal this is categorical data
with a set order or a scale to it uh and
you can think of salary range is a great
one uh movie ratings Etc you see here
the salary R if you have 10,000 to
20,000 number of employees earning that
rate is 150 20,000 to 30,000 100 and so
forth some of the terms you'll hear is
bucket uh this is where you have 10
different buckets and you want to
separate it into something that makes
sense into those 10 buckets and so we
start talking about ordinal a lot of
times when you get down to the Brass
bones again we're talking true false uh
so if you're a member of the 10 to 20K
range uh so forth those would each be
either part of that group or you're not
but now we're talking about buckets and
we want to count how many people are in
that bucket quantitative numerical data
uh falls into two classes discrete or
continuous and so data with a final set
of values which can be categorized class
strength questions answered correctly
and runs hit and Cricket a lot of times
when you see this you can think integer
uh and a very restricted integer I.E you
can only have 100 questions um on a test
so you can it's very discreet I only
have a 100 different values that it can
attain so think usually you're talking
about integers but within a very small
range they don't have an open end or
anything like that uh so discret is very
solid simple to count set number
continuous on the other hand uh
continuous data can take any numerical
value within a range so water pressure
weight of a person Etc usually we start
thinking about float values where they
can get phenomenally small in their in
what they're worth and there's a whole
series of values that falls right
between discret and
continuous um you can think of the stock
market you have dollar amounts it's
still discreet but it starts to get
complicated enough when you have like
you know jump in the stock market from
$525 33 to
$580 seven cense there's a lot of Point
values in there it' still be called
discret but you start looking at it as
almost continuous because it does have
such a variance in it now uh we talk
about no we did we went over nominal and
ordinal uh almost true false charts and
we looked at quantitative and numerical
data which we start to get into numbers
discret you can usually a lot of times
discret will be put into it could be put
into true false but usually it's not uh
so we want to address this stuff and the
first thing we want to look at is the
very basic which is your algebra so
we're going to take a look at linear
algebra you can remember back when your
ukian geometry uh we have a line well
let's go through this we have a linear
algebra is the domain of mathematics
concerning linear equations and their
representations in Vector spaces and
through matrixes I told you we're going
to talk about
matrixes uh so a linear equation is
simply um uh 2x + 4 y - 3 Z = 10 very
linear 10 x + 12.4 y = z and now you can
actually solve these two equations by
combining them uh and that's where we're
talking about a linear
equation in the vectors we have a plus
Bal C now we're starting to look at a
direction and these values usually think
of an XY zplot um so each one is a
direction and the actual distance of
like a triangle AB is C and then you're
Matrix can describe all kinds of things
um I find matrixes uh confuse a lot of
people not because they're particularly
difficult but because of the magnitude
and the different things they're used
for and a matrix is uh a chart or a um
you know think of a spreadsheet but you
have your rows and your columns and
you'll see here we have a * b equals c
very important to know your counts uh so
depending on how the math is being done
what you're using it for making sure you
have the same rows and number of columns
or a single number there's all kinds of
things that play in that that can make
matrixes confusing uh but really it has
a lot more to do with what domain you're
working in uh are you adding in multiple
polom where you have like uh uh ax^2
plus b y plus you know you start to see
that it can be very confusing versus a
very straightforward Matrix and let's
just go a little deeper into these
because these are such primary this is
what we're here to talk about is these
different math uh mathematical
computations that come up so we're
looking at linear equations let's dig
deeper into that one an equation having
a maximum order of one is called a
linear
equation uh so it's linear because when
you look at this we have uh ax plus Bal
C which is a one variable we have uh two
variable ax plus b y = c ax plus b y + z
c zal d and so forth but all of these
are to the power of one you don't see X
squ you don't see X cubed so we're
talking about linear equations that's
what we're talking about in their
addition if you have already dived into
say neural networks you should recognize
this ax plus b plus CZ um setup plus The
Intercept uh which is basically your
your neural network each node adding up
all the different inputs and we can
drill down into that most common formula
is your y = mx +
C so you have your uh y equals the M
which is your slope your X Value Plus C
which is your um Y intercept they kind
of labeled it wrong here uh threw me for
a loop but the the C would be your Y
intercept so when you set x equal to 0 y
equal C and that's that's your Y
intercept right there uh and that's they
they just had reversed value of y when x
equals 0 equals the Y intercept which is
C and your slope gradient line which is
your M so you get your y = 2x + 3 and
there's lots of easy ways to compute
this this why this is why we always
start with the most basic one when we're
solving one of these problems and then
of course the um one of the most
important takeaways is the slope
gradient of the line uh so the slope is
very important that M value uh in this
case we went ahead and solved this if
you have y = 2x + 3 you can see how it
has a nice line graph here on the
right so matrixes a matrix refers to a
rectangular representation of an array
of numbers arranged in columns and
rows so we're talking M rows by in
columns here A1 is denotes the element
of the first row in the First Column
similarly A2 and it's really pronounced
a11 in this particular setup so it's a
row one column one A2 is a uh Row one
column two uh first row and second
column and so
on and there's a lot of ways to denote
this I've seen these as like a capital
letter a smaller case a for the top row
or I mean you can see where they can go
all kinds of different directions as far
as the
value you just take a moment to realize
there needs to be some designation as
far as what row it's in and what column
it's
in and we have our uh basic operations
we have addition so when you think about
addition you have uh two matrixes of 2x
two and you just add each individual
number in that Matrix and then when you
get to the bottom you have uh in this
case the solution is 12 10 + 2 is 12 5 +
3 is 8 and so on and the same thing with
subtraction now again you're counting
matrixes you want to check your um
dimensions of the Matrix the shape
you'll see shape come up a lot in
programming so we're talking about
Dimensions we're talking about the shape
if the two shapes are equal this is what
happens when you add them together or
subtract them and we have multiplication
when you look at the multiplication you
end up with a very slightly different
setup going now if we look at our last
one we um uh we're like why this always
gets to me when we get to matrixes they
don't really say why you multiply
matrixes um you know my first thought is
1 * 2 4 * 3 but if you look at this we
we get 1 * 2 + 4 * 3 1 * 3 + 4 * 5 uh 6
* 2 + 3 * 3 6 * 3 + 3 * 5 if you're
looking at these matrixes uh think of
this more as an equation and so we have
uh if you remember when we back up here
for our multiple line equations let's
just go back up a couple slides where we
were looking at uh two variable so this
is a two variable equation ax + b y =
c um and this is a way to make it very
quick to solve these variables and
that's why you have the Matrix and
that's why you
do the multiplication the way they do
and this is the dotproduct of uh 1 * 2 +
4 *
3 1 * 3 + 4 *
5 uh 6 * 2 + 3 * 3 6 * 3 + 3 * 5 and it
gives us a nice little uh 1423 21 and 33
over here which then can be used and
reduced down to a sample um formula as
far as solving the variables as you have
enough inputs uh and then in Matrix
operations when you're dealing with a
lot of matrixes uh now keep in mind
multiplying matrixes is different than
finding the product of two matrixes okay
so we're talking about multiplication
we're talking about solving uh for
equations when you're finding the
product you are just finding 1 * 2 keep
that in mind cuz that does come up I've
had that come up a number of times where
I am altering data and I get confused as
to what I'm doing with it uh transpose
flipping the Matrix over its diagonal
comes up all the time where you have you
still have 12 but instead of it being uh
128 it's now 124 821 you're just
flipping the columns and the rows uh and
then of course you can do an inverse um
changing the signs of the values across
this main diagonal and you can see here
we have the inverse a theus1 and ends up
with uh instead of 12 8 14 12 it's now -
22
-2 vectors uh Vector just means we
have a value and a direction and we have
down four numbers here on our
Vector uh in mathematics a
one-dimensional matrix is called a
vector uh so if you have your X plot and
you have a single value that value is
along the x- axis and it's a single
dimension if you have two Dimensions you
can think about putting them on a graph
you might have X and you might have y
and each value denotes a direction and
then of course the actual distance is
going to be the hypothesis of that
triangle uh and you can do that with
three dimensionals x y and z uh and you
can do it all the way to nth Dimensions
so when they talk about the K means uh
for categorizing and how close data is
together they will compute that based on
the Pythagorean theorem so you would
take
uh the square of each value add them all
together and find the square root and
that gives you a distance as far as
where that point is where that Vector
exists or an actual point value and then
you can compare that point value to
another one and it makes a very easy
comparison versus comparing uh 50 or 60
different numbers and that brings us up
to ige vectors and ige values uh igene
vectors the vectors that don't change
their span while transformation
and IG values the scalar values that are
associated to the vectors conceptually
you can think of the vector as your
picture you have a picture it's um uh
two Dimensions X and Y and so when you
do those two dimensions and those two
values or whatever that value is um that
is that point but the values change when
you skew it and so if we take and we
have a vector a and that's a set value
uh B is um your is your you have a and b
which is your IG Vector two is the igene
value so we're altering all the values
by two that means we're um maybe we're
stretching it out One Direction making
it tall uh if you're doing picture
editing um that that's one of the places
this comes in but you can see when
you're
transforming uh your different
information how you transform it is then
your IG value and you can see here
Vector after line trans transition uh we
have 3A a is the igene vector 3 is the
igene value so a doesn't change that's
whatever we started with that's your
original picture and three uh is skewing
it One Direction and maybe uh B is being
skewed another Direction and so you have
a nice tilted picture because you've
altered it by those by the igene
values so let's go ahead and pull up a
demo on linear algebra and to do this
I'm going to go through my trusted
Anaconda into my Jupiter notebook and
we'll create a new uh notebook called
linear algebra since we are working in
Python uh we're going to use our numpy I
always import that as NP or numpy array
probably the most popular um module for
doing matrixes and things
in given that this is part of a series
I'm not going to go too much into numpy
uh we are going to go and create two
different variables a for a numpy array
1015 and b
29 we'll go ahead and run this and you
can see there's our two arrays 105 29
and I went and added a space there in
between so it's easier to read and since
it's the last line we don't have to put
the print statement on it unless you
want we can simp but we can simply do a
plus b so when I run this uh we have 105
29 and we get 30 24 which is what you
expect 10 + 20 15 + 9 you could almost
look at this addition as being
um just adding up the columns on here
coming down and if we wanted to do it a
different way we could also do
A.T plus B.T remember that t flips them
and so if we do that we now get them uh
we now have 324 going the other way we
could also do something kind of fun
there's a lot of different ways to do do
this uh as far as a plus b I can also do
A+
B.T and you're going to see that that
will come out the same the 3024 whether
I transpose A and B or transpose them
both at the
end and likewise we can very easily
subtract two vectors I can go a minus B
and we run that and we get min - 106 now
remember this is the last line in this
particular section that's why I don't
have to put the print around it
um and just like we did before we can
transpose either the individual or we
can transpose the main setup and then we
get a minus 106 going the other
way now we didn't mention this in our
notes but you can also do a scalar
multiplication let me just put down the
scaler so you can remember that uh what
we're talking about here is I have uh
this array here U and if I go a *
U uh we'll take the value two we'll
multiply it by every value in here so 2
* 30 is 60 2 *
15 and just like we did
before um this happens a lot because
when you're doing matrixes you do need
to flip them you get 6030 coming this
way so in numpy uh we have what they
call Dot
product and with this this is in a
twodimensional vectors it is the
equivalent of to matrix
multiplication remember we were talking
about matrix
multiplication uh where it is the well
let's walk through
it we'll go ahead and start by defining
two um numpy arrays we'll have uh 10 20
25 6 or our U and our V uh and then
we're going to go ahead and do if we
take the values uh and if you remember
correctly an array like this would be 10
* 25 + 20 * 6 we'll go ahead and uh
print
that there we
go and then we'll go ahead and do the
np. dot of VI
comma
V and we'll find when we do this we go
and run this uh we're going to get uh
370
370 so this is a strain multiplication
where they use it to
solve uh linear algebra uh when you have
multiple numbers going across and so
this could be very complicated we could
have a whole string of different
variables going in here but for this we
get a nice uh value for our Dot
multiplication and we did um addition
earlier which was just your basic
addition uh and of course the Matrix you
can get very complicated on these or in
this case we'll go ahead and do um let's
create two complex
matrixes this one is a matrix of um you
know 1210 46
431 we'll just print out a so you can
see what that looks like here's print
a we print a out you can see that we
have a um
2x3 layer Matrix for a and we can also
put together always kind of fun when
you're playing with print values uh we
could do something like this we could go
in here there we go uh we could print a
we have it end with uh equals a run and
this kind of gives it a nice look uh
here's your Matrix that's all this is
comma and means it just tags it on the
end that's all all that is doing on
there and then we can simply add in what
is a plus b and you should already guess
because this is the same as what we did
before there's no difference uh we do a
simple vector addition we have 12 + 2 is
14 10 + 8 is 18 and so on and just like
we did the uh Matrix addition we can
also do a minus B and do our Matrix
subtraction and we look at this uh we
have what 12 minus 2 is 10 10 minus 8 um
where are
we oh there we go 8 H confusing what I'm
looking at I should have reprinted out
the original numbers uh but we can see
here 12 - 2 is of course 10 10 - 8 is 2
uh 4 - 46 is - 42 and so forth so same
as a subtraction as before we just call
it Matrix subtraction it's
identical now if you remember up here we
had a scalar addition where we're adding
just one number to a matrix you can also
do scalar
multiplication uh and so simply if you
have a single value
a and you have B which is your array we
can also do a * B when we run that uh
you can see here we have 2 * 4 is 8 uh 5
* 4 is 20 and so forth you're just
multiplying the four across each one of
these values and this is an interesting
one that comes up a little bit of a
brain teaser is uh Matrix and Vector
multiplication and so we're looking at
this uh we are
just do regular arrays it doesn't
necessarily have to be a numpy array we
have a which has our um array of arrays
and B which is a single array and so we
can from
here do the
dot
ab and this is going to return two
values and the first value is that it's
you could say it's like uh um we're
doing the this array B array first with
a and then with a second one and so it
splits it up so you have a matrix of
vector multiplication and you can mix
and match when you get into really
complicated uh backend stuff this
becomes more common because you're now
you got layers upon layers of data and
so you you'll end up with a matrix and a
set of uh Vector matrices do you want to
multiply now keep in mind that if you're
doing data science a lot of times you're
not looking at this this is what's going
on behind the scenes so if you're in uh
the S kit looking at sklearn where
you're doing linear regression models
this is some of the math that's hidden
behind the scenes that's going on other
times you might find yourself having to
do part of this and manipulate the data
around so it fits right and then you go
back in and you run it through the S kit
and if we can do um up here where we did
a uh Matrix and Vector multiplication we
can also do Matrix to matrix
multiplication and if we run this where
we have the two matrixes uh you can see
we have a very complicated array that of
course comes out on there for our DOT
and just to reiterate it we have our
transpose of Matrix which is your T and
so if we create a matrix a and we do a
transpose it you can see how it flips it
from 5 10 15 20 25 30 2 5 15 25 10 20 30
uh rows and
columns and certainly with the math uh
this comes up a lot um it also comes up
a lot with XY plotting when you put into
Pi plot you have one format where
they're looking at Pairs and numbers and
then they want all of x's and all y's so
you know the transpose is an important
tool both for your math and for plotting
and all kinds of things another tool
that we didn't discuss uh is your
identity
Matrix uh and this one is more
definition uh the identity Matrix um we
have here one where we just did uh two
so it comes down as one 0 1 uh 1 0 0 1 0
it creates a diagonal of one and what
that is is when you're doing your
identities you can be comparing all your
different features to the different
features and how they correlate and of
course when you have uh feature one
compared to feature one to itself it is
always one uh where usually it's between
zero one depending on how well
correlates so when we're talking about
identity Matrix that's what we're
talking about right here is that you
create this preset Matrix and then you
might adjust these numbers depending on
what you're working with and what the
domain is and then another thing we can
do uh to kind of wrap this up we'll hit
you with the most complicated uh um
piece of this puzzle here is an inverse
um a matrix and let's just go ahead and
put the um it's a lengthy
description let's go and put the
description this is straight out of the
uh
the website for numpy uh so given a
square Matrix a here's our Square Matrix
a which is 2 1 0 0 1
0121 keep in mind 3x3 it's Square it's
got to be equal it's going to return the
Matrix a inverse satisfying a um a
inverse so here's our matrix
multiplication um and then of course it
equals the dot uh yeah a inverse of a um
with an identity shape of uh a do shaped
zero this is just reshaping the
identity that's a little complicated
there uh so we go and have our here's
our array uh we'll go and run this and
you can see what we end up with is we
end up with uh an array 0.5 minus5 and
so farth with our 211 going down 2 1 0
01
0121 um getting into a little deep on
the map
understanding when you need this is
probably really is is what's really
important when you're doing data science
versus uh handwriting this out and
looking up the math and handwriting all
the pieces out you do need to know about
the linear algorithm inverse of a uh so
if it comes up you can easily pull it up
or at least remember where to look it up
you took a look at the algebra side of
it let's go ahead and take a look at the
calculus side of uh what's going on here
with the machine learning so calculus oh
my goodness differential equations you
got to throw that in there CU that's all
part of the bag of tricks especially
when you're doing large neural networks
but it also comes up in many other areas
the good news is most of it's already
done for you in the back end uh so when
it comes up you really do need to
understand from the data science not
data analytics data analytics means
you're digging deep into actually
solving these math equations u and a
neural network is just a giant
differential
equation uh so we talk about calculus uh
we're going to go ahead and understand
it by talking about cars versus time and
speed uh so helps to calculate the
spontaneous rate of
change uh so suppose we plot a graph of
the speed of a car with respect to time
so as you can see here going down the
highway probably merged into the highway
from an on-ramp so I had to accelerate
my speed went way up uh stuck in traffic
merged into the traffic traffic opens up
and I accelerate again up to the speed
limit and uh maybe a Peters off up there
so you can look at this as as um the
speed versus time I'm getting faster and
faster because I'm continually
accelerating and if I hit the brakes it'
go the other way so the rate of change
of speed with respect of time is nothing
but acceleration how fast are we
accelerating the acceleration is the
area between the start point of X and
the end point of Delta x uh so we can
calculate a simple if you had X and
Delta X we could put a line there and
that slope of the line is our
acceleration now that's pretty easy when
you're doing linear algebra but I don't
want to know it just for that line in
those two points I want to know it
across the whole of what I'm working
with that's where we get into calculus
so when we talk about the distance
between X and Delta X it has to be the
smallest possible near to zero in order
to approximate the acceleration
uh so the idea is that instead of I mean
if you ever did took a basic calculus
class they would draw bars down here and
you would divide this area up um let's
go back up a screen you divide this area
of this time period up into maybe 10
sections and you'd use that and you
could calculate the acceleration between
each one of those 10 sections kind of
thing uh and then we just keep making
that space smaller and smaller until
Delta X is almost uh infinitism small
and so we get a function of a uh equals
a limit as H goes to zero of a function
of a plus h minus a function of a over H
and that is you're Computing the slope
of the
line we're just Computing that slope
under smaller and smaller and smaller
samples uh and that's what calculus is
calculus is the integral you can see
down here we have our nice uh integral
sign looks like a giant s and that's
what that means is that we taken this
down to as small as we can for that
sampling uh so we're talking about
calculus we're finding the area under
the slope is the main process in the
integration similar small intervals are
made of the smallest possible length of
X Plus Delta X where Delta X approaches
almost an infinitism small space and
then it helps to find the overall
acceleration by summing up all the
lengths together uh so we're summing up
all the accelerations from the beginning
to theend end and so here's our integral
we sum of a ofx * D ofx = A + C uh that
is our basic calculus here so when we
talk about multivariate calculus uh
multivariate calculus deals with
functions that have multiple variables
and you can see here we start getting
into some very complicated equations um
uh changeing W over change of time
equals change of w over change of Z the
differential of Z to DX differential of
x to DT it gets pretty complicated uh
and it really translates into the
multivariate integration using double
integrals and so you have the the sum of
the sum of f ofx of Y of D of a equals
the sum from C to D and A to B of f ofx
y dxdy equals uh the sum of a to B sum
of C to D of fxy Dy
DX understanding the very specifics of
everything going on in here and actually
doing the math is use the calculus one
calculus 2 and differential equations uh
so you're talking about three F length
courses to dig into and solve these math
equations what we want to take from here
is we're talking about calculus uh we're
talking about summing of all these
different slopes and so we're still
solving a linear uh expression we're
still solving y = mx + b but we're doing
this for infinitism small x's and then
we want to sum them up that's what this
integral sign means the the sum of a of
x d ofx equal a plus
c and when you see these very
complicated uh multivariate
differentiation using the chain rule uh
when we come in here and we have the
change of w to the change of T equals
the change of w DZ uh and so forth
that's what's going on here that's what
these means we're basically looking for
the area under the curve which really
comes to how is the change changing
speed's going up how is that changing
and then you end up with a multiple
layer so if I have three layers of
neural networks how is the third layer
changing based on the second layer
changing which is based on the first
layer changing and you get the picture
here that now we have a very complicated
uh multivariate integration um with
integrals the good news is we can solve
this uh mathematically and that's what
we do when you do neural networks in
rever
propagation uh so the nice thing is that
you don't have to solve this on paper
unless you're a data analysis and you're
working on the back end of integrating
these formulas and building the script
to actually build them so we talk about
applications of calculus uh it provides
us the tools to build an accurate
predictive model um so it's really
behind the scenes we want to guess at
what the change of the change of the
change
is that's a little goofy I I know I just
threw that out there it's kind of a
metat term but if you can guess how
things going to change then you can
guess what the new numbers are
multivariate calculus explains the
change in our Target variable in
relation to the rate of change in the
input variables so there's are multiple
variables going in there if uh one
variable is changing how does it affect
the other
variable and then in gradient descent
calculus is used to find the local and
Global Maxima and this is really big uh
we're going actually going to have a
whole section here on gradient descent
because it is really I mean I talked
about neural networks and how you can
see how the different layers go in there
but gradient descent is one of the most
key things for trying to guess the best
answer to something so let's take a look
at the code behind gradient descent and
uh before we open up the code let's just
do real quick uh gradient
descent let's say we have a curb like
this and most common is that this is
going to represent your error
oops error there we go error uh hard to
read there and I want to make the error
as low as possible and so what I'm
looking at it is I want to find this
line here which is the minimum value so
we're looking for the minimum and it
does that by uh sampling there and and
then it based on this it guesses it
might be someplace here and it goes hey
this is still going down it goes here
and then goes back over here and then
goes a little bit closer and it's just
playing a high low until it gets to that
spot that bottom spot and so we want to
minimize the error and uh on the flip
note you could also want to be
maximizing something you want to get the
best output of it uh that's simply uh
minus the value uh so if you're looking
for for where the peak is this is the
same as a negative for where the valley
is I'm looking for that Valley uh that's
all that is and this is a way of finding
it so the cool thing is um all the heavy
lifting is done um I actually ended up
putting together one of these a while
back is when I didn't know about
sidekick and I was just starting boy
it's a long while back and uh is playing
high low how do you play high low not
get stuck in the valleys figure out
these curves and things like that well
you do that in the back end is all the
calculus and differential equations to
calculate this out the good news is you
don't have to do
those uh so instead we're going to put
together the code and let's go
ahead and see what we can do with
that so uh guys in the back put together
a nice little piece of code here which
is kind of fun uh some things we're
going to note and this is this is really
important stuff because when you start
doing your data science and digging into
your machine learning models uh you're
going to find these things are stumbling
blocks uh the first one is current X
where do we start at uh keep in mind
your model that you're working with is
very generic so whatever you use to
minimize it the first question is where
do we start um and we started at this
because the algorithm starts at x equals
3 so we arbitrarily picked five learning
rate is uh how many bars to skip going
one way or the other I'm in fact I'm
going to separate that a little bit
because these two are really important
um if we're dealing with something like
this where we're talking about um well
here's our here's the function we're
going to use our U gradient of our
function um 2 * x + 5 keep it simple so
that's a function we're going to work
with so if I'm dealing with increments
of a th000 0.1 is going to be a very
long time and if I'm dealing with
increments of
0.001 uh 0.1 is going to skip over my
answer so I won't get a very good answer
um and then we look at Precision this
tells us when to stop the algorithm so
again very specific to what you're
working on uh if you're working with
money and you don't convert it into a
float value uh you might be dealing with
0.001 which is a penny that might be
your Precision you're working with um
and then of course the previous step
size Max iterations uh we want something
to cut out at a certain point usually
that's built into a lot of minimization
functions and then here's our actual uh
formula we're going to be working with
and then we come in we go while previous
step size is greater than precision and
it is less than Max Max
it say that 10 times fast um we're just
saying if it's uh if we're if we're
still greater than our Precision level
we still got to keep digging deeper um
and then we also don't want to go past a
thou or whatever this is a million or
10,000 uh running that's actually pretty
high um we almost never do Max
iterations more than like a 100 or 200
rare occasions you might go up to four
500 if it's depending on the problem
you're working with uh so we have our
previous equals our current that way we
can track
TimeWise uh the current now equals the
current minus the rate times the formula
of our previous text so now we've
generated our new version uh previous
step size equals the absolute current
previous uh so we're looking for the
change in X it equals iterations plus
one that's so we know to stop if we get
too far and then we're just going to
print the local minimum occurs at X on
here and if we go ahead and run
this uh you can see right here it gets
down to this point and it says hey um
local minimum is -
33222 for this particular series we
created uh and this is created off of
our formula here Lambda X2 * x + 5 now
when I'm running this stuff uh you'll
see this come up a
lot in uh with the SK learn kit and and
one of the nice reasons of breaking this
down the way we did is I could go over
those top pieces uh those top pieces are
everything when you start looking at
these minimization tool kits in built-in
code and so from um we'll just do it's
actually
docs. cci.org and we're looking at the
scikit there we go um optimize
minimize you can only minimize one value
you have the function that's going in
this function can be very complicated uh
so we used a very simple function up
here it could be yeah there's all kinds
of things that could be on there and
there's a number of methods to solve
this as far as how they shrink down uh
and your X notot there's your there's
your start value so your function your
start value um there's all kinds of
things that come in here that we can
look at which we're not going to um
optimization automatically creates
constraints bounds some of this it does
automatically but you really the big
thing I want to point out here is you
need to have a starting point you want
to start with something that you already
know is mostly the answer uh if you
don't then it's going to have a heck of
a time trying to calculate it
out or you can write your own little
script that does this and and does a
high low guessing and tries to find the
max value that brings us to statistics
what this is kind of all about is
figuring things out lot of vocabulary
and statistics uh so statistics well I
guess it's all relative it's definitely
not an edel class uh so a bunch of stuff
going on statistics statistics concerns
with The Collection organization
analysis interpretation and presentation
of data that is a mouthful um so we have
from end to end where where does it come
from is it valid what does it mean how
do we organize it um how do we analyze
it then you got to take those analysis
and interpret it into something that uh
people can use kind of reduce it to
understandable um and nowadays you have
to be able to present it if you can't
present it then no one else is going to
understand what the heck you
did so when we look at the terminologies
uh there's a lot of terminologies
depending on what domain you're working
in so clearly if you're working in um a
domain that deals
with viruses and te- cells and and how
does you know where does it come from
and you're studying the different people
then you're going have a population if
you are working with um mechanical gear
um you know a little bit different if
you're looking for the wobbling
statistics uh to know when to replace a
rotor on a machine or something like
that uh that could be a big deal you
know we have these huge fans that turn
in our sewage processing systems and so
those fans they start to wobble and hum
and do different things that the sensors
pick up at one point do you replace them
instead of waiting for it to break in
which case it costs a lot of money
instead of replacing a bushing you're
replacing the whole fan unit uh an
interesting project that came up for our
city a while back uh so population all
objects are measurements whose
properties are being observed uh so
that's your population all the objects
it's easy to see it with people because
we have our population and large um but
in the case of the sewer fans we're
talking about how the fan units that's
the population of fans that we're
working
with you have a parameter a matric uh
that is used to represent a population
or
characteristic you have your sample a
subset of the population studied you
don't want to do them all because then
you don't have a if you come up with a
conclusion for everyone you don't have a
way of testing it so you take a sample
uh sometimes you don't have a choice you
can only take a sample of what's going
on you can't uh study the whole
population and a variable a metric of
interest for each person or object in a
population types of sampling we have
probabilistic approach uh selecting
samples from a larger population using a
method based on the theory of
probability and we'll go into a little
bit more deeper on these we have random
systematic stratified and then you have
nonprobabilistic approach selecting
samples based on the subjective Judgment
of the researcher rather than random
selection uh it has to do with
convenience trying to reach a quota um
or
snowball um and they're very biased
that's one of the reasons you'll see
this big stamp on it says biased uh so
you got to be very careful on that so
probabilistic sampling uh when we talk
about a random sampling we select random
siiz samples from each group or category
so we it's as random as you can get uh
we talk about systematic sampling we're
selecting random siiz samples from each
group or category with a fixed periodic
interval uh so we kind of split it up
this would be like a Time setup or our
different categories and you might ask
your question what is a category or
group uh if you look at I'm going to go
back of a window let's say we're
studying um economics of different of an
area um we know pretty much that based
on their culture where they came from
they might need to be separated and so
uh and when I say separated I don't mean
separated from their their uh place
where they live I mean as far as the
analysis we want to look at the
different groups and make sure they're
all represented so if we had like
80% uh of a group that is uh say
Hispanic and or Indian and also in that
same area we have 20 20% who are let's
call our exp Patriots they left America
and they're nice and uh your Caucasian
group we might want to sample a group
that is representative of both uh so
we're talking about stratified sampling
and we're talking about groups those are
the groups we're talking about and that
brings us to stratified sampling
selecting approximately equal size
samples from each group or
category uh this way we can actually
separate the categories and give us an
insight into the different cultures and
how that might affect them in that area
uh so you can see these are very very
different kind of depends on what you're
working with um as far as your data and
what you're studying and so we can see
here just to go a little bit more we'd
have selecting 25 employees from a
company of 250 employees randomly don't
care anything about them what groups are
in which officer in nothing uh and we
might be selecting one employee from
every 50 unique employees in a company
of 250 employees and then we have
selecting one employee from every branch
in the company office so we have all the
different branches there's our group or
our categories by the branch and the
category could depend on what you're
studying so it has a lot of variation on
there you see this kind of grouping and
categorizing is also used to generate a
lot of
misinformation uh so if you only study
one group and you say this what it
is so types of Statistics uh when we
talk about statistics we're going talk
about particular setup so we're
talking is pry solid you're describing
the data what does it look
like this
drug % better survival rate than the
people um do not have a drug so we can
infer that that drug will work in
going to affect the greater
population a measure of central
Tendencies we have your mean median in
mode and then we have a major of spread
range uh but one of you can think of is
um how the data difference differences
you know what's the max in range all
that stuff is and tendencies maor
Central Tendencies so we talk about the
mean the average of the Val Med the
higher half and the lower half of data
uh so where's the center point of all
your different data points so your mean
might have a couple really big numbers
that skew it so the average is much
higher might give you a much lower
number Medan because you have some
outliers why is it so much lower and
then the mode is the most frequent
appear value uh this is really
interesting you're economics and how
people um income like in the US was 1.
24,000 a year where the average was
close 80,000 and wow that up so the
average person is not making money at
the average so create a very interesting
way of looking at the data again these
are all uh central tendency single
numbers you can look at for the spread
of the dayes the mean is the average
marks of a students in a classroom so
here we have the mean some of the marks
of the students total number of students
and as we talked about the median have Z
10 half the numbers on the other side of
the line uh we end five in midd mode
test you know simple case where most
people scored like an 82% fure not so
easy when you have different areas where
like you have
economy and a slightly bigger group that
makes 26,000 so what do you put the
most
varations uh range what's the difference
between the highest and the lowest
value 100% or maybe 60 to could not get
100% rank ordered data set into four
equal parts very common do all the basic
pages in Scala whether you're working in
R um you'll see this come up r X and
then it'll have your inter quartile
range how does it look like in each
quarter of data variance measur how far
each number in the set is from the mean
and therefore from every other number uh
so you have like how much Tri is going
on in this
data values from the mean and you'll
usually see if I'm doing a graph I might
have the value graphed um then based on
the the error on the graph as a
background so you can see how far off it
is uh so standard deviations used a lot
so me spread uh marks of a student out
of 100 we here from to 63 or 50 to 90 so
the range maximum marks minimum marks we
have 90 to 45 and the spread of that is
45 90 minus 45 and then we have the
inter quartile range using the same
marks over there you can see here where
the median is
and find out the mean uh so here you
know calculating the average there we
end up at approximately 66 for the
average and then we look at that the
variance once know the means we can do
equal marks minus mean squared Why is it
squared uh because one you want to make
sure it's you don't have like if you if
you're putting all this stuff together
you end up with error as far as one
negative one's positive one's higher
lower uh so you always see the squared
value and over the total observations
and so the standard deviation equals the
square root of the variance which is
approximately
16 model You' be looking at the
deviation again really important to know
if you're if you're predicting being way
off a little bit off at the tools let's
go ahead and pull up a little demo
that's like pyth hands on here for that
go back our almost all of this you can
do in
numpy and if here this is basically a
printf.
head you see we have the name Jane
Michael William Rosie Hannah and their
salaries on here and of course instead
of having to do all
calculations use the command mean in
pandas and so if I go and do this print
DF check our column salary because we
want to find the means that Cy we want
to find the means of that column uh and
we go and print this out and you can see
that the average
in and let's just go ahead and do this
we'll go ahead and put in
means and if we're going to do that we
also might want to find the is very
similar actually Med we're used to means
and average interesting that those are
they use the two different words uh
there can be in some computation slight
differences but for the most part the
means is the average uh in the median
oops let's put a
median here DF salary that way it
displays a little better you see the
median is
54 why somebody
189,000 darn you Rosie for throwing off
our numbers uh but that's something you
want to notice this is this is the
difference between is huge me looking at
uh the different data coming in and of
course we also want to find out hey
what's the most uh common income that
people
make example into the mode you can see
50,000 so this is this is very telling
that most people are making 50,000 the
middle point is at 54,000 so half the
people are making more than that what
that tells me is that if the most common
income is way is below the median then
there's a few there's a you know there's
a lot of high salaries going up but
there's some really low salaries in
there and so this trend which is very
common in statistic when you're
analyzing the economy in different
people's income is pretty common and the
bigger difference between these is also
very important when we're studying
statistics uh and when you hear someone
just say hey the average income was you
might start asking questions at that
point why aren't you talking about the
median income why aren't you talking
about the mode the most common income
what are you hiding uh and if you're
doing these analysis you should be
looking at these saying hey why why are
this discrepancies why are these so
different and of course with any uh
analysis it's important to find out the
minimum
and the maximum so we'll go ahead it's
just simply uh
Min straight on as far as um you know
put the your lowest value what your
highest value is here later on and real
quick on no mode uh note that it puts
mode zero like I said there's a couple
different ways you can compute mode
range which is your max minus your Min
so now we have
a feature in pandas and P can this you
can see we have there our standard
deviation which we didn't compute
computes it it looks for axes and things
like that uh we have our minimum
value our maximum value and of course a
name salary uh these are these are the
basic statistics you can pull them up
and like just describe this is a
dictionary so I could actually do
something like um in here I could
actually go uh count and run and now it
just prints the count uh so because this
is a dictionary you can pull any one of
these values out of here it's kind of a
quick and dirty way to pull all the
different information and then split it
up depending on what you need now if I
just walked in and gave you this
information um in a meeting at some
point you would just kind of fall
asleep that's what I would do anyway um
so we want to go ahead and and see about
graphing it here we'll go ahead and put
it into a history gram and plot that
graph on it of the salaries and let's
just go ahead and put put that in here
so we do our map plot inline remember
that's a Jupiter's notebook thing uh a
lot of the new version of the matplot
library does it automatically but just
in case I always put it in there uh
import map plot Library pip plot is PLT
that's my
plotting and then we have our data frame
uh I don't I guess I really don't need
to respell the data frame maybe we could
just remind ourself what's in it so
we'll go ahead and just uh
print DF that way we still have it and
then we have our salary DF salary
salary. plot history title salary
distribution color
gray uh plot axv line salary the mean
value so we're going to take the mean
value um color violet line style Dash
this is just all making it pretty uh
what color dash line line width of two
that kind of thing and the median and
let's go ahead and run this just so you
can see what we're talking
about and so up here we
are see it with the salaries we're
looking at the salary distribution and
just look at case we did Let's see we
had red for the median we have
Violet for our
average here's our outlier here's our
person who makes a lot of money here's
the um average Med based on the average
it really doesn't tell you much about
what people are really taking home all
it does is tell you how you know
so it's very easy to plot um an axv line
these are these
marely do a histogram and pictur
thousand words do a basic describe which
pulls all this information from the
describe uh because this is a
dictionary and so if we want to go ahead
and look up um the mean value we also
describe being able
to doesn't have the print on there so
it's only going to print you very easily
reference any one of these and then you
can also you don't need just the basics
you can come through and pull any one of
the
individual references from the from the
pandas on here so now we've had a chance
to describe our data uh let's get into
inferential statistics inferential
statistics allows you to make
predictions or inferences from data and
you see here we have a nice little
picture movie ratings and and said hey
how many people like the movie dislike
it can't say and then you mie who hasn't
been in the study you caner that 55%
chance of saying liked 35% chance of
saying disliked so that's real basics of
what we're talking about is you're going
to infer that the next person is
follow uh so let's look at point value
for a population
parameter and flu bugs all that it's a
pretty big thing of how do you test
these out and make sure they're going
to are generalized for the whole
population so here's A's small here s
results work for the population nice
diagram with the arrows going back and
forth in the very scary of inferential
statistics very Central is what they
call hypothesis
testing uh and the confidence interval
which go with that and then as we get
into probability we get into our
binomial the our normal distribution in
central limit theorem hypothesis testing
hypothesis testing is used to measure
the plausibility of a hypothesis
assumption by using sample data now when
we talk about theorems Theory
hypothesis uh keep in mind that if you
are in a philosophy class theory is the
same as hypothesis where theorem is a
scientific uh statement that is
something that has been proven although
it is always up for debate because in
science we always want to make sure
things are up to debate so hypothesis is
the same as a Phil philosophical class
calling a theory where theory in science
is not the same theory in science says
this has been well proven gravity is a
theory uh so if you want to debate the
theory of gravity try jumping up and
down if you want to have a theory about
why the economy is collap collapsing in
your area that is a philosophical debate
very important I've heard people mix
those up and it is a pet peeve of mine
when we talk about hypothesis testing
the steps involved in hypothesis testing
is first we formulate a hypothesis we
figure out the right test to test our
hypothesis and we make a decision and so
when you're talking about hypothesis
you're usually trying to disprove it if
you can't disprove it and it works for
all the facts then you might call that a
theorem at some point so in a use case
uh let's consider an example we have
four students who are given a task to
clean a room every day sounds like
working with my kids they decided to
distribute the job of cleaning the room
among themselves they did so by making
four chits which has their names on it
and the name that gets picked up has to
do the cleaning for that day Rob took
the opportunity to make chits and wrote
everyone's name on it so here's our four
people Nick Rob Emilia imia and
summer now Rick emia and summer are
asking us to decide whether Rob has done
some Mischief in preparing the chits I.E
whether Rob has written his name on one
of the chit for that we will find out
the probability of Rob getting the
cleaning job on first day second day
third day and so on till 12 days the
probability of Rob getting the job
decreases every day I.E his turn never
comes up then definitely he has done
some Mischief while making the chits so
the probability of not doing work on day
one is uh three out of four there's A7
75 chance that he didn't do uh two days
34 * 34 =
.56 3 days you have 34 34 34s which
equals
042 uh when you get to day 12 it's 0.032
Which is less than
0.05 remember this 0.005 uh that comes
up a lot when we're talking about um
certain values when we're looking at
statistics Rob is cheating as he wasn't
chosen for 12 consecutive days that's a
very
phomen or no association among the
groups alternative
hypothesis whenever something is
happening hypothesis go hand in hand uh
so your null this is always interesting
in in we're talking about data science
and the math behind it it's about
proving that the things have no
correlation no hypothesis
z p value the P value is the probability
of finding the observed or stud question
is and units of standard error greater
the magnitude of T the greater the
evidence against the null hypothesis and
you look at the T to the test you're
doing where P
value 05 showing that it has a high
correlation so di in deeper
let's that you don't have any drug the
no value here than the existing drug now
if we get that uh that says our null hyp
direct there is no correlation and new
Dr the alternative hypothesis new
positive results which will reject the N
hypothesis POS test results and finding
means of different samples in order to
test they have and this leads us to the
confidence intervals vales of
observations how many cans of food do
you buy for your per on an average 95%
of the people bought around 200 hence we
can say that we have a confidence
interval of 2300 or 9 % of our values
lie in that a lot so you can start
seeing what you're looking at here where
you have the
95% curve equal on both sides it's not
asymmetrical then you have your outliers
the 2.5% going each way so we touched
upon hypothesis uh we're going to move
to probability uh so you have your
hypothesis once you've generated your
hypothesis we want to know the
probability of something occurring occur
any event can be predicted with total
certainty only lik of urr so any event
cannot be predicted with total certainty
be predicted as a likelihood of its
occurrence uh score prediction how good
you're going to do in whatever sport
you're in weathered physics and Chaos
Theory even the location of the chair
you're sitting on
has under one in trillions upon
trillions so probability it's going to
happen there are some things that have
such a low probability that we don't see
them so we talk about a random variable
uh random variable is a variable whose
possible values outcomes random
phenomena so uh we have the coin toss
how many heads will occur in the series
of 20 coin flips probably you know the
on average they're 10 but you really
can't know cuz it's very random how many
times a red ball is picked from a bag of
balls if there's equal number of of red
balls and blue balls and green balls in
there how many times do sum of digits on
two dice uh result or five each um so
you know there's how often you going to
roll two fives on your pair of dice so
in a used case uh let's consider the
example of rolling two dice we have a
random variable outcome equals y you can
take values 2 3 4 5 6 7 8 9 10 11 12 so
we have a random variable and a
combination of dice and instead of
looking at how many times um both dice
for rle five let's go ahead and look at
a total sum of five and you have in as
far as your random variables you can
have a 14 = 5 4 1 2 3
32 so four of those roles can be or if
you look at all the different options
you have four of those random rolls can
be a five and if we look at the total
number which happens to be 36 different
options uh you can see that we have four
out of 36 chance every time you roll the
dice that you're going to roll a total
of five you're going to have an outcome
of five and uh we'll look a little
deeper as to what that means uh but you
could think of that at what point if
someone never rolls a five or they
always roll a five can you you say hey
that person's probably cheating uh we'll
look a little closer at the math behind
that but let's just consider this is one
of the cases is rolling two dice and
gambling there's also a binomial
distribution it is the probability of
getting success or failure as an outcome
in an experiment or trial that is
repeated multiple times and the key is
is by meaning to binomial uh so passing
or failing an exam winning or losing a
game and getting either head or tails so
if you ever see by omal distribution is
based on a um true false kind of setup
you win or lose let's consider a use
case and let's consider the game of
football between two clubs Barcelona and
Dortmund the teams will have to play a
total of four matches and we have to
find out the chances of Barcelona
winning the
series
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e e
so we look at the total games and we're
looking at five different games or
matches let's say that the winning
chance for Barcelona is 75% or 75 that
means at each game they have a 75% %
chance that they're going to win that
game and losing chances are 25% or 0.25
clearly 75 plus 0.25 equals one so that
and you'll see this as B theem uh and
this is red I mean you have these funky
looking little p brackets a this is the
probability of a being true while B is
already
true probability of a being true divided
by the probability of B being true and
we talk about B theorem which occurred
back in the
an important formula and it's really
it's not if you actually do the math you
could just kind of do um um XY equals JK
and then you divide them out and you're
going to see the same math but it works
with prob uh eight or nine different
studies going on in done the studies
together um atus the virus spread
certainly the done in China us data is
different in each of those studies but
if you can find a place together you can
then compute the changes that you need
to make in one study to make them equal
and this um one group and you want to
find out more about it so this formula
is very powerful uh it really has to do
with the data collection part of the
math and data science and understanding
where your data is coming from and how
you're going to combine different
studies in different
groups and we'll go and go into a use
case uh let's find out the chance of a
person getting lung disease due to
smoking uh and this is kind of
interesting the way they word the um
let's say that according to medical
report provided by the hospital patients
they treated suffered long so we have
kind of a generic medical report they
further found out by a survey that 15%
of the patients visit them smoke so we
have 10% that are lung disease and um
15% of the patients smoke and finally 5%
of the people continued smoke even when
they had lung disease uh not the
brightest Choice um but you know it is
an addiction so it can be really
difficult to kick and so we can look at
the probability of a uh prior
probability of 10% people having lung
disease and then probability B
prob
e
e
e
e
e
e
e
e
e
e e
probability that a patient smokes is
15% uh and the probability of b um if B
then a the probability of a patient
smokes even though they have lung
disease is
5% and probability of a is B probability
of the patient will have lung disease if
they smoke and then when you put the
formulas get you can't create a 47 comma
4 it'll delete the four out so it's only
unique values and if you use
dictionaries quick reminder this should
look familiar because it is a dictionary
uh where you have a value and that value
is assigned to or that key is assigned
to a value uh so you can have a key
value set up as a dictionary so it's
like a dictionary without the value it's
just the keys and they all have to be
unique and if we run this we have a set
of
47 we can also take a list a regular um
setup and I'm going to go ahead and just
throw in another number in here four and
run it uh and you can see here if I take
my list 1 2 3 4 4 and I convert it to a
set and here it is my set from list
equals set my
list the result is 1 2 3 4 so it just
deletes that last four right out of
there
and with the sets you can also go in
there and um print here is my set my Set
uh three is in the set and then if you
do three in my
set e
that's going to be a logic function uh
and one in my set six is not in the set
and so forth if we run
this we get uh three is in the set true
one is in the set false cuz 357 is
another one six is in the set uh six is
not in the set so not in my set you can
also use this with the list we could
have just use 357 and it would to have
um the same response on there is three
and usually do if three is in but three
in my set is still works on just a
regular list and we'll go ahead and do a
little iteration we're going to do kind
of the dice one remember um uh 1 2 3 4 5
6 and so we're going to bring in an
iteration tool and and import product is
product and uh I'll show you what that
means in just a second so we have our
two dice we have dice a and it's going
to be a set of values uh you can only
have one value for each one that's why
they put it in a set and if you remember
from range it is up to seven so this is
going to be 1 2 3 4 5 six it will not
include the seven and the same thing for
our dice
B and then we're going to do is we're
going to create a list
which is the product of A and B so
what's um a plus b and if we go ahead
and run this uh it'll print that out and
you'll see um in this case when they say
product because it's an iteration
tool we're talking about creating a
tuple of the two so we've now created a
tuple of all possible outcomes of the
dice where dice a is 1 2 3 1 to six and
dice B is 1 to six and you can see 1: 1
1: two 1 to 3 and so so forth you
remember we had a slide on this
earlier we can do in dice equals 2 div
dice faces 1 2 four five six uh another
way of doing before and then we create
space where we have set which is the
product of dice
faes and here it just again put it
through all the different possible
variables we can
have like we had before uh can go
through four outcome and event space
outcome versus going through and putting
them in a nice line
and we do something let's go print since
we have the in printing with a comma
that just means it's not going to hit
the return going down the next line uh
and we'll go ahead and do the
length of our event space variable we're
going to want to know in a minute and of
course I get Carri with my typing of
L and we might want to calculate
something like um what about the what if
we want to have the probability of the
mulle three in our setup and so we can
put together the code for the outcoming
event space of x y equals outcome if x +
y remainder three so we're going to
divide by three and look at the
remainder and it equals
zero then it's a faval outcome and we're
going to pop that outcome on the end
there and we'll turn it into a set so
the favor outcome equals a
set but just in case we'll go ahead and
do
that and if we want to print out the
outcome we can go ahead and see what
that looks like you can see here these
are all U multiples of three uh 1 + 2 is
3 5 + 4 is 9 which divided by 3 is 3 and
so
forth and just like we looked up the
length uh of the one before let's go
ahead and print the
length of our F outcome so we can see
what that looks
like there we
go and of course I did forget to add the
print in the middle because looping
through there and if I run this you can
see we end up with 12 so we have 36 to
we have 12 that are multiple that add up
to a multiple of three and we can easily
comp the probability of this of our
faval outcome over the length of the
event with a 3333
chance the probability of getting the
sum which is a mul is
3333 and go run it you see we just have
a huge amount of choices go on down here
and we look at
the
7,776 choices that's a lot of
choices uh what is the S where the sum
is a multiple of five but not a multiple
of three we can go through all these
different options and then you can see
here D1 D2 D3 outcome and if and
division by five does not have a
remainder of zero but the remainder is
also of a division by three is not equal
to zero so the multiple of five is equal
to zero but the multiple of three is not
we can just appin that on here and then
we can look at that uh favorable outcome
we'll go ahead and set that and we'll
just take a look at this what's our
length of our favorable
outcome
e
e e
it's always good to see what we're
working with and so we have 904 out of
770 six and then of course we can just
do a simple division to get the
probability on here what's the
probability that we're going to roll a
multiple of five when you add them
together but not positive true positive
um and then we have false positive and
you can think of this as your predicted
model what does that mean that means if
you divided your data and
you third to see how well it comes out
how many times was it uh true positive
versus uh false positive I gave a false
positive response and you can imagine in
medical situations you might adjust your
model accordingly so you don't have a
false positive say better a false
negative and they go back and get
retested then to have 30% false positive
where then the test is pretty much
invalid so in a used case uh like cancer
prediction let's consider an example
where a cancer prediction model is put
to the test for its accuracy and
precision actually result of a person's
medical
report and so you can see here here's
our actual predicted whether cancer not
cancer you don't want to have a false
positive I mean false negative in other
words you don't want to have it tell you
that you don't have cancer when you do
so that would be something you'd really
be looking for in this particular domain
you don't want a false
negative uh and this is again you know
you've created a model you have hundreds
of people or thousands of pieces of data
that come in there's a real famous case
study where they have the imagery and
all the measurements they take and
there's about 36 different measurements
they take and then if you run the a
basic model you want to know just how
accurate is how many um negative results
do you have that are either telling
people they have cancer that don't or
telling people that don't have cancer
that they do and then we can take these
numbers
and e
we can feed them into our accuracy our
precision and our
recall uh so accuracy precision and
recall accuracy metric to measure how
accurately the results are predicted and
this is your um total um true where you
got the right results you add them
together the true positive the true
negative over all the results so what
percentage positive plus the false
negative on there and we'll want to go
ahead and do a demo on the naive Bas
classifier before I get too far into uh
naive Bas class going pull it from the
SK learn or the psyit uh let's go ahead
kind of an interesting P
classif I'll just zoom up in here so you
can see some of titles uh everything
from the nearest neighbor linear uh but
we're going to be focusing on the na Bas
over here and this is a sample dat set
that they put together these na base
remember is set up as probably most
simplified calculator and so with true
false and stuff that between the
features where the features assume to
have some then we can go ahead and use
that for the prediction and so that's
what we're using as an B classifier
versus many of the other classifiers
that are out
there for this we're going to use uh the
social netor Network ads it's a little
data set on here and let me go and just
open that up the file uh here we go it
has user ID gender age estimated salary
uh purchased and so we you see the user
ID mail 19 uh estimated salary
19,000 and purchase zero uh so it's
either going to make a purchase or not
so look at that last one 01 we should be
thinking of binomials we should be
thinking of uh simple naive phase
classifier kind of
setup so if we close this out we're
going to go ahead and import our numpy
as
NP we're nice to have a a good visual of
our data so we'll put in our map plot
Library here's our pandas our data
frame uh and then we're going to go
ahead and import the data set and the
data set's going to be we're going to
read it from The Social Network ads. CSV
then we're going to print the head just
so you can see it again uh even though I
showed you it in the file and x equals
the data set ication uh two three values
and Y is going to be the four uh column
four let me just run this so it's a
little easier to go over that um you can
see right here we're going to be looking
at uh 012 is age and estimated salary so
23 and that's what iocation just means
um that we're looking at the number
versus a regular location uh regular
location you'd actually say age and
estimated salary
and then column four is did they make a
purchase they purchased something uh so
those are the three columns we're going
to be looking at when we do this and
we've gone ahead and imported these
and
e e
imported the data so now our data set is
all set with this information in
it and we'll need to go ahead and split
the data up uh so we need our from the
sklearn model selection we can import
train test split uh this does a nice job
we can set the random state so randomly
picks the data and we're just going to
take uh 25% of it going to go into the
test our X test and our y test and the
75% will go to XT train and Y train that
way once we create our model we can then
have data to see just how accurate or
how well it has performed with our um
prediction the next step in
pre-processing our data is to go ahead
and do feature scaling now a lot of this
is start to look familiar if you've done
a number of the other modules and setup
you should start noticing that we bring
in our data we take a look at what we're
working with um we go ahead and split it
up into training and testing uh in this
case we're going to go ahead and scale
it scale it means we're putting it
between a value of minus1 and one uh or
someplace in that middle ground there
this way if you have any huge set you
don't have this huge um setup if we go
back up to here where salary salary
is well there's a good chance with a lot
of the backend math that 20,000
properly and finally we get to actually
create our naive B model um and we're
going to go ahead and import the gazan
naive
Bay and the gazan is is the most basic
one that's what we're looking at now it
turns out though if you go to the SK um
learn kit uh they have a number of
different ones you can pull in there
there's a um beri I I've never used that
one
gazan uh so there's a number of
different options you can look at gazan
to commonly used uh so we're talking
about the naive B that's usually what
people are talking about when they're
pulling this in and one of the nice
things about the gajian if you go to
their website um to sklearn the naive
Bay gazan there's a lot of cool features
one of them is you can do partial fit on
here um that means if you have a huge
amount of data you don't have to process
it all a want you once you can batch it
into the gazan can with it as far as
fitting the data how you it we're just
doing the basics so we're going to go
ahead and create our classifier we're
going to equal the gazan
NB and then we're going to do a fit
we're going to fit our training data and
our training solution so XT train y
train and we'll go ahead and run this uh
it's going to tell us that it it ran the
code right
there and now we have our trained
classifier model so the next step is we
need to go ahead and run a prediction
we're going to do our y predict
equal the classifier predict X test so
here we fit the dat data and now we're
going to go ahead and
predict and now we get to our confusion
Matrix uh so from the SK learned Matrix
metrix you can import your confusion
Matrix just as saves you from doing all
the simple math that does it all for you
and then we'll go ahead and create our
confusion metric with the Y test and the
Y predict so we have our actual and we
have our predicted
value and you can see from here this
this is the chart we looked at here's
predicted so true positive false
positive false negative true
negative and if we go ahead and run this
there we have it 65
3725 and in this particular uh
prediction we had 65 uh or predicted the
truth as far as a a purchase they're
going to make a purchase and we guessed
three wrong and we had 25 we predicted
would not purchase
and at this point if you were with your
shareholders or a board meeting um you
would start to hear some snoozing they
looking at the numbers and say hey
here's my confusion M
Matrix so let's go ahead and visualize
the
results we're going to pull from the map
plot Library colors import listed color
map um and this is actually my machine's
going to throw an error because this is
being um because of the way the setup is
I have a newer version on here than when
they put together the demo and we need
our um X set and our y set which is our
X train and Y train and then we'll
create our X1 X2 and we'll put that into
a grid uh and we set our exet minimum
stop and our ex set max stop and if we
come all the way over here we're going
to step 0.01 this is going
to give us a nice line uh is what that's
doing and then we're going to plot the
Contour
uh plot the X limit plot the Y limit and
put the scatter plot in there let's go
ahead and run this uh to be honest when
I'm doing these graphs there's so many
different ways to do that there's so
many different ways to put this code
together to show you what we're doing
it's uh a lot easier to pull up the
graph and then go back up and explain it
so the first thing we want to note here
when we're looking at the
data is this is the training set
and so we have those who didn't make a
purchase we've drawn a nice area for
that it's defined by the naive Bay setup
and then we have those who did make a
purchase the green and you can see that
some of the green dots fall into the red
area and some of the red dots fall into
the green so even our training set isn't
going to be 100% uh we couldn't do that
and so we're looking at our different
data coming down uh we can kind of
arrange our X1 X2 so we have a nice plot
going on and we're going to create the
um
Contour that's that nice line that's
drawn down the middle on here with the
red green um that's where that's what
this is doing right here with the
reshape and notice that we had to uh do
the dot T if you remember from numpy um
if you did the numpy module um you end
up with pairs you know x uh X1 X2 X1 X2
next row and so forth you have to flip
it so it's all one row you have all your
x1's and all your X twos um so that's
what we're kind of looking for right
here on this
setup uh and then the scatter plot is of
course um your scattered data across
there we're just going through all the
points that puts these nice little dots
onto our setup on here and we have our
estimated salary and our age and then of
course the dots are did they make a
purchase or not and just a quick note
this is kind of funny you can see up
here where it says X set y set equals uh
XT train y train which seems kind of a
little weird to do um this is because
this was probably originally a
definition uh so it's its own module
that could be called over and over again
and which is really a good way to do it
because the next thing we're going to
want to do is do the exact same thing
when we're going to visualize the test
set results uh that way we can see what
happened with our test group our
25% and you can see down here we have um
the test Set uh and it if you look at
the two graphs next to each other this
one obviously has um 75% of the data so
it's going to show a lot more this is
only 25% of the data you can see that
there's a number that are kind of on the
edge as to whether they could guess by
age and income they're going to make a
purchase or not um but that said it
still is pretty clear it's pretty good
as far as how much the estimate is and
how good it
does now graphs are really effective for
showing people what's going on but you
also need to have the numbers and so
we're going to do from SK learn we're
going to import metrics and then we're
going to print our metrics
classification Port from the Y test and
the Y
predict and you can see here we have
Precision uh Precision of zeros is 90
there's our recall .96 we have an F1
score and a support and we have our
Precision the recall on getting it right
uh and then we can do our accuracy the
macro average and the weighted average
uh so you can see that it pulls in
pretty good as far as um how accurate it
is you could say it's going to be about
90% is going to guess correctly um that
that they're not going to purchase and
we had an 89% chance that they are going
to purchase um and then the other
numbers as you get down have a little
bit different meaning but it's pretty
straightforward on here here's our
accuracy and here's our micro average
and the weighted average and everything
else you might need and if you forgot
the exact definition of accuracy it is
the true positive true negative over all
of the different setups Precision is
your true positive over all positives
true and false and recall is a true
positive over true positive plus false
negative and we can just real quick flip
back there so you can see those numbers
on here uh here's our Precision here's
our
recall and here's our accuracy on this
multiple linear regression let's take a
brief look at what happens when you have
multiple inputs so in multiple linear
regression we have uh well we'll start
with the simple linear regression where
we had y = m + x + C and we're trying to
find the value of y now with multip
linear regression we have multiple
variables coming in so instead of having
just X we have X1 X2 X3 and instead of
having just one slope each variable has
its own slope attached to it as you can
see here we have M1 M2 M3 and we still
just have the single coefficient so when
you're dealing with multiple linear
regression you basically take your
single linear regression and you spread
it out so you have y = M1 * X1 + M2 * X2
so on all the way to m to the nth x to
the nth and then you add your
coefficient on there implementation of
linear regression now we get into my
favorite part let's understand how
multiple linear regression works by
implementing it in Python if you you
remember before we were looking at a
company and just based on its R&D trying
to figure out its profit we're going to
start looking at the expenditure of the
company we're going to go back to that
we're going to predict his profit but
instead of predicting it just on the R&D
we're going to look at other factors
like Administration costs marketing
costs and so on and from there we're
going to see if we can figure out what
the profit of that company's going to be
to start our coding we're going to begin
by importing some basic libraries and
we're going to be looking through the
dat before we do any kind of linear
regression we're going to take a look at
the data see what we're playing with
then we'll go ahead and format the data
to the format we need to be able to run
it in the linear regression model and
then from there we'll go ahead and solve
it and just see how valid our solution
is so let's start with importing the
basic libraries now I'm going to be
doing this in Anaconda Jupiter notebook
a very popular IDE I enjoy it it's such
a visual to look at it's so easy to use
um just any ID for python will work just
fine for for this so break out your
favorite python IDE so here we are in
our Jupiter notebook let me go ahead and
paste our first piece of code in there
and let's walk through what libraries
we're importing first we're going
to these are very common tools most your
line regression the whichs
foron it does such a wonderful job
importing data setting into a DAT frame
so use P when we can and I'll show you
what that looks like the other three
lines are R to this and take a look so
we're going to import matplot library.
pip plot as M plot Library so you have
to always import M plot library then
caborn sits on top of it and we'll take
a look at what that looks like you could
use any of your own plotting libraries
you want there's all kinds of ways to
look at the data these are just very
common ones and the caborn is so easy to
use it just looks beautiful is the igned
m plot Library inline that is only
because I'm doing inline IDE my
interface in the Anaconda Jupiter
notebook requires I put that in there or
you're not going to see the graph when
it comes up let's go ahead and run this
it's not going to be that interesting so
we're just setting up variables in fact
it's not going to do anything that we
can see but it is importing these
different libraries and setup the next
step is load the data set and extract
independent and dependent variables now
here in the slide you'll see companies
equals pd. read CSV and it has a long
line there with the file at the end
1,000 companies. CSV you're going to
have to change this to fit
whatever e
setup you have in the file itself you
can request just go down to the
commentary below this video and put a
note in there and simply learn we'll try
to get in contact with you and Supply
you with that file so you can try this
coding yourself so we're going to add
this code in here and we're going to see
that I have companies equals pd. reader
CSV and I've changed this path to match
my computer c/s simplylearn
1000 companies. CSV and then below there
we're going to set the x equals to
companies under the ication and because
this is companies is a PD data set I can
use this nice notation that says take
every row that's what the colon the
first colon is comma except for the last
column that's what the second part is
where we have a colon minus one and we
want the values set into there so X is
no longer a data set a pandas data set
but we can easily extract the data from
our Panda's data set with this notation
and then y we're going to set equal to
the last row well the question is going
to be what are we actually looking at so
let's go ahead and take a look at that
and we're going to look at the
companies. head which lists the first
five rows of data and I'll open up the
file in just a second so you can see
where that's coming from but let's look
at the data in here as far as the way
the pandas sees it when I hit run you'll
see it breaks it out into a nice setup
this is what pandas one of the things
pandas is really good about is it looks
just like an Excel spreadsheet you have
your rows and remember when we're
programming we always start with zero we
don't start with one so it shows the
first five rows 0 1 2 3 4 and then it
shows your different columns R&D spend
Administration marketing spend State
profit it even notes that the top are
column names it was never told that but
Pand is is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
opened it up as a text editor and you
can see at the top we have R&D spin
comma administ ation comma marketing
spin comma State comma profit carriage
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
uh
$165,300 compared to the administration
cost of
$136,800 so on so on helps to create the
profit of
$2,261 83 that makes no sense to me
whatsoever no pun intended so let's flip
back here and take a look at our next
set of code where we're going to graph
it so we can get a better understanding
of our data and what it mean so at this
point we're going to use a single line
of code to get a lot of information so
we can see where we're going with this
let's go ahead and paste that into our
notebook and see what we got going and
so we have the visualization and again
we're using SNS which is pandas as you
can see we imported the map plot
library. pylot is PLT which then the
caborn uses and we imported the caborn
as SNS and then that final line of code
helps us show this in our um inline
coding without this it wouldn't display
and you display it to a file in other
means and that's the matap plot library
in line with the Amber sign at the
beginning so here we come down to the
single line of code Seaborn is great
because it actually recognizes the panda
data frame so I can just take the
companies. core for coordinates and I
can put that right into the Seaborn and
when we run this we get this beautiful
plot and let's just take a look at what
this plot means if you look at this plot
on mine the colors are probably a little
bit more purplish and blue than the
original one uh we have the column in
the rows we have R and D spending we
have Administration we have marketing
spending and profit and if you cross
index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so R&D spending is going
to be the same as R&D spending and the
same thing with Administration cost so
right down the middle you get this dark
row or dark um diagonal row that shows
that this is the highest corresponding
data that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with R&D spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so now that we
have a nice look at the data let's go
ahead and dig in and create some actual
useful linear regression model so that
we can predict values and have a better
profit now that we've taken a look at
the visualization of this data we're
going to move on to the next step
instead of just having a pretty picture
we need to generate some hard data some
hard values so let's see what that looks
like we're going to set up our linear
regression model in two steps the first
one is we need to prepare some of our
data so it fits correctly and let's go
ahead and paste this code into our
Jupiter notebook and what we're bringing
in is we're going to bring in in the
sklearn pre-processing where we're going
to import the label encoder and the one
hot encoder to use the label encoder
we're going to create a variable called
label encoder and set it equal to
capital L label capital E encoder this
creates a class that we can reuse for
transferring the labels back and forth
now about now you should ask what labels
are we talking about let's go take a
look at the data we processed before and
see what I'm talking about here if you
remember when we did the companies. head
and we printed the top five rows of data
we have our columns going across we have
column zero which is R&D spending column
one which is Administration column two
which is marketing spending and column
three is State and you'll see under
State we have New York California
Florida now to do a linear regression
model it doesn't know how to process New
York it knows how to process a number so
the first thing we're going to do is
we're going to change that New York
California and Florida and we're going
to change those to numbers that's what
this line of code does here x equals and
then it has the colon comma 3 in
Brackets the first part the colon comma
means that we're going to look at all
the different rows so we're going to
keep them all together but the only row
we're going to edit is the third row and
in there we're going to take the label
coder and we're going to fit and
transform the X also the third row so
we're going to take that third row we're
going to set it equal to a
transformation and that transformation
basically tells it that instead of
having a uh New York it has a zero or
one or a two and then finally we need to
do a one hot encoder which equals one
hot encoder categorical features equals
three and then we take the X and we go
ahead and do that equal to one hot
encoder fit transform X to array this
final transformation preps our data for
us so it's completely set the way we
need it as just a row of numbers even
though it's not in here let's go ahead
and print X and just take a look what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just do row zero you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers next on setting up our
data we have avoiding dummy variable
trap this is very important why because
the computer has automatically
transformed our header into the setup
and it's automatically transformed all
these different variables so when we did
the encoder the encoder created two
columns and what we need to do is just
have the one because it has both the
variable and the name that's what this
piece of code does here let's go ahead
and paste this in here and we have x = x
colon comma 1 colon all this is doing is
removing that one extra column we put in
there when we did our one hot encoder
and our label en coding let's go ahead
and run that and now we get to create
our linear regression model and let's
see what that looks like here and we're
going to do that in two steps the first
first step is going to be in splitting
the data now whenever we create a uh
predictive model of data we always want
to split it up so we have a training set
and we have a testing set that's very
important otherwise we'd be very
unethical without testing it to see how
good our fit is and then we'll go ahead
and create our multiple linear
regression model and train it and set it
up let's go ahead and paste this next
piece of code in here and I'll go ahead
and shrink it down a size or two so it
all fits on one line so from the sklearn
module selection we're going to import
train test split and you'll see that
we've created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually referenes when
we're doing different uh models you
usually see that a capital x and you see
the train and the test and the lowercase
Y what this is is X is our data going in
that's our R&D spin our Administration
our Market
and then Y which we're training is the
answer that's the profit because we want
to know the profit of an unknown entity
so that's what we're going to shoot for
in this tutorial the next part train
test split we take X and we take y we've
already created those X has the columns
with the data in it and Y has a column
with profit in it and then we're going
to set the test size equals 0. 2 that
basically means 20% So 20% of the rows
are going to be tested we're going to
put them off to the side side so since
we're using a th000 lines of data that
means that 200 of those lines we're
going to hold off to the side to test
for later and then the random State
equals zero we're going to randomize
which ones it picks to hold off to the
side we'll go ahead and run this it's
not overly exciting it's setting up our
variables but the next step is the next
step we actually create our linear
regression model now that we got to the
linear regression model we get that next
piece of the puzzle let's go ahe and put
that code in there and walk through it
so here we go we're going to paste it in
there and let's go ahead and since this
is a shorter line of code let's zoom up
there so we can get a good look and we
have from the SK learn. linear model
we're going to import linear regression
now I don't know if you recall from
earlier when we were doing all the math
let's go ahead and flip back there and
take a look at that do you remember this
or we had this long formula on the
bottom and we were doing all this suiz
and then we also looked at setting it up
with the different lines and then we
also looked all the way down to multiple
linear regression where we're adding all
those form formulas together all of that
is wrapped up in this one section so
what's going on here is I'm going to
create a variable called regressor and
the regressor equals the linear
regression that's a linear regression
model that has all that math built in so
we don't have to have it all memorized
or have to compute it individually and
then we do the regressor do fet in this
case we do X train and Y train because
we're using the training data X being
the data in and Y being profit what
we're looking at and this does all that
math for us so within one click and one
line we've created the whole linear
regression model and we fit the data to
the linear regression model and you can
see that when I run the regressor it
gives an output linear regression it
says copy x equals True Fit intercept
equals true in jobs equal one normalize
equals false it's just giving you some
general information on what's going on
with that regressor model now that we've
created our linear regression model
let's go ahead and use it and if you
remember we kept a bunch of data aside
so we're going to do a y predict
variable and we're going to put in the X
test and let's see what that looks like
scroll up a little bit paste that in
here predicting the test set results so
here we have y predict equals regressor
do predict X test going in and this
gives us y predict now because I'm in
Jupiter in line I can just put the
variable up there and when I hit the Run
button it'll print that array out I
could have just as easily done print y
predict so if you're in a different IDE
this not an inline setup like the
jupyter notebook you can do it this way
print y predict and you'll see that for
the 200 different test variables we kept
off to the side it's going to produce
200 answers this is what it says the
profit are for those 200 predictions but
let's don't stop there let's keep going
and take a couple look we're going to
take just a short detail here and
calculating the coefficients and the
intercepts this gives a quick flash
what's going on behind the line we're
going to take a short detour here and
we're going to be calculating the
coefficient and intercepts so you can
see with those look like what's really
nice about our regressor we created is
it already has a coefficients for us and
we can simply just print regressor do
coefficient underscore when I run this
you'll see our coefficient here and if
we can do the regressor coefficient we
can also do the regressor intercept and
let's run that and take a look at that
this all came from the multiple
regression model and we'll flip over so
you can remember where this is going
into and where it's coming from you can
see the formula down here where y = M1 *
X1 + M2 * X2 and so on and so on Plus C
the coefficient so these variables fit
right into this formula y = slope 1 *
column 1 variable plus slope 2 * column
2 variable all the way to the m into the
n and x to the N plus C the coefficient
or in this case you have -
8.89 to the power of 2 etc etc times the
First Column and the second column and
the third column and then our intercept
is the minus
10309 Point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the r s value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from SK learn. metrics we're going
to import R2 score that's the r SAR
value we're looking at the error so in
the R2 score we take our y test versus
our y predict y test is the actual
values we're testing that was the one
that was given to us that we know are
true the Y predict of those 200 values
is what we think it was true and when we
go ahead and run this we see we get a
9352 that's the R2 score now it's not
exact a straight percentage so it's not
saying it's
93% correct but you do want that in the
upper 90s oh and higher shows that this
is a very valid prediction based on the
R2 score and if r squar value of 91 or
92 as we got on our model remember it
does have a random generation involved
this proves the model is a good model
which means success yay we successfully
trained our model with certain
predictors and estimated the profit of
the companies using linear regression
let's take an example and see how we can
apply logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into Jupiter notebook and uh show
the code but before that let me take you
through a couple of slides to explain
what we're trying to do so let's say you
have an 8 by8 image and the the image
has has a number 1 2 3 4 and you need to
train your model to predict what this
number is so how do we do this so the
first thing is obviously in any machine
learning process you train your model so
in this case we are using logistic
regression so and then we provide a
training set to train the model and then
we test how accurate our model is with
the test data which means that like any
machine learning process we split our
set mod and
then so that is typical methology of
train testing mod so let's uh take a
look at the code and uh see what we are
doing so I'll not go line by line but
just take you through some of the blocks
so first thing we do is import all the
libraries and then we basically take a
look at the images and
see I mentioned earlier and we can
and and the heat map and use heat map
for visualizing this and I show you in
the code what exactly is we can use for
finding the accuracy in our example we
get an
accuracy good so what isus Matrix this
is an example ofus and for idy accuracy
of
classification so the most important
part in a confusion Matrix is that first
of all this as you can see this is a
matrix and the size of the Matrix
depends on how many outputs we are
expecting right that
because the diagonals should have
maximum numbers and other should have
very few numbers so here that's what is
happening so there's a two here there
are there's a one here but most of them
are along the diagonal this what does
mean this means that the number that has
been fed is zero and the number that has
being detected is also zero so the
predicted value and the actual value are
the same so along the diagonals that is
true which means that let's let's take
this diagonal right if if the maximum
number is here that me that is 34 which
means that4
of four and out of which 34 have been
predicted correctly as number four and
one has been predicted as number eight
and another one has been predicted as
number nine so these are two
misclassifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100% accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like a
100% accurate model okay so that's uh
just of how to use this Matrix how to
use this uh confusion Matrix I know the
name uh is a little funny sounding
confusion Matrix but actually it is not
very confusing it's very straightforward
so you are just plotting what has been
predicted and what is the labeled
information or what is the actual data
that's also known as the ground truth
sometimes okay these are some fancy
terms that are used so predicted label
and the actual label that's all it is
okay yeah so we are showing a little bit
more information here so 38 have been
predicted and here you will see that all
of them have been predicted correctly
there have been 38 zeros and the
predicted value and actual
val7 + 5 yeah 42 have been fed the
images 42 images are of Digit three and
uh the accuracy is only 37 of them have
been accurately predicted three of them
have been predicted as number seven and
two of them have been predicted as
number eight and so on and so forth okay
so this is the code in in Jupiter
notebook for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to 5 or 0 to 9 and
um and then we will see how well it is
trained and whether it is able to
predict these numbers correctly or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and uh then
the last line in this block is to load
the digits so let's go ahead and run
this code then here we will visualize
the shape of these uh digits so we can
see see here if we take a look this is
how the shape is
797 by 64 these are like 8 by8 images so
that's that's what is reflected in this
shape now from here onwards we are
basically once again importing some of
the libraries that are required like
numai and M plot and we will take a look
at uh some of the sample images that we
have loaded so this one for example
creates a figure uh and then we go ahead
and take a few sample images to see how
they look so let's
me
e e
run this code and so that it becomes
easy to understand so these are about
five images sample images that we are
looking at 0 1 2 3 4 so this is how the
images this is how the data is okay and
uh based on this we will actually train
our logistic regression model and then
we will test it and see how well it is
able to recognize so the way it works is
the pixel information so as you can see
here this is an 8 by 8 pixel kind of a
image and uh the each pixel whether it
is activated or not activated that is
the information available for each pixel
now based on the pattern of this
activation and non-activation of the
various pixels this will be identified
as a zero for example right similarly as
you can see so overall each of these
numbers actually has a different pattern
of the pixel activation and that's
pretty much that our model needs to
learn for which number what is the
pattern of the activation of the pixels
right so that is what we are going to
train our model okay so the first thing
we need to do is to split our data into
training and test data set right so
whenever we perform any training we
split the data into training and test so
that the training data set is used to
train the system so we pass this prob
multiple times uh and then we test it
with the test data set and the split is
usually in the form of and there are
various ways in which you can split this
data it is up to the individual
preferences in our case here we are
splitting in the form of 23 and 77 so
when we say test size as 20 23 that
means 23% of the entire data is used for
testing and the remaining 77 10% is used
for training so there is a readily
available function which is uh called
train test split so we don't have to
write any special code for the splitting
it will automatically split the data
based on the proportion that we give
here which is test size so we just give
the test size automatically training
size will be determined and uh we pass
the data that we want to split and the
the results will be stored in xcore
train and Y Yore train for the training
data set and what is xcore train this
are these are the features right which
is like the independent variable and
Yore train is the label right so in this
case what happens is we have the input
value which is or the features value
which is in xor train and since this is
labeled data for each of them each of
the observations we already have the
label information saying whether this
digit is a zero or a one or a two so
that this this is what will be used for
comparison to find out whether the the
system is able to recognizes it
correctly or there is an error for each
observation it will compare with this
right so this is the label so the same
way xcore train Yore train is for the
training data set xcore test Yore test
is for the test data set okay so let me
go ahead and execute this code as well
and then we can go and check quickly
what is the how many entri are there and
in each of this so xcore train the shape
is
1383 by 64 and ycore train has 1383
because there is uh nothing like the
second part is not required here and
then xcore test shape we see is 414 so
actually there are 414 observations in
test and 1383 observations in train so
that's basically what these four lines
of code are are saying okay then we
import the uh logistic regression
library and uh which is a part of
psychic learn so we we don't have to
implement the logistic regression
process itself we just call these the
function and uh let me go ahead and
execute that so that uh we have the
logistic regression Library imported now
we create an instance of logistic
regression right so logistic RR is a is
an instance of logistic regression and
then we use that for training our model
so let me first execute this code so
these two lines so the first line
basically creates an instance of
logistic regression model and then the
second line is where we are passing our
data the training data set right this is
our the the predictors and uh this is
our Target we are passing this data set
to train our model all right so once we
do this in this case the data is not
large but by by and large uh the
training is what takes usually a lot of
time so we spend in machine learning
activities in machine learning projects
we spend a lot of time for the training
part of it okay so here the data set is
relatively small so it was pretty quick
so all right so now our model has been
trained using the training data set and
uh we want to see how accurate this is
so what we'll do is we will test it out
in probably faces so let me first try
out how well well this is working for
one image okay I will just try it out
with one image my the first entry in my
test data set and see whether it is uh
correctly predicting or not so and in
order to test it so for training purpose
we use the fit method there is a method
called fit which is for training the
model and once the training is done if
you want to test for a particular value
new input you use the predict method
okay so let's run the predict method and
we pass this particular image and uh we
see that the shape is or the prediction
is four so let's try a few more let me
see for the next 10 uh seems to be fine
so let me just go ahead and test the
entire data set okay that's basically
what we will do so now we want to find
out how accurately this has um performed
so we use the score method to find what
is the percentage of accuracy and we see
here that it has performed up to 94%
Accurate okay so that's uh on this part
now what we can also do is we can um
also see this accuracy using what is
known as a confusion Matrix so let us go
ahead and try that as well uh so that we
can also visualize how well uh this
model has uh done so let me execute this
piece of code which will basically
import some of the libraries that are
required and um we we basically create a
confusion Matrix an instance of
confusion matrix by running confusion
Matrix and passing these uh values so we
have so this confusion underscore Matrix
method takes two parameters one is the
Yore test and the other is the
prediction so what is the Yore test
these are the labeled values which we
already know for the test data set and
predictions are what the system has
predicted for the test data set okay so
this is known to us and this is what the
system has uh the model has generated so
we kind of created a confusion Matrix
and we will print it and uh this is how
the confusion Matrix looks as the name
suggests it is a matrix and um the key
point out here is that the accuracy of
the model is determined by how many
numbers are there in the diagonal the
more the numbers in the diagonal the
better the accuracy is okay and first of
all the total sum of all the numbers in
this whole Matrix is equal to the number
of observations in the test data set
that is the first thing right so if you
add up all these numbers that will be
equal to the number of observations in
the test data set and then out of that
the maximum number of them should be in
the diagonal that means the accuracy is
pretty good if the the numbers in the
diagonal are L and in all other places
there are a lot of numbers which means
the accuracy is very low the diagonal
indicates a correct prediction that this
means that the actual value is same as
the predicted value here again actual
value is same as the predicted value and
so on right so the moment you see a
number here that means the actual value
is something and the predicted value is
something else right similarly here the
actual value is something and the
predicted value is something else so
that is basically uh how we read the
confusion Matrix now how do we find the
accuracy you can actually add up the
total values in the diagonal so it's
like 38 + 44 Plus 43 and so on and
divide that by the total number of test
observations that will give you the
percentage accuracy using a confusion
Matrix now let us visualize this
confusion Matrix in a slightly more
sophisticated way uh using a heat map so
we will create create a heat map with
some We'll add some colors as well it's
uh it's like a more visually visually
more appealing so that's the whole idea
so if we let me run this piece of code
and this is how the heat map looks uh
and as you can see here the diagonals
again are are all the values are here
most of the values so which means
reasonably this seems to be reasonably
accurate and yeah basically the accuracy
score is 94% this is calculated as I
mentioned by adding all these numbers
divided by the total test values or the
total number of observations in test
data set okay so this is the confusion
Matrix for logistic
regression all right so now that we have
seen the confusion Matrix let's take a
quick sample and see how well uh the
system has classified and we will take a
few examples of the data so if we see
here we we picked randomly a few of them
so this is uh number four which is the
actual value and also the predicted
value both are four this is an image of
zero so the predicted value is also zero
actual value is of course zero then this
is the image of nine so this has also
been predicted correctly nine and actual
value is nine and this is a image of one
and again this has been predicted
correctly as like the actual value okay
so this was a quick demo of of logistic
regression how to use logistic
regression to identify images need for
confusion matrixes classification models
have multiple output categories most
error measures will tell us the total
error in our model but we cannot use it
to find out individual instances of
errors in our models so you have your
input coming in you have your classifier
uh it measures the error and it says oh
53 of these are correct but we don't
don't know which 53% are correct is it
53% correct uh on guessing on the spam
is it 23% guessing on spam and 27%
guessing on what's not spam this is
where the confusion Matrix comes in so
during the classification we also have
to overcome the limitations of accuracy
accuracy can be misleading for
classification problems if there is a
significant class imbalance a model
might predict the majority class for all
cases and have a high accuracy score and
so you can see here we have our emo
coming in and there's two spams the
classifier comes in and goes hey it only
catches one of those spams and it
misclassifies one that's not spam so our
model predicted eight out of 10
instuments and will have an accuracy of
80% but is it classifying
correctly a confusion Matrix represents
a table layout of the different outcomes
of prediction and results of a
classification problem and helps
visualize its
outcomes and so you see here we have our
uh uh simple chart predicted and actual
the confusion Matrix helps us identify
the correct predictions of a model for
different individual classes as well as
the errors so you'll see here that the
values predicted by our classifier are
along the rows this is what we're going
to guess it is or our our model is
guessing what this is based on its
training so we've already trained the
model to um guess whether it's spam or
not spam or whatever it is you're
working on and then the actual values of
our data set are along the
columns so this is the actual value it's
supposed to be people who can speak
English will be classified as positives
so because they have a remember 01 do
you speak English yes no and you could
extend this that they might have do you
speak uh French do you speak whatever
languages and so you might have a whole
lot of classifiers that you would look
at each one of these
people who cannot speak English will be
classified as negatives so they'll be a
zero so you know zero ones the number of
times our actual positive values are
equal to predicted positive values gives
us true positive TP the number of times
our actual negative values are equal to
predictive negative values gives us true
negative
TN the number of times our model wrongly
predicts negative values as
positives gives us a false positive
FP and you'll see when you're working
with these a lot you know memorizing
that is false positive you can easily
figure out what that is and pretty soon
you're just looking at the FP or the TP
depending on what you're working on and
the number times our model wrongly
predicts positive values as negatives
gives us a false negative
FP now I'm going to do a quick step out
here let's say you're working in the
medical and we're talking about cancer
uh do you really want a bunch of false
negatives you want zero under false
negative uh so when we look at this
confusion Matrix if you have 5% false
positives and 5% false negatives it'd be
much better to even have 20% false
positives because they go in and test it
and zero false negatives the say might
be true if you're working on uh uh say
uh a car driving is this a safe place
for the car to go well you really don't
want any false positives you know yes
this is safe right over the cliff so
again when you're working on the project
or whatever it is you're working on this
chart suddenly has huge value uh we were
talking about spam email how many
important emails say from your banking
overdraft charge coming in that you want
to be uh a true a false negative you
don't want it to go in the spam folder
likewise you want to get as much as the
spam out of there but you don't want to
miss anything really
important confusion Matrix metrics are
performance measures which help us find
the accuracy of our classifier there are
four main metrics accuracy precision
recall and F1 score the F1 score is the
one I usually hear the most and accuracy
is usually what you put on your chart uh
when you're standing in front of the
shareholders how accurate is it people
understand
accuracy um F1 score is a little bit
more on the math side and so you got to
be a little careful when you're quoting
F1 scores in the when you're sitting
there with all the shareholders because
a lot of them will just glaze over so
confusion Matrix metrics are performance
measures which help us find the accuracy
of our classifier there are four main
metrics accuracy the accuracy is used to
find the portion of the correctly
classified values it tells us how often
our classifier is right it is the sum of
all True Values divided by the total
values
and this makes sense uh again it's one
of those
things I don't want to F you know
depends on what you're looking for are
you looking for uh not to miss any spam
mails are you looking to drive down the
road and not run anybody over Precision
is used to calculate the model's ability
to classify positive values correctly it
answers the question when the model
predicts a positive value how often is
it right it is the true positive divided
by the total number of predicted
positive values again this one uh
depends on what project you're working
on whether this is what you're going to
be focusing on uh so recall it is used
to calculate the model's ability to
predict positive values how often does
the model actually predict the correct
positive values it is the true positives
divided by the total number of actual
positive values and then your F1 score
it is the Harmon IC mean of recall and
precision it is useful when you need to
take both precision and recall into
account consider the following two
confusion Matrix derived from two
different classifier to figure out which
one performs better we can find the
confusion Matrix for both of them and
you can see we're back to uh does it
classify whether they can speak English
or or non-speaker they speak some they
don't know the English language and so
we put these two uh uh confusion
matrixes out here we can go ahead and do
the math behind that we can look up the
accuracy that's a tpn plus TN over the
TF plus TN plus FP plus FN and so we get
an accuracy of
8125 and we have a Precision if you do
the Precision which is your TP truth
positive over TP plus
FP uh we get
891 and if we do the call we'll end up
with the 0825 that's your TP over TP
plus FN and then of course your F1 score
which is 2 * Precision Time recall over
Precision plus
recall and we get the 0
857 and if we do that um with another
model let's say we had two different
models and we're trying to see which one
we want to use uh for whatever reason uh
we might go ahead and compute the same
things we have our accuracy our
precision and our recall and our F1
score and uh as we're looking at this we
might uh look at the accuracy because
that's really what we're interested in
is uh how many people are we able to
classify as being able to speak English
I really don't want to know if I you
know I I I I really don't want to know
if I if they're non-speakers um I'd
rather Miss 10 people speaking English
instead of 15 and so you can see from
these charts we'd probably go with the
first model because it does a better job
guessing who speaks English and has
higher accuracy because in this case
that is what we're looking
for so uh with that we'll go ahead and
pull up a demo so you can see what this
looks like in the python uh setup in in
the actual coding for this we'll go into
Anaconda Navigator if you're not
familiar with Anaconda uh it's a really
good tool to use as far as doing display
and demos and for quick development um
as a data scientist I just love the
package now if you're going to do
something heavier lifting uh there's
some limitations with anaconda and with
the setup but in general you can do just
about anything in here with your Python
and for this we'll go with Jupiter
notebook uh Jupiter lab is the same as
Jupiter notebook you'll see they now
have integration with uh py charm if you
work in py charm uh certainly there's a
lot of other Integrations that Anaconda
has and we've opened up um I simply
learn files I work on and create a new
file called confusion Matrix demo
and the first thing we want to note is
the data we're working with uh here I've
opened it up in a word pad or notepad or
whatever uh you can see it's got a row
of uh headers uh comma separated and
then all the data going down below and
then I saved this in the same file so I
don't have to remember what path I'm
working on uh of course if you have your
data separated and you're working with a
lot of data you probably want to put in
a different folder or file depending on
what you're doing and the first thing we
want to do is go ahead and import our
tools uh we're going to use the pandas
that's our data frame if you haven't had
a chance to work with the data frame
please review Panda data
frame and then we're going to use uh the
denoted as SK learn and I just pull this
in you can see here's the site learn.org
with the stable version that you can
import into your
Python and from here we're going to use
the train test split for splitting our
data we're going to do some
pre-processing we're going to do use the
logist regression
model the confusion Matrix and the
classifier report so let me go ahead and
run that and bring all that information
in and just like we opened the file we
need to go ahead and load our data in
here uh so we're going to go ahead and
do our pandas read
CSV and then just because we're in
jupyter Notebook we can just put data to
read the data in here a lot of times
Well actually let me just do this I
prefer to do the just the head of the
data or the top
part and you can see we have age sex um
I'm not sure what CP stands for test BPS
colesterol uh so a lot of different
measurements uh if you were in this
domain you'd want to know what all these
different measurements mean
I don't want to focus on that too much
because when we're talking about data
science a lot of times you have no idea
what the data means if you've ever
looked up the breast cancer measurement
it's just a bunch of measurements and
numbers uh unless you're a doctor you're
going to have no idea what those
measurements mean but if it's your
specialty and your domain you better
know them so we're going to go ahead and
create Y and it's going to we're going
to set it equal to the Target uh so
here's our Target value here and it's
either one or
zero so we have a classifier if you're
dealing with one zero true false what do
you have you have a
classifier and then our X is going to be
uh everything except for the Target uh
so we're going to go ahead and drop the
target axis equals one remember that's
columns versus uh the end index for rows
ax is equal zero would would give you an
error but you would drop like row two
and then we'll go ahead and just print
that out so you can see what we're
looking at and uh here we have um Y data
X data uh and you can see from the X
data we have the X head and we can go
ahead and just do
print the Y head
data and run
that so this is all loading the data
we've done so far uh if there's a
confusion in there go back and rewind
the tape and review it and then we need
to go ahead and split our data into our
X train X test y train y test and then
keep in mind you always want to split
the data before we do the scaler and the
reason is is that uh you want the scaler
on the training data uh to be set on the
training data data or fit to it but not
on the test data think of this as being
out in the field uh you're not it could
actually alter your results uh so it's
always important to do make sure
whatever you do to the training data or
whatever um fit you're doing is always
done on the training not on the test and
then we want to go ahead and scale the
data now we are working with um linear
regression model I'll mention this here
in a minute when we get to the actual
model uh so some sometimes you don't
need to scale it when you're working
with linear regression models it's not
going to change your result as much as
say a neural network where it has a huge
impath uh but we're going to go ahead
take we create our the scale is going to
fit the X TR and we're take TR transform
it and to take our X test and transform
it based on the scale on here so that
our X is now between that nice minus one
to one and so this is all uh our pre
data setup and hopefully all of that
looks fairly familiar to you if you've
done a number of our other classes and
you're up to the setup on
here and then we want to go ahead and do
is create our model and we're going to
use the logistic regression model and
from the logistic regression model uh
we're going to go ahead and fit our X
train and Y train and then we'll run our
predicted value on here and so let's go
ahead and run that and so now we are uh
we actually have like our X test and our
prediction so if you remember from
our Matrix we're looking for the actual
versus the prediction and how those
compare and if I take this back up here
you're going to notice that we imported
the accuracy score the confusion Matrix
and the classification report uh and
there's of course our logistic
regression the model we're using for
this and I did mention I was going to
talk a little bit about scaler and the
regression
model the scaler on a lot of your
regression models uh your basic Mass
standard regression models and I'd have
to look it up for the logistic
regression model when you're using a
standard regression model you don't need
to scale the data uh it's already just
built in by the way the model
Works in most Cas uh but if you're Neal
Network and there's a lot of other
different setups then you really want to
take this and fit that on
there and so we can go in and do the
accuracy uh and this is if you remember
correctly we were looking at the
accuracy with the English speaking uh so
this is saying our accuracy as to
whether this person is I believe this is
the heart data
set
um it's going to be accurate about 85%
of the time as far as whether it's going
to predict the person's going to have um
heart condition or the one as it comes
up with the 01 on there which would mean
at this point that you have an 85% uh
being correct on telling someone they're
extremely high risk for a heart attack
kind of
thing and so we want to go ahead and uh
create our confusion Matrix and let me
just do
that of course the software does
everything for us so we'll go ahead and
run this and you can see right here um
here's our 25
uh prediction uh correct predictions
right
here and if you remember from our slide
I'll just bring this over so it's a nice
visual we have our true positive false
positive uh so we had 25 which were true
that it said hey this person's going to
be high risk at um heart and we had four
that were still high risk that it said
were false um so out of these 25 people
or out of these 29 people and that that
makes sense cuz you have 085 uh out of
29 people it was correct on 25 of them
and so uh here's our accuracy score we
were just looking at that our accuracy
is your true positive and your true
negative over all of them so how true is
it there was our accuracy um coming up
here 085 and then we have our nice
Matrix generated from that uh and you
can see right here is a similar Matrix
we had going for from the slide and this
starts to this should start asking
questions at this point um so if you're
in a board meeting or you're working
with this you really want to start
looking at this data here and saying
well is this good enough is uh this
number of people and hopefully you'd
have a much larger data set it my is my
confusion Matrix showing for the true
positive and uh false positive is that
acceptable for what we're doing uh and
of course if you're going to put
together whatever data you're putting
out you might want to separate the uh
true negative false positive false
negative true positive and you can
simply do that uh by doing the confusion
Matrix uh and then of course the Ravel
part let you um set that up so you can
just split that right up into a nice
tupal and the final thing we want to
show you here in the coding on this part
is the confusion Matrix
metrix and so we can come in here and
just use the Matrix equals
classification report the Y test and the
predict and then we're going to take
that classification report and go ahead
out and you can see here do a nice job
uh giving you your accuracy uh your
micro average your weighted average um
you have your Precision your recall your
F1 score and your support all in one
window so you can start looking at this
data and saying oh okay
83 uh 87 for getting a a positive and 83
for the negative side for a zero and we
start talking about whether this is a
valid information or not to use and when
we're looking at a heart attack
prediction we're only looking at one
aspect what's the chances of this person
having a heart attack or not um you
might something back to langu maybe also
know whether they speak English or Hindi
or French and you can see right here
that we can now take our confusion
Matrix and just expand it as big as we
need to depending on how many different
classifiers we're working on decision
tree important terms before we dive in
further we need to look at some basic
terms we need to have some definition
techs to go with our decision tree in
the different parts we're going to be
using we'll start with entropy entropy
is a measure of Randomness or
unpredictability in the data set for
example we have a group of animals in
this picture there's four different
kinds of animals and this data set is
considered to have a high entropy you
really can't pick out what kind of
animal it is based on looking at just
the four animals as a big clump of of uh
entities so as we start splitting it
into sub our second definition which is
Information Gain information it's a me
of decrease
this Cas based on the yellow we split
yellow as false as we continue down the
yellow side we split based on the height
true or false equals 10 and on the other
side is l 10 true or false and as you
see as we split it the entropy continues
to be less and less and less and so our
Information Gain is simply the entropy
E1 on the top and how it's changed E2
bottom we look at the deeper math
although you really don't need to know a
huge amount of math when you actually do
the programming in Python cuz they'll do
it for you but we'll look on the actual
math how compute entropy finally we went
over the different parts of tree and
they call the leaf node Leaf node
carries the classification or the
decision so it's a final end at the
bottom the decision node has two or more
branches this is where we're breaking
the group up into different parts and
finally you have the root node the
topmost decision node is known as the
root
node how does a decision tree work
wonder what kind of animals I'll get the
jungle today maybe you're the hunter
with a gun or if you're more into
photography you're a photographer with a
camera so let's look at this group of
animals and let's try to classify
different types of animals based on
their Fe
using a decision tree so the problem
statement is to classify the different
types of animals based on their features
using a decision tree the data set is
looking quite messy and the entropy is
high in this case so let's look at a
training set or a training data set and
we're looking at color we're looking at
height and then we have our different
animals we have our elephants our
giraffes our monkeys and our tigers and
they're of different colors and shapes
let's see what that looks like and how
do we split the data we have to frame
the conditions that split the data in
such a way that the Information Gain is
the highest note gain is the measure of
decrease in entropy after splitting so
the formula for entropy is the sum
that's what this symbol looks like that
looks like kind of like a uh e funky e
of K where I equals 1 to k k would
represent the number of animal the
different animals in there where value
or P value of I would be the percentage
of that animal times the log base 2 of
the same the percentage of that animal
let's try to calculate the entropy for
the current data set and take a look at
what that looks like and don't be afraid
of the math you don't really have to
memorize this math just be aware that
it's there and this is what's going on
in the background and so we have three
giraffes two tigers one monkey two
elephants a total of eight animals
gathered and if we plug that into the
formula we get an entropy that equals 3
over 8 so we have three drafts a total
of eight times the L usually they use
base two on the log so log base 2 of 3
over8 plus in this case let's say it's
the elephants 2 over8 two elephants over
total of 8 * log base 2 2 over 8 plus
one monkey over total of 8 log base 2 1
over 8 and plus 2 over 8 of the Tigers
log base 2 over 8 and if we plug that
into our computer our calculator I
obviously can't do logs in my head we
get an entropy equal to
.571 the pro will actually calculate the
entropy of the data set similarly after
every split to calculate the gain now
we're not going to go through each set
one at a time to see what those numbers
are we just want you to be aware that
this is a Formula or the mathematics
behind it gain can be calculated by
finding the difference of the subsequent
entropy values after a split now we will
try to choose a condition that gives us
the highest gain we will do that by
splitting the data using each condition
and checking that the gain we get out of
them the condition that gives us the
highest gain will be used to to make the
first split can you guess what that
first split will be just by looking at
this image as a human it's probably
pretty easy to split it let's see if
you're right if you guessed the color
yellow you're correct let's say the
condition that gives us the maximum gain
is yellow so we will split the data
based on the color yellow if it's true
that group of animals goes to the left
if it's false it goes to the right the
entropy after the splitting has
decreased considerably however we still
need some splitting at both the branches
to attain an entropy value equal to zero
so we decided to split both the nodes
using height as a condition since every
Branch now contains single label type we
can say that entropy in this case has
reached the least value and here you see
we have the giraffes the Tigers the
monkey and the elephants all separated
into their own groups this tree can now
predict all the classes of animals
present in the data set with 100%
accuracy that was easy use case loan
repayment prediction let's get into my
favorite part and open up some Python
and see what the programming code and
the scripting looks like in here we're
going to want to do a prediction and we
start with this individual here who's
requesting to find out how good his
customers are going to be whether
they're going to repay their loan or not
for his bank and from that we want to
generate a problem statement to predict
if a customer will repay loan amount or
not and then we're going to be using the
decision tree algorithm in Python let's
see what that looks like and let's dive
into the code in our first few steps of
implementation we're going to start by
importing the the necessary packages
that we need from Python and we're going
to load up our data and take a look at
what the data looks like so the first
thing I need is I need something to edit
my Python and run it in so let's flip on
over and here I'm using the Anaconda
Jupiter notebook now you can use any
python IDE you like to run it in but I
find the jupyter notebooks really nice
for doing things on the Fly and let's go
ahead and just paste that code in the
beginning and before we start let's talk
a little bit about what we're bringing
in and then we're going to do a couple
things in here we after to make a couple
changes as we go through this first part
of the import the first thing we bring
in is numpy as NP that's very standard
when we're dealing with mathematics
especially with uh very complicated
machine learning tools you almost always
see the numpy come in for your num your
numers it's called number python it has
your mathematics in there in this case
we actually could take it out but
generally you'll need it for most of
your different things you work with and
then we're going to use pandas as PD
that's also a standard the pandas is a
data frame set and you can liken this to
taking your basic data and storing it in
a way that looks like an Excel
spreadsheet so as we come back to this
when you see NP or PD those are very
standard uses you'll know that that's
the pandas and I'll show you a little
bit more we explore the data in just a
minute then we're going to need to split
the data so I'm going to bring in our
train test and split and this is coming
from the sklearn package cross
validation in just a minute we're going
to change that and we'll go over that
too and then there's also the sk. tree
import decision tree classifier that's
the actual tool we're using remember I
told you don't be afraid of the
mathematics it's going to be done for
you well the decision tree classifier
has all that mathematics in there for
you so you don't have to figure it back
out again and then we have SK learn.
metrics for accuracy score we need to
score our our setup that's the whole
reason we're splitting it between the
training and testing data and finally we
still need the sklearn import tree and
that's just the basic tree function is
needed for the decision tree classifier
and finally we're going to load our data
down here and I'm going to run this and
we're going to get two things on here
one we're going to get an error and two
we're going to get a warning let's see
what that looks like so the first thing
we had is we have an error why is this
error here well it's looking at this it
says I need to read a file and when this
was written the person who wrote it this
is their path where they stored the file
so let's go ahead and fix
that and I'm going to put in here my
file path I'm just going to call it full
file name and and you'll see it's on my
C drive and this this very lengthy setup
on here where I stored the data 2. CSV
file don't worry too much about the full
path because on your computer it'll be
different the data. 2 CSV file was
generated by simply learn if you want a
copy of that you can comment down below
and request it here in the
YouTube and then if I'm going to give it
a name full file name I'm going to go
ahead and change it here to to
full file name so let's go ahead and run
it now and see what
happens and we get a
warning when you're coding understanding
these different warnings and these
different errors that come up is
probably the hardest lesson to learn so
let's just go ahead and take a look at
this and use this as a uh opportunity to
understand what's going on here if you
read the warning it says the cross
validation is depreciated so it's a
warning on it's being removed and it's
going to be moved in favor of the model
selection so if we go up here we have
sklearn Doc crossvalidation and if you
research this and go to sklearn site
you'll find out that you can actually
just swap it right in there with model
selection and so when I come in here and
I run it again that removes a warning
what they've done is they've had two
different Developers develop it in two
different branches and then they decided
to keep one of those and eventually get
rid of the other one that's all that is
and very easy and quick to
fix before we go any further I went
ahead and opened up the data from this
file remember the the data file we just
loaded on here the dataor 2. CSV let's
talk a little bit more about that and
see what that looks like both as a text
file because it's a comma separated
variable file and in a spreadsheet this
is is what it looks like as a basic text
file you can see at the top they've
created a header and it's got 1 2 3 4
five columns and each column has data in
it and let me flip this over cuz we're
also going to look at this uh in an
actual spreadsheet so you can see what
that looks like and here I've opened it
up in the open Office calc which is
pretty much the same as um Excel and
zoomed in and you can see we've got our
columns and our rows of data a little
easier to read in here we have a result
yes yes no we have initial payment last
payment credit score house number if we
scroll way
down we'll see that this occupies a,1
lines of code or lines of data with uh
the first one being a column and then
1,000 lines of
data now as a
programmer if you're looking at a small
amount of data I usually start by
pulling it up in different sources so I
can see what I'm working
with but in larger data you won't have
that option it'll just be um two too
large so you need to either bring in a
small amount that you can look at it
like we're doing right now or we can
start looking at it through the python
code so let's go ahead and move on and
take the next couple steps to explore
the data using python let's go ahead and
see what it looks like in Python to
print the length and the shape of the
data so let's start by printing the
length of the database we can use a
simple lend function from Python and
when I run this you'll see that it's a
th long and that's what we expected
there's a thousand lines of data in
there if you subtract the column head
and this is one of the nice things when
we did the balance data from the panda
read CSV you'll see that the header is
row zero so it automatically removes a
row and then shows the data separate it
does a good job sorting that data out
for us and then we can use a different
function and let's take a look at that
and again we're going to utilize the
tools in Panda and since the balance
uncore data was put as hand a data frame
we can do a shape on it let's go ahead
and run the shape and see that looks
like what about the shape link of data
we have a thousand lines it also tells
me there's five colums so we're looking
at the data we have five columns of data
and then let's take one more step to
explore the data we take a look at the
link and shap let's go ahead and use the
end module for head another thing in the
we can utilize put that on our sheet
here and we have print data set and
balance data. head this is a handa's
print statement of his own so it has it
own print feature in there and then we
went ahead and gave a label for a print
job here of data set just a simple print
statement and we run that let's just
take a closer look at that let me zoom
in here
there we go pandas did such a wonderful
job of making this a very clean readable
data set so you can look at the data you
can look the column headers you can have
it the first five lines of the data and
we always start with zero so we have
five lines we have 0 one 2 3 4 instead
of 1 2 3 4 5 that's a standard scripting
and programming set is you want to start
with the zero position and that is what
the data head does it pulls the first
five rows of data puts in a nice format
that you can look at and view very
powerful tool to view the data so
instead of going to flip open up an
Excel spreadsheet or open Office Cal or
trying to look at a word dock where it's
all scrunched together and hard to read
you can now get a nice open view of what
you're working with we're working with a
shape of a thousand long five wi so we
have five columns and we do the full
data head you can actually see what this
data looks like the initial payment last
payment credit scores house number so
let's take this now that we've explored
the data and let's start digging into
the decision tree so in our next step
we're going to train and build our data
tree and to do that we need to First
first separate the data out we're going
to separate into two groups so that we
have something to actually train the
data with and then we have data on the
side to test it to see how good our
model is remember with any of the
machine learning you always want to have
some kind of test set to to weigh it
against so you know how good your model
is when you distribute it let's go ahead
and break this toe down and look at it
in pieces so first we have our X and Y
where do X and Y come from well X is
going to be our data and Y is going to
be the answer or the target you can look
at it source andar Target in this case
we're using X and Y to denote the data
in and the data that we're actually
trying to guess what the answer is going
to be and so separate we can simply put
in x equals the balance of the data dot
values the first brackets means that
we're going to sel all the lines in the
database all data one through five
remember always start with zero zero is
a yes or no and that's whether the loan
went default or not so we want to start
with one if we go back up here that's
the initial payment and it goes all the
way through the house number well if we
look at 1 through five we can do the
same thing for Y which is the answers
and we're going to set that just equal
to the the zero row so it's just the
zero row and then it's all rows going in
there so now we've divided this into two
different data sets one of them with the
data going in and one with the
answers next we need to split the
data and here you'll see that we have it
split into four different parts the
first one is your X training your X test
your y train your y test simply put we
have X going in we're going to train it
we have to the answer train it with and
then we have X test test that data and
we have to know in the end what the Y
was supposed to be and that's where this
train test split comes in that we loaded
earlier in the modules this does it all
for us you see they set the test size
equal to .3 so it's roughly 30% will be
used in the test and then we use a
random state so it's completely random
which rows it takes out of there and
then finally we actually build our
decision and call theore entropy that's
the actual decision tree or decision
tree classifier and in here they've
added a couple variables which we'll
explore in just a minute and then
finally we
cre we fit the X train and since we know
the TR y train we go ah and put those in
and let's go ahead and run this and what
most of these sklearn modules do is when
you set up varable this set the C
decision tree classifier prints out
what's in that decision tree there's a
lot of variables you can play with in
here and it's quite beyond the scope of
this tutorial to go through all of these
and how they work but we're working on
entropy that's one of the we've added
this compl random state of 100 100% we
have a max depth of three now the max
depth if you remember above when we were
doing the different graphs of animals
means it's only going to go down three
layers before it stops and we have
minimal samples of leaves is five so at
least five Le and at least five in leav
with the result at the bottom now that
we've created our decision tree
classifier created it trained it let's
go ahead and apply it and see what that
looks like so let's go ahead and make a
prediction and see what that looks like
we're going to past our pred code in
here and before do here we have that
we're going to do and we're going to use
our variable that we created and then
you'll see predict and it's very common
in the SK learn modules that the
different tools have a PR a prediction
in this case we're going to put test
data in here now if you delivered this
for use an actual commercial use the
person's going to be pay them back or
not in this case so we need to test out
the data and just see how good our
sample is how good of our tree does
at put the Y predict print I dis as
easily
print 300 answers in here tell each
those lines of Y came out so let's move
on the next we're going to take this
learn all all the math we and let's go
ahead and go through that and see what
that means and what that looks like go
ahead and paste this in and let me zoom
in a little bit there we go you have a
nice full picture and we'll see here
we're just going to do a print accuracy
is and then we do the accuracy score and
this was something we imported um
earlier if you remember at the very
beginning let me just scroll up there
real quick so you can see where that's
coming from that's coming from here down
here from SK learnmetrics import
accuracy score and you could probably do
this very easily how accurate is it how
many out of 300 do we get right and so
we put in our y test that's WECT on and
then we put in our y predict
that's the answers we got and we're just
going to multiply that by 100 because
this is just going to give us an answer
as a decimal and we want to see it as a
percentage and let's run that and see
what it looks like and if you see here
we got an accuracy of 93.
66667 so when we look at the number of
loans and we look at
93.6 fitting to it so just a quick recap
on that we now have accuracy setup on
here and so we have created a model that
uses a decision tree algorithm to
predict whether a customer will repay
the loan or not the accuracy of the
model is about
94.6% the bank can now use this model
whether it should approve the loan
request from a particular customer or
not and so this information is really
powerful we might not be able to as
individuals understand all these numbers
because they have thousands of numbers
that come in but you can see that this
is a smart decision for the bank to use
a tool like this to help them to predict
how good their uh profits going to be
off of the loan balances and how many
are going to default or not how does a
random Forest work as a whole so to
begin our um random Forest classifier
let's say we already have built three
trees and we're going to start with the
first tree that looks like this just
like we did in the example this tree
looks at the diameter greater or equal
to three it's true otherwise it's false
so one side goes to the smaller diameter
one side goes to larger diameter and if
the color is orange it's going to go to
the right true we're using oranges now
instead of lemons and if it's red it's
going to go to the left false and we
build a second tree very similar but
split differently instead of the first
one being split by a diameter uh this
one when they created it if you look at
that first Bowl it has a lot of red
objects so it says is the color red
because that's going to bring our
entropy down the fastest and so of
course if it's true it goes to the left
if it's false it goes to the right and
then it looks at the shape false or true
and so on and so on and tree three is
the diameter equal to one and it came up
with this because there's a lot of
cherries in this bowl so that would be
the biggest
SPL to the right let's go ahead and
bring these three trees so you can see
them all in one image so this would be
three completely different trees
categorizing a fruit and let's take a
fruit you can't see the color on so it's
missing data remember one of the things
talked about earlier is that a random
for data you're missing Maybe person had
a blacka picture and we're look at this
and it's going to have um put a color in
there so ignore the color down there but
the diameter equals
we find out it grows in the summer
equals yes and the shape is a circle and
if you go to the right you can look at
what one of the decision trees did this
is the third one is the diameter greater
than equal to three is the color orange
well it doesn't really know on this one
but it if you look at the value say true
and it go to the right tree 2 classifies
it as cherries is a color equal red is a
shape a circle true it is a circle so
this would look at it and say oh that's
a cherry and is it di equal one well
that's false
true go a cherry and the third one says
H it's an orange and you can guess that
if you have two oranges and one says
it's a cherry uh when you add that all
together the majority of the vote says
orange so the answer is it's classified
as an orange even though we didn't know
the color and we're missing data on it I
don't know about you but I'm getting
tired of fruit so let's switch and I did
promise you we start looking at a case
example and get into some python coding
today we're going to use the case the
iris flower analysis
this is the exciting part as we roll up
our sleeves and actually look at some
python coding before we start the python
coding we need to go ahead and create a
problem statement wonder what species of
Iris do these flowers belong to let's
try to predict the species of the
flowers using machine learning Inon
let's see how it can be done so here we
begin to go ahead and Implement our
python code and you'll find that the
first half of our implementation is all
about organizing and exploring the data
coming in let's go ahead and take this
first step which is loading the
different modules into python andless we
put that in our favorite editor whatever
your favorite editor is in this case I'm
going to be using the Anaconda jupyter
notebook which is one of my favorites
certainly there's notepad++ and eclipse
and dozens of others or just even using
the python terminal window any of those
will work just fine to go ahead and
explore this python coding so here we go
let's go ahead and flip over to our
Jupiter notebook and I've already opened
up a new page for
python dat sets import load Iris this is
the load Iris and Iris popular it's been
around since 1936
when a random Forest classifier we need
go ahead import from the SK learn
module. Ensemble import ROM force
classifier and we want to bring in two
more modules um and these are probably
the most commonly used modules in Python
with any of the other modules we bring
in one pandas import pandas as PD PD is
a common term used for pandas and pandas
is basically creates a where you pandas
spret you see
minut mathematical sets on here we'll
see right off theat we're take our NP
we're see the this code doesn't actually
show anything we're going to go ahead
and run it because I need to make sure I
have all those loaded and let's take a
look at the next module on here this one
this code right here the script let's
get that over to our jup notebook and
here we go we've gone ahead and run the
Imports let's take a look
and dat to
Iris names and let's
here EXL spreadsheet we have set the
columns so on the top you can see the
colums and without knowing where this
data is coming from so let's look at the
bigger picture and I'm going to go print
I'm just going to change this for a
moment and we're going to print all of
irus and see what that looks like so
when I print all of irus I get this long
list of information and you can scroll
through here and see all the different
titles on there what's important to
notice is that first off there's a
brackets at the beginning so this is a
python diary and in a python dictionary
you'll have a after it so feature
names these colums if you scroll down
far dat is equal to the different data
we looking at a lot things in here like
T minut Target further down
that back to of pandas and P is you do
or the first five lines of the in this
case we have the column header features
and in here you'll see that we have 0 1
2 3 4 in Python most arrays always start
at zero so when you look at the first
five it's going to be 0 1 2 3 4 not 1 2
3 4 5 so now we've got our Iris dat
imported into a data frame let's take a
look at the next piece of code in here
and so in this section here
of the code we're going to take a look
at the Target and let's go ahead and get
this into our notebook this piece of
code so we can discuss it a little bit
more in detail so here we are in our
Jupiter notebook I'm going to put the
code in here and before I run it I want
to look at a couple things going on so
we have uh DF species and this is
interesting CU right here you'll see
where I have DF species in Brackets
which is uh the key code for creating
another column and here we have iris.
Target now these are both in the pandas
setup on here so in pandas we can do
either one I could have just as easily
done Iris and then in Brackets Target
depending on what I'm working on both
are um acceptable let's go ahead and run
this code and see how this changes and
what we've done is we target from the
Irish data set column on the
end now what species is this is what
we're trying to predict tell the answer
for all these different pieces and then
we've added a column with the answer
that way when we do our fin setup the
ability to program our our neural
network to look for this different data
and know what a Sosa is or a verac color
which we'll see in just a minute or
virginica those are the three that are
in there and now we're going to add one
more column we're organizing all this
data over and over again it's kind of
fun there's a lot of ways to organize it
what's nice about putting everything
onto one data frame is I can then and it
shows me exactly what I'm looking at and
I'll show you where you where that's
different where you can alter that and
do it slightly differently but let's go
ahead and put this into our script up to
that now and here we go we're going to
put that down here and we're going to
run that and let's talk a little bit
about what we're doing now we're
exploring data and one of the challenges
is knowing how good your model is did
your into two different parts the
training and the testing and so here
we're going to go ahead and put that in
our database so you can see it clearly
and we've set it DF remember you can put
brackets this is creating another column
is train so we're going to use part of
it for training and this equals NP
remember that stands for numpy random.
uniform so we're generating a random
number between zero and one and we're
going each com from so each row gets a
generated number and if it's less than
75 it's true and if it's greater than 75
it's false this means we're going to
take 75% of the data roughly there's
Randomness involved we're going to use
that to train it and the other 25% we're
going to hold off to the side and use
that to test it later on so let's play
on over and see what the next step is so
now that we've labeled our database for
which is training and which is testing
variables train and test and let's take
this code let's bring it into our
project here we past it down and before
I run this let's take a quick look of
what's going on here is we have up above
we created remember there's our def.
head which prints the first five rows we
added a column is train at the end and
so we're going to take that we're going
to create two variables we're going to
create two new data frames one's called
train one's called test 75% in train 25%
in test we're going to do
that and if DF is train equals true
that's going to go in the train and if
DF is train equals false it goes into
the test and so when I run this we're
going to print out the number in each
one let's see what that looks like and
you'll see that it puts 118 the training
module and it puts 32 in the testing
module which lets us know that there was
150 lines of data in here so if you look
at the original data you see there's 150
lines and that's roughly 75% one 25% for
us to test our model on afterward so
let's jump back to our code and see
where this goes in the next two steps we
want to do one more thing with our data
and let make it readable to humans I
don't know about you but I hate looking
at zeros and ones so let's start with
the features and let's go ahead and take
those and make those
readable let's see here we go paste it
in and you'll see here we've done a
couple B things data frame again this is
a panda thing the four of them 0 one two
three the first four areat you'll see
down here that it creates an index sea
length sea width pedal length and pedal
width titles going across here's the
first four line notebook
or this is the
same and the short hand is you just put
features in here if you're actually
writing a code and saving the script and
running it by remote thing but for this
we go ahead we'll just leave features
because it doesn't really matter and is
one of the fun thing about Jupiter
notebooks is I'm just building the code
as we go and then we need to go ahead
and create the labels for the other part
so let's take a look and see what that
for our final St testing is we're going
to go ahead and convert the species on
here into something the computer
understands so let's put this code into
our script and see where that takes
us all right here we go we set y equal
to pd.
factorize train species of zero so let's
break this down just a little bit we
have our pandas right here PD factorize
what is factorize doing I'm going to
come back to that in just a second let's
look at what trained species is and why
we're looking at the group zero on there
and let's go here and here is our
species remember this we created this
whole column here for species and then
it has
satos virginica and Vera color we need
to convert that into something the
computer understands zeros and ones so
the train species of zero because this
is in the format of a of an array of
arrays so you have to have the zero on
the end and then species is just that
column factorize goes in there and looks
at the fact that there's only three of
them so when I run this you'll see that
y generates an array that's equal to in
this case it's the training set and it's
zeros ones and twos representing the
three different kinds of flowers we have
so now we have something the computer
understands and we have a nice table
that we can read and understand and now
finally we get to actually start doing
the predicting so here we go uh we have
two lines of code oh my goodness that
was a lot of work to get to two lines of
code but there is a lot in these two
lines of code so let's take a look and
see what's going on here and put this
into our full script we're running and
let's paste this in here and let's take
a look and see what this is we have
we're creating a variable clf and we set
equal to the random forest classifier
and we're passing two variables in here
and there's a lot of variables you can
play with as far as these two are
concerned they're very standard in jobs
all that does is to prioritize it not
something really worry about usually
when you're doing this on your own
computer you do jobs equals 2 if you're
working a larger or big data and you
need to prioritize it differently this
is what that number does is it changes
your priorities and how it's going to
run across the system and things like
that and then the random state is just
how it starts zero is fine for
here but uh let's go ahead and run
this we also have cf. fit train features
comma Y and before we run it let's talk
about this a little bit more
cf. fit so we're fitting we're training
it we are actually creating our random
for classifier right here this is a code
that does everything and we're going to
take our training set remember we kept
test off to the side we're going to take
our training set with the features and
then we're going to go ahead and put
that in y so the Y is 01 and two we just
created and let's go ahe run
that and this is kind of an interesting
thing because it printed out the random
force
classifier and everything around it and
so when you're running this in your
terminal window or in a script like this
this automatically treats this like just
like when we were up here and I typed in
y and I printed out y instead of print y
this does the same thing it treats this
as a variable and prints it out but if
you were actually running your code that
wouldn't be the case and what is printed
out is it shows us all the different
variables we can change and if we go
down here you can actually see in jobs
equals 2 you can see the random State
equals z those two that we sent in there
you would really have to dig deep to
find out all these different meanings of
all these different settings on here
some of them are self-explanatory if you
kind of think about it a little bit like
Max features is auto so all the features
that we're putting in there is just
going to automatically take all four of
them whatever we send it it'll take some
of them might have features because
you're processing words there might be
like 1.4 million features in there
because you're doing legal documents
that's how many different words are in
there at that point you probably wanton
to limit the maximum features that
you're going to process and leaf nodes
that's the end nodes remember we had the
fruit and we're talking about the leaf
nodes like I said there's a lot in this
we're looking at a lot of stuff here but
you might have in this case there's
probably only think three leaf nodes
maybe four you might have thousands of
leaf notes which point you do need to
put a cap on that and say okay it only
go so far and we're going to use all of
our resources on processing this and
that really is most are about is process
and making sure we don't over the system
and there's another in
here we're not we're not going to have
like we're not going to continually to
train here that you'll want to look up
more detail from the SK learn and here
for today though all we need to do is
fit or train our features and our Target
why so now we have our training model
what's next if we're going to create a
model we need side the test feature test
group data so take this code into our
script and see what that looks
like represents the three type of
flowers the satsa the virginica and the
Versa color and what we're putting into
our predict is the test features and I
always kind of like to know what it is I
am looking at so real
quick
link colum features so features like
play with P data frames you'll see it's
an index so when you put index into
features into test it then takes those
columns and creates a panda data frames
from those columns and in this case
we're going to go ahead and put those
into our predict so we're going to put
each one of these lines of data the 5.0
3.4.2 and predict what our new four come
and this is whated predict
now that we've taken our test features
let's explore that let's see exactly
what that data means to us so the first
thing we can do with our predicts is we
can actually generate a different
prediction model when I say different
we're going to view it differently it's
not that the data itself is different so
let's take this next piece of code and
put it into our
script so we're pasting it in here and
you'll see that we're doing uh predict
and we've added underscore proba for
probability so there's f. predict
probability so we're we're running it
just like we ran it up here but this
time with this we're going to get
slightly different result and we're only
going to look at the first 10 so you'll
see down here instead of looking at all
of them uh which was uh what 27 you'll
see right down here that this generates
a much larger field on the probability
and let's take a look and see what that
looks like and what that means so when
we do the predict underscore pra for
probability it generates three numbers
so we had three leaf nodes at the end
and if you remember from all the theory
we did this is the predictors the first
one is predicting a one for satsa it
predicts a zero for Virgin and it
predicts a zero for Versa color and so
on and so on and so on and let's um you
know what I'm going to change this just
a little bit let's look at
10 to 20 just because we
can and we start to get in a little
different of data and you'll see right
down here it gets to this one this line
right here and this line has 0 0.5
0.5 and so if we're going to vote and we
have two equal votes it's going to go
with the first one so it says uh satsa
gets zero votes virginica gets 0.5 votes
versacolor gets .5 votes but let's just
go with the virginica since these two
are equal and so on and so on down the
list you can see how they vary on here
so now we've looked looked at both how
to do a basic predict of the features
and we've looked at the predict
probability let's see what's next on
here so now we want to go ahead and
start mapping names for the plants we
want to attach names so that it makes a
little more sense for us and that's what
we're going to do in these next two
steps we're going to start by setting up
our predictions and mapping them to the
name so let's see what that looks like
and let's go head and paste that code in
here and run it and this goes along with
the next piece of code so we'll skip
through this quickly and then come back
to it a little bit so here's
iris. Target
names and uh if you remember correctly
this was the the names that we've been
talking about this whole time the satsa
virginica versac color and then we're
going to go ahead and do the prediction
again we run we could have just set a
variable equal to this instead of
rerunning it each time but we're going
ahead and run it again cf. predict test
features remember that Returns the zeros
the ones and the twos and then we're
going to set that equal to predictions
so this time we're actually putting it
in a variable and when I run
this
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e e
it distributes and it it comes out as an
array and the array is sosa Sosa Sosa
Sosa Sosa we're only looking at the
first five we could actually do let's do
the first 25 just so we can see a little
bit more more on there and you'll see
that it starts mapping it to all the
different flower types the versacolor
and the virginica in there and let's see
how this goes with the next around
people although I do have a cat that
does that where dogs do and we can look
at these we can say we can evaluate the
sharpness of the claws how sharp are
their claws and we can evaluate the
length of the ears and we can usually
sort out cats from dogs based on even
those two
characteristics now tell me if it is a
cat or a dog odd question usually little
kids know cats and Hugs by
now
e
e
e
e
e
e
e
e e
unless they live a place where there's
not many cats or dogs so if we look at
the sharpness of the claws the length of
the ear
and we can see that the cat has smaller
ears and sharper claws than the other
animals its features are more like cats
it must be a cat sharp claws length of
ears and it goes in the cat group
because KNN is based on featur similar
the different wines they've tested and
where they fall on that graph based on
how much sulfur dioxide and how much
chloride K and KNN is a perimeter that
refers to the number of nearest
neighbors to include in the majority of
the voting process and so if we add a
new glass of wine there red or white we
want to know what the neighbors are in
this case we're going to put k equal 5
we'll talk about K in just a minute a
data point is classified by the majority
of votes from its five nearest neighbors
here the unknown point would be
classified as red since four out of five
neighbors are red so how do we choose K
how do we know k equals five I mean
that's was the value we put in there so
we're going to talk about
it e
how do we choose the factor K KN andn
algorithm is based on feature similarity
choosing the right value of K is a
process called parameter tuning and is
important for better accuracy so at k
equal 3 we can classify we have a
question mark in the middle as either a
as a square or not is it a square or is
it in this case a triangle and so if we
set K say this square
and depending on where that point is
that drastically changes your answer and
uh we jump here we go how do we choose
the factor of K you'll find this in all
machine learning choosing these factors
that's the face you get you like oh my
gosh did I choose the right K did I set
whatever you're looking at so that you
don't have a huge bias in One Direction
or the other and in terms of K andn the
number of K if you choose it too low the
bias is based on it's too cple things
going to pick those things and you might
get a skute answer and if you're
processing issues and resource issues so
we do the most common use and there's
other options for choosing K is to use
the square root of n so N is a total
number of values you have you take the
square root of it in most cases you also
if it's an even number so if you're
using uh like in this case squares and
triangles if it's even you want to make
your K value odd that help helps it
select better so in other words you're
not going to have a balance between two
different factors that are equal so
usually take the square root of N and if
it's even you add one to it or subtract
one from it and that's where you get the
K value from that is the most common use
and it's pretty solid it works very well
when do we use KNN we can use KNN when
data is labeled so you need a label on
it we know we have a group
pictures with dogs dogs cats cats data
is Noise free and so you can see here
when we have a class and we have like
underweight 140 23 Hello Kitty normal
that's pretty confusing we have a High
variety of data coming in so it's very
noisy and that would cause an issue data
set is small so we're usually working
with smaller data sets where I you might
get into gig of data if it's really
clean doesn't have a lot of noise
because KNN is a lazy learner I.E it
doesn't learn a discriminative function
from the training set so it's very lazy
so if you have very complicated data and
you have a large amount of it you're not
going to use the KNN but it's really
great to get a place to start even with
large data you can sort out a small
sample and get a an idea of what that
looks like using the KNN and also just
using for smaller data sets KNN works
really good how does the KNN algorithm
work consider a data set having two
variables height in centimeters and
weight in kilograms and each point is
classified as normal or underweight so
we can see right here we have two
variables you know true false they're
either normal or they're not they're
underweight on the basis of the given
data we have to classify the below set
as normal or underweight using KNN so if
we have new data coming in that says 57
kilog and 177 cm is that going to be
normal or underweight to find the
nearest neighbors we'll calculate the
ukian distance according to the ukian
distance formula the distance between
two points in the plane with the
coordinates XY and ab is given by
distance D equals the square root of x -
a^ 2 + y - b^ 2 and you can remember
that from the two edges of a triangle
where Computing the third Edge since we
know the X side and the yide let's
calculate it to understand clearly so we
have our unknown point and we placed it
there in red and we have our other
points where the data is scattered
around the distance D1 is a square root
of 170 - 167 2 + 57 - 51 2ar which is
about 6.7 and distance 2 is about 13 and
distance three is about 13.4 similarly
we will calculate the ukian distance of
unknown data point from all the points
in the data set and because we're
dealing with small amount of data that's
not that hard to do and it's actually
pretty quick for a computer and it's not
a really complicated Mass you can just
see how close is the data based on the
ukian distance hence we have calculated
the ukian distance of unknown data point
from all the points as showing where X1
and y1 equal 57 and 170 whose class we
have to classify so now we're looking at
that we're saying well here's the ukian
distance
who's going to be their closest
neighbors now let's calculate the
nearest neighbor at k equals 3 and we
can see the three closest neighbors puts
them at normal and that's pretty
self-evident when you look at this graph
it's pretty easy to say okay what you
know we're just voting normal normal
normal three votes for normal this is
going to be a normal weight so majority
of neighbors are pointing towards normal
hence as per K&N algorithm the class of
57170 should be normal so recap of knnn
positive integer K is specified along
with a new sample we select the K
entries in our database which are
closest to the new sample we find the
most common classification of these
entries this is the classification we
give to the new sample so as you can see
it's pretty straightforward we're just
looking for the closest things that
match what we got so let's take a look
and see what that looks like in a use
case in Python so let's dive into the
predict diabetes use case so use case
predict diabetes the objective predict
whether a person will be diagnosed with
diabetes are not we have a data set of
768 people who were or were not
diagnosed with diabetes and let's go
ahead and open that file and just take a
look at that data and this is in a
simple spreadsheet format the data
itself is comma separated very common
set of data and it's also a very common
way to get the data and you can see here
we have columns a through I that's what
1 2 3 four five 6 78 um eight columns
with a particular tribute and then the
ninth column which is the outcome is
whether they have diabetes as a data
scientist the first thing you should be
looking at is insulin well you know if
someone has insulin they have diabetes
because that's why they're taking it and
that could cause issue on some of the
machine learning packages but for a very
basic setup this works fine for doing
the KNN and the next thing you notice is
it it didn't take very much to open it
up um I can scroll down to the bottom of
the data there's
768 it's pretty much a small data set
you know at 769 I can easily fit this
into
my ram on my computer I can look at it I
can manipulate it and it's not going to
really tax just a regular desktop
computer you don't even need an
Enterprise version to run a lot of this
so let's start with importing all the
tools we need and before that of course
we need to discuss what IDE I'm using
certainly you can use any particular
editor for python but I like to use for
doing uh very basic visual stuff the
Anaconda which is great for doing demos
with the Jupiter notebook and just a
quick view of the an Konda Navigator
which is the new release out there which
is really nice you can see under home I
can choose my application we're going to
be using python 36 I have a couple
different uh versions on this particular
machine if I go under environments I can
create a unique environment for each one
which is nice and there's even a little
button there where I can install
different packages so if I click on that
button and open the terminal I can then
use a simple pip install to install
different packages I'm working with
let's go ahead and go back under home
and we're going to launch our notebook
and I've already you know kind of like
uh the old cooking shows I've already
prepared a lot of my stuff so we don't
have to wait for it to launch because it
takes a few minutes for it to open up a
browser window in this case I'm going
it's going to open up Chrome because
that's my default that I use and since
the script is pre-done you'll see I have
a number of windows open up at the top
the one we're working in and uh since
we're working on the KNN predict whether
a person will have diabetes or not let's
go and put that title in there and I'm
also going to go up here and click on
Cell actually we want to go ahead and
first insert to cell below and then I'm
going to go back up to the top cell and
I'm going to change the cell type to
markdown that means this is not going to
run as python it's a markdown language
so if I run this first one it comes up
in nice big letters which is kind of
nice remind us what we're working on and
by now you should be familiar with doing
all of our Imports we're going to import
the pandas as PD import numpy as NP
pandas is the panda data frame and numpy
is a number array very powerful tools to
use in here so we have of our Imports so
we've brought in our pandas our numpy
our two general python tools and then
you can see over here we have our train
test split by now youed to be familiar
with splitting the data we want to split
part of it for training our thing and
then training our particular model and
then we want to go ahead and test the
remaining data just see how good it is
pre-processing a standard scaler
pre-processor so we don't have a bias of
really large numbers remember in the
data we had like number pregnancies
isn't going to get very large where the
amount of insulin they take and get up
to 256 so 256 versus 6 that will skew
results so we want to go ahead and
change that so they're all uniform
between minus one and one and then the
actual tool this is the K neighbors
classifier we're going to use and
finally the last three are three tools
to test all about testing our model how
good is it me just put down test on
there and we have our confusion Matrix
our F1 score and our accuracy so we have
our two general python modules we're
importing and then we have our six
modules specific from the sklearn setup
and then we do need to go ahead and run
this so these are actually imported
there we go and then move on to the next
step and so in this set we're going to
go ahead and load the database we're
going to use pandas remember pandas is
PD and we'll take a look at the data in
Python we looked at it in a simple
spreadsheet but usually I like to also
pull it up so that we can see what we're
doing so here's our data set equals pd.
read PV that's a pandas command and the
diabetes folder I just put in the same
folder where my IPython script is if you
put in a different folder you need the
full length on there we can also do a
quick length of uh the data set that is
a simple python command Len for length
we might even let's go ahead and print
that we'll go print and if you do it on
its own line link. dat set in the
Jupiter notebook it'll automatically
print it but when you're in most of your
different setups you want to do the
print in front of there and then we want
to take a look at the actual data set
and since we're in pandas we can simply
do data set head and again let's go
ahead and add the print in there if you
put a bunch of these in a row you know
the data set one head data set two head
it only prints out the last one so I us
always like to keep the print statement
in there but because most projects only
use one data frame Panda's data frame
doing it this way doesn't really matter
the other way works just fine and you
can see when we hit the Run button we
have the 768 lines which we knew and we
have our pregnancies it's automatically
given a label on the left remember the
head only shows the first five lines so
we have zero through four and just a
quick look at the data you can see it
matches what we looked at before we have
pregnancy glucose blood pressure all the
way to Ag and then the outcome on the
end and we're going to do a couple
things in this next step we're going to
create a list of columns where we can't
have zero there's no such thing as zero
skin thickness or or zero blood PR
pressure zero glucose uh any of those
you'd be dead so not a really good
Factor if they don't if they have a zero
in there because they didn't have the
data and we'll take a look at that cuz
we're going to start replacing that
information with a couple of different
things and let's see what that looks
like so first we create a nice list as
you can see we have the values we talked
about glucose blood pressure skin
thickness uh and this is a nice way when
you're working with columns is to list
the columns you need to do some kind of
transformation on very common thing to
do and then for this particular setup we
certainly could use the there's some
Panda tools that will do a lot of this
where we can replace the na but we're
going to go ahead and do it as a data
set column equals data set column.
replace this is this is still pandas you
can do it direct there's also one that
that you look for your n a lot of
different options in here but the Nan
nump Nan is what that stands for is is
non doesn't exist so the first thing
we're doing here is we're replacing the
zero with a numpy none there's no data
there that's what that says that's what
this is saying right here so put the
zero in and we're going to replace zeros
with no data so if it's a zero that
means the person's well hopefully not
dead hopefully they just didn't get the
data the next thing we want to do is
we're going to create the mean which is
the in integer from the data set from
the column do mean where we skip Nas we
can do that that is a panda's command
there the skip na so we're going to
figure out the mean of that data set and
then we're going to take that data set
column and we're going to replace all
the
npn with the means why did we do that
and we could have actually just uh taken
this step and gone right down here and
just replace zero and Skip anything
where except you could actually there's
a way to skip zeros and then just
replace all the zeros but in this case
we want to go ahead and do it this way
so you can see that we're switching this
to a non-existent value then we're going
to create the mean well this is the
average person so if we don't know what
it is if they did not get the data and
the data is missing one of the tricks is
you replace it with the average what is
the most common data for that this way
you can still use the rest of those
values to do your computation and it
kind of just brings that particular
value or those missing values out of the
equation let's go ahead and take this
and we'll go ahead and run it doesn't
actually do anything so we're still
preparing our data if you want to see
what that looks like we don't have
anything in the first few lines so it's
not going to show up but we certainly
could look at a row let's do that let's
go into our our data set with printed
data set and let's pick in this case
let's just do glucose and if I run this
this is going to print all the different
glucose levels going down and we
thankfully don't see anything in here
that looks like missing data at least on
the ones it shows you can see it skipped
a bunch in the middle because that's
what it does if you have too many lines
in Jupiter notebook it'll skip a few and
and go on to the next in a data set let
me go and remove this and we'll just
zero out that and of course before we do
any processing before proceeding any
further we need to split the data set
into our train and testing data that way
we have something to train it with and
something to test it on and you're going
to notice we did a little something here
with the Panda's database code there we
go my drawing tool we've added in this
right here of the data set and what this
says is that the first one in pandas
this is from the PD pandas it's going to
say within the data set we want to look
at the iocation and it is all rows
that's what that says so we're going to
keep all the rows
but we're only looking at 0 column 0 to
8 remember column this is actually 0 to
7 it doesn't include the last one and
then we go down here to Y which is our
answer and we want just the last one
just column 8 and you can do it this way
with this particular notation and then
if you remember we imported the train
test split that's part of the SK learn
right there and we simply put in our X
and our y we're going to do random State
equals zero you don't have to
necessarily seed it that's a seed number
number I think the default is one when
you SE it I'd have to look that up and
then the test size test size is 0.2 that
simply means we're going to take 20% of
the data and put it aside so that we can
test it later that's all that is and
again we're going to run it not very
exciting so far we haven't had any print
out other than to look at the data but
that is a lot of this is prepping this
data once you prep it the actual lines
of code are quick and easy and we're
almost there with the actual writing of
our KNN we need to go ahead and do a
scale the data if you remember correctly
we're fitting the data in a standard
scaler which means instead of the data
being from you know 5 to 303 in one
column and the next column is 1 to six
we're going to set that all so that all
the data is between minus one and one
that's what that standard scaler does
keeps it standardized and we only want
to fit the scaler with the training set
but we want to make sure the testing set
is the X test going in is also
transformed so it's processing it the
same so here we go with our standard
scaler we're going to call it scor X for
the scaler and we're going to import the
standard scaler into this variable and
then our X train equals score x. fit
transform so we're creating the scaler
on the XT train variable and then our X
test we're also going to transform it so
we've trained and transformed the XT
train and then the X test isn't part of
that training it isn't part of the of
training the Transformer it just gets
transformed that's all it does and again
we're going to go and this if you look
at this we've now gone through these
steps all three of them we've taken care
of replacing our zeros for key columns
that shouldn't be zero and we replace
that with the means of those columns
that way that they fit right in with our
data models we've come down here and we
split the data so now we have our test
data and our training data and then
we've taken and we scaled the data so
all of our data going in now no we don't
TR we don't train train the Y part the Y
train and Y test that never has to be
trained it's only the data going in
that's what we want to train in there
then Define the model using K neighbors
classifier and fit the train data in the
model so we do all that data prep and
you can see down here we're only going
to have a couple lines of code where
we're actually building our model and
training it that's one of the cool
things about Python and how far we've
come it's such an exciting time to be in
machine learning because there's so many
automated tools let's see before we do
this let's do a quick length of and
let's do y we want let's just do length
of Y and we get 768 and if we import
math we do math. square root let's do y
train there we go it's actually supposed
to be X train before we do this let's go
ahead and do import math and do math
square root length of Y test and when I
run that we get
12.49 I want you to see show you where
this number comes from we're about to
use 12 is an even number so if you know
if you're ever voting on things remember
the neighbors all vote don't want to
have an even number of neighbors voting
so we want to do something odd and let's
just take one away we'll make it 11 let
me delete this out of here that's one of
the reasons I love Jupiter notebook
because you can flip around and do all
kinds of things on the fly so we'll go
ahead and put in our classifier we're
creating our classifier now and it's
going to be the K Neighbors classifier
in Neighbors is equal 11 remember we did
12 minus 1 for 11 so we have an odd
number of of neighbors P equals 2
because we're looking for is it are they
diabetic or not and we're using the
ukian metric there are other means of
measuring the distance you could do like
square square means value there's all
kinds of measure this but the ukian is
the most common one and it works quite
well it's important to evaluate the
model let's use the confusion Matrix to
do that and we're going to use the
confusion Matrix wonderful tool and then
we'll jump into the F1 score and finally
accuracy score which is probably the
most commonly used quoted number when
you go into a meeting or something like
that so let's go ahead and paste that in
there and we'll set the cm equal to
confusion Matrix y test y predict so
those are the two values we're going to
put in there and let me go ahead and run
that and print it out and the way you
interpret this is you have the Y
predicted which would be your title up
here we could do uh let's just do
p predicted across the top and act
going down actual it's always hard to to
write in here actual that means that
this column here down the middle that's
the important column and it means that
our prediction said 94 and prediction
and the actual agreed on 94 and 32 this
number here the 13 and the 15 those are
what was wrong so you could have like
three different if you're looking at
this across three different variables
instead of just two you'd end up with
the third row down here in the column
going down the middle so in the first
case we have the the and I believe the
zero is a 94 people who don't have
diabetes the prediction said that 13 of
those people did have diabetes and were
at high risk and the 32 that had
diabetes it had correct but our
prediction said another 15 out of that
15 it classified as incorrect so you can
see where that classification comes in
and how that works on the confusion
Matrix then we're going to go ahead and
print the F1 score we just run that and
you see we get a 69 in our F1 score the
F1 takes into account both sides of the
balance of false positives where if we
go ahead and just do the accuracy
account and that's what most people
think of is it looks at just how many we
got right out of how many we got wrong
so a lot of people when you're data
scientist and you're talking to other
data scientists they're going to ask you
what the F1 score the F score is if
you're talking to the general public or
the uh decision makers in the business
they're going to ask what the accuracy
is and the accuracy is always better
than the F1 score but the F1 score is
more telling it lets us know that
there's more false positives than we
would like on here but 82% not too bad
for a quick flash look at people's
different statistics and running an
sklearn and running the knnn the K
nearest neighbor on it so we have
created a model using KNN which can
predict whether a a person will have
diabetes or not or at the very least
whether they should go get a checkup and
have their glucose checked regularly or
not the print accurate score we got the
0 818 was pretty close to what we got
and we can pretty much round that off
and just say we have an accuracy of 80%
tells us it is a pretty fair fit in the
model so far so clear but a question
should be coming up we have our sample
data set but instead of looking like
this what if it looked like this where
we have two sets of data but one of them
occurs in the middle of another set you
can see here where we have the blue and
the yellow and then blue again on the
other side of our data line in this data
set we can't use a hyperplane so when
you see data like this it's necessary to
move away from a ond view of the data to
a two-dimensional view of the data and
for the transformation we use what's
called a kernel function the kernel
function will take the 1D input and
transfer it to a two-dimensional output
as you can see in this picture here the
one when transferred to a
two-dimensional makes it very easy to
draw a line between the two data sets
what if we make it even more complicated
how do we perform an svm for this type
of data set here you can see we have a
two-dimensional data set where the data
is in the middle surrounded by the green
data on the outside in this case we're
going to segregate the two classes we
have our sample data set and if you draw
a line through it's obviously not an
optimal hyper plane in there so to do
that we need to transfer the 2D to a 3D
array and when you translate it into a
three-dimensional array using the kernel
you can see where you can place a hyper
plane right through it and easily split
the data before we start looking at a
programming example and dive into the
script let's look at the advantage of
the support Vector machine we'll start
with high-dimensional input space or
sometimes referred to as the curse of
dimensionality we looked at earlier one
dimension two Dimension three dimension
when you get to a thousand dimensions a
lot of problems start occurring with
most algorithms that have to be adjusted
for the svm automatically does that in
high dimensional space one of the high
dimensional space one high dimensional
space that we work on is sparse document
vectors this is where we tokenize the
words in documents so we can run our
machine learning algorithms over them
I've seen ones get as high as 2.4
million different tokens that's a lot of
vectors to look at and finally we have
regularization parameter the
regularization parameter or Lambda is a
parameter that helps figure out whether
we're going to have a bias or
overfitting of the data whether it's
going to be overfitted to very specific
instance or it's going to be biased to a
high or low value with the svm it
naturally avoids the overfitting and
bias problems that we see in many other
algorithms these three advantages of the
support Vector machine make it a very
powerful tool to add to your repertoire
of machine learning tools now we did
promise you a used case study we're
actually going to dive into some Python
Programming and so we're going to go
into a problem statement and start off
with the zoo so in the zoo example we
have um family members going to the zoo
and we have the young child going dad is
that a group of crocodiles or alligators
well that's hard to differentiate and
zoos are a great place to start looking
at science and understanding how things
work especially as a young child and so
we can see the parents sitting here
thinking well what is the difference
between a crocodile and an alligator
well one crocodiles are larger in size
alligators are smaller in size snout
width the crocodiles have a narrow snout
and alligators have a wider snout and of
course in the modern day and age the
father sitting think thinking how can I
turn this into a lesson for my son and
he goes let a support Vector machine
segregate the two groups I don't know if
my dad ever told me that but that would
be funny now in this example we're not
going to use actual measurements and
data we're just using that for imagery
and that's very common in a lot of
machine learning algorithms and setting
them up but let's roll up our sleeves
and we'll talk about that more in just a
moment as we break into our python
script so here we arrive in our actual
coding and I'm going to move this into a
python editor in just a moment but let's
talk a little bit about what we're going
to cover first we're going to cover in
the code the setup how to actually
create our svm and you're going to find
that there's only two lines of code that
actually create it and the rest of it is
done so quick and fast that it's all
here in the first page and we'll show
you what that looks like as far as our
data because we're going to create some
data I talked about creating data just a
minute ago and so we'll get into the
creating data here and you'll see this
nice correction of our two blobs and
we'll go through that in just a second
and then the second part is we're going
to take this and we're going to bump it
up a notch we're going to show you what
it looks like behind the scenes but
let's start with actually creating our
setup I like to use the Anaconda jupyter
notebook because it's very easy to use
but you can use any of your favorite
python editors or setups and go in there
but let's go ahead and switch over there
and see what that looks like so here we
are in the Anaconda python notebook or
anaconda Jupiter notebook with python
we're using Python 3 I believe this is
3.5 but it should be worked in any of
your 3x versions and uh you'd have to
look at the sklearn and make sure if
you're using a 2X version an earlier
version let's go and put our code in
there and one of the things I like about
the Jupiter notebook is I go up to view
and I'm going to go ahead and toggle the
line numbers on to make it a little bit
easier to talk about and we can even
increase the size because this is edited
in in this case I'm using Google Chrome
Explorer and that's how it opens up for
the editor although anyone any like I
said any editor will work now the first
step is going to be our Imports and
we're going to import four different
parts the first two I want you to look
at are line one and line two are numpy
as NP and matplot library. pyplot as PLT
now these are very standardized Imports
when you're doing work the first one is
the numbers python we need that because
part of the platform we're using uses
that for the numpy array and I'll talk
about that in a minute so you can
understand why we want to use a numpy
array versus the standard python array
and normally it's pretty standard setup
to use NP for numpy the map plot library
is how we're going to view our data so
this has uh you do need the NP for the
SK learn module but the map plot library
is purely for our use for visualization
and so you really don't need that for
the svm but we're going to put it there
so you have a nice visual aid and we can
show you what it looks like that's
really important at the end when you
finish everything so you have a nice
display for everybody to look at and
then finally we're going to I'm going to
jump one ahead to line number four
that's the sklearn dod sets. samples
generator import make blobs and I told
you that we were going to make up data
and this is a tool that's in the SK
learn to make up data I personally don't
want to go to the zoo get in trouble for
jumping over the fence and probably get
eaten by the crocodiles or alligators as
I work on measuring their snouts and
width and length instead we're just
going to make up some data and that's
what that make blobs is It's a Wonderful
tool if you're ready to test your your
uh setup and you're not sure about what
data you're going to put in there you
can create this blob and it makes it
really easy to use and finally we have
our actual svm the sklearn import svm on
line three so that covers all our
Imports we're going to create remember I
used the make blobs to create data and
we're going to create a capital x and a
lowercase y equals make blobs in samples
equals 40 so we're going to make 40
lines of data it's going to have two
centers with a random State equals 20 so
each each each group is going to have 20
different pieces of data in it and the
way that looks is that we'll have under
X um an XY plane so I have two numbers
under X and Y will be0 one that's the
two different centers so we have yes or
no in this case alligator or crocodile
that's what that represents and then I
told you that the actual SK learner the
svm is in two lines of code and we see
it right here with clf equals svm do SVC
kernel equals linear and I set Cal to
one although in this example since we
are not uh regularizing the data because
we want it to be very clear and easy to
see I went ahead you can set it to a
thousand a lot of when you're not doing
that but for this thing linear because
it's a very simple linear example we
only have the two dimensions and it'll
be a nice linear hyper plane will'll be
a nice linear line instead of a full
plane so we're not dealing with a huge
amount of data and then all we have to
do is do cf. fit X comma Y and that's it
clf has been created and then we're
going to go ahead and display it and I'm
going to talk about this display here in
just a second but let me go ahead and
run this code and this is what we've
done is we've created two blobs you'll
see see the blue on the side and then
kind of an orangish uh on the other side
that's our two sets of data they
represent one represents crocodiles and
one represents alligators and then we
have our measurements in this case we
have like the uh width and length of the
snout and I did say I was going to come
up here and talk just a little bit about
our plot and you'll see PLT that's what
we imported we're going to do a scatter
plot that means we're just putting dots
on there and then look at this notation
I have the capital x and then in
brackets I have a colon comma 0o that's
from from numpy if you did that in a
regular array you'll get an error in a
python array you have to have that in a
numpy array it turns out that our make
blobs returns a numpy array and this
notation is great because what it means
is the first part is the colon means
we're going to do all the rows that's
all the data in our blob we created
under capital x and then the second
Point has a comma zero we're only going
to take the first value and then if you
notice we do the same thing but we're
going to take the second value remember
we always start with zero and then one
so we have column zero and column one
and you can look at this as our XY plots
the first one is the X plot and the
second one is the Y plot so the first
one is on the bottom 0 2 4 6 8 and 10
and then the second one X of the one is
the 4 5 6 7 8 9 10 going up the left
hand side s equal 30 is just the size of
the dot so we can see them instead real
tiny dots and then cmap equals plt.com
paired and you'll also see the t equal y
That's the color we're using two colors
01 and that's why we get the nice blue
and the two different colors for the
alligator and the crocodile now you can
see here that we did this the actual H
fit was done in two lines of code a lot
of times there'll be a third line where
we regularize the data we set it between
like minus one and one and we reshape it
but for this it's not necessary and it's
also kind of nice cuz you can actually
see what's going on and then if we
wanted to we wanted to actually run a
prediction let's take a look and see
what that looks like and to predict some
new data and we'll show this again as we
get towards the end of digging in deep
you can simply assign your new data in
this case I'm giving it a uh width and
length 34 and a width and length 56 and
note that I put the data as a set of
brackets and then I have the brackets
inside and the reason I do that is
because when we're looking at data it's
designed to process a large amount of
data coming in we don't want to just
process one line at a time and so in
this case I'm processing two lines and
then I'm just going to print and you'll
see cf. predict new data so the clf and
the predict part is going to give us an
answer and let's see what that looks
like and you'll see 01 so predicted the
first one the 34 is going to be on the
one side and the 56 is going to be on
the other side so one came out as a
alligator and one came out as a
crocodile now that's pretty short
explanation for this setup but really we
want to dig in and see what going on
behind the scenes and let's see what
that looks like
so the next step is to dig in deep and
find out what's going on behind the
scenes and also put that in a nice
pretty graph we're going to spend more
work on this than we did actually
generating the original model and you'll
see here that we go through a few steps
and I'll move this over to our editor in
just a second we come in we create our
original data it's exactly identical to
the first part and I'll explain why we
redid that and show you how not to redo
that and then we're going to go in there
and add in those lines we're going to
see what those lines lines look like and
how to set those up and finally we're
going to plot all that on here and show
it and you'll get a nice graph with the
what we saw earlier when we were going
through the theory behind this where it
shows the support vectors and the hyper
plane and those are done where you can
see the support vectors as the dash
lines and the solid line which is the
hyperplane let's get that into our
Jupiter notebook before I scroll down to
a new line I want you to notice line 13
it has Plot show and we're going to talk
about about that here in just a second
but let's scroll down to a new line down
here and I'm going to paste that code in
and you'll see that the plot show has
moved down below let's scroll up a
little bit and if you look at the top
here of our new Section 1 2 3 and four
is the same code we had before and let's
go back up here and take a look at that
we're going to fit the values on our svm
and then we're going to plot scatter it
and then we're going to do a plot show
so you should be asking why are we
redoing the same code well when you do
the plot show that blanks out what's in
the plot so once I've done this Plot
show I have to reload that data now we
could do this simply by removing it up
here rerunning it and then coming down
here and then we wouldn't have to rerun
these first four lines of code now in
this it doesn't matter too much and
you'll see the plot show was down here
and then removed right there on line
five I'll go ahead and just delete that
out of there cuz we don't want to blank
out our screen we want to move on to the
next setup so we can go ahead and just
skip the first four lines cuz we did
that before and let's take a look at the
ax equal pl. GCA now right now we're
actually spending a lot of time just
graphing that's all we're doing here
okay so this is how we display a nice
graph with our results and our data ax
is very standard not used variable when
you're talking about PLT and it's just
setting it to that axis the last axis in
the PLT they can get very confusing if
you're working with many different
layers of data on the same graph and
this makes it very easy to reference the
ax so this reference is looking at the
PLT that we created and we already
mapped out our two blobs on and then we
want to know the limits so we want to
know how big the graph is we can find
out the X limit and the Y limit simply
with the get X limit and get y limit
commands which is part of our met plot
library and then we're going to create a
grid and you'll see down here we have
we've set the variable XX equal to np.
linespace X limit 0 x limit 1 comma 30
and we've done the same thing for the
yspace and then we're going to go in
here and we create a mesh grit and this
is a numpy command so we're back to our
numbers python let's go through what
these numpy commands mean with the line
space and the mesh grid we've taken XX
small s XX equals NP line space and we
have our X limit zero and our X limit
one and we're going to create 30 points
on it and we're going to do the same
thing for the y axis now this has
nothing to do with our evaluation
it's all we're doing is we're creating a
grid of data and so we're creating a set
of points between zero and the X limit
we're creating 30 points and the same
thing with the Y and then the mesh grid
Loops those all together so it forms a
nice grid so if we were going to do this
say between the limit 0 and 10 and do 10
points we would have a 0 0 1 1 0 1 02 03
04 to 10 and so on you can just imagine
a point at Each corner of one of those
boxes and the mesh grid comb BS them all
so we take the y y and the XX we created
and creates the full grid and we've set
that grid into the Y Y coordinates and
the XX coordinates now remember when
we're working with numby and python we
like to separate those we like to have
instead of it being X comma 1 you know X
comma Y and then X2 comma Y 2 and in the
next set of data it would be a column of
x's and a column of y's and that's what
we have here is we have a column of y's
we put it as a capital y y and a column
of x Capital XX with all those different
points being listed and finally we get
down to the numpy vstack just as we
created those in the mesh grid we're now
going to put them all into one array XY
array now that we've created the stack
of data points we're going to do
something interesting here we're going
to create a value Z and the Z equals the
clf that's our uh that's our support
Vector machine we created and we've
already trained and we have a DOT
decision function and we're going to put
the XY in there so here we have all this
data we're going to put that XY in there
that data and we're going to reshape it
and you'll see that we have the XX do
shape in here this literally takes the
XX resets it up connected to the Y and
the Z value lets us know whether it is
the left hand side it's going to
generate three different values the zv
value does and it'll tell us whether
that data is a support Vector to the
left the hyper plane in the middle or
the support vector to the right so it
generates three different values for
each of those points and those points
have been reshaped so they're right on a
line on those three different lines so
we've set all of our data up we've
labeled it to three different areas and
we've reshaped it and we've just taken
30 points in each direction if you do
the math you have 30 * 30 so that's 900
points of data and we separated it
between the three lines and reshaped it
to fit those three lines we can then go
back to our matap plot Library where
we've created the ax and we're going to
create a contour and you see here we
have Contour Capital XX capital y y
these have been reshaped to fit those
lines Z is the labels so now we have the
three different points with the labels
in there and we can set the colors
equals K and I told you we had three
different labels but we have uh three
levels of data the alpha is just makes
it kind of seethrough so it's only 0
five of the value in there so when we
graph it the data will show up from
behind it wherever the lines go and
finally the line Styles this is where we
set the two support vectors to be dash
dash lines and then a single one is just
a straight line that's what all that
setup does and then finally we take our
ax. scatter we're going to go ahead and
plot the support vectors but we've
programmed it in there so that they look
nice like the Das dash line and the dash
line on that grid and you can see here
when we do the clf do support vectors we
are looking at column zero and column 1
and then again we have the S equal 100
so we're going to make them large and
the line width equals one face colors
equals none let's take a look and see
what that looks like when we show it and
you can see when we get down to our end
result it creates a really nice graph we
have our two support vectors and dash
lines and they have the near data so you
can see those two points or in this case
the Four Points where those lines nicely
cleave the data and then you have your
hyperplane down the middle which is as
far from the two different points as
possible creating the maximum distance
so you can see that we have our nice
output for the size of the body and the
width of the snout and we've easily
separated the two groups of crocodile
and alligator congratulations you've
done it we've made it of course these
are pretend data for our crocodiles and
alligators but this Hands-On example
will help you to encounter any support
Vector machine projects in the future
and you can see how easy they are to set
up and look at in depth hello everyone I
am M and welcome to Simply YouTube
channel today we are diving into the
fascinating world of lstms or long
short-term memory Network NS in the
machine learning if you are into Tech
and data you probably heard about lstms
they are a game changer for dealing with
sequential data so what's the big deal
about lstm well they are a special kind
of neural network designed to handle
task where data comes in sequence like
predicting stock prices understanding
human speech or even translating
languages traditional neural network
struggle with these task because they
can't remember information over long
sequence this is where lstms come to the
rescue lstms have a unique structure
with memory cells and three key Gates
input forgate and output Gates these
gates work together to decide what
information to keep what information to
throw away and what to Output this
clever mechanism allows lstm to remember
important details for long periods
making them super effective for task
where context really matters whether
it's predicting the next word in the
sentence or analyzing Trends in Time
series lsms have proven to be incredibly
powerful stick around as we explore more
about how they work and why they are
such a big deal in the machine learning
Bo in the meantime I would like to let
you know that we regularly post updates
on various Technologies if you are a
tech Enthusiast looking for latest Trend
consider subscribing to our YouTube
channel and press that Bell icon to
never miss any update from Simply learn
so great let's get started so now let's
start with what the LSM model is long
short-term memory is a type of recurrent
neural network RNN designed to capture
long-term dependencies in sequential
data lstms can process and analyze
sequential data like time series text
and speech they use memory cells and
gate to control information flow
allowing them to selectively retain or
discard information as needed thus
avoiding the vanishing gradient problem
found in traditional RNs lstms are
widely used in applications such as
natural language processing spee
recognition time series forecasting and
many more craving a career upgrade
subscribe like and comment
below dive into the link in the
description to FasTrack your Ambitions
whether you're making a switch or aiming
higher simply learn has your
back so before moving forward as you
guys know lstm is a concept of machine
learning and if you want to learn Ai and
ml from industry expert and want to gain
on Hands-On in top industry project then
try simply as post graduate program in
Ai and machine learning from P
University in collaboration with IBM
this core teaches in demand skills such
as machine learning deep learning and LP
computer vision reinforcement learning
chat gbt and many more so don't forget
to check out the course link from the
description box below and the pin
comment so let's get started so now what
is RNN RNN are a type of neural network
that are designed to process sequential
data they can analyze data with temporal
Dimensions such as time series speech
and text RN can do this by using a
hidden State pass from one time stem to
the next the next hidden state is
updated at each other time step based on
the input and the previous hidden State
RNN are able to capture short-term
dependencies in sequential data but they
struggle with capturing long-term
dependencies why the lstms are made so
moving forward let's discuss types of
LSM Gates so LSM models have three types
of gates the input gate the forgate gate
and the output gate so let's first
discuss the input gate the input gate
controls the flow of information into
the memory cell deciding what to store
the input gate determines which values
from the input should be updated in the
memory set it uses a sigmoid activation
function to scale the values between 0o
and one and then applies pointwise
multiplication to decide what
information to store next is forget G
controls the flow of information out of
the memory cell deciding what to discard
the foret gate decides what information
should be discarded from the memory cell
it also uses a zmo activation function
to scale the values between 0o and one
followed by Point wise multiplication to
determine what information to forget the
last one is output gate controls the
flow of information out of the lstm
deciding what to use for the output the
output gate determines the output of the
lstm M unit it uses a sigmoid activation
function to scale the values from 0 to
one then applies Point F multiplication
to produce the output of the lstm unit
so these get implemented using sigmoid
function are trained using back
propagation they open and close based on
the input and the previous hidden State
allowing the lstm to selectively retain
or discard information effectively
capturing long-term dependencies so now
let's discuss application of lstm lstm
models are high effective and used in
various application including video
analysis analyzing video frames to
identify action object and scenes the
second is language simulation tasks like
language modeling machine translation
and text summarization the third one is
time series prediction so predicting
future values in a Time series the
fourth is voice recognition tasks such
as speech to text transcription and
command
recognition the last one is sentiment
analysis classifying text sentiment as
positive negative or neural so there are
many more examples of LSD so now let's
move forward and understand LSM model
and how it works with example let's
consider the task of predicting the next
word in a sentence this is a common
application of lstm networks in natural
language processing so I will break it
down step by step using the analogy of
remembering a story and deciding what
comes next based on the context so
imagine you are reading a story as you
read you need to remember what has
happened so far to predict what what
might happen next so let's illustrate
with the simple example sentence the cat
sat on the dash so you want to predict
the next word which could be mat or roof
or something else an lstm Network helps
this make prediction by remembering
important parts of the story and
forgetting irrelevant details so now
let's dive into step by step process so
step by step exclamation using lstm the
first one is reading the story input
sequence as you read each word in the
word sentence you process it and store
relevant information for example the you
understand it's determiner CAD you know
it's a noun and the subject of the
sentence set indicates the action
performed by the subject on preposition
indicating the relationship between the
cat and the next noun so this sequence
diagram showing the words being read and
processed so second comes forget G As
you move through the sentence you might
decide that some details are no longer
important for instance you might decide
that knowing the is less important now
that you have get and said so the word
forget gets help discard this less
important information so this sequence
diagram you can see on the screen
showing how relevant information is
discarded so the third one input gate
when you read on you need to decide how
relevant this information is so this
sequence diagram in the screen is
showing how new information is
integrated with the last one okay the
fourth one is the cell state memory part
so this is like your memory of the story
so far it carries the information about
the subject cat and the action set on so
it updates the new information as you
read each word okay the cat set on the
retaining the important context so this
sequence diagram showing how the memory
is updated with the new information so
the last one is output gate when you
need to predict the next word the output
get helps you decide based on the
current memory Cate so it uses the
context the cat set on the so the
predict the next word might be mat
because cat and mat are often associated
with the context so it can predict
anything the cat sat on the table or on
the sofa anything but Matt why I'm
saying Matt because cat and Matt are
often associated in the same context so
this diagram is showing the pred ition
of the next word based on the current
memory so there are many applications
where you can use lstm in predictive
time series or next word in the sentence
so by the using of LSM Gates input gate
forget gate and output gate and updating
the cell State the network can predict
the next word in a sequence by
maintaining relevant context and
discarding unnecessary information this
is step-by step process allow LSM
Network to effectively handle sequence
and make accurate prediction based on
the context
today data analysis has taken a big leap
forward with the new AI tool called chat
jpd 40 that is developed by open and
this tool isn't just for chatting it can
look at large amounts of data and find
important patterns and information in
this tutorial we will show you how chat
GPT 4 can help with data analysis for
instance a small business might use it
to figure out which products are selling
best and predict what will be the
popular in future this Mak makes Chad
gp4 and 40 a great tool for anyone who
wants to make better decisions based on
data so join us to see how this powerful
AI can make analyzing data easier and
more effective so let's start exploring
the official document and those who want
to jump straight to data analysis using
chat j40 can directly jump to it with
the time stamp mentioned in the
description box craving a career upgrade
subscribe like and comment
below d into the link in the description
to FastTrack your Ambitions whether
you're making a switch or aiming higher
simply learn has your
back so here's the open a documentation
and you could see the new features
introduced with the CH GPD 40 so these
are the improvements uh one is the
updated and interactive bar graphs or
pie charts that you can create and these
are the features that you could see here
you could change the color you could
download it and what we have is you
could update the latest file versions
directly from Google Drive and Microsoft
One drive and we have the interaction
with tables and charts in a new
expandable view that I showed you here
that is here you can expand it in the
new window and you can customize and
download charts for presentations and
documents moreover you can create the
presentation also that will see in
further and here we have how data
analysis Works in chat jbt
you could directly upload the files from
Google Drive and Microsoft One drive I
will show you guys how we can do that
and where's this option is and we can
work on tables in real
time and there we have customized
presentation ready charts that is you
can create a presentation with all the
charts based on a data provided by you
and moreover a comprehensive security
and privacy feature so with that guys
we'll move to chat JB
and here we have the chat gbt 40 version
so before commencing guys there's a
quick info for you if you're one of the
aspiring data analyst looking for online
training and graduating from the best
universities or a professional who
elicits to switch careers with data
analytics by learning from the experts
then try giving a show to Simply learn
Purdue post-graduate program in data
analytics in collaboration with IBM you
can find the link in the description box
and pin comment so let's get started
with data analysis part so this is the
PIN section or the insert section where
you can have the options to connect to
Google Drive connect to Microsoft One
drive and you can upload it from the
computer this option was already there
that is upload from computer and you can
upload at least or at Max the 10 files
that could be around Excel files or
documents so the max limit is 10 and if
you have connected to Google Drive I'll
show you guys uh I'm not connecting you
but you guys can connect it to and you
could upload it from there also and
there's another cool update that is
ability to quote directly in your chat
uh so while chatting with chat gbt I'll
show you guys how we can do that and you
could find some new changes that is in
the layout so this is the profile
section it used to be at the left bottom
but now it's move to the top right and
make it more accessible than ever so
let's start with the data analysis part
and the first thing we need is data so
you can find it on kle or you could ask
chat gp4 to provide the data I'll show
you guys so this is the kagle website
you can sign in here and click on data
sets you can find all the data sets here
that would be around Computer Science
Education classification computer vision
or else you could move back to chat
GPD and you could ask the chat GP for
model to generate a data and provide it
in Excel format so we'll ask him we will
not ask him can you we'll just ask him
provide a data
set that I can use for data
analysis
and provide in CSV
format so you could see that it has
responded that I can provide a sample
data set and he has started generating
the data set
here so you could see that he has
provided only 10 rows and he is saying
that I will now generate this data set
in CSV format first he has provided the
visual presentation on the screen and
now he generating the CSV format so if
you want more data like if you want 100
rows or thousand rows you could specify
in the prompt and chat jpt will generate
that for
you so we already have the data I will
import that data you could import it
from here or else you can import it from
your Google Drive so we have a sales
data here we will open
it so we have the sales data here so the
first step we need to do is data
cleaning so this is the crucial step to
ensure that the accuracy of our analysis
is at its best so we can do that by
handling missing values that is missing
values can distort our analysis and here
chat gb4 can suggest methods to impute
these vales values such as using the
mean median or a sophisticated approach
Based on data patterns and after
handling the missing values we will
remove duplicates and outlier detection
so we'll ask CH
jpt clean the
data if
needed so we can just write a simple
prom that would be clean the data if
needed and this is also a new feature
you can see the visual presentation of
the data here that we have 100 rows here
and the columns provided that is sales
ID date product category quantity and
price per unit and total sales so this
is also a new feature that
okay uh we just headed
back we'll move back to our chat GPT
chat here
okay so here we are so you could see
that jjy has cleaned the data and he has
provided that it has checked for missing
values checked for duplicates and ensure
consistent formatting and he's
saying okay okay so now we will ask him
that
execute these
steps and
provide the clean
data as CHT has provided that these
would the steps to clean the data and
let's
see so he has provided a new CSV file
with the clean sales data we will
download
it and ask him to use the same file
only use this new
cleaned sales data CSV file
for further
analysis so you could see that he is
providing what analysis we can do
further but once our data is clean the
next step is visualization so
visualizations help us understand the
data better by providing a graphical
representation so the first thing we
will do is we will create a prompt for
generating the histograms and we'll do
that for the age distribution part so
we'll write a prompt that generate a
histogram generator histogram to
visualize the distribution of customer
ages to
visualize the
distribution of customer
ages and what I was telling you guys is
this code button if you just just select
the text and you would find this reply
section just click on that and you could
see that it has selected the text or
what you want to get all the prompts
started with chat jpd so we'll make it
Closs and you could see that it has
provided the histogram
here and these are the new features here
and we could see that he's providing a
notification that interactive charts of
this type are not yet supported that is
histogram don't have the color Change
option I will show you the color Change
option in the bar chart section so these
features are also new you can download
the chart from here only and this is the
expand chart if you click on that you
could see that you could expand the
chart here and continue chat with chat
GPT here so this is the interactive
section so you could see that he has
provided the histogram that is showing
the distribution of customer ages and
the age range
are from 18 to 70 years with the
distribution visualized in 15 bins that
he has created 15 bins Here and Now
moving to another visualization that
we'll do by sales by region so before
that I will open the CSV file that is
provided by the chat GPT so you guys can
also see what data he has
provided so this is the clean sales data
and you could see that we have columns
sales ID date product category quantity
price per item total sales region and
salesperson so now moving back to chat
chity so now we will create a bar chart
showing total sales by region so we'll
enter this prompt that create a bar
chart showing total
sales by
region so what we are doing here is we
are creating bar charts or histogram
charts but we can do that for only two
columns if we want to create these data
visualization charts we need two columns
to do so so you could see that he has
provided the response and created the
bar chart here and this is the
interactive section you could see that
here's an option to switch to static
chart if we click on that we can't like
we are not getting any information we
scroll on that and if I enable this
option you could see that I can visually
see how many numbers this bar is
indicating and after that we have the
change color
section you can change the color of the
data set
provided so we can change it to any
color that is provided here or you could
just write the color code
here and similarly we have other two
options that is download and that is the
expand chart
section and if you need uh what code it
has done to figure out this bar graph so
this is the code you could use any ID to
do so if you don't want the
presentations or the visualizations of
the bar charts here you could use your
ID and use the Python language and he
will provide the code for you just take
your data set and read it through pandas
and generate the bar
jarts so moving to next section that is
category wise sales section so here we
will generate a pie chart showing the
proportion of sales for each product
category so for that we'll write a
prompt generate a pie
chart showing the proportion of
sales for each product category
so you could see that it has started
generating the P chart and this is also
an interactive
section if you click on that you would
be seeing a static pie chart and if you
want to change the color you can change
for any section that could be clothing
Electronics furniture or
kitchen and similarly we have the
download section and the expand chart
section so this is how this new chat jpd
4 model is better than chat jpd
4 that you could use a more interactive
by charts you could change the colors
for that and you can just H over these
bar charts and found all the information
according to them so after this data
visualization now we'll move to
statistical analysis so this will help
us uncover patterns and relationships in
the data so the first thing we'll do is
correlation analysis and for that will
write the prompt analyze the correlation
between age and purchase amount so this
correlation analysis help us understand
the relationship between two variables
so this can indicate if older customers
tend to spend more or less so we will
find out that by analyzing the data and
we provide a PR to chat gyy that analyze
the correlation between age and purchase
amount
so let's see what it
provides uh so here's the response by CH
gity you could see a scatter plot that
shows the relationship between customer
age and total sales that is with a
calculated correlation coefficient of
approximately 0.16 so this indicates a
weak positive correlation between age
and purchase amount suggesting that as
customer age increases there's a slight
tendency for total sales to increase as
well so you could just see the scatter
PL here that if the age increases so it
is not correlated to sales as you would
see an empty graph here so till 40 to 50
years of age or the 70 years of age you
could find what amount they have spent
here that is the total sales accumulated
by these
ages so now mov to sales Trend so here
we will perform a Time series analysis
of purchase amount or the given dates so
what does this do is time series
analysis allows us to examine how sales
amount changes over time helping us
identify Trends and seasonal patterns so
for that we'll write a
prompt
performer time series analysis
of purchase
amount or given
dates so you could see that chat gbt has
provided us the response and here is the
time series plot showing total sales
over the given dates and each point on
the plot represents the total sales for
a particular day
so through this you can find out and the
businesses find out which is the
seasonal part of the year and where to
stock up the stocks for these kind of
dates and after that you could also do
customer segmentation
so what does this do is so we can use
clustering here to segment customers
based on age income and purchase amount
so clustering groups customers into
segments based on similarities this is
useful for targeted marketing and
personalized
services and after that we have the
advanced usage for data analysis here we
can draw a predictive modeling table and
do the Market Basket analysis and
perform a customer lifetime value
analysis so we will see one of those and
what we'll do is we'll perform a Market
Basket analysis and perform an
association rule mining to find
frequently bought together
products so the theor behind this is the
association rule mining helps identify
patterns of products that are often
purchased together aiding in Inventory
management and cross selling strategies
so for that we'll write a prompt that so
perform an association rule mining to
find frequently board together products
so for that we'll write a prompt here
performance Association rule
mining to find frequently bought
products
together so let's see for this prompt
what does CH4 respond to
us uh so you could see that he is
providing a code here but we don't need
a code here we need the
analysis
don't provide
code do the market pket
analysis and provide
visualizations so you could see that uh
chat JT has
provided the response that given the
limitations in this environment so he is
not able to do the market bus analysis
here so but he can help us how we can
perform this in an ID so he providing
you can install the required libraries
then prepare the data and here is
providing the example code so you could
see there are some limitations to chat
GT4 also that he can't do Advanced Data
analysis so you could use the code in
your ID and do the Market Basket
analysis there so there are some
limitations to char4 also
and now we will ask chat GPT can you
create a presentation based on the data
set and we'll provide a data set to it
also so we will provide a sample sales
data and we'll ask him can you create a
presentation or PowerPoint present
a based on this data
set and only
provide data
visualization
graphs so you can see that J GT4 has
started analyzing the data and he
stating that and he will start by
creating a data visualization from the
provided data set and compile them into
PowerPoint
presentation so you could see that CH4
has provided us the response and these
are all the presentations or the bar
graphs that he has created and now we
have downloaded the presentation here we
will open
that and here's the
presentation that is created by Chad JP
40 in today's video we are diving deep
into the world of machine learning
interview preparation as we GE up for
2024 it's crucial to be well prepared
for the questions that can come your way
in any machine learning interview we
have compiled 30 essential interview
questions and answers thoughtfully
categorized into beginner intermediate
and advanced levels craving a career
upgrade subscribe like and comment
below dive into the link in the
description to FastTrack your Ambitions
whether you're making a switch or aiming
higher simply learn has your
back but before we jump into question
and answer here is a quick info for you
if you want to learn Ai and ml from
industry experts then try simply learns
postgraduate program in Ai and machine
learning from Purdue University in
collaboration with IBM this course
teaches in demand skills such as machine
learning deep learning NLP computer
vision reinforcement learning generative
AI prom engineering chat GPT and many
more so don't forget to check out the
course link from the description box and
pin comment so without any further Ado
let's get started so let's start with
beginner level questions and number one
is what is machine learning so machine
learning is a subset of artificial
intelligence that involves the use of
algorithms and statistical models to
enable computer comps to perform task
without explicit instructions that is by
relying on patterns and interference and
now moving to number second question
that is what are the different types of
machine learning so the three main types
of machine learning are number one is
supervised learning and then comes
unsupervised learning and then there is
reinforcement learning now moving to
next question that is third that is what
is supervised learning so supervised
learning involves training a model on a
label data set which means each training
example is paired with an output lbel
the model learns to predict the output
from the input data now moving to the
fourth question that is what is
unsupervised learning so unsupervised
involve training a model on data that
does not have labeled responses the
model tries to learn the patterns and
the structure from the input data so
Guys these are the beginner level
questions and now we'll move to the
fifth question that is what is
reinforcement learning so reinforcement
learning is a type of machine learning
where an agent learns to make decisions
by performing actions and receiving
Rewards or penalties the goal is to
maximize the cumulative reward so now
moving to the sixth question that is
what is a model in machine learning so a
model in machine learning is a
mathematical representation of a real
well process it is trained on data to
recognize patterns and make predictions
or decisions based on new data so now
moving to seventh question that is what
is overfitting so overfitting occurs
when a machine learning model performs
well on the training data but poorly on
new unseen data it indicates that the
model has learned the noise and details
in the training data instead of the
actual patterns so now coming to
question number eight that is what is
underfitting so underfitting occurs when
a machine learning model is too simple
to capture the underlying patterns in
the data it performs poorly on both the
training data and new data now move to
the next question that is n9th question
and the question is what is a confusion
Matrix so confusion Matrix is a table
used to evaluate the performance of a
classification model it summarizes the
number of correct and incorrect
predictions made by the model and that
is categorized by each class now moving
to the 10th question that is what is
cross validation so cross validation is
a technique for assessing how the
results of a statistical analysis will
generalize to an independent data set it
involves partitioning the data into
subsets training the model on some
subsets and validating it on the
remaining subsets this was all about
that is the 10th question or the overall
1 to 10 questions for beginner level now
we'll move to intermediate level and
here we'll cover 10 questions so we'll
start with 11th question that is what is
a Roc curve so Roc that is receiver
operating characteristic curve it is a
graphical representation of a classifier
per performance across different
thresholds it plots the true positive
rate that is tpr against a false
positive rate that is fpr now moving to
12th question that is what is precision
and Reco so Precision is the ratio of
correctly predicted positive
observations to the total predicted
positives and recall is the ratio of
correctly predicted positive
observations to all actual positives so
the formula is precision equal to
tp/ TP Plus FP and the recall is tp/ TP
+ FN so now we'll move to the 13th
question that is what is the F1 score so
the F1 score is the harmonic mean of
precision and record it provides a
balance between the two matrics and is
useful when you need to balance
precision and recall F1 score is equal
to twice into Precision into recall and
that is divided by Precision plus Recall
now we'll move to 14th question and here
we will cover
regularization so the question is what
is regularization so it is a technique
used to prevent overfitting by adding a
penalty to the model's complexity and
the common types of regularization
include L1 that is lasso and L2 Ridge
regularization now we'll move to the
15th question that is what is the bias
variance tradeoff so the bias variance
trade-off is a fundamental issue in
machine learning that involes mods
balancing the error introduced by the
model's assumptions and the error due to
model complexity so a good model should
have low bias and low variance now we'll
move to the question number 16 that is
what is feature engineering so feature
engineering is the process of creating
new features or modifying existing ones
to improve the performance of a machine
learning model it involves techniques
like normalization encoding categorical
variables and creating interaction terms
so now we'll move to question number 17
and that is about gradient descent so
the question is what is gradient descent
and your answer is gradient descent is
an optimization algorithm used to
minimize the cost function in machine
learning models and it iteratively
adjust the model parameters in the
direction of the steepest Descent of the
coost function so with this we'll move
to the 18th question and that will cover
with the difference between bagging and
boosting so the question is what is
difference between bagging and boosting
and you could answer this with starting
with bagging that is bootstrap
aggregating that involves training
multiple models on different subsets of
the data and averaging their predictions
then comes boosting that involves
training models sequentially with each
new model focusing on correcting the
errors of the previous ones and then we
have the question number 19 that is what
is a decision tree so a decision tree is
a known parametric supervised learning
algorithm used for classification and
regression it splits the data into
subsets based on the value of input
features resulting in a tree like
structure of decisions Now we move to
question number 20 that is what is a
random Forest So Random Forest is an
ensemble learning method that combines
multiple decision trees to improve the
accuracy and robustness of the model it
builds each tree using a random subset
of features and data points and then
averages their predictions so so these
were the questions that are for the
intermediate level and these are just
the basic questions or I will just say
the theoretical questions that can be
asked in an interview so be prepared for
that now we'll move to the advanced
level interview questions and we'll
start with question number 21 and here
also we'll cover the 10 questions so
number one question or that is 21th
question and the question is what is a
support Vector machine so support Vector
machine is a supervised learning
algorithm used for classification and
regression it finds the optimal hyper
plane that maximizes the margin between
different classes in the feature space
and then comes question number 22 that
is what is principal component analysis
so principal component analysis is a
dimensionality reduction technique that
transforms High dimensional data into a
lower dimensional Space by finding the
directions that is principal components
that maximize the variance in the data
and then comes the question number 23
that is what is a neural network so a
neural network is a series of algorithms
that attempt to recognize underlying
relationships in a set of data through a
process that mimics the way the human
brain operates it consists of layer of
interconnected nodes or neurons and then
comes the question number 24 that is
what is deep learning so deep learning
is a subset of machine learning that
involves neural networks with many
layers that is deep neural networks and
it is particularly effective for tasks
like image and speech recog
now I'll move to question number 25 that
is what is convolutional neural network
that is CNN so we will start the answer
by answering the interviewer that a
convolutional neural network is a type
of deep learning model specifically
designed for processing structured grid
data like images it uses convolutional
layers to extract special features or
the spal features and patterns from the
input data now we'll move to the
question number 26 that is what is a
recurrent neural network or RNN so a
recurent neural network is a type of
neural network designed for sequential
data and it has connections that form
directed Cycles allowing it to maintain
a memory of previous inputs and process
sequences of data so this was all about
question number 26 and now we will cover
the question number 27 that is what is
the difference between batch gradient
descent and stochastic gradient descent
so batch gradient descent computes the
gradient of the cost function using the
entire training data set while
stochastic gradient descent that is SGD
computes the gradient using only one
training example at a time so SGD is
faster but noisier now we'll move to
question number 28 that is what is
Dropout in neural networks so Dropout is
a regularization technique used in
neural networks to prevent overfitting
and it involves randomly setting a
fraction of the neur Rons to zero during
training for forcing the network to
learn more robust features and now we'll
move to question number 29 and that will
be about transfer learning and your
question is what is transfer learning so
we'll answer this to the interviewer by
starting that transfer learning is a
technique in machine learning where a
model developed for one task is reused
as the starting point for a model on a
second related task it is particularly
useful when there is limited data
available for the second task now we'll
move to the last question and the 30th
question so that is what is a generative
ADV verial Network that is g so you can
start answering this so generative
adversary network is a type of deep
learning model consisting of two neural
networks a generator and a discriminator
that are trained simultaneously the
generator creates fake data while the
discriminator tries to distinguish
between real and fake data leading to
the generator producing increasingly
realistic data and these question
questions and answers are over and these
covers a wide range of topics in machine
learning and should help prepare for
interviews at various levels thank you
for joining simply learns course on
machine learning we hope you enjoyed
learning about this powerful technology
and its many uses now you have the
skills to create algorithms predict
outcomes and automate task these skills
are valuable and in high demand so
whether you are advancing in your
current career or exploring New
Opportunities mastering machine learning
gives you a significant Edge so keep
practicing and experimenting with what
you have learned good luck on your
machine Learning Journey and see you in
the next course thank you staying ahead
in your career requires continuous
learning and upskilling whether you're a
student aiming to learn today's top
skills or a working professional looking
to advance your career we've got you
covered explore our impressive catalog
of certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click here