foreign
YouTube channel today we are going to
create a chatbot using Python and before
proceeding I have an insight for you
guys that is if you are aiming to get
certified as an AI and I'm an engineer
and want to build a career in AI in that
case you can check this professional
certificate program in Ai and machine
learning that is in collaboration with
IBM and Purdue University
that is by simply learn that will open
the doors for the best care
opportunities to find the details of
this PCP program in Ai and machine
learning when check the link in the
description box below I will also update
the description and the pin comment with
the link
you can check this course
I will scroll you down for you
and
just give it a shot check it out with
the link in the description box below
and moving on
I have an quiz question for you guys and
your question is
among the following option identify the
one which is NoDa type of learning and
your options are the first option is
supervised learning second is semi
unsupervised learning third is
reinforcement learning and the fourth is
unsupervised learning please answer in
the comment section below and we'll
update the correct answer in the pen
comment you can pause the video give it
a thought and answer in the comment
section and today we'll train the
chatbot on a data set that will store in
a file will call intents and that would
be the Json file and the data set that
would contain path tons of words that
are associated with different intensive
texts
so for that
first we'll create a folder
and name it as
chat Port using python
so we will name as chat hold only
and here we have our intense file we'll
just
copy paste into a chatbot
and first here we'll open a command
prompt
first we'll create a virtual environment
because virtual environment is the best
thing when you are creating a project
your libraries won't desktop any of your
other projects or any other libraries
won't disturb you this project
so to create that
you will write python
minus m
n v and the name of the folder
or on which name you want to create a
virtual environment I will create with
the name chatbot
and it has been created
you can check here
then inside the chatbot folder we have
the chatbot and we have include lip and
scripts folder
that holds
all the python and its necessary files
that is interpreter and other files so
now we want to activate this
so to activate it
we'll write the name of the
virtual environment then we'll go inside
scripts and after that we'll write
activate to activate our virtual
environment you can see here that in
circular basis we have chatbot written
here this shows that our virtual
environment has been activated and now
we can install any libraries that we
want for this project
now before that this is the intense
Source oh sorry intents dot Json file
so for this
I will get you guys Visual Studio code
open
and I will show you guys the
intense.json file
so this is the Nintendo's intense node
Json file which hold the data set for
our chatboard so this data set contains
patterns of words that are associated
with different intents or texts after
that
we will create a file and name it as new
DOT py and there we will write the code
that will pre-process this data by
tokenizing levertising and removing stop
words
then we will create a bag of words that
will represent the data which is used to
train a neural network model the model
has three layers with the first layer of
being a dense layer with 128 neurons and
second layer being a Dropout layer and
the third layer being a dense layer the
number of neurons equal to the number of
intents
then this model will be compiled with
the stochastic gradient descent that is
SGD Optimizer with a learning rate of
0.01 and momentum of 0.9 and nestro
acceleration the model is then print
with the training data for 200 epochs
and the train model is saved in a file
that will name chat Port model
so first we'll start
with the Json file you guys can see the
Json file I have updated some data set
here I will just upload this file on the
GitHub page and you can download it if
you want to use this also
now moving on
now we will create a file and we'll
create the file
inside this chatbot
and we'll name it as new DOT py
and inside this file
we will perform all the other
operations that is tokenizing limitizing
and removing stop words
and before that the first thing we need
to do is we need to install and import
libraries as we have created the virtual
environment and we don't have any
libraries that is instead of built-in
library is installed into our virtual
environment so we will install the Nexus
is a necessity libraries
for our virtual environment
so
here we have created the Json file and
that has all the possible outcomes of
user interactions with a chatbot we
first need a set of tags that users can
use to categorize their queries these
tags include name age and many others
every new tag would require a unique new
pattern as we have the tag goodbye
thanks query greetings and these are the
patterns and these are the responses
that our chatbot would give us
so identifying these Trends can help the
chatbot train itself on how people query
about a chatbots name allowing it to be
more responsive the chatbot will return
pre-program responses that are these
responses
and now
we'll start with
installing and importing libraries
for that
we'll get back to our Command Prompt in
which you have activated the virtual
environment and the first Library we
need is numpy
so for that we'll write the command
installed
numpy
and here we are collecting the data and
if you want
uh like if you want to know the command
to install these libraries you can just
go to your browser
and you can open a new tab
and there you can write the name of the
library that is numpy and then you can
write
P by p i
that is the PIP website to install any
Library so it will show you the command
to install the libraries here we have we
can see that numpy is installed now in
store tensorflow
so for that we have written the one whip
install tensorflow
so it's downloading
and you can see that it's being
installed in our virtual environment
that we have named as chatbot
you can show you guys
how to get
numpy
pypi
so this is the website you can just
click on that and you can see the
command to install a particular Library
similarly you can do for tensorflow and
the next we will install is nltk
so we'll wait until it installs
now the tensorflow library has been
installed now I'll install the nltk
so for that I'm writing one pip
install
and press enter
so now it would be installed
is installed now I will come back to our
ID and I am using visual studio code as
you guys have seen it you can use any
that you have Hands-On and now we'll
start with importing the libraries and
we'll import required modules such as
random Json pickle numpy tensorflow and
ltk and we'll import
wordnet limitizer class that is from
nltk.stem module and will initialize an
instance of the worldnet limitizer class
and we'll load the Json data that is
from the intense.json file that you guys
have seen that we have stored the
data set into that and store it in a
variable and name it as intents
so we'll start with importing
first of all import random
and after that we will
import Json
and after that we'll import
so I have written equal
import critical and after that we will
import
numpy as NP
and after that we'll import
enter flow as TF
and here we will import nltk
and
from nltk dot stem
import word net limit ISO
okay
now
we have imported the Json file that is
used to utilize the work with Json data
then we have random that is for various
distributions and this module implements
pseudo random number generators and the
word net parameterizer it can lamidize
and tensorflow it's a multi-dimensional
array of elements that is represented by
the symbol
now
we will create our variable it is
limitizer
and
that will initialize it
and now we'll create a variable for
to load the intense.json file so I will
name it intense and we'll use Json
dot loads loaded
and we'll use open
and
intends to
Json
and
fight
to read it
okay
now we'll initialize empty lists that is
for Words classes and documents and now
we will also create a list of characters
to ignore that would be
also initialized into a list
so when initialize lists
for words
we have initialized now for classes
and after that we'll do for documents
and
now we'll create a list for ignore
letters
so inside this we will
all right what we need to ignore okay
so slash question mark
and exclamation mark
these would be neglected from a data set
that is Json file
we get
and over
so these would be neglected now we will
look through the intents and their
patterns and after that we'll tokenize
each pattern into a list of words using
nltk dot word underscore tokenized
that is if we open the
Json file you can see the tags patterns
and responses
now
and after adding each word in the word
list that is we have created
then we'll add a tuple containing the
word list and the intent tag to
documents then we'll add it into the
documents and if the intent tag is not
already in classes we'll add it to the
list if the intent tag is not already in
classes
so I will show you how we'll do it
so
for intent
in
intents
and for
pattern
in
tent
and
inside will pass patterns
we will create a variable and name it as
word list
and that would be equal to nltk
dot will use the word underscore
tokenize
recognize it and inside will pass the
pattern
after that we'll use
words dot extend
an inside will pass word list word list
and now
and now we'll append the documents
that is we'll add a tuple containing the
word list and then 10 text to documents
so for that we will append it it is
documents Dot
a band and inside and pass
word list
and
intent
tag
okay
and now if
intent
tag
not in classes
that is if it's not present in classes
then classes will be appended
and they would be dependent by
tax okay
now
we will limitize each word in words
using wordnet limitizer and we'll remove
the characters in ignore Letters From
words that is if there is ignore let us
present in any of the word will remove
them and then we'll sort the words in
alphabetical order and removing
duplicates using set and after that we
will be sorting the classes in
alphabetical order and removing
duplicates from set and after that we'll
pickling the sorted words and classes
list and save them to files called
words.pkl and classes dot pkl
respectively okay
so for that
first of all limitize each word in words
using wordnet parametizer
so for that
we will create a variable
and name it as words
and have
we'll use limitizer
create an list for that also rametizer
and we'll limitize use the limit as
function
and inside we will pass word
and
forward in words
not in ignore letters that is if it's
not present in known letters will sort
it out
so
ignore
shown
and that is we have named model letters
okay
and inside this
now we will sort this
words
equal to
sorted and inside will create a set
and inside we will create
classes
okay
no
what we will do in classes as
well
sort the classes only
that would be in set that is if there
are any repetitions
that would be animated and it would be
sorted
so we'll sort the classes in
alphabetical order and we'll remove any
duplicates
if they are present and nucleus they
would be removed now we will use
pickle and
use the term function
and inside will pass words
and will
move one
save words Dot pkl
ift we
and the next we'll do is
people don't dump that is all pickling
the sorted words and classes
that would be classes list and saving
them to files that would be words.pkl
and classes dot pkl okay
so now we have done for the words now
we'll do for the classes so for that we
will write pickle dot dump and inside
will pass
in which we want to
save them
so first we'll pass the classes
and after that we will
open
classes Dot pcal
and write WP
okay
and now
we'll initialize an empty list that
would be called training
and after that we'll create a list of
zeros with length equal to classes and
sort it in a variable called output
empty and looping through each document
in documents and will initialize an
empty list called back after that we'll
tokenize each word in the document and
limitizing it using wordnet limitizer
and we'll create a bag of words
representation of the document by
setting the value of each word in words
to one if it happens in the document and
0 otherwise
so to do that
we'll start by creating a
praying list
okay
and after that
all right output
empty
and do
length of classes
now we'll run a loop that is
we'll check in the document
for document and documents
will create a bag
and what
patterns
that is
document
and what patterns equal to
limitizer
Dot
limit is
and that is
word dot lower
okay
and
now
all that all right
for word and word patterns
not enough for Loop that is
forward and
words
and we'll abandon back
that is back Dot
append
and we'll show output row
and before output row we will back dot a
pen one first
that is banknote append one
and
that is
f
what
in board patterns
else
will backdoor depend zero
okay
we have done this dotument zero
now what we'll do is we will create a
variable and name it as output row
and inside we will create list
and that would be will pass output empty
okay
okay
okay
so now
we have created output row and now
inside
output row
open when it passes classes
and that would be
the index and
the particular document
okay and that should be
equal to one
and after that if it's there then we'll
append IT training dot append
and inside
back plus output run
okay
and after that load random dot Shuffle
and we'll pass training inside that and
training
equal to NP Dot
array
and
this sign will pass
training
after that
we'll create a train X
and inside when pass
training
colon
length
words
trained by
and
this will create a training list
and inside will pass colon
and length
of what
okay
now
we have created this we have created a
bag of words
for the representation of the document
by setting the value of each word in
words to one and if it appears in the
document and we have set it zero
otherwise
and now we will get a new sequential
Keras model which is a linear stack of
layers
for that we'll write model
equal to TF dot Keras
code
sequential
and after that we'll add a densely
connected neural network layer to the
model this layer has 128 units and the
input shape is determined by the length
of the first training example the
activation function used in this layer
is re value
so I will show you how to add this
we have added the model now the model
we'll add
and for that we'll write TF Dot
Keras dot layer
and that is the dense layer
and inside we have
128
and we'll
input shape
that is equal to
length of
train X
okay
and what will
pass is
X zero
and this
and here we will have activation
activation
and that is of r value
foreign
no
we will add a Dropout layer to the model
which will randomly set 50 of the inputs
to 0 at each update during training to
reduce overfitting so to do that we will
write
model
not add
TF dot Keras
Dot
layers
and which level
drop out
okay and by zero point
five that is 50 percent
okay it will set inputs to 0 for the
fifty percent inputs and now
we'll add another densely connected
neural network to the model
and this layer will have 64 units and
the activation function used in this
layer will be r value
okay
so for that we'll add model load
add
and inside we will pass TF plot
Keras node layers
note and we'll use the dense one
and inside will pass 64.
and activation
equal to re Lu
okay
that is done
and now we'll add another Dropout layer
to the model with the same configuration
as the previous one
that is model Dot
add
and inside pass TF dot Keras
dot layers
and this would be the dense layer
and inside will pass length
of
chain y
01
and set the activation equal to softmax
so this add some final densely connected
neural network layer to the model this
layer has a number of units equal to the
number of unique classes in the training
data and the activation function used in
this layer is softmax softmax ensures
that the outputs of the layer represent
a probability distribution over the
possible classes
so now we'll create a stochastic
gradient descent Optimizer with a
learning rate of 0.01 and momentum of
0.9 and stroke momentum maneuver
so for that
we'll write sjd
equal to TF dot Keras
Dot optimizers
and node SGD
and inside and right learning rate
that is equal to
0.01
and we'll set the momentum
that is 0.9
and next drop
that is equal to
true
so now
we have created a sdd
now
will compile the model specifying the
lows function as categorical cosentropy
the optimizer as the STD Optimizer
created earlier and the metric to be
used to evaluate this module as accuracy
to evaluate this model as accuracy we
will compile the model
so for that we will write
model
dot compile
and inside
pass loss equal to categorical
close entropy
and we will set the optimizer
equal to SUV
okay
and Matrix
equal to
accuracy
okay
now
this will train the model and no mainly
to
train it will create a variable and name
it as haste and to train this model
using the training data for 200 ebooks
with a batch size of 5. the verbose
parameter set to 1 specifies that
progress messages should be printed
during training the training data is
passed as numpy arrays to the model's
fit method so for that
we'll create a variable and named as
hist and inside will pass model Dot
fit
and inside will pass NP Dot
add a
inside will pass train X
and
P Dot
array and inside will pass train y
and
equal to 200
and
batch size
that will set as
5. and verbose
equal to 1 okay
so now we will save the train model to a
file and name the file something
so we will set what we'll name it
so to save this model
write model load save
and we'll write the name of this model
as
chat port
underscore simply learn
model dot H5
okay
and
pass test
so this will save the train model to a
file that is new
chatbot underscore simply learn model
dot H5 along with the training history
so
as this would be done we want something
to print
we will print executed
okay
so this will simply print the executed
to the console to indicate that the
model has been trained and saved
successfully
so I'll save this and run it
so we'll save this
any error or not modify model is
executed successfully
wait for it
okay so here you can see that it has
shown that it's running in Virtual
environment we have
turn it again
and see it has shown us some errors but
we'll wait
for the execution once again
yes so in 45th line okay
so this will be in Brackets
okay
[Music]
yeah for this okay
so let's hear this and run it again
again
and
run it again
so we have a new terminal here
and we are executing the file
so you could see
it is downloading
the models and it has printed executed
so our model is running fine
so now we'll create another file and
name it as chat.py
that will create a file cheat code.py
and here we'll execute the functioning
of chatbot and what it will provide us
back
so before that I will give you a quick
recap of the new DOT py file
so this new node py file is an
implementation of a simple chatbot using
tensorflow which is a machine learning
framework developed by Google and the
chatbot is trained using neural network
to classify user inputs in predefined
intents Android appropriate responses
based on the detected intake
and the code starts by importing the
necessary libraries including numpy
tensorflow and ltk the word net
limitizer from the analytical library is
used to reduce words to the base
to improve classification accuracy that
is called limitize the code also loads
the intense.json file which contains the
predefined intents and Associated
patterns and responses
now the code then prepares the data for
training by iterating through each
pattern in the intense.json file
tokenizing it into individual words
limitizing the words and adding them to
a list called words
in each pattern and its Associated
intent are also added to a list called
documents additionally a list called
classes is created to store the
different intents next the words list is
clean by removing any punctuation
characters and duplicates
and then sorted alphabetically similarly
the classes list are sorted these clean
and sorted lists are saved to files
called
words.pkl and classes Dot pkl
respectively using the Picker Library
and the code then I did through the
documents list and converts each pattern
into Bag upwards that is a list of
binary values indicating whether each
word in the words is list is present or
not in the pattern the corresponding
intent for each pattern is also
converted into a one hole encoded Vector
indicating which intent it belongs to
these bag of words and one hot encoded
vectors are then combined and added to a
list called training the training list
is then shuffled randomly to ensure the
neural network does not learn any older
dependencies in the training data
finally the training list is converted
into a numpy array and split into
separate arrays from the bag of words
that is strain X and one hot encoded
vectors that is train y
you can see them and the neural network
is defined using the tensorflow
sequential API which allows the layers
to be added in the sequence the neural
network consists of three fully
connected layers the First with 128
nodes the second one is 64 nodes and
third with the number of nodes equal to
the number of classes the first two
layers also include drop out
regularization with a dropout rate of
0.5 which helps to prevent overfitting
the model is compiled using stochastic
gradient descent that is SGD as the
optimizer and categorical cross entropy
as the loss function the accuracy Matrix
is also calculated during training
finally the model is trained for 200
epochs with a batch size of 5 and
verbose set to 1 to display progress
during print after training the model is
saved to a file that is chatbot
underscore simply learned dot H5 along
with its training history and the
program Trends execute to indicate that
the training is complete
now we'll move to chatbot.py and inside
this file first of all import the
necessary libraries to run the code
and the necessary libraries here are
random This One Import random and after
that we will import Json
and after that we'll import
pickle
and then
showing error in Json
no okay
the import numpy
S and P
and after that we will import
nlpk
and now
from
nrtk dot stem
well import
word netlimitizer
okay now so
Keras dot models
will import node
underscore model
okay now we have imported these
let us load underscore model
so
random is used for generating random
responses from the bot and Json is used
for reading the intense file in Json
format the color is used for reading the
words.pkl and classes dot pkl files
which are pickled python objects numpy
is used for creating arrays and nltk is
used for natural language processing and
wordnet nematizer is used to limitize
words load underscore model is used to
load the pre-trained chatbot model
now
create a variable and initialize the
word net limitizer
and after that we will create an intense
file and load the Json file that is
intense node.json file for that we will
use json.loads
and inside will open the file
and here we'll name the file that is
intents dot Json
and after we will use the read function
to read this file
now we'll create a
variable and name it as words and inside
we'll load the words.pkl file using the
pickle Library
that will write pickle Dot
Dot and inside we will open the file
that is words Dot pkl
and
pass
are
and after that
load the classes file
and that is pickle Dot
we'll just copy it
change it to
classes
Dot pkl
okay now
load the model
that is
load underscore model that is the
pre-trained model
and
we have set the name of the model as
chat code underscore simply on model dot
H5 so we'll copy it
and get back to our chatbot.py file and
inside dot load store model will paste
it
so this initialize the limitizer object
that creates the intense.json file and
loads it into a python dictionary using
the Json Library the words.pkl and
classes dot pkl files are loaded into
python list using the Pica library and
finally the pre-trained chatbot model is
loaded using load underscore model
now
what we'll do is we'll create a function
and that function will take a sentence
as input tokenize it using nltk dot word
underscore tokenize limitizes each word
in the sentence using the limitizer
object and returns a list of limitized
words
so for that we will create a function
and name the function as cleanup
sentence
okay and in that
will pass the sentence
and now we'll create a variable
sentence words
and inside this variable we'll use
tokenize and pass the sentence to
tokenize it
that will use nltk Dot
word underscore
tokenize
and will pass sentence
and
after that
with the same variable will limitize it
so to limitize it
use limitizer
dot limit is
and inside will pass
is there any air
okay what
underscore tokenize
and in the element eyes we'll pass the
word
and
forwarding sentence words
what we'll return is limitizes sentence
words
so we'll return
sentence underscore words
okay
so we have done that and now we'll
create a function that takes a sentence
as input calls the cleanup sentence
function
to get a list of limitized words then
creates a bag of words representation of
the sentence and it turns it as a numpy
array the bag of word supplementation is
a binary Vector that has a one in the
position corresponding to each word in
the vocabulary that appears in the
sentence and zeros elsewhere
so for that we will create a function
and we'll name it as bag of words
and inside will pass the sentence
okay
now we'll use the same variable sentence
words
and here we'll call the cleanup sentence
function
and inside will pass the sentence
okay
and now we'll use Tag variable
and the zero position into
length of words
and now we will run a loop
for sentence words that is for WN
sentence underscore words
and
in this Loop
will enumerate
the whole words
that is
for I comma word
n
enumerate
and will pass words here
and inside if
word
equal to equal to w
n
bag of I equal to 1 else
and here it will return the array with
the
fluids
return
and that is NB Dot
array
and we'll pass back here
now
we'll create another function that takes
a sentence as input creates a bag of
words representation of the sentence
using the bag of words function feeds
the bags of words representation to the
pre-trained chatbot model using model
dot predict function and returns a list
of intents with the probabilities and
the error underscore threshold variable
that we will create that will set the
minimum probability required for an
intent to be considered the list of
intents is sorted in descending order of
probability and each intent is returned
as a dictionary containing the intent so
for that we will create these functions
and we'll start with
the function predict class
and inside will pass
sentence
and here we'll create a variable
and named as bow
and inside
we will call back off
words function and pass the sentence
and after that we'll create a variable
RDS and use the model
predict
use NP dot array
and inside will pass vow
and
it's
zeroth position
okay and after that we'll set the error
threshold equal to 0.25
and we'll make it in capital letters
only
get threshold
that is equal to 0.25
okay
now we will create a
variable and name it as results
and inside
we will pass a comma r
that is
for
I comma R in enumerate
and we'll pass
this and
that is
if
R is greater than error threshold
okay
and
if that's the case then we'll sort it
and short term results Dot
sort and inside will pass
equal to
Lambda
X
and that is
position one
and reverse
equal to
true
okay
and
return underscore list
that is
and for RN results
return list
dot append
will append
and inside
intent
and
classes
are
and probability
that is Str
f
and the brackets are passed
and after that we will return
underscore
list
okay
and after this we will create another
function and that will be get response
and this function will take two
parameters intense underscore list and
intense underscore Json
and this intense underscore list that
will contain the predicted intents of
the user input while intense undisco
Json contains a Json file that lists all
the possible intents and their
corresponding responses the function
then matches the predicted intent with
the intent in the Json file selects a
random response from the list of
responses associated with the intent and
it's a and returns it as a chatbot
response okay
so for that
we'll create another function
and we'll name it as get response
and inside this we'll pass intents
underscore list
and intense underscore Json
okay
for this we will
create a
variable named as list of
intents
and in this
intense underscore Json
and inside
intense
and we'll also create a variable tag
and
set this will pass intense underscore
list
the zero position
and
and
okay
with intent only
okay now we'll run a loop that is for i
n
stuff
intents
f
I
equal to equal to
tag
and result equal to random Dot
choice
and
okay
random number choice
okay and inside we have there is
responses
and after that we'll
break
and
then we have the
result
that is we will return the result
okay
and after that we will print
that
create
what is running
and you can remove that also
and after this the main program will use
an infinite Loop that waits for user
input through the input function and
once the user enters a message it calls
the predict underscore class function
that is not shown in the code to predict
the intents of the message
it then calls the get underscore
response function to retrieve the
appropriate response from the Json file
and print it as the chatboard's response
using the print function so first it
will call the underscore class function
so
for that we will create a loop
while true
message
equal to
input
and
have
create
ends and the predict class function and
inside will pass the message
and
in the highest variable it will call
get response and inside will pass
and intents
and after that we'll print
Arias
okay
now we are completed
with a code so we'll save it
and run it
so we are waiting to always get executed
structure terminal and execute it again
okay now it has
shown the
virtual environment so we will run it
again
okay so in line 10 it shows some error
okay
we'll see that
okay
now take any we are just initializing it
so I'll run it again
so you can see that a code is executed
and it has printed great photos running
so we'll ask
I do
I do
okay we will just okay we have to run it
again
get what is running so we'll get high
okay if there is some error just cut
this file
and cut this intense.json file inside
this chatbot only
and we have done that
now we'll run it
internal and now run it
oh sorry we are running the lens
node.json file
that's why
why
Dot py
and okay it says there it is an error
and
10th intense.json
okay and don't see any error in this
once
see if there is any error
okay and showing error so what we'll do
is we'll copy the path
of the intense.json file and run it
so for that
we have from here we'll copy the path
get back to
paste the path
and Slash
intense node.json file
and now we will run it again
now you can see that the file is
successfully executed
first like without path also it has been
executed and the file is in the same
directory it should be but it was
showing error so we have done that
and we'll do the same for the
chatbot.py and here also we'll copy the
same path
Json save it and run the chatbot
now you can see
go what is running
you have written great or go
it is saying okay no worries
so well same hi
and he responders to good to see you
again that is from the intense.json file
is giving us the render
responses so this is our intense.json
file
and from here like if we
say him greetings that is patterns hi
or how are you here selected good to see
you again and hit as
responded us that is he is using the
random function RC voltage will random
function and
we say him buy or will same thanks
he's saying good to see you again
thank you now he's saying you are
welcome so
I don't know why he said good to see you
again and thanks
see thanks for helping me there's only
my pleasure and you're welcome
is there any error
I'm asking what is Big Bang
okay he's responding correctly now maybe
there could be a slight Arrow in the
pre-processing so no worries
our code is running fine and I will
update all these files that is
intense.json file chatbot.py and new DOT
py into the GitHub repository you can
download it from there
you can create your own chatbot and you
can provide data to it that is you can
provide other tags and the patterns that
you will inquire him and the responses
accordingly
so here we have done with the project
and with that we have come to the end of
this session
and I hope the session both interesting
and informative if you liked it please
let us know in the comment section below
also do subscribe to our Channel and
stay tuned for more from Simply learn
till then stay safe and keep learning
foreign
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here