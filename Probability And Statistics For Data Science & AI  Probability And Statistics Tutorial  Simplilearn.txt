Welcome to our video on the foundational
concepts of probability and statistics
by simply learn probability and
statistics are essential tools for
understanding and analyzing data helping
us make informed decisions in various
Fields probability measures the
likelihood of events occurring providing
a mathematical framework to handle
uncertainty and Randomness it allows us
to quantify risks make predictions and
understand the behavior of Random
processes from simple events like
rolling a dice to complex phenomena in
finance and science statistics involves
systematic collection analysis
interpretation and presentation of data
by examining data statistics help us
draw meaningful conclusions identify
Trends test hypothesis and make data
driven decisions this field is crucial
in areas like healthcare engineering
economics and social science where
analyzing data accurately can lead to
significant insights and advancement
craving a career upgrade subscribe like
and comment
below dive into the link in the
description to FastTrack your Ambitions
whether you're making a switch or aiming
higher simply learn has your
back and if you want to make your career
in Ai and machine learning here is a
quick info for you try simply learns
postgraduate program in Ai and machine
learning from puru University in
collaboration with IBM this course
teaches in demand skills such as machine
learning deep learning NLP computer
vision reinforcement learning generative
AI prompt engineering chat GP and many
more so don't forget to check out the
course link from the description box and
pin comments so without any further Ado
let's get started
[Music]
learning objectives welcome to math
refresher probability and
statistics in this lesson we are going
to explain the concepts of statistics
and
probability describe conditional
probability Define the chain rule of
probability discuss the measure of
variance identify the types of gsan
distribution basic of statistics and
probability probability and statistics
data science relies heavily on estimates
and predictions a significant portion of
data science is made up of evaluations
and
forecast statistical methods are used to
make estimates for further analysis
probability theory is helpful for making
predictions statistical methods are
highly dependent on probability Theory
and all probability and statistics are
dependent on data data is information
acquired for reference or research via
observations facts and measurements data
is a set of facts structured in the form
that computers can interpret such as
numbers words estimations and Views
importance of data data AIDS in seeing
more about the information by
identifying possible connections between
two features data assists in the
detection of distortion by uncovering
hidden patterns based on prior
information patterns data may be
utilized to anticipate the future or
predict the current state of affairs
also data AIDS in determining whether
two pieces of information have any
instance in common or not types of data
data might be quantitative that is data
that can be measured or counted in
numbers or it may be qualitative which
is data which is generally divided into
groups or in simpler Words which cannot
be counted or measured in numbers let's
consider an example a customer
information data of a bank may contain
quantitative and qualitative data
consider this snapshot where we have
customer ID surname geography gender age
balance has CR or card is active member
amongst these variables we can see
surname is mostly qualitative as it
cannot be counted and measured in
numbers geography and gender are also
qualitative as they cannot be counted in
numbers and are mostly groups has Seer
card that is has credit card and his
active member although are containing
numerical inform form but these are
categorical that means these have been
divided into groups of one and zero that
represent yes and no as an answer hence
these two variables are also
qualitative customer ID is again all
although a numerical data however the
significance or intuition behind
customer ID is
categorical hence it may be kept in the
qualitative data also however age and
balance these are numerical information
which have been measured or counted and
numerical operations can be performed on
them hence these are Under quantitative
data categories introduction to
descriptive statistics descriptive
statistics
a descriptive measurement is summary
measure that quantitatively portrays the
most important features of a set of data
allowing for a better comprehension of
the information data can be measured as
different levels the levels of
measurement describe the nature of
information stored in the data assigned
to the variables qualitative data can be
measured as nominal or ordinal
quantitative data can be measured in
terms of interval and ratio Type n Al
data the data is categorized using names
labels or qualities for example brand
name zip code and gender ordinal data
can be arranged in order or ranked and
can be compared examples include grades
Star reviews position and race and date
interval data is the data that is
ordered and has meaningful differences
between the data points example
temperature in celsius and year of birth
ratio data is similar to the interval
level with the added property of
inherent zero mathematical calculations
can be performed on both interval as
well as ratio data for example height
age and weight population versus sample
before analyzing the data it's important
to figure out if it's from a population
or a sample population is a collection
of all available items as well as each
unit in our study
sample is a subset of the population
that contains only a few units of the
population population data is used for
study when the data pool is very small
and can give all the required
information samples are collected
randomly and represent the entire
population in the best possible way
measures of central
tendency the central tendency is a
single value that aids in the
description of the data by by
determining its Center position measures
of central tendency are sometimes known
as summary statistics or measures of
central
location the most popular measurements
of central tendency are mean median and
mode the normal distribution is a
bell-shaped symmetrical distribution in
which mean median and mode all are equal
the curve over here shows the
bell-shaped curve or the normal
distribution of variable X
the point over here that is
X1 is the point which represents the
mean median and mode of this
distribution mean mean is calculated by
dividing the sum of all data values by
the total number of data values it gets
affected when there are unusual or
extreme values it is sensitive to the
outliers mean can be calculated as
summation over all the values of X in a
collection divided by the size of the
collection for example we have a
collection where we have values as 7 3 4
1 6 and
7 we find out the sum of these values
which is 28 and there are total of six
values so 28 / 6 gives us a mean value
of
4.66
median it is the middle value in the set
of the data that has been sorted in
ascending order order it is a better
alternative to mean since it is less
impacted by outliers and
skewness it is closer to the actual
Central
value median is calculated differently
for different sizes of
data differentiated as if the total
number of values is odd or if the total
number of values is even if the size of
the data is odd for example in this case
we have five
elements after sorting whatever middle
value we
get that means n + 1 by twoth term in
this
case 5 + 1 /
2 that is the third term which is four
is the median
value in case when the total number of
values is even like here there are six
values the average or the mean of the
two cent values is considered as the
median in this case the median is the
mean of 6 and four which is five
mode mode represents the most common
value in the data set it is not at all
affected by extreme
observations it is the best measure of
central tendency for highly skewed or
non-normal
distribution mode for categorical data
is determined by estimating the fre
frequencies for each
categories and then the category with
the highest frequency is considered to
be
mode like in this case s has the highest
frequency hence s becomes the mode value
however in case of continuous data or
quantitative data the calculation of
mode is slightly different the first
step in calculation of mode is dividing
the data into classes which are equal
with then getting the frequency of data
points lying in within that range of
classes and finally selecting the class
with the highest
frequency using the range of that class
and the frequencies we can get the final
mode
value using the formula
l+ fmus fub1 multiplied to h / fmus fub1
Plus FM minus
F2 here L is the lower limit or the
lower observation of the mode class H is
the size of the mode
class FM is the frequency of the mode
class F1 is the frequency of the class
proceeding to mode and F2 is the
frequency of the class succeeding to
mode this gives us the final mode
value mean versus
expectation now let's talk about mean
versus
expectation so in general we use the
expected value or expectation when we
want to calculate the mean of a
probability distribution that represents
the average value we expect to occur
before collecting any data and mean on
the other hand mean is basically used
when we want to calculate the average
value of a given sample this represents
the average value of raw data that we
may have already
collected we can understand this by
using a simple
example now to calculate the expected
value of this probability distribution
we can use a specific formula from the
previous
discussion this is going to be the
expected value where X is going to be
the data value and this PX is the
probability of
value for example we could calculate the
expected value for this probability
distribution to be his shown
so here it will be 1.45
goals so this represents the expected
number of goals that the team will score
in any given
game and then if you talk about
calculating mean so we typically
calculate the mean after we have
actually collected raw
data for example suppose we record the
number of goals that a soccer team will
score in 15 different
games now to calculate the mean number
of goals scored per
game we can use the following
formula where sum of X is basically the
sum of all the goals divided by n and
the number of Records or we can say the
sample
size it is as shown on the screen
so this represents the mean number of
goals scored per game by the
team measures of
asymmetry the difference between the
three distinct curves can be studied in
this
image the central curve is the normal or
no skewness curve here mean median and
mode all lie on the same point
this normal curve is symmetrical about
its mean median and
mode that means the left hand side of
the curve is a mirror image of the right
hand side of the
curve however in case of negatively
skewed data the tail is elongated on the
left hand
side and the mean is smaller than the
mode and the median values or is on the
left hand side of the mode
hence indicating that the outliers are
in the negative
Direction on the other hand in case of
positively skewed the data is
concentrated on the left hand side of
the
curve while the tail is elongated or
longer on the right hand side of the
curve the mean is greater than the mode
and
median or is on the right hand side of
the mode and median indicating that the
outliers are in the positive
direction let's consider an
example the graph here shows the global
income distribution for the year 2003
2013 and a projection for
2035 if we see the global income
distribution statistics for 2003 it is
highly right
skewed we can observe in the previous
graph that in
2003 the mean of
$3,451 was higher than the median of
$10.90 the global income is definitely
not evenly distributed the majority of
people make less than $22,000 each
year while only a small percentage of
the population earns more than $14,000
measures of
variability measures of variability
dispersion the measure of central
Tendencies provide a single value that
addresses the full worth however the
central tendency cannot depict the
Viewpoint entirely the metric of
dispersion helps us focus on the
inconsistency in the data spread
measures of dispersion describe the
spread of the data
the range inter cordal range standard
deviation and variance are examples of
dispersion
measures
range the range of distribution is the
difference between the largest and the
smallest amount of
data the range for example does not
include all of a series positive
aspects it concentrates on the most
shocking aspects and ignores that aren't
considered critical for example for a
set 13 33 45 67
70 the range is
57 that is the maximum of this which is
70 minus the minimum over
here which is
13
variance variance is the average of all
squar
deviations it is defined as the sum of
square distance between each point and
the mean or the dispersion around the
mean the standard deviation is used as
variance suffers from a unit
difference variance can be computed as
Sigma Square summation / x - mu^
s divided by
n where mu is the mean of the data X is
the individual data
point and N is the size of the data
this representation is for a population
data for a sample data variance can be
computed as x
minus xar whole Square
summation over it divided by n minus
one here xar is the mean of these sample
data and N is the sample
size the units of values and variance
are not equal so another variability
measure is
used standard
deviation standard deviation is a
statistical term used to measure the
amount of variability or dispersion
around a
mean the standard deviation is
calculated as the square root of
variance it depicts the concentration of
the data around the mean of the data
set standard deviation as indicated
previously can be computed as square
root of
variance for a population data standard
deviation Sigma can be computed as
square root of summation over X IUS mule
squ /
n where mu is the mean of the data XI
are the data points and N is the size
let's consider an
example let's find out the mean variance
and standard deviation for this data the
data values are 3 5 6 9 and 10 to find
out the mean we first find the sum of
all these data
values that is 33 and divide it by the
count which is
5 we get the mean of 6.6 to compute the
variance we start by Computing the
deviation
that is x minus the mean of X here three
is one of the values of the data and 6.6
is the
mean so 3 - 6.6 squ and we do
that to find out some of all the
deviations divided by the
count which is
five we end up getting an overall
variance of
6.64 standard deviation as we know is
measured at square root of variance that
is square root of
6.64 which amounts to
2576 measures of
relationship measures of relationship
covariance covariance is the measure of
joint variability of two
variables it measures the direction of
the relationship between the variables
it determines if one variable will cause
the other to alter in the same
way Co variance between variable X and Y
can be computed as summation over the
product of x i -
xar and y i- y bar the whole divided by
n minus
one here xar and Y Bar are the mean of X
and Y respectively the value of
covariance can range from minus infinity
to a plus
Infinity
correlation correlation is normalized
covariance it measures the strength of
association between two variables the
most common measure for correlation is
the pieron correlation
coefficient correlation between two
variables X and Y can be measured with
respect to covariance as covariance
between
X and Y / by the standard deviation of X
and standard deviation of
Y the value of correlation ranges from a
negative one to positive
one types of
correlation correlation can be either a
positive
correlation zero correlation or a
negative
correlation the first picture over here
represents a perfect positive of
correlation we're in a straight line
with a positive
slope is representing the relationship
between the two
variables zero correlation means that
the line representing the relationship
between the two variables is horizontal
to the X
AIS perfect negative correlation can be
represented by a straight line with a
negative
slope corelation equals to one implies a
positive
relationship that is when one variable
increases the other variable also
increases a correlation value of
negative one implies a negative
relationship that is when one variable
increases the other
decreases the correlation coefficient of
zero shows that the variables are
completely independent of each
other let's consider an example
here we have two variables height and
weight to compute the correlation
between height and
weight we use the correlation formula as
covariance of
X and Y divided by standard deviation of
X and standard deviation of
Y here height is the X variable and
weight is the Y
variable first to compute Co variant we
compute the x - xar and y- Y Bar values
and then the product of
them we then compute x - x
s and Y - Y Bar Square values to compute
the standard deviations of height and
weight respectively correlation as we
know has been defined as covariant of X
and I and Y divided by standard
deviations of X and
Y this can also be represented as
summation over x - xar multiplied to y -
Y
Bar divided square root of summation
over sum of squared
deviations that is x - xar s multiplied
to square root of summation over y - Y
Bar whole square that is sum of square
deviations for y
now let's find out values to put into
this
formula first we find out the overall
sum of height to get the mean of height
which is
5.14 similarly we get the sum of weight
to get the mean of weight as 50 we now
get the summation over x - xar
multiplied to y - Y Bar to get the
numerator for the
formula then we compute x - xar s
and Y - y bar squ that is sum of squared
deviation of X and Y
respectively now we put in the values in
this final correlation formula to get a
correlation value of
0.889 this indicates that height and
weight have a positive
relationship it is evident that as
height grows weight also increases
in this module we will be talking about
expectation and
variance so the expected value or we can
say mean of a given variable that we can
denote by X is a discrete random
variable where it is a weighted average
of the possible values that X can take
and each value is going to be according
to the probability of that specific
event
occurring so usually the expected value
of x is denoted by a simple formul
formula where we can Define the
expectation based on the X
parameter which is going to be the sum
of each possible outcome multiplied by
the probability of the outcome
occurring so in more concrete terms the
expectation is what we would expect the
outcome of an experiment to be on
average we can take an example for the
coin if a coin is being cost 10 times
then one is most likely to get five
heads and five
tails same logic can be discussed if we
talk about another example of rolling a
die so there are six possible outcomes
when you roll a die 1 2 3 4 5 6 and each
of these has a probability of 1x six of
occurring so we can say that the
expectation is going to be one
multiplied by the probability of that
happening which is going to be 1X 6 + 2x
6 + 3x 6 + 4x 6 + 5x 6 + 6x 6 and that
is going to give us 3.5 as an output the
expected value is
3.5 so if you think about it 3.5 is
halfway between the possible values that
I can take and this is what we should
have
expected next we talk about the con
concept of variance so variance of a
random variable allows us to know
something about the spread of the
possible values of the variable so for a
discrete random variable X the variances
of X is going to be denoted by using a
simple formula that is going to be varx
equal e x - M the whole Square where m
is basically the expected value of the
expectation of X so this is more like a
standard deviation of x which can also
be represented by using this formula so
the variance does not behave in the same
way as expectation when we multiply and
add constants to random
variables so now there are two different
type of variance that we can have a fair
understanding on first of all we have
low variance and then we have high
variance so low variance simply means
that there is a small variation
in the production of the target function
with changes in the trading data set and
at the same time high variance as we can
see here High variance shows a large
variation in prediction of the target
function with changes in the trading
data set so a model that shows High
variance learns a lot and perform well
with the trading data set and it does
not generalize well with the Unseen data
set and that's why as a result such a
model gives good results results with
training data set but shows High error
rates on the test data set and since the
high variance a model learns to much
from the data set it leads to an
overfitting of the model so model with
high variance will be having couple of
issues like it may lead to overfitting
or it may also lead to increase in model
complexities next we have
skewness so skewness in simple terms is
is basically a measure of asymmetry of a
distribution so distribution is
asymmetrical when it's left and right
sides are not the mirror
images right now this is a mirrored
image and a distribution can have right
positive or we can say negative or it
can have zero
skewness so right skewed in this
scenario is basically the distribution
is longer on the right side of its
peak and a left skew distribution is
going to be we can say where it is
longer on the left
side so we can see we have this one as a
part of right side it is more elongated
towards the right side and this one is
more elongated towards the left side so
we can think of skewness in terms of
Tails a tail is long tampering and the
end of a distribution so it simply
indicates that they are observations at
one end of the distribution but that
they are relatively infrequent so a
right skew distribution has a long tail
on the right side as you can see here so
the number supports observed let's say
we have a data on a perear basis so
again we can have a more skewness
towards the right side where data is
being dropping as we continue to
increase the number of years for example
we may have a high sales towards the
beginning of year suppose in
2022 but again as we proceed to
2023 second half we are seeing the dip
in performance so that is rightly skewed
and same way let's suppose if we started
with the sales figure it was really Less
in suppose
2002 but again as we proceeded to 2023
now our sales have been gradually
increasing so it's more like skew
towards the left section as a part of
negative skew next we have curtosis
so curtosis is basically a measure of
the tailedness of a
distribution so tailedness is how often
the outliers occur and acts as curtes is
the tailedness of the distribution
related to a normal distribution so a
distribution with medium curtosis is
called as mesokurtic a distribution with
low curtosis like this one this is
called as the platic and then
distribution with high curtosis like
this one
this is called as the lepto
kurtic so Tails here they are tapering
ends on either side of a distribution
like this so they represent the
probability or the frequency of values
that are extremely high or extremely low
to the
mean in other words tals here represents
how often the outliers
occur so there are three type of
curses we have platic kurtic which is
negative Le kurtic which is a positive
towards the upper end and then we have
mesokurtic which is a normal
distribution so mesokurtic is the medium
tail so normal distributions they have a
curtois of three so any distribution
with a curtois of a prox value of three
is going to be mesokurtic and curtosis
is described in terms of excess curtosis
which is curtosis minus 3 and since
normal distribution they have a curtosis
of three axis curtes makes comparing a
distribution curtois to a normal
distribution even easier introduction to
probability probability Theory
probability is a measure of the
likelihood that an event will
occur let's consider an example of coin
toss where the chances of getting heads
on a coin are 1 by two or
50% the prob probability of each given
event is between zero and 1 both
inclusive sum of an events cumulative
probability cannot be greater than
one hence the probability of an event X
lies between 0o and 1 this means that
the integral of probability of
distribution over xal to
1 conditional
probability conditional probability of
any event a is defined as the
probability of occurrence of a given
that event B has previously
occurred condition probability of event
a given B can be estimated as
probability of a intersection B that is
probability of both A and B happening
together divided by the probability of
B it is also written as that probability
of a and intersection b equals to
probability of a given B multiplied to
probability of
B let's consider an
example in a coin we are doing a two
coin flip coin one gets heads Tails
heads and tails in subsequent
flips while coin 2 gets Tails heads
heads and tails in the subsequent flips
now the prob probability that coin 1
will get a head is two out of four while
the probability that coin 2 will get
heads is again two out of
four the probability that both coin 1
and coin 2 will have a heads is just one
out of the four
flips hence the probability that coin
one will get heads given that coin 2 is
already heads can be computed as
probability of coin one Edge
intersection coin 2 Edge
that is 1x4 divided by probability of
coin
2 that's a given that is 2x4 which is
going to be 0.5 or 50%
based base theorem base theorem
calculates the conditional probability
of an event based on its prior
probabilities basically base theorem
incorporates the prior probability
distrib ution to predict the posterior
probability base theorem for conditional
probability can be expressed as
probability of a given b equals
probability of B given a divided by
probability of B multiplied to
probability of
a base theorem allows updating the
probability values by using new
information or evidence here probability
of a is known as prior probability that
is the probability of event before any
new data is collected probability of a
given B is known as the posterior
probability it is the revised
probability of an event occurring after
taking into consideration the new
information probability of B given a is
known as the likelihood and probability
of B is probability of observing an
Evidence Baye model an example consider
an example for calcul ating the
likelihood of having diabetes based on
frequency of fast food consumption here
is the observed data let's say the fast
food audience is 20% diabetes prevalence
is 10% and 5% is fast food and
diabetes the chances of diabetes given
fast food that is the conditional
probability of D given B can be
calculated as probability of diabetes
and fast food together divided by
probability of fast food that means 5%
ided
20% that equals
25% Define an analysis can State eating
fast food increases the chance of having
diabetes by
25% the multiplication rule of
probability if events A and B are
statistically independent and
probability of a intersection B can be
given as probability of a given B
multiplied to probability of B however
probability of a intersection B is also
given as probability of a multiplied to
probability of B here probability of a
given b equals to probability of a when
we assume that probability of B is non
zero similarly probability of b equals
probability of B given a assuming
probability of a is non Z chain rule of
probability joint probability
distributions over many random variables
can be reduced into conditional
distributions over a single variable it
can be expressed as probability of X1 X2
so on until xn equals probability of X1
intersection probability of x i given
probability of X1 till x i minus
one for example the joint probability of
a b and c can be given as probability of
a given b c multiplied to probability of
B given C multiply to probability of C
logistic
sigmoid the logistics function is a type
of sigmoid function that aims to predict
the class to which a particular sample
belongs its outcome is discrete binary
value a probability between 0 and one
the logistics sigmoid is a useful
function that follows the yes curve it
saturates when the input is very large
or very small logistic sigmoid is
expressed as Sigma of x = 1 upon 1 + e
to the power minus
X the logistic sigmoid can be expressed
as sigmoid function of X is given as 1
upon 1 + e^ Min - x where e is the ool's
number
gsan
distribution the gossing distribution is
a type of distribution in which data
tends to Cluster around a central value
with little or no bias to the left or
right it is often referred to as normal
distribution in absence of Prior
information the normal distribution is
frequently a fair assumption in machine
learning
equation the formula for calculating
gsan distribution is described as the
normal distribution of
X that is the function of x given mean
as Mu and variance is Sigma Square can
be calculated as 1 upon Sigma Square <
TK of 2 pi e to^ - x - mu / Sigma whole
s where mu is the mean or Peak value
which also is the expected value of
x Sigma is the standard deviation Sigma
square is the
variance a standard normal distribution
has a mean of zero and a standard
deviation of
one gsan distribution can be
univariate which describes the
distribution of a single variable
X it can also be multivariate where it
can just Ed to describe the distribution
of several
variables it is represented in 3D of ND
formats law of large
numbers now let's talk about law of
large numbers the law of large numbers
states that an observed sample average
from a large sample will be close to the
true population average and that it will
get closer in the larger sample so the
law of large number does not guarantee
that a given sample spatially a small
sample will reflect the true population
characteristics or that a sample does
not reflect the true population will be
balanced by a subsequent sample this is
for the law of large numbers to express
the relationship between scale and
growth
rate so there are multiple examples
through which we can
understand and it is widely used in
statistical analysis in working with the
central limit theorem in terms of the
business growth
so there are multiple real times set up
in which these are going to be used so
if you talk about tossing a coin so
tossing a coin in a number of times will
give us two different type of
outcomes the result will spread evenly
between head and Tails and the expected
average value is going to be
half that means 50 * tails and 30 times
heads but again if you toss a coin 1,000
times then the result can be in
different manners because out of 1,000
let's say 850 times it has been head and
only 150 times it has been taals and so
on so that's why the possibility of one
event occurring is going to be changed
in large sample sets as compared to a
small sample sets as in let's say 10
times so the number of heads and tails
unbalanced for lower number of Trials so
we can see it is unbalanced
but again as soon as we toss more number
of coins more leans towards the balance
value or we can see the observed
averages next we have P
value so P value is basically a number
calculated from the statistical test
that describes How likely we are to have
found a particular set of observations
if the null hypothesis were true so P
values are used in hypothesis testing to
to help decide whether to reject the
null hypothesis and the smaller the P
value the more likely we are to reject
the null
hypothesis so we have a term called as
null hypothesis so all statistical tests
they have null hypothesis so for most
tests the null hypothesis is that there
is no relationship between our variables
of INF first or that there is no
difference among groups for example in a
two-tail T Test the non-hypothesis is
that the difference between two groups
is going to be
zero so P value is going to tell us how
likely it is that our data could have
occurred under the null
hypothesis it is done by calculating the
likelihood of a test
statistic which is the number calculated
by a statistical test using our data so
P value tell us how often we would
expect to see a test statistic as
extreme or more extreme
than one calculated by a statistical
test if the null hypothesis of the test
was
true so there are multiple limitations
as well so first one is the results can
be significant but again they are they
may not be practical as we have compared
it can be based on multiple hypothesis
for a game for the healthc care test if
the test is going to be positive or not
it may show even values of the effect of
a variable but not the magnitude in real
life what exactly is going to be the
application of a drug test being failed
in Pharma company therefore it is
recommended to use confidence and levels
in addition to the P values to quantify
or we can say to give a solid figure to
the reserve which we are going to get
the P values they are interpreted as
supporting or we can say refuting the
alternative
hypothesis so P value can only tell you
whether or not the null hypothesis is
supported it cannot tell us whether our
alternative hypothesis is true or why so
the risk of rejecting the null
hypothesis is often higher than the P
value so especially when we are looking
at a single study or when using small
sample sizes so this is because the
smaller frame of reference the greater
are the chance that as we stumble across
a statistically significant pattern
completely by
accident key
takeaways key takeaways probability and
statistics structure the premise of the
data the data helps in anticipating the
future or gauging in view of the past
patterns of
information the central tendency is a
single value that helps to describe the
data by identifying these Central
positions the mean median and mode are
the measures of central tendencies
the distribution where the data tends to
be around a central value with a lack of
bias or minimal bias towards the left or
right is called as gshi and distribution
thank
you staying ahead in your career
requires continuous learning and
upscaling whether you're a student
aiming to learn today's top skills or a
working professional looking to advance
your career we've got you covered
explore our impressive catalog of
certification programs in cuttingedge
domains including data science cloud
computing cyber security AI machine
learning or digital marketing designed
in collaboration with leading
universities and top corporations and
delivered by industry experts choose any
of our programs and set yourself on the
path to Career Success click the link in
the description to know more
hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to ner up and get certified click
here