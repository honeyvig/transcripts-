welcome to s's complete course on
machine learning in today's Tech World
machine learning is changing how
industry works and shaping the future
what makes it so powerful it allows
machine to learn from data predict
results and automate difficult task
helping businesses run more smoothly and
making life easier with machine learning
you can create algorithm that sport
pattern make decision and solve problems
without needing human input this makes
machine learning a highly valued skills
and professional in this field are in
high demand often earning top salaries
and finding exciting career path machine
learning isn't just for data scientist
it is a useful skills that can benefit
almost any Tech job whether you are a
developer analyst engineer or
entrepreneur knowing machine learning
can give you an advantage and help you
and stand out in the job market in this
course we will explore topic like what
is machine learning what is deep
learning linear algebra for machine
learning how to learn AIML and key
machine learning interview Portion by
end of this course you will know how to
use machine learning to improve your
work and explore new career options
ready to level up your skills and stay
competitive in Tech world let's jump in
and unlock the power of machine learning
craving a career upgrade subscribe like
and comment
below dive into the link in the
description to FastTrack your Ambitions
whether you're making a switch or aiming
higher simply learn has your
back if you interested in making a
Korean machine learning and AI unlock
your potential in Ai and machine
learning with simply lar professional
certificate course in generative Ai and
machine learning in collaboration with I
designed for expiring AI professional
this program offers hands-on experience
with machine learning algorithm deep
learning and NLP Guided by industry
expert and ID can't faculty so don't
forget to check out the course link from
the description box below and the pin
comment so without any further Ado let's
get started we know humans learn from
their past experiences and machines
follow instructions given by humans but
what if humans can train the machines to
learn from their past data and do what
humans can do and much faster well
that's called machine learning but it's
a lot more than just learning it's also
about understanding and reasoning so
today we will learn about the basics of
machine learning so that's Paul he loves
listening to new
songs he either likes them or dislikes
them Paul decides this on the basis of
the song's Tempo genre intensity and the
gender of voice for Simplicity let's
just use Tempo and intensity for now so
here Tempo is on the x-axis ranging from
relaxed to fast whereas intensity is on
the y- axis ranging from light to
Soaring we see that Paul likes the song
with fast tempo and soaring intensity
while he dislikes the song with relaxed
Tempo and light intensity so now we know
Paul's choices let's say Paul listens to
a new song Let's let's name it as song a
song a has fast tempo and a soaring
intensity so it lies somewhere here
looking at the data can you guess
whether Paul will like the song or not
correct so Paul likes this song by
looking at Paul's past choices we were
able to classify the unknown song very
easily right let's say now Paul listens
to a new song Let's label it as song b
so song b lies somewhere here with
medium Tempo and medium intensity
neither relaxed nor fast neither light
nor soaring now can you guess whether
Paul likes it or not not able to guess
where the Paul will like it or dislike
it are the choices unclear correct we
could easily classify song A but when
the choice became complicated as in the
case of song b yes and that's where
machine learning comes in let's see how
in the same example for song b if we
draw a circle around the song b we see
that there are four wordss for like
whereas one vote for dislike if we go
for the majority votes we can say that
Paul will definitely like the song
that's all this was a basic machine
learning algorithm also it's called K
nearest neighbors so this is just a
small example in one of the many machine
learning algorithms quite easy right
believe me it is but what happens when
the choices become complicated as in the
case of song b that's when machine
learning comes in it learns the data
builds the prediction model and when the
new data point comes in it can easily
protect for it more the data better the
model higher will be the accuracy there
are many ways in which the machine
learns it could be either supervised
learning unsupervised learning or
reinforcement learning let's first
quickly understand supervised learning
suppose your friend gives you 1 million
coins of three different currencies say
one rupee 1 euro and 1 dirham each coin
has different weights for example a coin
of 1 rupe weighs 3 g 1 EO weighs 7 G and
one Dam weighs 4 G your model will
predict the currency of the coin here
your weight becomes the feature of coins
while currency becomes their label when
you feed this data to the machine
learning model it learns which feature
is associated with which label for
example it will learn that if a coin is
of 3 G it will be a 1 rupe coin let's
give a new coin to the machine on the
basis of the weight of the new coin your
model will predict the currency hence
supervised learning uses label data to
train the model here the machine knew
the features of the object and also the
labels associated with those features on
this note let's move to unsupervised
learning and see the difference suppose
you have Cricket data set of various
players with their respective scores and
the wickets taken when we feed this data
set to the machine the machine
identifies the pattern of player
performance so it plots this data with
the respective wickets on the x-axis
while runs on the Y AIS while looking at
the data you'll clearly see that there
there are two clusters the one cluster
are the players who scored higher runs
and took less wickets while the other
cluster is of the players who scored
less runs but took many wickets so here
we interpret these two clusters as
batsman and Bowlers the important point
to note here is that there were no
labels of batsmen and Bowlers hence the
learning with unlabeled data is
unsupervised learning so we saw
supervised learning where the data was
labeled and the unsupervised learning
where the data was unlabeled and then
there is reinforcement learning which is
reward-based learning or we can say that
it works on the principle of feedback
here let's say you provide the system
with an image of a dog and ask it to
identify it the system identifies it as
a cat so you give a negative feedback to
the machine saying that it's a dog's
image the machine will learn from the
feedback and finally if it comes across
any other image of a dog it'll be able
to classify it correctly that is
reinforcement learning to generalize
machine learning model let's see a
flowchart input is given to to a machine
learning model which then gives the
output according to the algorithm
applied if it's right we take the output
as a final result else we provide
feedback to the training model and ask
it to predict until it learns I hope
you've understood supervised and
unsupervised learning so let's have a
quick quiz you have to determine whether
the given scenarios uses supervised or
unsupervised learning simple right
scenario one Facebook recognizes your
friend in a picture from an album of
tagged photographs
scenario 2 Netflix recommends new movies
based on someone's Past movie
choices scenario three analyzing Bank
data for suspicious transactions and
flagging the fraud transactions think
wisely and comment below your answers
moving on don't you sometimes wonder how
is machine learning possible in today's
era well that's because today we have
humongous data available everybody's
online either making a transaction or
just surfing the internet and that's
generating a huge huge amount of data
every minute and that data my friend is
the key to analysis also the memory
handling capabilities of computers have
largely increased which helps them to
process such huge amount of data at hand
without any delay and yes computers now
have great computational Powers so there
are a lot of applications of machine
learning out there to name a few machine
learning is used in healthcare where
Diagnostics are predicted for doctor's
review the sentiment analysis that the
tech Giants are doing on social media is
another interesting application of
machine learning fraud detection in the
finance sector and also to predict
customer CH in the e-commerce sector
while booking a gap you must have
encountered search pricing often where
it says the fair of your trip has been
updated continue booking yes please I'm
getting late for office well that's an
interesting machine learning model which
is used by Global Taxi giant Uber and
others where they have differential
pricing in real time based on demand the
number of cars available bad feather
Rush R Etc so they use the searge
pricing model to ensure that those who
need a cab can get one also it uses
predictive modeling to predict where the
demand will be high with a goal that
drivers can take care of the demand and
search pricing can be minimized great
hey Siri can you remind me to book a cab
at 6:00 p.m. today okay I'll remind you
thanks no problem comment below some
interesting everyday examples around you
where machines are learning and doing
amazing jobs so that's all for machine
learning Basics today from my site what
is machine learning machine learning is
the science of making computers learn
and act like humans by feeding data and
information without being explicitly
programmed we see here we have a nice
little diagram where we have our
ordinary system uh your computer
nowadays you can even run a lot of this
stuff on a cell phone because cell
phones advance so much and then with
artificial intelligence and machine
learning it now takes the data and it
learn learns from what happened before
and then it predicts what's going to
come next and then really the biggest
part right now in machine learning
that's going on is it improves on that
how do we find a new solution so we go
from descriptive where it's learning
about stuff and understanding how it
fits together to predicting what it's
going to do to post scripting coming up
with a new solution and when we're
working on machine learning there's a
number of different diagrams that people
have posted for what steps to go through
a lot of it might be very domain
specific so if you're working on Photo
identification versus language versus
medical or physics some of these are
switched around a little bit or new
things are put in they're very specific
to The Domain this is kind of a very
general diagram first you want to Define
your objective very important to know
what it is you're wanting to predict
then you're going to be collecting the
data so once you've defined an objective
you need to collect the data that
matches you spend a lot of time in data
science collecting data and the next
step preparing the data you got to make
sure that your data is clean going in
there's the old saying bad data in bad
answer out or bad data out and then once
you've gone through and we've cleaned
all this stuff coming in then you're
going to select the algorithm which
algorithm are you going to use you're
going to train that algorithm in this
case I think we're going to be working
with svm the support Vector machine then
you have to test the model does this
model work is this a valid model for
what we're doing and then once you've
tested it you want to run your
prediction you want to run your
prediction or your choice or whatever
output it's going to come up with and
then once everything is set and you've
done lots of testing then you want to go
ahead and deploy the model and remember
I said domain specific this is very
general as far as the scope of doing
something a lot of models you get
halfway through and you realize that
your data is missing something and you
have to go collect new data because
you've run a test in here someplace
along the line you're saying hey I'm not
really getting the answers I need so
there's a lot of things that are domain
specific that become part of this model
this is a very general model but it's a
very good model to start with and we do
have some basic divisions of what
machine learning does it's important to
know for instance do you want to predict
a category well if you're categorizing
thing that's classification for instance
whether the stock price will increase or
decrease so in other words I'm looking
for a yes no answer is it going up or is
it going down and in that case we'd
actually say is it going up true if it's
not going up it's false meaning it's
going down this way it's a yes no 01 do
you want to predict a quantity that's
regression so remember we just did
classification now we're looking at
regression these are the two major
divisions in what data is doing for
instance predicting the age of a person
based on the height weight health and
other factors So based on these
different factors you might guess how
old a person is and then there are a lot
of domain specific things like do you
want to detect an anomaly that's anomaly
detection this is actually very popular
right now for instance you want to
detect money withdrawal anomalies you
want to know when someone's making a
withdrawal that might not be their own
account we've actually brought this up
because this is really big right now if
you're predicting the stock whether to
buy stock or not you want to be able to
know if what's going on in the stock
market is an anomaly use a different
prediction model because something else
is going on you got to pull out new
information in there or is this just the
norm I'm going to get my normal return
on my money invested so being able to
detect anomalies is very big in data
scien SE days another question that
comes up which is on what we call
untrained data is do you want to
discover structure in unexplored data
and that's called clustering for
instance finding groups of customers
with similar Behavior given a large
database of customer data containing
their demographics and past buying
records and in this case we might notice
that anybody who's wearing certain set
of shoes go shopping at certain stores
or whatever it is they're going to make
certain purchases by having that
information it helps us to Market or
group people together so then we can now
explore that group and find out what it
is we want to Market to them if you're
in the marketing world and that might
also work in just about any Arena you
might want to group people together
whether they're uh based on their
different areas and Investments and
financial background whether you're
going to give them a loan or not before
you even start looking at whether
they're valid customer for the bank you
might want to look at all these
different areas and group them together
based on unknown
data so you're not you don't know what
the data is going to tell you but you
want to Cluster people together that
come together let's take a quick DeTour
for quiz time oh my favorite so we're
going to have a couple questions here
under our quiz time and um we'll be
posting the answers in the part two of
this tutorial so let's go ahead and take
a look at these quiz times questions and
hopefully you'll get them all right and
it'll get you thinking about how to
process data and what's going on can you
tell what's happening in the following
case pces of course you're sitting there
with your cup of coffee and you have
your check box and your pen trying to
figure out what's your next step in your
data science analysis so the first one
is grouping documents into different
categories based on the topic and
content of each document very big these
days you know you have legal documents
you have uh maybe it's a Sports Group
documents maybe you're analyzing
newspaper postings but certainly having
that automated is a huge thing in
today's world B identifying handwritten
digits in images correctly so we want to
know whether they're writing an A or
capital A B C what are they writing out
in their hand digit their handwriting C
behavior of a website indicating that
the site is not working as designed D
predicting salary of an individual based
on his or her years of experience HR
hiring uh setup there so stay tuned for
part two we'll go ahead and answer these
questions we get to the part two of this
tutorial or you can just simply write at
the bottom and send a note to Simply
learn and they'll follow up with you on
it back to our regular content and these
last few bring us into the next topic
which is another way of dividing our
types of machine learning and that is
with supervised
unsupervised and reinforcement learning
supervised learning is a method used to
enable machines to classify predict
objects problems or situations based on
labeled data data fed to the machine and
in here you see we have a jumble of data
with circles triangles and squares and
we label them we have what's a circle
what's a triangle what's a square we
have our model training and it trains it
so we know the answer very important
when you're doing supervised learning
you already know the answer to a lot of
your information coming in so you have a
huge group of data coming in and then
you have new data coming in so we've
trained our model the model now knows
the difference between a circle a square
a triangle and now that we've trained it
we can send in in this case a square and
a circle goes in and it predicts that
the top one's a square and the next
one's a circle and you could see that
this is uh being able to predict whether
someone's going to default on a loan
because I was talking about Banks
earlier supervised learning on stock
market whether you're going to make
money or not that's always important and
if you are looking to make a fortune in
the stock market keep in mind it is very
difficult to get all the data correct on
the stock market it is very uh it
fluctuates in ways you really hard to
predict dict so it's quite a roller
coaster ride if you're running machine
learning on the stock market you start
realizing you really have to dig for new
data so we have supervised learning and
if you have supervised we should need
unsupervised learning in unsupervised
learning machine learning model finds
the hidden pattern in an unlabeled data
so in this case instead of telling it
what the circle is and what a triangle
is and what a square is it goes in there
looks at them and says for whatever
reason it groups them together maybe
it'll group it by the number of corners
ERS and it notices that a number of them
all have three corners a number of them
all have four corners and a number of
them all have no corners and it's able
to filter those through and group them
together we talked about that earlier
with looking at a group of people who
are out shopping we want to group them
together to find out what they have in
common and of course once you understand
what people have in common maybe you
have one of them who's a customer at
your store or you have five of them are
customer at your store and they have a
lot in common with five others who are
not customers at your store how do you
Market to those five who aren't
customers at your store yet they fit the
demograph of who's going to shop there
and you'd like them to shop at your
store not the one next door of course
this is a simplified version you can see
very easily the difference between a
triangle and a circle which is might not
be so easy in marketing reinforcement
learning reinforcement learning is an
important type of machine learning where
an agent learns how to behave in an
environment by performing actions and
seeing the result we have here where the
in this case a baby it's actually great
that they used an infant for this slide
because the reinforcement learning is
very much in its infant stages but it's
also probably the biggest machine
learning demand out there right now or
in the future it's going to be coming up
over the next few years is reinforcement
learning and how to make that work for
us and you can see here where we have
our action in the action in this one it
goes into the fire hopefully the baby
didn't it was just a little candle not a
giant fire pit like it looks like here
when the baby comes out and the new
state is the baby is sad and crying
because they got burned on the fire and
and then maybe they take another action
the baby's called the agent because it's
the one taking the actions and in this
case they didn't go into the fire they
went a different direction and now the
baby's happy and laughing and playing
reinforcement learning is very easy to
understand because that's how as humans
that's one of the ways we learn we learn
whether it is you you burn yourself on
the stove don't do that anymore don't
touch the stove in the big picture being
able to have machine learning program or
or an AI be able to do this is huge
because now we're starting to learn how
to learn that's a big jump in the world
of computer and machine learning and
we're going to go back and just kind of
go back over supervise versus
unsupervised learning understanding this
is huge because this is going to come up
in any project you're working on we have
in supervised learning we have labeled
data we have direct feedback so
someone's already gone in there and said
yes that's a triangle no that's not a
triangle and then you predicted outcome
so you have a nice prediction this is
this this new set of data is coming in
and we know what it's going to be and
then with unsupervised training it's not
labeled so we really don't know what it
is there's no feedback so we're not
telling it whether it's right or wrong
we're not telling it whether it's a
triangle or a square we're not telling
it to go left or right all we do is
we're finding hidden structure in the
data grouping the data together to find
out what connects to each other and then
you can use these together so imagine
you have an image and you're not sure
what you're looking for so you go in and
you have the unstructured data find all
these things that are connected together
and then somebody looks at those and
labels them now you can take that label
data and program something to predict
what's in the picture so you can see how
they go back and forth and you can start
connecting all these different tools
together to make a bigger picture there
are many interesting machine learning
algorithms let's have a look at a few of
them hopefully this give you a little
flavor of what's out there and these are
some of the most important ones that are
currently being used we'll take a look
at linear regression
decision tree and the support Vector
machine let's start with a closer look
at linear regression linear regression
is perhaps one of the most well-known
and well understood algorithms in
statistics and machine learning linear
regression is a linear model for example
a model that assumes a linear
relationship between the input variables
X and the single output variable Y and
you'll see this if you remember from
your algebra classes y = mx + C imagine
we are predicting distance traveled y
from speed X our linear regression model
representation for this problem would be
y = m * x + C or distance equals m *
speed plus C where m is the coefficient
and C is the Y intercept and we're going
to look at two different variations of
this first we're going to start with
time is constant and you can see we have
a bicyclist he's got a safety gear on
thank goodness speed equals 10 m/s and
so over a certain amount of time his
distance equals 36 km we have a second
bicyclist who's going twice the speed or
20
m/s and you can guess if he's going
twice the speed and time is a constant
then he's going to go twice the distance
and that's easily to compute 36 * 2 you
get 72 kilm and so if you had the
question of how fast would somebody who
going three times that speed or 30 m/
second is you can easily compute the
distance in our head we can do that
without needing a computer but we want
to do this for more comp licated data so
it's kind of nice to compare the two but
let's just take a look at that and what
that looks like in a graph so in a
linear regression model we have our
distance to the speed and we have our m
equals the ve slope of the line and
we'll notice that the line has a plus
slope and as speed increases distance
also increases hence the variables have
a positive relationship and so your
speed of the person which equal y = mx
plus C distance traveled in a fixed
interval of time and we could very
easily comp compute either following the
line or just knowing it's 3 times 10 m/s
that this is roughly 102 km distance
that this third bicep has traveled one
of the key definitions on here is
positive relationship so the slope of
the line is positive as distance
increase so does speed increase let's
take a look at our second example where
we put distance is a constant so we have
speed equals 10 m/ second they have a
certain distance to go and it takes them
100 seconds to travel that distance and
we have our second bicyclist who's still
doing 20 m/ second since he's going
twice the speed we can guess he'll cover
the distance in about half the time 50
seconds and of course you could probably
guess on the third one 100 divided by 30
since he's going three times the speed
you could easily guess that this is
33.33 3 seconds time when we put that
into a linear regression model or a
graph if the distance is assumed to be
constant let's see the relationship
between speed and time and as time goes
up the amount of speed to go that same
distance goes down so now your m equals
a minus ve slope of the line as the
speed increases time decreases hence the
variable has a negative relationship
again there's our definition positive
relationship and negative relationship
depending on the slope of the line and
with a simple formula like this um and
even a significant amount of data Let's
uh see with the mathematical
implementation of linear regression and
we'll take this data so suppose we have
this data set where we have xyx = 1 2 3
45 standard series and the Y value is 3
22 43 when we take that and we go ahead
and plot these points on a graph you can
see there's kind of a nice scattering
and you could probably eyeball a line
through the middle of it but we're going
to calculate that exact line for linear
regression and the first thing we do is
we come up here and we have the mean of
XI and remember mean is basically the
average so we added 5+ 4+ 3+ 2+ 1 and
divide by five that simply comes out as
three and then we'll do the same for y
we'll go ahead and add up all those
numbers and divide by five and we end up
with the mean value of y of I equals 2.8
where the XI references it's an average
or means value and the Yi also equals a
means value of y and when we plot that
you'll see that we can put in the yal
2.8 and the x equals three in there on
our graph we kind of gave it a little
different color so you could sort it out
with the dash lines on it and it's
important to note that when we do the
linear regression the linear regression
model should go through that dot now
let's find our regression equation to
find the best fit line remember we go
ahead and take our yal MX plus C so
we're looking for M and C so to find
this equation for our data we need to
find our slope of M and our coefficient
of c and we have y = mx + C where m
equals the sum of x - x average * y - y
average or y means and X means over the
sum of x - x means squared that's how we
get the slope of the value of the line
and we can easily do that by creating
some columns here we have XY computers
are really good about iterating through
data and so we can easily compute this
and fill in a graph of data and in our
graph you can easily see that if we have
our x value of one and if you remember
the XI or the means value is three 1 - 3
= -2 and 2 - 3 = a -1 so on and so forth
and we can easily fill in the column of
x - x i y - Yi and then from those we
can compute x - x i^ 2 and x - x i * y -
Yi and you can guess it that the next
step is to go ahead and sum the
different columns for the answers we
need so we get a total of 10 for our x -
x i^ 2 and a total of 2 for x - x i * y
- y i and we plug those in we get 2/10
which equals .2 so now we know the slope
of our line equals 0.2 so we can
calculate the value of c that'd be the
next step is we need to know where
crosses the y axis and if you remember I
mentioned earlier that the linear
regression line has to pass through the
means value the one that we showed
earlier we can just flip back up there
to that graph and you can see right here
there's our means value which is 3 x = 3
and Y = 2.8 and since we know that value
we can simply plug that into our formula
y = 2x + C so we plug that in we get 2.8
= 2 * 3 + C and you can just solve for C
so now we know that our coefficient
equals 2.2 and once we have all that we
can go ahead and plot our regression
line Y = 2 * x + 2.2 and then from this
equation we can compute new values so
let's predict the values of Y using x =
1 2 3 4 5 and plot the points remember
the 1 2 3 4 5 was our original X values
so now we're going to see what y thinks
they are not what they actually are and
we plug those in we get y of designated
with Y of P you can see that x = 1 = 2.4
x = 2 = 2.6 and so on and so on so we
have our y predicted values of what we
think it's going to be when we plug
those numbers in and when we plot the
predicted values along with the actual
values we can see the difference and
this is one of the things is very
important with linear aggression in any
of these models is to understand the
error and so we can calculate the error
on all of our different values and you
can see over here we plotted um X and Y
and Y predict and we drawn a little line
so you can sort of see what the error
looks like there between the different
points so our goal is to reduce this
error we want to minimize that error
value on our linear regression model
minimizing the distance there are lots
of ways to minimize the distance between
the line and the data points like sum of
squared errors sum of absolute errors
root mean square error Etc we keep
moving this line through the data points
to make sure the best fit line has the
least Square distance between the data
points and the regression line so to
recap with a very simple linear
regression model we first figure out the
formula of our line through the middle
and then we slowly adjust the line to
minimize the error keep in mind this is
a very simple formula the math gets even
though the math math is very much the
same it gets much more complex as we add
in different dimensions so this is only
two Dimensions y = mx plus C but you can
take that out to X Z ijq all the
different features in there and they can
plot a linear regression model on all of
those using the different formulas to
minimize the error let's go ahead and
take a look at decision trees a very
different way to solve problems in the
linear regression model decision tree is
a tree-shaped algorithm used to
determine a course of action each branch
of a tree represents a possible decision
occurrence or reaction we have data
which tells us if it is a good day to
play golf and if we were to open this
data up in a general spreadsheet you can
see we have the Outlook whether it's a
rainy overcast Sunny temperature hot
mild cool humidity windy and did I like
to play golf that day yes or no so we're
taking a census and certainly I wouldn't
want a computer telling me when I should
go play golf or not but you could
imagine if you got up in the night
before you're trying to plan your day
and it comes up and says tomorrow would
be a good day for golf for you in the
morning and not a good day in the
afternoon or something like that this
becomes very beneficial and we see this
in a lot of applications coming out now
where it gives you suggestions and lets
you know what what would uh fit the
match for you for the next day or the
next purchase or the next uh whatever
you know next mail out in this case is
tomorrow a good day for playing golf
based on the weather coming in and so we
come up and let's determine if you
should play golf when the day is sunny
and windy so we found out the forecast
tomorrow is going to be sunny and windy
and suppose we draw our tree like this
we're going to have our humidity and
then we have our normal which is if it's
if you have a normal humidity you're
going to go play golf and if the
humidity is really high then we look at
the Outlook and if the Outlook is sunny
overcast or rainy it's going to change
what you choose to do so if you know
that it's a very high humidity and it's
sunny you're probably not going to play
golf cuz you're going to be out there
miserable fighting off the mosquitoes
that are out joining you to play golf
with you maybe if it's rainy you
probably don't want to play in the rain
but if it's slightly overcast and you
get just the right Shadow that's a good
day to play golf and be outside out on
the green now in this example you can
probably make your own tree pretty
easily because it's a very simple set of
data going in but the question is how do
you know what to split where do you
split your data what if this is much
more complicated data where it's not
something that you would particularly
understand like studying cancer they
take about 36 measurements of the
cancerous cells and then each one of
those measurements represents how
bulbous it is how extended it is how
sharp the edges are something that as a
human we would have no understanding of
so how do we decide how to split that
data up and is that the right decision
tree but so that's a question that's
going to come up is this the right
decision tree for that we should
calculate entropy and Information Gain
to important vocabulary words there are
the entropy and the Information Gain
entropy entropy is a measure of
Randomness or impurity in the data set
entropy should be low so we want the
chaos to be as low as possible we don't
want to look at it and be confused by
the images or what's going on there with
mixed data and the Information Gain it
is the measure of decrease in entropy
after the data set is split also known
as entropy reduction Information Gain
should be high so we want our
information that we get out of the split
to be as high as possible let's take a
look at entropy from the mathematical
side in this case we're going to denote
entropy as I of P of and N where p is
the probability that you're going to
play a game of golf and N is the
probability where you're not going to
play the game of golf now you don't
really have to memorize these formulas
there's a few of them out there
depending on what you're working with
but it's important to note that this is
where this formula is coming from so
when you see it you're not lost when
you're running your programming unless
you're building your own decision tree
code in the back and we simply have a
log squar of p over P plus n minus n /
p+ n * the log s of n of p+ n but let's
break that down and see what actually
looks like when we're Computing that
from the computer script side entropy of
a target class of the data set is the
whole entropy so we have entropy play
golf and we look at this if we go back
to the data you can simply count how
many yeses and no in in our complete
data set for playing golf days in our
complete set we find we have 5 days we
did play golf and N9 days we did not
play golf and so our I equals if you add
those together 9 + 5 is 14 and so our I
equals 5 over 14 and 9 over 14 that's
our P andn values that we plug into that
formula and you can go 5 over 14 = 36 9
over 14 = 64 and when you do the whole
equation you get the minus 36 logun 2ar
of 36 - 64 log < TK of 64 and we get a
set value we get
.94 so we now have a full entropy value
for the whole set of data that we're
working with and we want to make that
entropy go down and just like we
calculated the entropy out for the whole
set we can also calculate entropy for
playing golf and the Outlook is it going
to be overcast or rainy or sunny and so
we look at the entropy we have P of
Sunny time e of 3 of two and that just
comes out how many sunny days yes and
how many sunny days no over the total
which is five don't forget to put the
we'll divide that five out later on
equals P overcast equal 4 comma 0 plus
rainy equal 2 comma 3 and then when you
do the whole setup we have 5 over 14
remember I said there was a total of
five 5 over 14 * the I of 3 of 2 + 4
over 14 * the 4 Comm 0 and 514 over I of
23 and so we can now compute the entropy
of just the part it has to do with the
forecast and we get 693 similarly we can
calculate the entropy of other
predictors like temperature humidity and
wind and so we look at the gain Outlook
how much are we going to gain from this
entropy play golf minus entropy play
golf Outlook and we can take the
original 0.94 for the whole set minus
the entropy of just the rainy day in
temperature and we end up with a gain
of. 247 so this is our Information Gain
remember we Define entropy and we Define
Information Gain the higher the
information gain the lower the entropy
the better the information gain of the
other three attributes can be calculated
in the same way so we have our gain for
temperature equals
0.029 we have our gain for humidity
equals 0.152 and our gain for a windy
day equals
0048 and if you do a quick comparison
you'll see the 247 is the greatest gain
of information so that's the split we
want now let's build a decision tree so
we have the Outlook is it going to be
sunny overcast or rainy that's our first
split because that gives us the most
Information Gain and we can continue to
go down the tree using the different
information gains with the largest
information we can continue down the
nodes of the tree where we choose the
attribute with the largest Information
Gain as the root node and then continue
to split each sub node with the large
largest Information Gain that we can
compute and although it's a little bit
of a tongue twister to say all that you
can see that it's a very easy to view
visual model we have our Outlook we
split it three different directions if
the Outlook is overcast we're going to
play and then we can split those further
down if we want so if the over Outlook
is sunny but then it's also windy if
it's uh windy we're not going to play if
it's uh not windy will play so we can
easily build a nice decision treat to
guess what we would like to do tomorrow
give us a nice recommendation for the
day so we want to know if it's a good
day to play golf when it's sunny and
windy remember the original question
that came out tomorrow's weather report
is sunny and windy you can see by going
down the tree we go Outlook Sunny
Outlook windy we're not going to play
golf tomorrow so our little Smartwatch
pops up and says I'm sorry tomorrow is
not a good day for golf it's going to be
sunny and windy and if you're a huge
golf fan you might go uhoh it's not a
good day to play golf we can go in and
watch a golf game at home so we'll sit
in front of the TV instead of being out
playing golf in the wind now that we
looked at our decision tree let's look
at the third one of our algorithms we're
investigating support Vector machine
support Vector machine is a widely used
classification algorithm the idea of
support Vector machine is simple the
algorithm creates a separation line
which divides the classes in the best
possible manner for example dog or cat
disease or no disease suppose we have a
labeled sample data which tells height
and weight of males and females a new
data point arrives and we want to know
whether it's going to be a male or a
female so we start by drawing a line we
draw decision lines but if we consider
decision line one then we will classify
the individual as a male and if we
consider decision line two then it will
be a female so you can see this person
kind of lies in the middle of the two
groups so it's a little confusing trying
to figure out which line they should be
under we need to know which line divides
the classes correctly but how the goal
is to choose a hyperplane and that is
one of the key words that use when we
talk about support Vector machines
choose a hyper plane with the greatest
possible margin between the decision
line and the nearest Point within the
training set so you can see here we have
our support Vector we have the two
nearest points to it and we draw a line
between those two points and the
distance margin is the distance between
the hyper plane and the nearest data
point from either set so we actually
have a value and it should be equally
distant between the two um points that
we're comparing it to when we draw the
hyperplanes we observe that line one has
a maximum distance so we observe that
line one has a maximum distance margin
so we'll classify the new data point
correctly and our result on this one is
going to be that the new data point is
Mel one of the reasons we call it a
hyperplane versus a line is that a lot
of times we're not looking at just
weight and height we might be looking at
36 different features or dimensions and
so when we cut it with a hyper plane
it's more of a three-dimensional cut in
the data or multi-dimensional it cuts
the data a certain way and each plane
continues to cut it down until we get
the best fit or match let's understand
this with the help of an example problem
statement I always start with a problem
statement when you're going to put some
code together we're going to do some
coding now classifying muffin and
cupcake recipes using support Vector
machines so the cupcake versus the
muffin let's have a look at our data set
and we have the different recipes here
we have a muffin recipe that has a so
much flour I'm not sure what measurement
55 is in but it has 55 maybe it's
ounces but uh it has a certain amount of
flour certain amount of milk sugar
butter egg baking powder vanilla and
salt and So based on these measurements
we want to guess whether we're making a
muffin or a cupcake and you can see in
this one we don't have just two features
we don't just have height and weight as
we did before between the male and
female in here we have a number of
features in fact in this we're looking
at eight different features to guess
whether it's a muffin or a cupcake
what's the difference between a muffin
and a cupcake turns out muffins have
more flour while cupcakes have more
butter and sugar so basically the
cupcakes a little bit more of a dessert
where the muffins a little bit more of a
fancy bread but how do we do that in
Python how do we code that to go through
recipes and figure out what the recipe
is and I really just want to say
cupcakes versus muffins like some big
professional wrestling thing before we
start in our cupcakes versus muffins we
are going to be working in Python
there's many versions of python many
different editors that is one of the
strengths and weaknesses of python is it
just has so much stuff attached to it
it's one of the more popular data
science programming packages you can use
in this case we're going to go ahead and
use anaconda and Jupiter notebook the
Anaconda Navigator has all kinds of fun
tools once you're into the Anaconda
Navigator you can change environments I
actually have a number of environments
on here we'll be using python 36
environment so this is in Python version
36 although it doesn't matter too much
which version you use I usually try to
stay with the 3x because they're current
unless you have a project that's very
specifically in version 2x 27 I think is
usually what most people use in the
version two and then once we're in our
um Jupiter notebook editor I can go up
and create a new file and we'll just
jump in here
in this case we're doing spvm muffin
versus Cupcake and then let's start with
our packages for data
analysis and we almost always use a
couple there's a few very standard
packages we use we use import oops
import
import
numpy that's for number python they
usually denote it as NP that's very
comma that's very common and then we're
going to import pandas as
PD and numpy deals with number arrays
there's a lot of cool things you can do
with the numpy uh setup as far as
multiplying all the values in an array
in a numpy array data array pandas I
can't if we're using it actually in this
data set I think we do as an import it
makes a nice data frame and the
difference between a data frame and a
nump array is that a data frame is more
like your Excel spreadsheet you have
columns you have indexes so you have
different ways of referencing it easily
viewing it and there's additional
features you can run on a data frame and
pandas kind of sits on numpy so they you
need them both in there and then finally
we're working with the support Vector
machine so from sklearn we're going to
use the sklearn model import svm support
Vector
machine and then as a data scientist you
should always try to visualize your data
some data obviously is too complicated
or doesn't make any sense to the human
but if it's possible it's good to take a
second look at it so that you can
actually see what you're doing and for
that we're going to use two packages
we're going to import map plot library.
pyplot as PLT again very common and
we're going to import caborn as SNS and
we'll go ahead and set the font scale in
the SNS right in our import line that's
what this um semicolon followed by a
line of data we're going to set the s NS
and these are great cuz the the Seaborn
sits on top of matap plot Library just
like Panda sits on numpy so it adds a
lot more features and uses and control
we're obviously not going to get into
matplot library and Seaborn it' be its
own tutorial we're really just focusing
on the svm the support Vector machine
from sklearn and since we're in Jupiter
notebook uh we have to add a special
line in here for our met plot library
and that's your perent sign or Amber
sign map plot library in line now if
you're doing this in just a straight
code Project A lot of times I use like
notepad++ and I'll run it from there you
don't have to have that line in there
because it'll just pop up as its own
window on your computer depending on how
your computer's set up because we're
running this in the jupyter notebook as
a browser setup this tells it to display
all of our Graphics right below on the
page so that's what that line is for and
the first time I ran this I didn't know
that and I had to go look that up years
ago was quite a headache so map plot
library in line is just because we're
running this on the web setup and we can
go ahead and run this make sure all our
modules are in they're all imported
which is great if you don't have them
import you'll need to go ahead and pip
use the PIP or however you do it there's
a lot of other install packages out
there although pip is the most common
and you have to make sure these are all
installed on your python setup the next
step of course is we got to look at the
data can't run a model for predicting
data if you don't have actual data so to
do that let me go aad and open this up
and take a look and we have our uh
cupcakes versus muffins and it's a CSV
file or CSV meaning that it's comma
separated
variable and it's going to open it up in
a nice uh spreadsheet for me and you can
see up here we have the type we have
muffin muffin muffin cupcake cupcake
cupcake and then it's broken up into
flour milk sugar butter egg baking
powder vanilla and salt
so we can do is we can go ahead and look
at this data also in our
python let us create a variable recipes
equals we're going to use our pandas
module. read CSV remember is a comma
separated
variable and the file name happened to
be cupcakes versus muffins oops I got
double brackets
there do it this way
there we go cupcakes versus
muffins because the program I loaded or
the the place I saved this particular
Python program is in the same folder we
can get by with just the file name but
remember if you're storing it in a
different location you have to also put
down the full path on
there and then because we're in pandas
we're going to go ahead and you can
actually in line you can do this but let
me do the full print you can just type
in
recipes. head in the Jupiter notebook
but if you're running in code in a
different script You' need to go ahead
and type out the whole print recipes.
head and Panda knows is that's going to
do the first five lines of data and if
we flip back on over to the spreadsheet
where we opened up our CSV
file uh you can see where it starts on
line two this one calls it zero and then
2 3 4 5 six is going to match go and
close that out we need that anymore and
it always starts at zero and these are
it automatically indexes it since we
didn't tell it to use an index in here
so that's the index number for the
leftand side and it automatically took
the top row as uh labels so Panda's
using it to read a CSV is just really
slick and fast one of the reasons we
love our pandas not just because they're
cute and cuddly teddy
bears and let's go ahead and plot our
dat
and I'm not going to plot all of it I'm
just going to plot the uh sugar and
flour now obviously you can see where
they get really complicated if we have
tons of different features and so you'll
break them up and maybe look at just two
of them at a time to see how they
connect and to plot them we're going to
go ahead and use caborn so that's our
SNS and the command for that is SNS dolm
plot and then the two different
variables I'm going to plot is flour and
sugar data equals recipes the Hue equals
type and this is a lot of fun because it
knows that this is pandas coming in so
this is one of the powerful things about
pandas mixed with Seaborn and
doing
graphing and then we're going to use a
pallet set one there's a lot of
different sets in there you can go look
them up for Seaborn we do a regular a
fit regular equals false so we're not
really trying to fit anything
and it's a scatter
kws a lot of these settings you can look
up in Seaborn half of these you could
probably leave off when you run them
somebody played with this and found out
that these were the best settings for
doing a Seaborn plot and let's go ahead
and run that and because it does it in
line it just puts it right on the
page and you can see right here that
just based on sugar and flour alone
there's a definite split and we use
these models because you can actually
look at it and say hey if I drew a line
right between the middle of the blue
dots and the red dots we'd be able to do
an svm and and a hyperplane right there
in the
middle then the next St is to format or
pre process our
data and we're going to break that up
into two
parts we need a type label and remember
we're going to decide whether it's a
muffin or a cupcake well a computer
doesn't know muffin or cupcake it knows
zero and one so what we're going to do
is we're going to create a type label
and from this we'll create a nump array
and P where and this is where we can do
some logic we take our recipes from our
Panda and wherever type equals muffin
it's going to be zero and then if it
doesn't equal muffin which is cupcakes
it's going to be one so create our type
label this is the answer so when we're
doing our training model remember we
have to have a a training data this is
what we're going to train it with is
that it's zero or one it's a muffin or
it's
not and then we're going to create our
recipe
features and if you remember correctly
from right up here the First Column is
type so we really don't need the type
column that's our muffin or cupcake and
in pandas we can easily sort that out
we take our value
recipes do columns that's a pandas
function built into
pandas do values converting them to
values so it's just the column titles
going across the top and we don't want
the first one so what we do is since it
always starts at zero we want
one colon till the
end and then we want to go ahead and
make this a list and this converts it to
a list of
strings and then we can go ahead and
just take a look and see what we're
looking at for the features make sure it
looks right let me go ahead and run
that and I forgot the S on recipes so
we'll go ahead and add the s in there
and then run that and we can see we have
flour milk sugar butter egg baking
powder vanilla and salt and that matches
what we have up here where we printed
out everything but the type so we have
our features and we have our
label Now the recipe features is just
the titles of the columns and we
actually need the
ingredients and at this point we have a
couple options one we could rent it over
all the
ingredients and when you're dealing this
usually you do but for our example we
want to limit it so you can easily see
what's going on because if we did all
the ingredients we have you know that's
what um
seven eight different hyperplanes that
would be built into it we only want to
look at one so you can see what the svm
is
doing and so we'll take our recipes and
we'll do just flour and sugar again you
can replace that with your recipe
features and do all of them but we're
going to do just flour and sugar and
we're going to convert that to values we
don't need to make a list out of it
because it's not string values these are
actual values on there and we can go
ahead and just
print and ingredients and you can see
what that looks
like uh and so we have just the N of
flour and sugar just the two sets of
plots and just for fun let's go ahead
and take this over here and take our
recipe
features and so if we decided to use all
the recipe features you'll see that it
makes a nice column of different data so
it just strips out all the labels and
everything we just have just the values
but because we want to be able to view
this e easily in a plot later on we'll
go ahead and take that and just do flour
and
sugar and we'll run that you'll see it's
just the two
columns so the next step is to go ahead
and fit our
model we'll go and just call it model
and it's a svm we're using a package
called
SVC in this case we're going to go ahead
and set the kernel equals linear so it's
using a specific setup on there and if
we go to the reference on their website
for the
svm you'll see that there's about
there's eight of them here three of them
are for
regression three are for classification
the SVC support Vector classification is
probably one of the most commonly used
and then there's also one for detecting
outliers and another one that has to do
with something a little bit more
specific on the model but SVC and svr
are the two most commonly used standing
for support vector vector classifier and
support Vector regression remember
regression is an actual value a float
value or whatever you're trying to work
on and SBC is a classifier so it's a yes
no true
false but for this we want to know zer1
muff cupcake we go ahead and create our
model and once we have our model created
we're going to do model. fit and this is
very common especially in the sklearn
all their models are followed with the
fit
command and what we put into the fit
what we're training with it is we're
putting in the ingredients which in this
case we limited to just flour and sugar
and the type label is it a muffin or a
cupcake now in more complicated data
science series you'd want to split into
we won't get into that today we split it
into uh training data and test data and
they even do something where they split
it into thirds where a third is used for
where you switch between which one's
training and test there's all kinds of
things go into that it gets very
complicated when you get to the higher
end not overly complicated just an extra
step which we're not going to do today
because this is a very simple set of
data and let's go ahead and run this and
now we have our model fit and I got an
error here so let me fix that real quick
it's Capital SBC it turns
out I did it
lowercase support
Vector classifier there we go let's go
ahead and run that and you'll see it
comes up with all this information that
it prints out automatically these are
the defaults of the model you notice
that we changed the kernel to linear and
there's our kernel linear on the print
out and there's other different settings
you can mess
with we're going to just leave that
alone for right now for this we don't
really need to mess with any of
those so next we're going to dig a
little bit into our newly trained model
and we're going to do this so we can
show you on a graph
and let's go ahead and get the
separating we're going to say we're
going to use a W for our variable on
here we're going to do model.
coefficient
or0 so what the heck is that again we're
digging into the model so we've already
got a prediction and a train this is a
math behind it that we're looking at
right now and so the W is going to
represent two different coefficients and
if you remember we had y = mx + C so
these coefficients are connected to that
but in two-dimensional it's a
plane we don't want to spend too much
time on this because you can get lost in
the confusion of the math so if you're a
math Wiz this is great you can go
through here and you'll see that we have
a equal minus W of 0 over W of 1
remember there's two different values
there and that's basically the the slope
that we're
generating and then we're going to build
an XX what is XX we're going to set it
up to a numpy array there's our np.
linespace so we're creating a
line of values between 30 and 60 so it
just creates a set of numbers for
x and then if you remember correctly we
have our formula y equal the slope time
X plus the intercept well to make this
work we can do this as y y equals the
slope times each value in that array
that's the neat thing about numpy so
when I do a * XX which is a whole numpy
array of values it multiplies a across
all of them and then it takes those same
values and we subtract the model
intercept that's your uh remember we had
MX plus C so that'd be the C from the
formula yal MX plus
C and that's all these numbers come from
a little bit confusing because it's
digging out of these different arrays
and then we want to do is we're going to
take this and we're going to go ahead
and plot it so plot the parallels to
separating hyperplane that pass through
the support vectors and so we're going
to create b equals a model support
vectors pulling our support vectors out
there here's our YY which we now know is
a set of data and we have uh we're going
to create y y down equal a * XX Plus
B1 minus a * B 0 and then model support
Vector B is going to be set that to a
new value the minus one setup and y y up
equals a * XX + B1 - A * B 0 and we can
go ahead and just run this to load these
variables up if you want to know
understand a little bit more what's
going on you can see if we
print y y we just run that you can see
it's an array it's this is a line it's
going to have in this case between 30
and 60 so it's going to be 30 variables
in here and the same thing with y y up y
y down and we'll we'll plot those in
just a minute on a graph so you can see
what those look
like just go ahead and delete that out
of here and run that so it loads up the
variables nice clean slate I'm just
going to copy this from before remember
this our SNS our caborn plot LM plot
flow sugar
and I'll just go and run that real quick
so you can see what remember what that
looks like it's just a straight graph on
there and then one of the neat things is
because caborn sits on top of Pi plot we
can do the pi plot for the line going
through and that is simply PLT do
plot and that's our xx and y y our two
corresponding values XY and then
somebody played with this to figure out
that the line width equals two in the
color black black would look nice so
let's go ahead and run this whole thing
with the PIP plot on there and you can
see when we do this it's just doing
flower and sugar on
here corresponding line between the
sugar and the flour and the muffin
versus
Cupcake um and then we generated the um
support vectors the y y down and y y up
so let's take a look and see what that
looks
like so we'll do our PL
plot and again this is all against XX
the our x value but this time we have y
y
down and let's do something a little fun
with this we can put in a k dash dash
that just tells it to make it a dotted
line and if we're going to do the down
one we also want to do the up one so
here's our
YY up and when we run that it both sets
aligned and so here's our support and
this is what you expect you expect these
two lines to go through the nearest data
point so the dash lines go through the
nearest muffin and the nearest cupcake
when it's plotting it and then your svm
goes right down the middle so it gives
it a nice split in our data and you can
see how easy it is to see based just on
sugar and flour which one's a muffin or
a
cupcake let's go ahead and create a
function
to
predict muffin or
cupcake I've got my um recipes I pulled
off the um internet and I want to see
the difference between a muffin or a
cupcake so we need a function to push
that through and create a function with
de and let's call it muffin or cupcake
and remember we're just doing flour and
sugar today not doing all the
ingredients and that actually is a
pretty good split you really don't need
all the ingredients to know it's flour
and
sugar and let's go ahead and do an IFL
statement so if model
predict is of flour and sugar equals
zero so we take our model and we do run
a predict it's very common in sklearn
where you have a DOT predict you put the
data in and it's going to return a value
in this case if it equals zero then
print you're looking at a muffin recipe
else if it's not zero that means it's
one then you're looking at a cupcake
recipe that's pretty straightforward
for function or def for definition DF is
how you do that in Python and of course
if you're going to create a function you
should run something in it and so let's
run a cupcake and we're going to send it
values 50 and 20 a muffin or a cupcake I
don't know what it is and let's run this
and just see what it gives us and it
says oh it's a muffin you're looking at
a muffin recipe so it very easily
predicts whether we're looking at a
muffin or a cupcake recipe let's
plot this there we go plot this on the
graph so we can see what that actually
looks like and I'm just going to copy it
and pasted From Below where we plotting
all the points in there so this is
nothing different than what we did
before if I run it you'll see it has all
the points and the lines on there and
what we want to do is we want to add
another point and we'll do PLT
plot and if you remember correctly we
did for our test we did 50 and 20
and then somebody went in here and
decided we'll do yo for yellow or it's
kind of a orangest yellow color is going
to come out marker size nine those are
settings you can play with somebody else
played with them to come up with the
right setup so it looks good and you can
see there it is graph um clearly a
muffin in this case in cupcakes versus
muffins the muffin has won and if you'd
like to do your own muffin cupcake
Contender series you certainly can send
a note down below and the team at simply
learn will send you over the data they
use for the muffin and cupcake and
that's true of any of the data um we
didn't actually run a plot on it earlier
we had men versus women you can also
request that information to run it on
your data setup so you can test that
out so to go back over our setup we went
ahead for our support Vector machine
code we did a predict 40 Parts flour 20
Parts sugar I think it was different
than the one we did whether it's a
muffin or a cupcake hence we have built
a classifier using SPM which is able to
classify if a recipe is of a cupcake or
a muffin which wraps up our cupcake
versus muffin so what is deep learning
deep learning is a subset of machine
learning which itself is a branch of
artificial intelligence unlike
traditional machine learning models
which require manual feature extraction
deep learning models automatically
discovers representation from raw data
so this is made possible through neural
networks particularly deep neural
network which consist of multiple layers
of interconnected nodes so these neural
network are inspired by the structure
and the function of human brain each
layer in the network transform the input
data into more abstract and composite
representation for instance in image
recognition the initial layer might
detect simple features like edges and
textures while the deeper layer
recognize more complex structure like
shapes and objects so one of the key
advantage of deep learning is its
ability to handle large amount of
unstructured data such as images audios
and text making it extremely powerful
for various application so stay tuned as
we delve deeper into how these neural
networks are trained the types of deep
learning models and some exciting
application that are shaping our future
types of deep learning deep learning AI
can be applied supervised unsupervised
and reinforcement machine learning using
various methods for each the first one
supervis machine learning in supervised
learning the neural network learns to
make prediction or classify that data
using label data sets both input
features and Target variables are
provided and the network learns by
minimizing the error between its
prediction and the actual targets a
process called back propagation CNN and
RNN are the common deep learning
algorithms used for tasks like image
classification sentiment analysis and
language translation the second one
unsupervised machine learning in
unsupervised machine learning the neural
network discovers patterns or cluster in
unlabel data sets without Target
variables it identifies hidden pattern
or relationship within the data
algorithms like Auto encoders and
generative models are used for tasks
such as clustering dimensionality
reduction and anomaly detection the
third one reinforcement machine learning
in this an agent learns to make decision
in an environment to maximize a reward
signal the agent takes action observes
the records and learns policies to
maximize cumulative rewards over time
deep reinforement learning algorithms
like deep Q networks and deep
deterministic poly gradient are used for
tasks such as Robotics and gameplay
moving forward let's see what are the
artificial neural networks artificial
neural networks Ann inspired by the
structure and the function of human
neurons consist of interconnected layers
of artificial neurals or units the input
layer receives data from the external
resources and and it passes to one or
more hidden layers each neuron in these
layers computes a weighted sum of inputs
and transfers the result to the next
layer during training the weight of
these connection are adjusted to
optimize the Network's performance a
fully connected artificial neural
network includes an input layer or more
hidden layers and an output layer each
neuron in a hidden layer receives input
from the previous layer and sends its
output to the next layer so this process
continues until the fin fin output layer
produced and network response so moving
forward let's see types of neural
networks so deep learning models can
automatically learn featur from data
making them ideal to tasks like image
recognition speech recognition and
natural language processing so the most
common architecture in deep learnings
are the first one feed foral neural
network fnn so these are the simplest
type of neural network where information
flows linearly from the input to the
output they are widely used for such as
image classification speech recognition
and natural language processing NLP the
second one convolutional neural network
designed specifically for image and
video recognition CNN automatically
learn feature from images making them
ideal for image classification object
detection and image segmentation the
third one recurrent neural networks RNN
are specialized for processing
sequential data time series and natural
language they maintain an internal state
to capture information from previous
input making them suitable for task such
as spee recognition NLP and language
translation so now let's move forward
and see some deep learning application
the first one is autonomous vehicle deep
learning is changing the development of
self-driving car algorithms like CNS
process data from sensors and cameras to
detect object recognize traffic signs
and make driving decision in real time
enhancing safety and efficiency on the
road the second one is Healthcare
Diagnostics
deep learning models are being used to
analyze medical images such as x-rays
MRIs and CT scans with high accuracy
they help in early detection and
diagnosis of diseases like cancer
improving treatment outcomes and saving
lives the third one is NLP recent
advancement in NLP powered by Deep
learning models like Transformer chat
GPD have led to more sophisticated and
humanik text generation translation and
sentiment analysis so application
include virtual assistant chat BS and
automated customer service the fourth
one def technology so deep learning
techniques are used to create highly
realistic synthetic media known as defix
while this technology has entertainment
and creative application it also raises
ethical concern regarding misinformation
and digital manipulation the fifth one
predictive maintenance in Industries
like manufacturing and Aviation deep
learning models predict equipment
failures before they occur by analyzing
sensor data the proactive approach
reduces downtime lowers maintenance cost
and improves operational efficiency so
now let's move forward and see some
advantages and disadvantages of deep
learning so first one is high
computational requirements so deep
learning requires significant data and
computational resources for training
whereas Advantage is high accuracy
achieves a state-of-the-art performance
in task like image recognition and
natural language processing whereas deep
learning needs large label data set
often require extensive label data set
for training which can be costly and
time consuming together so second
advantage of deep learning is automated
feature engineering automatically
discovers and learn relevant features
from data without manual intervention
the third disadvantage is
overfitting so deep planning can overfit
to training data leading to poor
performance on new unseen data whereas
the third deep learning Advantage is
scalability so deep learning can handle
large complex data set and learn from
massive amount of data so in conclusion
deep learning is a transformative leap
in AI mimicking human neural networks it
has changed healthare Finance autonomous
vehicles and
[Music]
NLP learning objectives welcome to math
refresher linear
algebra in this lesson lesson we are
going to explain the concepts of linear
algebra solve a linear system of
equations describe Matrix forms of
Matrix and Matrix operations and Define
vectors and list down its properties
introduction to linear
algebra linear algebra linear algebra
refers to study of linear combinations
for linear transformations to be carried
out a study of vector spaces line and
planes as well as some mappings as
necessary it contains linear functions
vectors and matrices it is an
examination of characteristic of linear
set
Transformations linear equations linear
algebra's major goal is to establish
systematic techniques for solving
systems of linear
equations a linear equation within
variables can be represented as A1 X1
Plus plus A2 X2 plus A3 X3 and so on
till a n xn equals to B where X1 X2 X3
and so on till xn are the unknown
quantities that are to be found while A1
A2 A3 and so on until a in are the
coefficients B here is a constant term a
linear equation with two variables X and
Y is a linear con connection between X
and Y the value of one variable often Y
is determined by the value of the other
that is X the independent variable in
this situation is X while the term
dependent variable refers to Y since it
depends on X let's consider some
examples for linear
equations linear equations in one
variable can be written as 3x + 5 = 0 or
3x2 of x + 7 = to 0 or 98x = to
49 linear equations in two variables
examples can include y + 7 x = 3 or 3 a
+ 2 B = to 5 6 x + 9 y - 12 = to 0
linear equations can also be written
with three variables as x + y + z = to 0
hey minus 3 3 B = to C 3x + 12 y = to
half of Z identifying linear and
nonlinear
equations linear equations present
themselves as a straight line on a graph
however nonlinear equations represent
themselves as a non-straight line on the
graph here equations like y = to 8 x -
9 or y + 3x - 1 = to 0 are examples of
linear
equations however y = to x^2 - 7 is a
nonlinear equation because X is raised
to the^
2 hence denoting a nonlinear
relationship or a non-straight line
relationship between X and
Y similarly Square < TK of y + x = 6 or
Y raed to^ 2 - x = 9 our examp examples
of nonlinear equations as Y is raised to
the power half and two
respectively forms of linear
equation linear equations can be
represented in three forms a standard
form a slope intercept form or a point
slope
form linear equation in standard form a
linear equation in standard form for a
single variable can be represented as a
x + b equals to zero where A and B are
real integers and X is the variable
while a linear equation in standard form
for two variable can be represented as
ax + b y = 2 C where A and C are real
integers while X and Y are the
variables to summarize a linear equation
for variables can be written as ax + b y
equal to C where X and Y are the
variables C is the constant and a and b
are the coefficients of X and Y
respectively linear equation in the
slope intercept form a linear equation
slope can be calculated to see how one
variable varies in response to a unit
change in another variable it is
represented as yal to mx + b where m is
the slope B is the intercept on the Y
AIS and X and Y are the lines distance
from the X and the Y AIS
respectively consider this example the
line's point x y represents the distance
from the X and the Y AIS
respectively while the line intercept on
the Y AIS is at 0 comma
B linear equation slope the slope
indicates how steep a line is with
respect to Y AIS when seen from left to
right it indicates whether the line goes
up or
down the slope describes how independent
variable has been changed while the
dependent variable is
changing types of slope there are four
types of slopes based on the
relationship between the two variables X
and Y the first is the positive slope
here as X
increases Y is also increasing in a
negative slope as X increases y
decreases if a line is parallel to X AIS
it is a zero
slope on the other hand if the line is
parallel to Y AIS then the slope is
undefined linear equation in point slope
form a straight line is represented in
point slope form by its slope and a
point on the
line it is represented as y - y1 = to M
of x -
X1 where X1 and y1 are the coordinates
of the
point linear equation forms some
examples consider solving the linear
equation given as 2x - 10 / 2 = to 3x -
1
first we will clear the
fraction in this example we can see only
left hand side we have 2x - 10 / 2 if
from the numerator we take two as common
we will be left with 2 * x - 5 / 2 hence
x - 5 on the left hand side on the right
hand side we still have 3 * x -1 now as
a part of next step we simplify both
sides of the equation that has open up
the brackets when we do that on the
right side we end up getting 3 x - 3 by
multiplying 3 to X - 1 solving further
we can write as X = to 3x + 2 finally we
try to clear the fraction to solve for x
we can rearrange the equation as x - 3 x
=
2 solving it further we get - 2x =
2 and then we solve for x as
-1 system of linear equation a system of
linear equations is a finite collection
of linear equations usually involving
the same variables it can be represented
as a11 X1 plus A1 2x2 and so1 equals to
B1 a21 X1 + A2 2x2 so on equals to B2
and so on here this example is for n
variables a system of M
equations a linear system can have
unlimited number of solutions one
solution or no solution at all in case
of a single linear equation a consistent
linear system has a solution while an
inconsistent linear system has no
solution solving a linear equation
solving linear system of
equations various methods can be used to
solve linear system of equations which
include graphic method substitution
method linear combination or elimination
method or a matrix
method graphing method let's try and
solve these two linear equations using
the graphic method the first equation is
defined as y equal 0 .5x + 2 and the
second equation is Y = -2x - 3 the two
equations as we can see are in the slope
intercept form the first line has a
slope of 0.5 and a y intercept of pos2
while the second line has a slope of -2
and a y intercept of -3 let's have a
look at these lines through a
plot here the blue line is the first
line represented by y = 2.5 x +
2 and the red line is represented by y =
to -2X
-3 these two lines intersect at a point
at -2 comma
1 hence we can say that X = to -2 and Y
= to 1
solving systems of linear equation using
substitution for substitution we start
by putting one of the equations in the
form of variable equals
to we then substitute that variable in
the other equation in its place take the
other equation to
solve if there are more than two
variables involved we repeat the process
from steps 1 through three
let's consider an
example let's consider the equation 3x +
2 y = 19 and x + y = to 8 we could begin
with any equation and variable in this
case let's look at the second equation
with variable
y we start with subtracting X from both
sides of x + y =
8 and we end up getting y = 8 -
X the first equation still retains its
form now we take the value of y that is
8 - x and substitute in the first
equation in place of
Y we end up getting the equation 3 x + 2
* 8 - x = to
19 we further simplify and solve this
equation using algebra method to get 3x
+ 16 - 2x = 290
we solve this equation further and we
end up getting x + 16 = to 19 and
finally x equal to
3 now we take the value of x and
substitute in the new reformed second
equation as y = to 8 -
x that gives us y = to 8 - 3 hence y =
to 5 The Final Answer remains X = to 3
and and Y = to 5 solving systems of
linear equation using elimination in
this case we start by multiplying an
equation by a constant anything other
than
zero we then add or subtract an equation
onto the other
equation let's consider the same example
of 3x + 2 y = to 19 and x + y =
8 we first start by m multiplying the
second equation by 2 and we get 2x + 2 y
=
216 now we subtract the second equation
from the first
equation that is 3x -
2x 2 y - 2 Y and 19 -
60 on solving this we end up getting X =
to
3 Now using these steps of substitution
we we put the value of x in the original
second equation that is x + y =
8 and achieve a value of y = to
5 hence the answer remains x = 3 and Y =
to
5 we can verify the solution by using
the
graph here again the Blue Line
represents the point at which 3x + 2 y =
to 19 is true while the red line
represents the point where X+ y = to 8
is
true the solution is found at the point
where X = to
3 and Y = to
5 this is the point where both lines
intersect introduction to
matrices
Matrix a matrix is rectangular array or
table with rows and Columns of numbers
symbols Expressions used to represent a
mathematical object or an
attribute for example here a is a matrix
of two rows and three
columns as 213 minus one 2 and four well
B is a squared Matrix with two rows and
two columns given as 2 1 3 4 Matrix size
The Matrix size is expressed as M cross
n where m is the number of rows and N is
the number of columns a matrix with two
rows and three columns can be referred
to as 2x3 Matrix or a 2 cross3 Matrix or
a matrix of Dimension 2 cross
3 notation of a matrix a matrix with M
rows and N columns can be presented as
this here each element will be
identified by its row number and column
number for example the element over here
is in the third row and the First
Column hence it is identified as a
31 forms of Matrix The Matrix is termed
as a squared Matrix if n equals to M
that is the number of rows and number of
columns are
equal an element's entry in The Matrix
of from a a i i is located on the
diagonal hey is called a diagonal matrix
if hey I J equals to
zero when I is not equals to
J that is in a square
Matrix apart from diagonal at all places
we have zero as a value Matrix
operations addition addition of two
matrices is is adding their
corresponding elements as in a11 + b11
a12 + B12 a21 +
B21 a22 plus B22 and so on consider the
example over here we have a matrix a as
22 32 11 16 and Matrix b as 13 8 13 and
16 Matrix a is of the order two cross 2
and Matrix B is also of the same order 2
cross 2 hence the corresponding elements
can be quickly added as 22 +
13 32 + 8 11 + 13 and 16 + 16 giving us
the result in Matrix of 35 40 24 and
32 addition rules only matrices with the
same size can be added
that is the two matrices should have
same number of rows and same number of
columns Matrix addition follows
commutative property that means a + b is
equals to B +
a
subtraction Matrix subtraction is same
as Matrix
addition it is subtracting the
corresponding
values let's consider the same example
of Matrix A and B the result of a minus
B can be given as 22 - 13 that is a11 -
b11 32 - 8 that is a12 - B12 11 - 13 a21
- B21 and 16 - 16 that is a22 -
B22 giving us result as 9 24 -2 and 0
subtraction rules only matrices with the
same number of rows and columns can be
subtracted Matrix subtraction does not
follow commutative
property that means a minus B is not
equals to B minus
a
multiplication matrix multiplication
results in what is called as a product
Matrix in matrix multiplication
each row of the first Matrix is
Multiplied to each column of the second
Matrix to compute each of these element
in the Matrix the values are then
added for example consider the Matrix A
and
B to get the first element we will
multiply the first row of Matrix a to
the First Column of Matrix
B and then add the individual elements
that is 22 multiply to 13 and 32
multiply to 13 and then add them up to
get this particular
element for the second element we
multiply the first row of a to the
second column of B hence getting 22
multiplied to 8 + 32 multip to
16 similarly we compute all the
remaining elements that is the second
row multiplied to the First Column and
added 11 multiplied to 13 + 16
multiplied to 13 and the second row now
multiplied to the second column that is
11 multiplied to 8 + 16 multiplied to
16 this gives us the resulting Matrix as
72
688
351 and 344
multiplication
rules let us say C equals to
AB here C is our product
Matrix which can be computed as
summation over aj.
bjk for a matrix of size i j Matrix B of
size
JK to calculate the value of each
member in the I cross K Matrix of
C the matrix product a is defined only
when the number of columns in a equals
to the number of rows in
B the matrix product of ba is defined
only when the number of columns in B is
equals to the number of rows in a a do B
is not always equals to b.
a
transpose a transpose is a matrix form
by turning all the rows of a given
Matrix
into the columns and vice
versa the transpose of a matrix a is
denoted as
a for example if the Matrix a is 22 32
11 and 16 the transpose of a would be 22
11 32 and
16 here we can see the row has been
converted to
column inverse if it is a non- singular
Square Matrix
there exists n cross n Matrix a minus
one known as A's
inverse that satisfies a. a inverse
equal to a inverse. AAL to
I where I is the identity
Matrix and identity Matrix is
characterized as a diagonals one and all
other elements are
zero also Al AB equals to b.
a equals to i n where I in denotes the N
cross in identity
Matrix special Matrix types symmetric
Matrix a matrix a is set to be symmetric
if a is equals to transpose of a
diagonal matrix a matrix D is diagonal
only if D equals to zero for all I not
equals to J identity
Matrix identity Matrix is denoted as I
in where a in equals to I a equals to
A tensors tensors are arrays with more
than two
axes a tensor can have in dimensions a i
j k is the value in the tensor a at
coordinates i j and k the principal
component analysis
PCA which is a statistical technique
used to reduce the number of dimensions
in a given data set while we preserve
its
variability so there are different use
cases of PCA in realtime scenarios one
of the major use case is in the
healthcare domain so a technically known
as the spike triggered covariance
analysis it uses the principle of PCA in
Neuroscience to identify the specific
properties of a stimulus that increase a
neuron's probability of generating an
action
potential so it is using reduction
techniques which is going to be really
helpful in detecting the coordinated
activities of large neuronal
ensembles so it is basically used for
identifying a neuron from the shape of
its action potential then in terms of
quantitative Finance so PCA help us in
reducing the dimensionality of a complex
problem let's say a fund manager has 200
stocks in his portfolio now to analyze
these stocks quantitatively a stock
manager will require a correlational
metric of a size 200x 200 which makes
the problem very
complex however if he has to extract 10
principal components which best
represents the variance in the stock
price this will reduce the complexity of
the problem while still EXP explaining
the movement of all 200 stocks so we use
it for analyzing the shape of the yield
curve in terms of forecasting the
portfolio returns developing the asset
allocation algorithms and so on and it
is also widely used in image compression
so PCA is also used for image
compression so if we go ahead and use
any of the py charm ID e so here we can
work with matte plot lib image Pi plot
and the nump as NP and here we can also
import from sklearn and we can import
the PCA so basically we are trying to
print the shape of the image by reading
the image this is a sample
logo.png which has been placed in the
same directory we are trying to print
the image so if we run this particular
code then we will be able to see the
shape of the image return to us first
the current shape based on the image
which has been deducted which is
156 94 and 3 so what does this mean this
means image is in the form of
156 rows and each row containing 194
pixels and then having three channels
which is red green and blue so we had to
resize the image so that it is in a
format required for PCA input which is
196x 3 which is going to be 500
82 so here we can see we have performed
a simple operation as image R where we
are using the numy library for reshaping
the image and then we are printing the
shape of the image again so here we can
see
156 and
582 has been returned as a response now
running the PCA with 32 principle
components we can use this fit for the
image that has been reshaped and then we
are trying to TR the transform image so
with these 32 components we are able to
express
98.7% of the variance so inverse
transforming the PC output and reshaping
the visualization can also be done
forward and that's how PCA is widely
used as a part of the medical and the
other Sciences for example is used for
object detection model for the
healthcare domain for the transportation
Aviation and location BAS
Services introduction to
vectors objects with both magnitude and
direction are called as vectors the
magnitude of the vector determines its
size it is represented as a line with an
arrow where the length of the line
indicates the vector's magnitude and the
arrow points in the desired Direction
other names for it include ukian Vector
geometric Vector spatial vector or
simply vector
its length is indicated by the symbol
double bar vble bar and it begins at the
original 0 comma 0 notation of a vector
a vector can be represented in a
standard form as a equal to a i plus b
JC plus c k cap where a B and C are
numeric values the unit vectors along
the XY Z AIS are i j and k cap respect
effectively as we know in 2024 Ai and ml
are evolving at lightning speed AI tools
like chat GPD Google Gemini and apple
Cutting Edge AI are Transforming Our
World CH gbt is changing customer
service language translation and even
creative content whereas Google Geminis
is pushing the boundaries of AI
enhancing search experience and making
information access smarter and more
intuitive Apple CI with Innovations like
Ci and Apple's intelligence is is taking
user experience to the next level with a
smarter personal assistant enhanced
device performance and top-notch privacy
features with these advancement these
Technologies are driving a huge demand
for AI engineers and ml Engineers
companies across the globe are eager to
integrate these Advanced solution to
boost efficiency personalize customer
experience and innovate in the fields
like healthcare logistic and in many
others so the opportunities in AI anml
field are endless and Incredibly
exciting so imagine being a part of this
revolutionary wave shaping the future
with your skills so in this video we
will show you exactly how to get there
we will cover the essential skills you
need to learn the best learning path
exciting Hands-On project and the
hottest career opportunities whether you
are just starting out to looking to
advance your career this s will guide
you to successfully become Ai and ml
engineer so before moving forward let me
give you a quick information as a demand
for machine learning professionals keep
increasing it's a crucial for aspiring
individuals to have a clear road map to
navigate their Learning Journey so
without any further Ado let's get
started so how this machine learning
road map will help you this machine
learning road map offers a clear and
structured path to mastering machine
learning by following this roadmap you
will not only gain valuable knowledge
but also develop a mindset gear towards
Innovation and adaptability so what is
machine learning and AI imagine a
computer that learns from data like a
student that's machine learning and
machine learning is a subset of
artificial intelligence AI that enables
computer to learn from the data and make
prediction or decision without being
explicitly programmed it uses algorithm
and Stat models to improve performance
or specific task through experience or
data input so Artificial Intelligence on
the other hand covers a broader scope
and involves developing computer system
that perform tasks typically requiring
human intelligence so machine learning
is a crucial component of AI providing
the capability to learn and adapt from
the data so now let's move forward and
see the steps for AIML road map so step
byst step machine learning road map so
this stepbystep machine learning road
map guides you through mastering ml a
vital branch of AI typically over
several months to a year so depending on
your background so start with prequest
like programming python R stat and
linear algebra progress through
understanding data processing learning
algorithms and model evaluation and
optimization so this structured approach
which combined with Hands-On projects
will solidified you ml expertise while
preparing you for the advanced topics
and machine learning applications in the
dynamic field so now let's get started
this step one mastering mathematics so
to excel in machine learning a strong
foundation in mathematics is essential
so focus on the following areas the
first one is linear algebra and calculus
so linear algebra is the backbone of
many machine learning algorithms it
helps you understand how each algorithm
works whereas calculus is crucial for
optimizing algorithm used in machine
learning key concept include like
vectors metrics linear equations IGN
values differentiation integration and
gradient descent the second one is
probability and statistic so probability
and sets are fundamentals for analyzing
data and making prediction so here are
some important topics like probability
distribution descriptive stat hypothesis
testing regression analysis and basian
St so moving forward step two developing
programming skills Proficiency in
programming is essential focus on python
andr the top language for machine
learning the first one python python is
a popular due to it Simplicity and
extensive Library like numai Panda psych
learn so it's great for both beginners
and expert the second one is our
programming R is excellent for statical
analysis and data visualization
platforms like simply offer specialized
score words in the r programming the
third important python libraries learn
libraries like numai for numerical
operations pandas for data manipulation
M plot leave and seon for data
visualization and psychic learn for
machine learning the third one exploring
core machine learning ml algorithms so
once you have got a good handle on math
and programming it's time to learn core
machine learning algorithm so
understanding these will help you solve
real world problems so here are some key
algorithms to explore the first one is
unsupervised learning algorithm explore
clustering that is K means clustering
and dimensionally reduction that is PCA
for understanding patterns in unlabeled
data the second one is supervised
learning algorithms learn regression for
continuous outcomes and classification
for discrete labels covering methods
like linear regression logistic
regression K near neighbor and support
Vector machine svms and the third one is
model evaluation and validation
understand evaluation metrics like ACC
Precision recall and F1 score learn
cross validation and performance metric
to SS model performance and you can also
learn other important machine learning
algorithm like reinforcement learning
gradient descent and algorithm for slope
understanding step four learn Advanced
Topic in machine learning so as you
advance in machine learning the journey
it's important to delve into more
advanced topics so these areas will
deepen your understanding and help you
tackle complex problem so here are some
key topics to focus on the first one is
deep learning and neural networks the
second is anible learning technique
third one is generative models and
adversarial learning the fourth one is
recommendation system and collaborative
filtering the last one is time series
analysis and
forecasting so for step five there is
learn
deployment so you have to learn how to
deploy models using flask Jango cloud
services like AWS aure gcp then Docker
and kubernetes so deployment skills are
crucial for making your model accessible
and usable in real world applications so
moving forward let's see some machine
learning projects so work on real world
projects to apply your knowledge focus
on data collection preparation and
Capstone project in image recognition
NLP and there is one more predictive
modeling and anomaly deduction so
practical experience is the key to
solidifying your skills step seven
continuous learning and exploration so
stay updated with the latest development
by the following industry leaders
engaging in online communities and
working on your personal projects so
pursue Advanced learning through courses
and certification to keep your skills
sharp so now moving forward let's see
machine learning career opportunities
with sell so the job market for machine
learning professional is booming the
average annual salary for machine
learning Engineers can vary based on
location experience and Company so here
are some roles like machine learning
engine enger data scientist NLP engineer
computer vision engineer and AIML
researcher so let's see how much they
earn so the first one machine learning
engineer in us they earn around
$153,000 and in India they earn around
11 lakhs per the second is data
scientist they earn in us around
$157,000 and in India they earn around
12 lakhs per an the third one is NLP
engineer in us they earn around $17,000
and in India they earn around 7 lakh per
the fourth one come to Vision engineer
they earn around
$126,000 in the US and in India they
earn around 6.5 lakh per the last one Ai
and ml res Searcher they earn around
$130,000 in the US and 9 lakh perom in
India so note that these figures can
vary on the website to website and
changes frequently the machine learning
road map provides a structured guide to
help you navigate this Dynamic field by
following the step-by-step guide and
continuously honning your skills you can
embark on a successful career in machine
learning embrace the challenge stay
curious and equip yourself with the
necessary knowledge and expertise to
thrive in this ever evolving domain
before moving on to machine learning
skills let's take a quick look at who is
a machine learning engineer as machine
learning is intricately connected to
data science there are overlaps in the
roles of data scientist data analyst and
machine learning engineer however data
scientist and analyst primar concentrate
on extracting insights from data
presenting them for organized decision
making with some knowledge of machine
learning algorithms in contrast machine
learning Engineers are exclusively
dedicated to machine learning creating
software components for minimal human
intervention to derive insites from
provided data this underscores the
emphasis on computer fundamentals and
software development as their
specialized skills to become a machine
learning engineer you need a combination
of Technical and non-technical skills so
here are some essential skills required
to pursue a career as a machine learning
engineer skill number one is programming
languages strong programming skills are
essential you should be proficient in at
least one programming language such as
python or python is widely used in
machine Learning Community due to its
Rich libraries for example numpy pandas
tensorflow pyo that supports machine
learning tasks machine learning
algorithms and techniques you should
have a solid understanding of various
machine learning algorithms such as
linear regression listic regression
decision trees random forests neural
network and deep learning so familiarize
yourself with these algorithms
principles pros and conses and when to
use them moving on to skill number three
we have data pre-processing machine
learning models require clean and
well-prepared data you should know how
to handle missing data deal with
outliers normalize all standardized data
and perform feature engineer
understanding data pre-processing
techniques is crucial for Effective
machine learning model training moving
on to skill number four we have data
manipulation and Analysis data is the
foundation of machine learning models
you should be skilled in data
manipulation and Analysis using
libraries like numpy and pendas this
includes cleaning and transforming data
exploratory data analysis and
understanding the statistical properties
of the data the next skill we have in
our list is machine learning libraries
and Frameworks familiarity with popular
machine learning libraries and framework
is essential some commonly used ones
include numpy pandas tensorflow and pych
these libraries provide pre-implemented
machine learning algorithms neural
network architectures and tools for
model training and evaluation the next
skill we have in our list is computer
vision computer vision and machine
learning are fundamental branches of
computer science that can independently
fuel highly Advanced systems relying on
CV and machine learning algorithms
however their combination has the
potential to unlock even greater
possibilities and achievements moving on
to the next skills we have neural
network
like the human brains neurons neural
network are pivotal for machine learning
Engineers they consist of layers
processing input valuable output through
parallel and sequential computations
various types exist like feed forward
recurrent convolutional and while
detailed knowledge isn't mandatory
grasping core fundamentals is essential
for aspiring machine learning engineer
moving on to the next skill it's natural
language processing natural language
processing or NLP is a fundamental
aspect of Machining it's object
objective is to impart the complexities
of human language to computers enabling
them to comprehend and interpret human
communication more effectively numerous
libraries serve as the basis for natural
language processing offering functions
to help computers understand natural
language by passing text based on syntax
extracting key phrases eliminating
irrelevant words and one prominent
example is the natural language toolkit
a widely used platform for developing
NLP related applications now that we
have seen the Technical Machine learning
engineer skills SK let us have a look at
the non-technical skills machine
learning Engineers require the first is
industry knowledge machine learning
projects that effectively tackle genuine
challenges are likely to achieve great
success regardless of the industry you
are involved in it is crucial to
comprehensively understand its
operations and identify ways to optimize
business outcomes moving on to the last
scale for the video we have effective
communication effective communication
plays a crucial role in facilitating
these interactions companies seeking
skill machine learning Engineers value
candidates who can effectively convey
technical discoveries to non-technical
teams like marketing or sales
demonstrating Clarity and fluency in
their explanations remember that the
field of machine learning is constantly
evolving so continuous learning and
staying updated with the latest
developments and research papers are
essential to be a practical machine
learning engineer what's in it for you
we're going to cover clustering what is
clustering K means clustering which is
one of the most common used clustering
tools out there including a flowchart to
understand K means clustering and how it
functions and then we'll do an actual
python live demo on clustering of cars
based on Brands then we're going to
cover logistic regression what is
logistic regression logistic regression
curve and sigmoid function and then
we'll do another python code demo to
classify a tumor as malignant or benign
based on features and let's start with
clustering suppose we have a pile of
books of different genres
now we divide them into different groups
like fiction horror education and as we
can see from this young lady she
definitely is into heavy horror you can
just tell by those eyes in the maple
Canadian leaf on her shirt but we have
fiction horror and education and we want
to go ahead divide our books up well
organizing objects into groups based on
similarity is clustering and in this
case as we're looking at the books we're
talking about clustering things with
know categories but you can also use it
to explore data so you might not know
the categories you just know that you
need to divide it up in some way to
conquer the data and to organize it
better but in this case we're going to
be looking at clustering in specific
categories and let's just take a deeper
look at that we're going to use K means
clustering K means clustering is
probably the most commonly used
clustering tool in the machine learning
library K means clustering is an example
of unsupervised learning if you remember
from our previous thing it is used when
you have unlabeled data so we we don't
know the answer yet we have a bunch of
data that we want to Cluster into
different groups Define clusters in the
data based on feature similarity so
we've introduced a couple terms here
we've already talked about unsupervised
learning and unlabeled data so we don't
know the answer yet we're just going to
group stuff together and see if we can
find an answer of how things connect
we've also introduced feature similarity
features being different features of the
data now with books we can easily see
fiction and horror and history books but
a lot of times with data some of that
information isn't so easy to see right
when we first look at it and so K means
is one of those tools where we can start
finding things that connect that match
with each other suppose we have these
data points and want to assign them into
a cluster now when I look at these data
points I would probably group them into
two clusters just by looking at them I'd
say two of these group of data kind of
come together but in K means we pick K
clusters and assign random centroids to
clusters where the K clusters represents
two different clusters we pick K
clusters and S random centroids to the
Clusters then we compute distance from
objects to the centroids now we form new
clusters based on minimum distances and
calculate the centroids so we figure out
what the best distance is for the
centroid then we move the centroid and
recalculate those distances repeat
previous two steps iteratively till the
cluster centroids stop changing their
positions and become Static repeat keep
previous two steps iteratively till the
cluster centroid stop changing and the
positions become Static once the
Clusters become Static then K means
clustering algorithm is said to be
converged and there's another term we
see throughout machine learning is
converged that means whatever math we're
using to figure out the answer has come
to a solution or it's conversed on an
answer shall we see the flowchart to
understand make a little bit more sense
by putting it into a nice easy step by
step so we start we choose K we'll look
at the elbow method in just a moment we
assign random centroids to clusters and
sometimes you pick the centroids because
you might look at the data in a graph
and say oh these are probably the
central points then we compute the
distance from the objects to the
centroids we take that and we form new
clusters based on minimum distance and
calculate their centroids then we
compute the distance from objects to the
new centroids and then we go back and
repeat those last two steps we calculate
the distances so as we're doing it
brings into the new centroid and then we
move the centroid around and we figure
out what the best which objects are
closest to each centroid so the objects
can switch from one croid to the other
as the centroids are moved around and we
continue that until it is converged
let's see an example of this suppose we
have this data set of seven individuals
and their score on two topics A and B so
here's our subject in this case
referring to the person taking the uh
test and then we have subject a where we
see what they've scored on their first
subject and we have subject B and we can
see what they score on the second
subject now let's take two farthest
apart points as initial cluster
centroids now remember we talked about
selecting them randomly or we can also
just put them in different points and
pick the furthest one apart so they move
together either one works okay depending
on what kind of data you're working on
and what you know about it so we took
the two furthest points one and one and
five and seven and now let's take the
two farthest apart points as initial
cluster centroids each point is then
assigned to the closest cluster with
respect to the distance from the
centroids so we take each one of these
points in there we measure that distance
and you can see that if we measured each
of those distances and you use the
Pythagorean theorem for a triangle in
this case because you know the X and the
Y and you can figure out the diagonal
line from that or you just take a ruler
and put it on your monitor that'd be
kind of silly but it would work if
you're just eyeballing it you can see
how they naturally come together in
certain areas now we again calculate the
centroids of each cluster so cluster one
and then cluster two and we look at each
individual dot there's one two three
we're in one cluster uh the centroid
then moves over it becomes 1.8 comma 2.3
so remember it was at one and one well
the very center of the data we're
looking at would put it at the one point
roughly 22 but 1.8 2.3 and the second
one if we wanted to make the overall
mean Vector the average Vector of all
the different distances to that centroid
we come up with 4 comma 1 and 54 so
we've now moved the centroids we compare
each individual's distance to its own
cluster mean and to that of the opposite
cluster and we find can build a nice
chart on here that the as we move that
centroid around we now have a new
different kind of clustering of groups
and using ukian distance between the
points and the mean we get the same
formul you see new formulas coming up so
we have our individual dots distance to
the mean centr of the cluster and
distance to the mean croid of the
cluster only individual three is nearer
to the mean of the opposite cluster
cluster two than its own cluster one and
you can see here in the diagram where
we've kind of circled that one in the
middle so when we've moved the clust the
centroids of the Clusters over one of
the points shifted to the other cluster
because it's closer to that group of
individuals thus individual 3 is
relocated to Cluster 2 resulting in a
new Partition and we regenerate all
those numbers of how close they are to
the different clusters for the new
clusters we will find the actual cluster
centroids so now we move the centroids
over and you can see that we've now
formed two very distinct clusters on
here on comparing the distance of each
individual's distance to its own cluster
mean and to that of the opposite cluster
we find that the data points are stable
hence we have our final clusters now if
you remember I brought up a concept
earlier K me on the K means algorithm
choosing the right value of K will help
in less number of iterations and to find
the appropriate number of clusters in a
data set we use the elbow method and
within sum of squares WSS is defined as
the sum of the squared distance between
each member of the cluster and its
centroid and so you see we've done here
is we have the number of clusters and as
you do the same K means algorithm over
the different clusters and you calculate
what that centroid looks like and you
find the optimal you can actually find
the optimal number of clusters using the
elbow the graph is called as the elbow
method and on this we guessed at two
just by looking at the data but as you
can see the slope you actually just look
for right there where the elbow is in
the slope and you have a clear answer
that we want two different to start with
k means equals 2 A lot of times people
end up Computing K means equals 2 three
four five until they find the value
which fits on the elbow joint sometimes
you can just look at the data and if
you're really good with that specific
domain remember domain I mentioned that
last time you'll know that that where to
pick those numbers are where to start
guessing at what that K value is so
let's take this and we're going to use a
use case using K means clustering to
Cluster cars into Brands using
parameters such as horsepower cubic
inches make year Etc so we're going to
use the data set cars data having
information about three brands of cars
Toyota Honda and Nissan we'll go back to
my favorite tool the Anaconda Navigator
with the Jupiter notebook and let's go
ahead and flip over to our Jupiter
notebook and in our Jupiter notebook I'm
going to go ahead and just paste the uh
basic code that we usually start a lot
of these off with we're not going to go
too much into this code because we've
already discussed numpy we've already
discussed matplot library and pandas
numpy being the number array pandas
being the panda data frame and map plot
for the graphing and don't forget uh
since if you're using the Jupiter
notebook you do need the map plot
library in line so that it plots
everything on the screen if you're using
a different python editor then you
probably don't need that because it'll
have a popup window on your computer and
we'll go ahead and run this just to load
our libraries and our setup into here
the next step is of course to look at
our data which I've already opened up in
a spreadsheet and you can see here we
have the miles per gallon cylinders
cubic inches horsepower weight pounds
how you how heavy it is time it takes to
get to 60 my card is probably on this
one at about 80 or 90 what year it is so
this is you can actually see this is
kind of older cars and then the brand
Toyota Honda Nissan so the different
cars are coming from all the way from
1971 if we scroll down to uh the 80s we
have between the 70s and 80s a number of
cars that they've put out and let's uh
we come back here we're going to do
importing the data so we'll go ahead and
do data set equals and we'll use pandas
to read this in and it's uh from a CSV
file remember you can always post this
in the comments and request the data
files for these either in the comments
here on the YouTube video or go to
Simply learn.com and request that the
cars CSV I put it in the same folder as
the code that I've stored so my python
code is stored in the same folder so I
don't have to put the full path if you
store them in different folders you do
have to change this and double check
your name variables and we'll go ahead
and run this and uh We've chosen data
set arbitrarily because you know it's a
data set we're importing and we've now
imported our car CSV into the data set
as you know you have to prep the data so
we're going to create the X data this is
the one that we're going to try to
figure out what's going on with and then
there is a number of ways to do this but
we'll do it in a simple Loop so you can
actually see what's going on so we'll do
for i n x. columns so we're going to go
through each of the columns and a lot of
times it's important I I'll make lists
of the columns and do this because I
might remove certain columns or there
might be columns that I want to be
processed differently but for this we
can go ahead and take X of I and we want
to go fill Na and that's a panda's
command but the question is when are we
going to fill the missing data with we
definitely don't want to just put in a
number that doesn't actually mean
something and so one of the tricks you
can do with this is we can take X of I
and in addition to that we want to go
ahead and turn this into an integer
because a lot of these are integers so
we'll go ahead and keep it integers and
me add the bracket here and a lot of
editors will do this they'll think that
you're closing one bracket make sure you
get that second bracket in there if it's
a double bracket that's always something
that happens regularly so once we have
our integer of X of Y this is going to
fill in any missing data with the
average and I was so busy closing one
set of brackets I forgot that the mean
is also has brackets in there for the
pandas so we can see here we're going to
fill in all the data with the average
value for that column so if there's
missing data is in the average of the
data it does have then once we've done
that we'll go ahead and loop through it
again
and just check and see to make sure
everything is filled in correctly and
we'll print and then we take X is null
and this returns a set of the null value
or the how many lines are null and we'll
just sum that up to see what that looks
like and so when I run this and so with
the X what we want to do is we want to
remove the last column because that had
the models that's what we're trying to
see if we can cluster these things and
figure out the models there is so many
different ways to sort the X out for one
we could take the X and we could go data
set our variable we're using and use the
iocation one of the features that's in
pandas and we could take that and then
take all the rows and all but the last
column of the data set and at this time
we could do values we just convert it to
values so that's one way to do this and
if I me just put this down here and
print X it's a capital x we chose and I
run this you can see it's just the
values we could also take out the values
and it's not going to return anything
because there's no values connected to
it what I like to do with this is
instead of doing the iocation which does
integers more common is to come in here
and we have our data set and we're going
to do data set dot or data set. columns
and remember that list all the columns
so if I come in here let me just Mark
that as red and I print data set.
columns you can see that I have my index
here I have my MPG cylinders everything
including the brand which we don't want
so the way to get rid of the brand would
be to do data Columns of Everything But
the last one minus one so now if I print
this you'll see the brand disappears and
so I can actually just take data set
columns minus one and I'll put it right
in here for the columns we're going to
look
at and uh let's unmark this and unmark
this and now if I do an x. head I now
have a new data frame and you can see
right here we have all the different
columns except for the brand at the end
of the year and it turns out when you
start playing with the data set you're
going to get an error later on and it'll
say cannot convert string to float value
and that's because for some reason these
things the way they recorded them must
have been recorded as strings so we have
a neat feature in here on pandas to
convert and it is simply convert
objects and for this we're going to do
convert oops convert
underscore numeric numeric equals true
and yes I did have to go look that up I
don't have it memorized the convert
numeric in there if I'm working with a
lot of these things I remember them but
um depending on where I'm at what I'm
doing I usually have to look it up and
we run that oops I must have missed
something in here let me double check my
spelling and when I double check my
spelling you'll see I missed the first
underscore in the convert objects and
when I run this it now has everything
converted into a numeric value because
that's what we're going to be working
with is numeric values down
here and the next part is that we need
to go through the data and eliminate
null values most people when they're
doing small amounts working with small
data pools discover afterwards that they
have a null value and they have to go
back and do this so you know be aware
whenever we're formatting this data
things are going to pop up and sometimes
you go backwards to fix it and that's
fine that's just part of exploring the
data and understanding what you
have and I should have done this earlier
but let me go ahead and increase the
size of my window one
notch there we go easier to see
so we'll do 4 I in working with x.
columns we'll page through all the
columns and we want to take X of I and
we're going to change that we're going
to alter it and so with this we want to
go ahead and fill in X of I pandis Has
the fill in a and that just fills in any
non-existent missing data and we'll put
my brackets up and there's a lot of
different ways to fill this data if you
have a really large data set some people
just void out that data because if and
then look at it later in a separate
exploration of data one of the tricks we
can do is we can take our column and we
can find the
means and the means is in our quotation
marks so we take the columns we're going
to fill in the the non-existing one with
the means the problem is that returns a
decimal float so some of these aren't
decimals certainly need to be a little
careful of doing this but for this
example we're just going to fill it in
with the integer version of this keeps
it on par with the other data that isn't
a decimal
point and then what we also want to do
is we want to double check A lot of
times you do this first part first to
double check then you do the fill and
then you do it again just to make sure
you did it right so we're going to go
through and test for missing data and
one of the re ways you can do that is
simply go in here and take our X ofi
column so it's going to go through the X
ofi column it says is null so it's going
to return any any place there's a null
value it actually goes through all the
rows of each column is null and then we
want to go ahead and sum that so we take
that we add the sum value and these are
all pandas so is null's a panda command
and so is sum and if we go through that
and we go ahead and run
it and we go ahead and take and run that
you'll see that all the columns have
zero null values so we've now tested and
double checked and our data is nice and
clean we have no null values everything
is now a number value we turned it into
numeric and we've removed the last
column in our data and at this point
we're actually going to start using the
elbow method to find the optimal number
of clusters so we're now actually
getting into the sklearn part uh the K
means clustering on here I guess we'll
go ahead and zoom it up one more notot
so you can see what I'm typing in
here and then from sklearn going to or
sklearn cluster we're going to import K
means I always forget to capitalize the
K and the M when I do this so capital K
capital M K
means and we'll go and create a um array
wcss equals let make it an empty array
if you remember from the elbow method
from our
slide within the sums of squares WSS is
defined as the sum of square distance
between each member of the cluster and
its centroid so we're looking at that
change in differences as far as a squar
distance and we're going to run this
over a number of K mean
values in fact let's go for I in range
we'll do 11 of
them range Zer of 11
and the first thing we're going to do is
we're going to create the actual we'll
do all lower
case and so we're going to create this
object from the K means that we just
imported and the variable that we want
to put into this is in clusters we're
going to set that equals to I that's the
most important one because we're looking
at how increasing the number of clusters
changes our answer there are a lot of
settings to the K means our guys in the
back did a great job just kind of
playing with some of them the most
common ones that you see in a lot of
stuff is how you init your K means so we
have K means plus plus plus this is just
a tool to let the model itself be smart
how it picks it centroids to start with
its initial syroid we only want to
iterate no more than 300 times we have a
Max iteration we put in there we have a
the inth a knit the random State equals
zero you really don't need to worry too
much about these when you're first
learning this as you start digging in
deeper you start finding that these are
shortcuts that will speed up the process
as far as a setup but the big one that
we're working with is the in clusters
equals I so we're going to literally
train our K means 11 times we're going
to do this process 11 times and if
you're working with uh Big Data you know
the first thing you do is you run a
small sample of the data so you can test
all your stuff on it and you can already
see the problem that if I'm going to
iterate through a terabyte of data 11
times and then the K means itself is
iterating through the data multiple
times that's a heck of a process so you
got to be a little careful with this a
lot of times though you can find your
elbow using the elbow method find your
optimal number on a sample of data
especially if you're working with larger
data sources so we want to go ahead and
take our K means and we're just going to
fit it if you're looking at any of the
SK learn very common you fit your model
and if you remember correctly our
variable we're using is the capital x
and once we fit this value we go back to
the um array we made and we want to go
just toin that value on the
end and it's not the actual fitware
pining in there it's when it generates
it it generates the value you're looking
for is inertia so K means. iner so we'll
pull that specific value out that we
need and let's get a visual on this
we'll do our PLT plot and what we're
plotting
here is first the xais which is range
01 so that will generate a nice little
plot there and the wcss for our Y
axis it's always nice to give our plot a
title and let's see we'll just give it
the elbow method for the title and let's
get some labels so let's go ahead and do
PLT X
label and what we'll do we'll do number
of clusters for that and PLT y
label and for that we can do oops there
we go wcss since that's what we're doing
on the plot on there and finally we want
to go ahead and display our graph which
is simply PLT do
oops. show there we go and because we
have it set to inline it'll appear in
line hopefully I didn't make a type
error on there
and you can see we get a very nice graph
you can see a very nice elbow joint
there at uh two and again right around
three and four and then after that
there's not very much now as a data
scientist if I was looking at this I
would do either three or four and I'd
actually try both of them to see what
the um output look like and they've
already tried this in the back so we're
just going to use three as a setup on
here and let's go ahead and see what
that looks like when we actually use
this to show the different kinds of
cars and so let's go ahead and apply the
K means to the cars data set and
basically we're going to copy the code
that we looped through up above where K
means equals K means number of clusters
and we're just going to set that number
of clusters to three since that's what
we're going to look for you could do
three and four on this and graph them
just to see how they come up differently
be kind of curious to look at that but
for this we're just going to set it to
three go ahead and create our own
variable y k means for our answers and
we're going to set that equal to whoops
double equal there 2 K means but we're
not going to do a fit we're going to do
a fit predict is the setup you want to
use and when you're using untrained
models you'll see um a slightly
different usually you see fit and then
you see just the predict but we want to
both fit and predict the K means on this
and that's fitcore predict and then our
capital x is the data we're working
with and before we plot this data we're
going to do a little pandas trick we're
going to take our x value and we're
going to set XS Matrix so we're
converting this into a nice rows and
columns kind of set up but we want the
we're going to have columns equals none
so it's just going to be a matrix of
data in here and let's go ahead and run
that a little warning you'll see this
warnings pop up because things are
always being updated so there's like
minor changes in the versions and Future
versions instead of Matrix now that it's
more common to set it values instead of
doing as Matrix but M Matrix works just
fine for right now and you'll want to
update that later on but let's go ahead
and dive in and plot this and see what
that looks like and before we dive into
plotting this data I always like to take
a look and see what I am plotting so
let's take a look at why K means I'm
just going to print that out down here
and we see we have an array of answers
we have 2 1 02 one two so it's
clustering these different rows of data
based on the three different spaces it
thinks it's going to
be and then let's go ahead and print X
and see what we have for x and we'll see
that X is an array it's a matrix so we
have our different values in the array
and what we're going to do it's very
hard to plot all the different values in
the array so we're only going to be
looking at the first two or positions
zero and one
and if you were doing a full
presentation in front of the board
meeting you might actually do a little
different than and dig a little deeper
into the different aspects because this
is all the different columns we looked
at but we only look at columns one and
two for this to make it easy so let's go
ahead and clear this data out of here
and let's bring up our plot and we're
going to do a scatter plot here so PLT
scatter
and this looks a little complicated so
let's EXP explain what's going on with
this we're going to take the X
values and we're only interested in y of
K means equals zero the first cluster
okay and then we're going to take value
zero for the x axis and then we're going
to do the same thing here we're only
interested in K means equals zero but
we're going to take the second column so
we're only looking at the first two
columns in our answer or in the data and
then the guys in the back played with
this a little bit to make it pretty
and they discovered that it looks good
with has a size equals 100 that's the
size of the dots we're going to use red
for this one and when they were looking
at the data and what came out it was
definitely the Toyota on this we're just
going to go ahead and label it Toyota
again that's something you really have
to explore in here as far as playing
with those numbers and see what looks
good we'll go ahead and hit enter in
there and I'm just going to paste in the
next two lines which is the next two
cars
and this is our Nissa and Honda and
you'll see with our scatter plot we're
now looking at where Yore K means equals
one and we want the zero column and y k
means equals two again we're looking at
just the first two columns zero and one
and each of these rows then corresponds
to Nissan and
Honda and I'll go ahead and hit enter on
there and uh finally let's take a look
and put the centroids on there again
we're going to do a scatter plot
and on the centroids you can just pull
that from our K means the uh model we
created do cluster centers and we're
going to just do
um all of them in the first number and
all of them in the second number which
is Zer one because you always start with
zero and
one and then they were playing with the
size and everything to make it look good
we'll do a size of 300 we're going to
make the color yellow and we'll label
them it's always good to have some good
labels centroids
and then we do want to do a title PLT
title and pop up there PLT title you
always make want to make your graphs
look pretty we'll call it clusters of
car
make and one of the features of the plot
library is you can add a legend it'll
automatically bring in it since we've
already labeled the different aspects of
the legend with Toyota Nissan and
Honda and finally we want to go ahead
and show
so we can actually see it and remember
it's in line uh so if you're using a
different editor that's not the Jupiter
notebook you'll get a popup of this and
you should have a nice set of clusters
here so we can look at this and we have
a clusters of Honda and green Toyota and
red Nissan and purple and you can see
where they put the centroids to separate
them now when we're looking at this we
can also plot a lot of other different
data on here as far cuz we only looked
at the first two columns this is just
column one and two or 01 as as you label
them in computer scripting but you can
see here we have a nice clusters of Carm
and we' were able to pull out the data
and you can see how just these two
columns form very distinct clusters of
data so if you were exploring new data
you might take a look and say well what
makes these different almost going in
reverse you start looking at the data
and pulling apart the columns to find
out why is the first group set up the
way it is maybe you're doing loans and
you want to go well why is this group
not defaulting on their loans and why is
the last group defaulting on their loans
and why is the middle group 50%
defaulting on their bank loans and you
start finding ways to manipulate the
data and pull out the answers you
want so now that you've seen how to use
K mean for clustering let's move on to
the next topic now let's look into
logistic regression the logistic
regression algorithm is the simplest
classification algorithm used for binary
or multiclassification problems and we
can see we have our little girl from
Canada who's into horror books is back
that's actually really scary we you
think about that with those big eyes in
the previous tutorial we learned about
linear regression dependent and
independent variables so to brush up y =
mx + C very basic algebraic function of
uh Y and X the dependent variable is the
target class variable we are going to
predict the independent variables X1 all
the way up to xn are the features or
attributes we're going to use to predict
the target class we know what a linear
regression looks like but using the
graph we cannot divide the outcome into
categories it's really hard to
categorize 1.5 3.6 9.8 uh for example a
linear regression graph can tell us that
with increase in number of hours studied
the marks of a student will increase but
it will not tell us whether the student
will pass or not in such cases where we
need the output as categorical value we
will use logistic regression and for
that we're going to use the sigmoid
function so you can see here we have our
marks 0 to 100 number of hours studied
that's going to be what they're
comparing it to in this example and we
usually form a line that says y = mx + C
and when we use the sigmoid function we
have P = 1 over 1 + eus y it generates a
sigmoid curve and so you can see right
here when you take the Ln which is the
natural logarithm I always thought it
should be NL not Ln that's just the
inverse of uh e your e to the minus y
and so we do this we get Ln of p over 1
minus P equal m * x + C that's the
sigmoid curve function we're looking for
and we can zoom in on the function and
you'll see that the function is it
derives goes to one or to zero depending
on what your x value is and the
probability if it's greater than 0.5 the
value is automatically rounded off to
one indicating that the student will
pass so if they're doing a certain
amount of studying they will probably
pass then you have a threshold value at
the0 five it automatically puts that
right in the middle usually and your
probability if it's less than 0.5 the
value rent it off to zero indicating the
student will fail so if they're not
studying very hard they're probably
going to fail this of course is ignoring
the outliers of that one student who's
just a natural genius and doesn't need
any studying to memorize everything
that's not me unfortunately have to
study hard to learn new stuff problem
statement to classify whether a tumor is
malignant or benign and this is actually
one of my favorite data sets to play
with because it has so many features and
when you look at them you really are
hard to understand you can't just look
at them and know the answer so it gives
you a chance to kind of dive into what
data looks like when you aren't able to
understand the specific domain of the
data but I also want you to remind you
that in the domain of medicine if I told
you that my probability was really good
it classified things that say 90% or 95%
and I'm classifying whether you're going
to have a malignant or a Bine tumor I'm
guessing that you're going to go get it
tested anyways so you got to remember
the domain we're working with so why
would you want to do that if you know
you're just going to go get a biopsy
because you know it's that serious this
is like an all or nothing just
referencing the domain it's important it
might help the doctor know where to look
just by understanding what kind of tumor
it is so it might help them or Aid them
in something they missed from before so
let's go ahead and dive into the code
and I'll come back to the domain part of
it in just a minute so use case and
we're going to do our noral Imports here
where we're importing numpy Panda
Seaborn the matplot library and we're
going to do matplot library in line
since I'm going to switch over to
Anaconda so let's go ahead and flip over
there and get this started so I've
opened up a new window in my anaconda
Jupiter
notebook and by the way Jupiter notebook
uh you don't have to use Anaconda for
the Jupiter notebook I just love the
interface and all the tools that
Anaconda brings so we got our import
numpy as in P for our numpy number array
we have our pandas PD we're going to
bring in caborn to help us with our
graphs as SNS so many really nice Tools
in both Seaborn and matplot library and
we'll do our matplot library. pyplot as
PLT and then of course we want to let it
know to do it in line and let's go and
just run that so it's all set
up and we're just going to call our data
data not creative today uh equals PD and
this happens to be in a CSV file so
we'll use a pd. read CSV and I happen to
name the file I renamed it data for
p2.png to supply that for
you and let's just um open up the data
before we go any further and let's just
see what it looks like in a
spreadsheet so when I pop it open in a
local spreadsheet and this is just a CSV
file comma separate variables we have an
ID so I guess they um categorizes for
reference of what id which test was done
the diagnosis M for malignant B for B9
so there's two different options on
there and that's what we're going to try
to predict is the m and b and test it
and then we have like the radius mean or
average the texture average perimeter
mean area mean smoothness I don't know
about you but unless you're a doctor in
the field most of the stuff I mean you
can guess what concave means just by the
term concave but I really wouldn't know
what that means in the measurements
they're taking so they have all kinds of
stuff like how smooth it is uh the
Symmetry and these are all float values
we just page through them real quick and
you'll see there's I believe 36 if I
remember correctly in this one
so there's a lot of different values
they take and all these measurements
they take when they go in there and they
take a look at the different growth the
tumorous growth so back in our data and
I put this in the same folder as a code
so I saved this code in that folder
obviously if you have it in a different
location you want to put the full path
in there and we'll just do uh
Panda's first five lines of data with
the data. head and we run that we can
see that we have
pretty much what we just looked at we
have an ID we have a
diagnosis if we go all the way across
you'll see all the different columns
coming across displayed nicely for our
data and while we're exploring the data
our caborn which we referenced as
SNS makes it very easy to go in here and
do a joint plot you'll notice the very
similar to because it is sitting on top
of the um plot Library so the joint plot
does a lot of work for us
and we're just going to look at the
first two columns that we're interested
in the radius mean and the texture mean
we'll just look at those two columns and
data equals data so that tells it which
two columns we're plotting and that
we're going to use the data that we
pulled in let's just run that and it
generates a really nice graph on here
and there's all kinds of cool things on
this graph to look at I mean we have the
texture mean and the radius mean
obviously the axes you can also
see and one of the cool things on here
is you can also see the histogram they
show that for the radius mean where is
the most commonest radius mean come up
and where the most common texture is so
we're looking at the tech the on each
growth its average texture and on each
radius its average uh radius on there
gets a little confusing because we're
talking about the individual objects
average and then we can also look over
here and see the the histogram showing
us the median or how common each
measurement is and that's only two
columns so let's dig a little deeper
into Seaborn they also have a heat map
and if you're not familiar with heat
Maps a heat map just means it's in color
that's all that means heat map I guess
the original ones were plotting heat
density on something and so ever since
then it's just called a heat map and
we're going to take our data and get our
corresponding numbers to put that into
the heat map and that's simply data. C
RR for that that's a panda expression
remember working in a pandas data frame
so that's one of the Cool Tools in
pandas for our data and this is pull
that information into a heat map and see
what that looks like and you'll see that
we're now looking at all the different
features we have our ID we have our
texture we have our area our compactness
concave points and if you look down the
middle of this chart diagonal going from
the upper left to bottom right it's all
white that's because when you compare
texture to texture they're identical so
there are 100 % or in this case perfect
one in their
correspondence and you'll see that when
you look at say area or right below it
it has almost a black on there when you
compare it to texture so these have
almost no corresponding data They Don't
Really form a linear graph or something
that you can look at and say how
connected they are they're very
scattered data this is really just a
really nice graph to get a quick look at
your data doesn't so much change what
you do but it changes very ter ifying so
when you get an answer or something like
that or you start looking at some of
these individual pieces you might go hey
that doesn't match according to showing
our heat map this should not correlate
with each other and if it is you're
going to have to start asking well why
what's going on what else is coming in
there but it does show some really cool
information on here I mean we can see
from the ID there's no real one feature
that just says if you go across the top
line that lights up there's no one
feature this says hey if the area is a
certain size then it's going to be B9 or
malignant it says there's some that sort
of add up and that's a big hint in the
data that we're trying to ID this
whether it's malignant or Bine that's a
big hint to us as data scientist to go
okay we can't solve this with any one
feature it's going to be something that
includes all the features or many of the
different features to come up with the
solution for it and while we're
exploring the data let's explore one
more area and let's look at data do is
null we want to check for null values in
our data if you remember from earlier in
this tutorial we did it a little
differently where we added stuff up and
summ them up you can actually with
pandas do it really quickly data. is
null and Summit and it's going to go
across all the columns so when I run
this you're going to see all the columns
come up with no null
data so we've just just to reash these
last few steps
we've done a lot of exploration we have
looked at the first two columns and seen
how they plot with the caborn with a
joint plot which shows both the
histogram and the data plotted on the X
Y coordinates and obviously you can do
that more in detail with different
columns and see how they plot together
and then we took and did the Seaborn
heat map the SNS do heatmap of the data
and you can see right here where it did
a nice job showing it some bright spots
where stuff correlates with each other
and forms a very nice combination or
points of scattering points and you can
also see areas that
don't and then finally we went ahead and
checked the data is the data null value
do we have any missing data in there
very important step because it'll crash
later on if you forget to do this step
it will remind you when you get that
nice error code that says null values
okay so not a big deal if you miss it
but it it's no fun to go back when
you're you're in a huge process and
you've missed this step and now you're
10 steps later and you got to go
remember where you were pulling the data
in so we need to go ahead and pull out
our X and our y so we just put that down
here and we'll set the x equal to and
there's a lot of different options here
certainly we could do x equals all the
columns except for the first two because
if you remember the first two is the ID
and the diagnosis so that certainly
would be an option but we're going to do
is we're actually going to focus on the
worst the worst radius the worst texture
parameter area smoothness compactness
and so on one of the reasons to start
dividing your data up when you're
looking at this
information is sometimes the data will
be the same data coming in so if I have
two measurements coming into my model it
might overway them it might overpower
the other measurements because it's
measur it's basically taking that
information in twice that's a little bit
past the scope of this tutorial I want
you to take away from this though is
that we are dividing the data up into
pieces and our team in the back went
ahead and said hey let's just look at
the worst so I'm going to create a an
array and you'll see this array radius
worst texture worst perimeter worst
we've just taken the worst of the worst
and I'm just going to put that in my X
so this x is still a pandas data frame
but it's just those columns and our y if
you remember correctly is going to be
oops hold on one second it's not X it's
data there we go so x equals data and
then it's a list of the different
columns the worst of the worst and if
we're going to take that then we have to
have our answer for our Y for the stuff
we know and if you remember correctly
we're just going to be looking
at the diagnosis that's all we care
about is what is it diagnosed is it Bine
or malignant and since it's a single
column we can just do diagnosis oh I
forgot to put the brackets the there we
go okay okay so it's just diagnosis on
there and we can also real quickly do
like x. head if you want to see what
that looks like and Y do head and run
this and you'll see um it only does the
last one I forgot about that if you
don't do print you can see that the the
y. head is just Mmm because the first
ones are all malignant and if I run this
the x. head is just the first five
values of radius worst texture worst
parameter worst area worst and so on
I'll go ahead and take that out so
moving down to the next step we've built
our two data sets our answer and then
the features we want to look
at in data science it's very important
to test your
model so we do that by splitting the
data and from sklearn model selection
we're going to import train test split
so we're going to split it into two
groups there are so many ways to do this
I noticed in one of the more modern ways
they actually split it into three groups
and then you model each group and test
it against the other groups so you have
all kinds of and there's reasons for
that which is past the scope of this and
for this particular example isn't
necessary for this we're just going to
split it into two groups one to train
our data and one to test our data and
the sklearn uh. model selection we have
train test split you could write your
own quick code to do this where you just
randomly divide the data up into two
groups but they do it for us
nicely and we actually can almost we can
actually do it in one statement with
this where we're going to generate four
variables capital x train capital X test
so we have our training data we're going
to use to fit the model and then we need
something to test it and then we have
our y train so we're going to train the
answer and then we have our test so this
is the stuff we want to see how good it
did on our model and we'll go ahead and
take our train test split that we just
UT orted and we're going to do X and our
y our two different data that's going in
for our split and then the guys in the
back came up and wanted us to go ahead
and use a test size equals. 3 that's
testore size random State it's always
nice to kind of switch a random State
around but not that important what this
means is that the test size is we're
going to take 30% of the data and we're
going to put that into our test
variables our y test and our X test and
we're going to do 70% into the X train
and the Y train so we're going to use
70% of the data to train our model and
30% to test it let's go ahead and run
that and load those up so now we have
all our stuff split up and all our data
ready to go now we get to the actual
Logistics Park we're actually going to
do our create our model so let's go
ahead and bring that in from sklearn
we're going to bring in our linear model
and we're going to import logistic
regression that's the actual model we're
using and this we'll call call it log
models there we go model and let's just
set this equal to our logistic
regression that we just imported so now
we have a variable log model set to that
class for us to use and with most the uh
models in the SK learn we just need to
go ahead and fix it fit do a fit on
there and we use our X train that we
separated out with our y train and let's
go ahead and run this so once we've run
this have a model that fits this data
that 70% of our training
data uh and of course it prints this out
that tells us all the different
variables that you can set on there
there's a lot of different choices you
can make but for word do we're just
going to let all the defaults sit we
don't really need to mess with those on
this particular example and there's
nothing in here that really stands out
as super important until you start
fine-tuning it but for what we're doing
the basics will work just fine and then
let's we need to go ahead and test out
our model is it working so let's create
a variable y predict and this is going
to be equal to our log model and we want
to do a predict again very standard
format for the sklearn library is taking
your model and doing a predict on it and
we're going to test y predict against
the Y test so we want to know what the
model thinks it's going to be that's
what our y predict is and with that we
want the capital x x test so we have our
train set and our test set and now we're
going to do our y predict and let's go
ahead and run
that and if we uh
print y predict let me go ahead and run
that you'll see it comes up and it preds
a prints a nice array of uh B and M for
B9 and
malignant for all the different test
data we put in there so it does pretty
good we're not sure exactly how good it
does but we can see that it actually
works and it's functional was very very
easy to create you'll always discover
with our data science that as you
explore this you spend a significant
amount of time prepping your data and
making sure your data coming in is good
uh there's a saying good data in good
answers out bad data in bad answers out
that's only half the thing that's only
half of it selecting your models becomes
the next part as far as how good your
models are and then of course
fine-tuning it depending on what model
you're using so we come in here we want
to know how good this came out so we
have our y predict here log model.
predict X
test so for deciding how good our model
is we're going to go from the SK
learnmetrics we're going to import
classification report and that just
reports how good our model is doing and
then we're going to feed it the uh model
data and let's just print this out and
we'll take our classification report
and we're going to put into
there our test our actual data so this
is what we actually know is true and our
prediction what our model predicted for
that data on the test side and let's run
that and see what that
does so we pull that up you'll see that
we have um a Precision for B9 and
malignant b& M and we have a Precision
of 93 and 91 a total of 92 so it's kind
of the average between these two of 92
there's all kinds of different
information on here your F1
score your recall your support coming
through on this and for this I'll go
ahead and just flip back to our slides
that they put together for describing it
and so here we're going to look at the
Precision using the classification
report and you see this is the same
print out I had up above some of the
numbers might be different because it
does randomly pick out which data we're
using so this model is able to predict
the type of tumor with
91% accuracy so we look back here that's
you will see where we have B9 and
malignant it actually is 92 coming up
here but we're looking about a 92 91%
precision and remember I reminded you
about domain so when we're talking about
the domain of a medical domain with a
very catastrophic outcome you know at 91
or 92% Precision you're still going to
go in there and have somebody do a
biopsy on it very different than if
you're investing money and there's a 92%
chance you're going to earn 10% and 8%
chance you're going to lose 8% you're
probably going to bet the money because
at that odds it's pretty good that
you'll make some money and in the long
run if you do that enough you definitely
will make money and also with this
domain I've actually seen them use this
to identify different forms of cancer
that's one of the things that they're
starting to use these models for because
then it helps the doctor know what to
investigate so that wraps up this
section we're finally we're going to go
in there and let's discuss the answer to
the quiz asked in machine learning
tutorial part one can you tell what's
happening in the following cases
grouping documents into different
categories based on the topic and
content of each document this is an
example of clustering where K means
clustering can be used to group the
documents by topics using bag of words
approach so if You' gotten in there that
you're looking for clustering and
hopefully you had at least one or two
examples like K means that are used for
clustering different things then give
yourself a two thumbs up B identifying
handwritten digits in images correctly
this is an example of classification the
traditional approach to solving this
would be to extract digit dependent
features like curvature of different
digits Etc and then use a classifier
like svm to distinguish between images
again if you got the fact that it's a
classification example give yourself a
thumb up and if you're able to go hey
let's use svm or another model for this
give yourself those two thumbs up on it
C behavior of a website indicating that
the site is not working as designed this
is an example of anomaly detection and
this case the algorithm learns what is
normal and what is not normal usually by
observing the logs of the website give
yourself a thumbs up if you got that one
and just for a bonus can you think of
another example of anomaly detection one
of the ones I use it for my own business
is detecting anomalies in stock markets
stock markets are very ficked and they
behave very radical so finding those
erratic areas and then finding ways to
track down why they're erratic was
something released in social media was
something released you can see where
knowing where that anomaly is can help
you to figure out what the answer is to
it in another area D predicting salary
of an individual based on his or her
years of experience this is an example
of regression this problem can be
mathematically defined as a function
between independent years of experience
and dependent variables salary of an
individual and and if you guess that
this was a regression model give
yourself a thumbs up and if you're able
to remember that it it was between
independent and dependent variables and
that terms give yourself two thumbs up
summary so to wrap it up we went over
what is K means and we went through also
the chart of choosing your elbow method
and assigning a random centroid to the
Clusters Computing the distance and then
going in there and figuring out what the
minimum centroids is and Computing the
distance and going through that Loop
until it gets the perfect centroid and
we looked into the elbow method to
choose K based on running our clusters
across number of variables and finding
the best location for that we did a nice
example of clustering cars with K means
even though we only looked at the first
two columns to make it simple and easy
to graph can easily extrapolate that and
look at all the different columns and
see how they all fit together and we
looked at what is logistic regression we
discussed the sigmoid function what is
logistic regression and we went into an
example of classifying tumors with
Logistics I hope you enjoyed part two of
machine learning and this is one of the
things is very important with linear
aggression in any of these models is to
understand the error and so we can
calculate the error on all of our
different values and you can see over
here we plotted um X and Y and Y predict
and we drawn a little line so you can
sort of see what the error looks like
there between the different points so
our goal is to reduce this error we want
to minimize that error value on our
linear regression model minimizing the
distance there are lots of ways to
minimize the distance between the line
and the data points like sum of squared
errors sum of absolute errors root mean
square error Etc we keep moving this
line through the data points to make
sure the best fit line has the least
Square distance between the data points
and the regression line so to recap with
a very simple linear regression model we
first figure out the formula of our line
through the middle and then we slowly
adjust the line to minimize the error
keep in mind this is a very simple
formula the math gets even though the
math is very much the same it gets much
more complex as we add in different
dimensions so this is only two
Dimensions y = mx plus C but you can
take that out to x z JQ all the
different features in there and they can
plot a linear regression model on all of
those using the different formulas to
minimize the error let's go ahead and
take a look at decision trees a very
different way to solve problems in the
linear regression model decision tree is
a tree-shaped algorithm used to
determine a course of action each branch
of a tree represents a possible decision
or currence or reaction we have data
which tells us if it is a good day to
play golf and if we were to open this
data up in a general spreadsheet you can
see we have the Outlook whether it's a
rainy overcast Sunny temperature hot
mild cool humidity windy and did I like
to play golf that day yes or no so we're
taking a sensus
and certainly I wouldn't want a computer
telling me when I should go play golf or
not but you can imagine if you got up in
the night before you're trying to plan
your day and it comes up and says
tomorrow would be a good day for golf
for you in the morning and not a good
day in the afternoon or something like
that this becomes very beneficial and we
see this in a lot of applications coming
out now where it gives you suggestions
and lets you know what would uh fit the
match for you for the next day or the
next purchase or the next uh whatever
you know next mail out in this case is
tomorrow a good day for playing golf
based on the weather coming in and so we
come up and let's uh determine if you
should play golf when the day is sunny
and windy so we found out the forecast
tomorrow is going to be sunny and windy
and suppose we draw our tree like this
we're going to have our humidity and
then we have our normal which is uh if
it's if you have a normal humidity
you're going to go play golf and if the
humidity is really high then we look at
the Outlook and if the Outlook is sunny
overcast or rainy it's going to change
what you choose to do so if you know
that it's a very high humidity and it's
sunny you're probably not going to play
golf cuz you're going to be out there
miserable fighting off the mosquitoes
that are out joining you to play golf
with you maybe if it's rainy you
probably don't want to play in the rain
but if it's slightly overcast and you
get just the right Shadow that's a good
day to play golf and be outside out on
the green now in this example you can
probably make your own tree pretty
easily CU it's a very simple set of data
going in but the question is how do you
know what to split where do you split
your data what if this is much more
complicated data where it's not
something that you would particularly
understand like studying cancer they
take about 36 measurements of the
cancerous cells and then each one of
those measurements represents how
bulbous it is how extended it is how
sharp the edges are something that as a
human we would have no understanding of
so how do we decide how to split that
data up and is that the right decision
tree but so that's the question is going
to come up is this the right decision
tree for that we should calculate
entropy and Information Gain two
important vocabulary words there are the
entropy and the Information Gain entropy
entropy is a measure of Randomness or
impurity in the data set entropy should
be low so we want the chaos to be as low
as possible we don't want to look at it
and be confused by the images or what's
going on there with mixed data and the
Information Gain it is a measure of
decrease in entropy after the data set
is split also known as entropy
reduction Information Gain should be
high so we want our information that we
get out of the split to be as high as
possible let's take a look at entropy
from the mathematical side in this case
we're going to denote entropy as I of P
of and N where p is the probability that
you're going to play a game of golf and
N is the probability where you're not
going to play the game of golf now you
don't really have to memorize these
formulas there's a few of them out there
depending on what you're working with
but it's important to note that this is
where this formula is coming from so
when you see it you're not lost when
you're running your programming unless
you're building your own decision tree
code in the back and we simply have a
log s of P Over p+ N minus n/ P plus n *
the log s of n of p plus n but let's
break that down and see what actually
looks like when we're Computing that
from the computer script side entb of a
target class of the data set is the
whole entropy so we have entropy play
golf and we look at this if we go back
to the data you can simply count how
many yeses and no in our complete data
set for playing golf days in our
complete set we find we have five days
we did play golf and nine days we did
not play golf and so our I equals if you
add those together 9 + 5 is 14 and so
our I equals 5 over 14 and 9 over 14
that's our p and N values that we plug
into that formula and you can go to 5
over 14 = 36 9 over 14 = 64 and when you
do the whole equation you get the minus.
36 logun 2 of 36 -64 log s < TK of 64
and we get a set value we get
.94 so we now have a full entropy value
for the whole set of data that we're
working with and we want to make that
entropy go down and just like we
calculated the entropy out for the whole
set we can also calculate entropy for
playing golf and the Outlook is is it
going to be overcast or rainy or sunny
and so we look at the entropy we have P
of Sunny time e of 3 of two and that
just comes out how many sunny days yes
and how many sunny days no over the
total which is five don't forget to put
the we'll divide that five out later on
equals P overcast = 4 comma 0 plus rainy
= 2 comma 3 and then when you do the
whole setup we have 5 over 14 remember I
said there was a total of five 5 over 14
time the I of 3 of 2 + 4 over 14 * the 4
comma 0 and 514 over I of 23 and so we
can now compute the entropy of just the
part it has to do with the forecast and
we get 693 similarly we can calculate
the entropy of other predictors like
temperature humidity and wind and so we
look at the gain Outlook how much are we
going to gain from this entropy play
golf minus entropy play golf Outlook and
we can take the original 0.94 for the
whole set minus the entropy of just the
U rainy day in temperature and we end up
with a gain of
0247 so this is our Information Gain
remember we Define entropy and we Define
Information Gain the higher the
information gain the lower the entropy
the better the information gain of the
other three attributes can be calculated
in the same way so we have our gain for
temperature equals
0.029 we have our gain for humidity
equals 0.15 two and our gain for a windy
day equals
0048 and if you do a quick comparison
you'll see the 247 is the greatest gain
of information so that's the split we
want now let's build the decision tree
so we have the Outlook is it going to be
sunny overcast or rainy that's our first
split because that gives us the most
Information Gain and we can continue to
go down the tree using the different
information gains with the largest
information we can continue down the
nodes of the tree where we choose the
attribute with the largest information
gain as the root node and then continue
to split each sub node with the largest
Information Gain that we can compute and
although it's a little bit of a tongue
twister to say all that you can see that
it's a very easy to view visual model we
have our Outlook we split it three
different directions if the Outlook is
overcast we're going to play and then we
can split those further down if we want
so if the over Outlook is sunny but then
it's also windy if it's uh windy we're
not going to play if it's uh not windy
we'll play so so we can easily build a
nice decision tra to guess what we would
like to do tomorrow and give us a nice
recommendation for the day so we want to
know if it's a good day to play golf
when it's sunny and windy remember the
original question that came out
tomorrow's weather report is sunny and
windy you can see by going down the tree
we go out look sunny out look windy
we're not going to play golf tomorrow so
our little Smartwatch pops up and says
I'm sorry tomorrow is not a good day for
golf it's going to be sunny and windy
and if you're a huge golf fan you might
go uhoh it's not a good day to play golf
we can go in and watch a golf game at
home so we'll sit in front of the TV
instead of being out playing golf in the
wind now that we looked at our decision
tree let's look at the third one of our
algorithms we're investigating support
Vector machine support Vector machine is
a widely used classification algorithm
the idea of support Vector machine is
simple the algorithm creates a
separation line which divides the
classes in the best possible manner for
example dog or cat disease or no disease
suppose we have a labeled samp sample
data which tells height and weight of
males and females a new data point
arrives and we want to know whether it's
going to be a male or a female so we
start by drawing a line we draw decision
lines but if we consider decision line
one then we will classify the individual
as a male and if we consider decision
line two then it will be a female so you
can see this person kind of lies in the
middle of the two groups it's a little
confusing trying to figure out which
line they should be under we need to
know which line divides the classes
correctly but how
the goal is to choose a hyperplane and
that is one of the key words they use
when we talk about support Vector
machines choose a hyper plane with the
greatest possible margin between the
decision line and the nearest Point
within the training set so you can see
here we have our support Vector we have
the two nearest points to it and we draw
a line between those two points and the
distance margin is the distance between
the hyperplane and the nearest data
point from either set so we actually
have a value and it should be equally
distant between the two um points that
we're comparing it to when we draw the
hyperplanes we observe that line one has
a maximum distance so we observe that
line one has a maximum distance margin
so we'll classify the new data point
correctly and our result on this one is
going to be that the new data point is
Mel one of the reasons we call it a
hyperplane versus a line is that a lot
of times we're not looking at just
weight and height we might be looking at
36 different different features or
dimensions and so when we cut it with a
hyper plane it's more of a
three-dimensional cut in the data
multi-dimensional it cuts the data a
certain way and each plane continues to
cut it down until we get the best fit or
match let's understand this with the
help of an example problem statement I
always start with a problem statement
when you're going to put some code
together we're going to do some coding
now classifying muffin and cupcake
recipes using support Vector machines so
the cupcake versus the muffin let's have
a look at our data set and we have the
different recipes here we have a muffin
recipe that has so much flour I'm not
sure what measurement 55 is in but it
has 55 maybe it's
ounces but uh it has certain amount of
flour certain amount of milk sugar
butter egg baking powder vanilla and
salt and So based on these measurements
we want to guess whether we're making a
muffin or a cupcake and you can see in
this one we don't have just two features
we don't just have height and weight as
we did before between the male and
female in here we have a number of
features in fact in this we're looking
at eight different features to guess
whether it's a muffin or a cupcake
what's the difference between a muffin
and a cupcake turns out muffins have
more flour while cupcakes have more
butter and sugar so basically the
cupcakes a little bit more of a dessert
where the muffins a little bit more of a
fancy bread but how do we do that in
Python how do we code that to go through
recipes and figure out what the recipe
is and I really just want to say cup
cupcakes versus muffins like some big
professional wrestling thing before we
start in our cupcakes versus muffins we
are going to be working in Python
there's many versions of python many
different editors that is one of the
strengths and weaknesses of python is it
just has so much stuff attached to it
and it's one of the more popular data
science programming packages you can use
in this case we're going to go ahead and
use anaconda and Jupiter notebook the
Anaconda navig has all kinds of fun
tools once you're into the anacon
Navigator you can change environments I
actually have a number of environments
on here we'll be using python 36
environment so this is in Python version
36 although it doesn't matter too much
which version you use I usually try to
stay with the 3x because they're current
unless you have a project that's very
specifically in version 2x 27 I think is
usually what most people use in the
version two and then once we're in our
um Jupiter notebook editor I can go up
and create a new file and we'll just
jump in here in this case we're doing
svm muffin versus Cupcake and then let's
start with our packages for data
analysis and we almost always use a
couple there's a few very standard
packages we use we use import oops
import
import
numpy that's for number python they
usually denoted do NP that's very comma
that's very common and then we're going
to import pandas as
PD and numpy deals with number arrays
there's a lot of cool things you can do
with the numpy uh setup as far as
multiplying all the values in an array
in a numpy array data array pandas I
can't remember if we're using it
actually in this data set I think we do
as an import it makes a nice data frame
and the difference between a data frame
and a nump array is that a data frame is
more like your Excel spreadsheet you
have columns you have indexes you have
different ways of referencing it easily
viewing it and there's additional
features you can run on a data frame and
pandas kind of sits on numpy so they you
need them both in there and then finally
we're working with the support Vector
machine so from sklearn we're going to
use the sklearn model import svm support
Vector
machine and then as a data scientist you
should always try to visualize your data
some data obviously is too complicated
or doesn't make any sense to the human
but if it's possible it's good to take a
second look at it so you can actually
see what you're doing now for that we're
going to use two packages we're going to
import matplot library. pyplot as
PLT again very common and we're going to
import caborn as SNS and we'll go ahead
and set the font scale in the SNS right
in our import line that's what with this
um semicolon followed by a line of data
we're going to set the SNS and these are
great cuz the the caborn sits on top of
matap plot Library just like Panda sits
on numpy so it adds a lot more features
and uses and control we obviously not
going to get into matplot library and
caborn it' be its own tutorial we're
really just focusing on the svm the
support Vector machine from sklearn and
since we're in Jupiter notebook uh we
have to add a special in here for our
matap plot library and that's your
percentage sign or Amber sign matplot
library in line now if you're doing this
in just a straight code Project A lot of
times I use like notepad++ and I'll run
it from there you don't have to have
that line in there because it'll just
pop up as its own window on your
computer depending on how your computer
set up because we're running this in the
Jupiter notebook as a browser setup this
tells it to display all of our Gra
graffics right below on the page so
that's what that line is for remember
the first time I ran this I didn't know
that and I had to go look that up years
ago it's quite a headache so map plot
library in line is just because we're
running this on the web setup and we can
go ahead and run this make sure all our
modules are in they're all imported
which is great if you don't have them
import you'll need to go ahead and pip
use the PIP or however you do it there's
a lot of other install packages out
there although pip is the most common
and you have to make sure these are all
in installed on your python setup the
next step of course is we got to look at
the data you can't run a model for
predicting data if you don't have actual
data so to do that let me goe and open
this up and take a look and we have our
uh cupcakes versus muffins and it's a
CSV file or CSV meaning that it's comma
separated
variable and it's going to open it up in
a nice uh spreadsheet for me and you can
see up here we have the type we have
muffin muffin muffin cupcake cupcake
cupcake and then it's broken up into
flour milk sugar butter egg baking
powder vanilla and salt so we can do is
we can go ahead and look at this data
also in our
python let us create a variable recipes
equals we're going to use our pandas
module. read CSV remember is a comma
separated
variable and the file name happened to
be cupcakes versus muffins oops I got
double brackets there
do it this
way there we go cupcakes versus
muffins because the program I loaded or
the the place I saved this particular
Python program is in the same folder we
can get by with just the file name but
remember if you're storing it in a
different location you have to also put
down the full path on
there and then because we're in pandas
we're going to go ahead and you actually
in line you can do this but let me do
the full print you can just type in
recipes. head in the Jupiter notebook
but if you're running in code in a
different script You' need to go ahead
and type out the whole print recipes.
head and pandas NOS is that's going to
do the first five lines of data and if
we flip back on over to the spreadsheet
where we opened up our CSV
file uh you can see where it starts on
line two this one calls it zero and then
2 3 4 5 6 is going to match go and close
that out cuz we don't need that anymore
and it always starts at zero and these
are it automatically indexes it since we
didn't tell it to use an index in here
so that's the index number for the
leftand side and it automatically took
the top row as uh labels so Panda's
using it to read a CSV is just really
slick and fast one of the reasons we
love our pandas not just because they're
cute and cuddly teddy bears
and let's go ahead and plot our data and
I'm not going to plot all of it I'm just
going to plot the uh sugar and
flour now obviously you can see where
they get really complicated if we have
tons of different features and so you'll
break them up and maybe look at just two
of them at a time to see how they
connect and to plot them we're going to
go ahead and use caborn so that's our
SNS and the command for that that is SNS
dolm plot and then the two different
variables I'm going to plot is flour and
sugar data equals recipes the Hue equals
type and this is a lot of fun because it
knows that this is pandas coming in so
this is one of the powerful things about
pandas mixed with Seaborn in
doing
graphing and then we're going to use a
pallet set one there's a lot of
different sets in there you can go look
them up for Seaborn
we do a regular a fit regular equals
false so we're not really trying to fit
anything and it's a scatter
kws a lot of these settings you can look
up in Seaborn half of these you could
probably leave off when you run them
somebody played with this and found out
that these were the best settings for
doing a Seaborn plot let's go ahead and
run that and because it does it in line
it just puts it right on the
page and you can see right here that
just based on sugar and flour alone
there's a definite split and we use
these models because you can actually
look at it and say hey if I drew a line
right between the middle of the blue
dots and the red dots we'd be able to do
an svm and and a hyperplane right there
in the
middle then the next step is to format
or
pre process our
data and we're going to break that up
into two parts
we need a type label and remember we're
going to decide whether it's a muffin or
a cupcake well a computer doesn't know
muffin or cupcake it knows zero and one
so what we're going to do is we're going
to create a type
label from this we'll create a numpy
array and P where and this is where we
can do some logic we take our recipes
from our Panda and wherever type equals
muffin it's going to be zero and then if
it doesn't equal muffin which is
cupcakes it's going to be one so we
create our type label this is the answer
so when we're doing our training model
remember we have to have a a training
data this is what we're going to train
it with is that it's zero or one it's a
muffin or it's
not and then we're going to create our
recipe
features and if you remember correctly
from right up here the First Column is
type so we really don't need the type
column that's our muffin or cupcake
and in pandas we can easily sort that
out we take our value
recipes dot columns that's a pandas
function built into
pandas do values converting them to
values so it's just the column titles
going across the top and we don't want
the first one so what we do is since it
always starts at zero we want
one colon till the
end and and then we want to go ahead and
make this a list and this converts it to
a list of
strings and then we can go ahead and
just take a look and see what we're
looking at for the features make sure it
looks right me go ahead and run
that and I forgot the S on recipes so
we'll go ahead and add the s in there
and then run that and we can see we have
flour milk sugar butter egg baking
powder vanilla and salt and that match
is what we have up here right where we
printed out everything but the type so
we have our features and we have our
label Now the recipe features is just
the titles of the columns and we
actually need the
ingredients and at this point we have a
couple options one we could rent it over
all the
ingredients and when you're doing this
usually you do but for our example we
want to limit it so you can easily see
what's going on because if we did all
the ingredients we have you know that's
what um seven eight different
hyperplanes that would be built into it
we only want to look at one so you can
see what the svm is
doing and so we'll take our recipes and
we'll do just flour and sugar again you
can replace that with your recipe
features and do all of them but we're
going to do just flour and sugar and
we're going to convert that to values we
don't need to make a list out of it
because it's not string values these are
actual Val values on there and we can go
ahead and just
print ingredients you can see what that
looks
like uh and so we have just the am of
flour and sugar just the two sets of
plots and just for fun let's go ahead
and take this over here and take our
recipe
features and so if we decided to use all
the recipe features you'll see that it
makes a nice column of different data so
it just strips out all the labels and
everything we just have just the values
but because we want to be able to view
this easily in a plot later on we'll go
ahead and take that and just do flour
and
sugar and we'll run that you'll see it's
just the two
columns so the next step is to go ahead
and fit our
model we'll go a and just call it model
and it's a svm we're using a package
called
SVC in this case we're going to go ahead
and set the kernel equals linear so it's
using a specific setup on there and if
we go to the reference on their website
for the
svm you'll see that there's about
there's eight of them here three of them
are for
regression three are for classification
the SVC support Vector classification is
probably one of the most commonly used
and then there's also one for detecting
outliers and another one that has to do
with something a little bit more
specific on the model but SBC and SV are
the two most commonly used standing for
support vector classifier and support
Vector regression remember regression is
an actual value a float value or
whatever you're trying to work on and
SBC is a classifier so it's a yes no
true
false but for this we want to know 01
muffin cupcake we go ahead and create
our model and once we have our model
created we're going to do model. fit and
this is very common especially in the SK
learn all their models are followed with
the fit
command and what we put into the fit
what we're training with it is we're
putting in the ingredients which in this
case we limited to just flour and sugar
and the type label is it a muffin or
cupcake now in more complicated data
science series you'd want to split into
we won't get into that today we split it
into training data and test data and
they even do something where they split
it into thirds where third is used for
where you switch between which one's
training and test there's all kinds of
things go into that it gets very
complicated when you get to the higher
end not overly complicated just an extra
step which we're not going to do today
because this is a very simple set of
data and let's go ahead and run this and
now we have our model fit and I got a
error here so let me fix that real quick
it's Capital SBC it turns
out I did it
lowercase support
vector classifier there we go let's go
ahead and run that and you'll see it
comes up with all this information that
it prints out automatically these are
the defaults of the model you notice
that we changed the kernel to linear and
there's our kernel linear on the
printout and there's other different
settings you can mess
with we're going to just to leave that
alone for right now for this we don't
really need to mess with any of
those so next we're going to dig a
little bit into our newly trained model
and we're going to do this so we can
show you on a
graph and let's go ahead and get the
separating and we're going to say we're
going to use a W for our variable on
here we're going to do
model. coefficient
or0 so what the heck is that again we're
digging into the model so we've already
got a prediction and a train this is a
math behind it that we're looking at
right now
and
so the W is going to represent two
different coefficients and if you
remember we had y equals MX plus C so
these coefficients are connected to that
but in two-dimensional it's a
plane we don't want to spend too much
time on this because you can get lost in
the confusion of the math so if you're a
math Wiz this is great you can go
through here and you'll see that we have
a = minus W 0 over w one remember
there's two different values there and
that's basically the slope that we're
generating and then we're going to build
an XX what is XX we're going to set it
up to a numpy array there's our np.
linespace so we're creating a
line plus The Intercept well to make
this work we can do this as y y equals
the slope times each value in that array
that's the neat thing about numpy so
when I do a * XX which is a whole numpy
array of values it multiplies a across
all of them and then it takes those same
values and we subtract the model
intercept that's your uh where we had MX
plus C so that'd be the C from the
formula yal MX plus
C and that's where all these numbers
come from a little bit confusing because
it's digging out of these different
arrays and then we want to do is we're
going to take this and we're going to go
ahead and plot it so plot the parallels
to separating hyper plane that pass
through the support vectors and so we're
going to create b equals a model support
vectors pulling our support vectors out
there here's our YY which we now know is
a set of data and we have uh we're going
to create YY down equal a * XX + B1 - A
*
b0 and then model support Vector B is
going to be set that to a new value the
minus - 1 set up and y y up = a * XX +
B1 - A * B 0 and we can go ahead and
just run this to load these variables up
if you want to know understand a little
bit more of what's going on you can see
if we
print y y we just run that you can see
it's an array it's this is a line it's
going to have in this case between 30
and 60 so it's going to be 30 variables
in here and the same thing with y y up y
y down and we'll we'll plot those in
just a minute on a graph so you can see
what those look
like you just go ahead and delete that
out of here and run that so it loads up
the variables nice clean slate I'm just
going to copy this from before remember
this our SNS our caborn plot LM plot
flow
sugar and I'll just go and run that real
quick so you can see what remember what
that looks like it's just a straight
graph on there and then one of the new
things is because caborn sits on top of
P plot we can do the P plot for the line
going through and that is simply PLT do
plot and that's our xx and y y our two
corresponding values XY and then
somebody played with this to figure out
that the line width equals two and the
color black would look nice so let's go
ahead and run this whole thing with the
PIP plot on there and you can see when
we do this it's just doing flower and
sugar on
here corresponding line between the
sugar and the flour and the muffin
versus
Cupcake um and then we generated the um
support vectors the y y down and y y up
so let's take a look and see what that
looks
like so we'll do our PL
plot and again this is all against
XX our x value but this time we have
YY
down and let's do something a little fun
with this we can put in a k dash dash
that just tells it to make it a dotted
line and if we're going to do the down
one we also want to do the up one so
here's our
YY up and when we run that it adds both
sets of
line and so here's our support and this
is what you expect you expect these two
lines to go through the nearest data
point so the Dash lines go through the
nearest muffin and the nearest cupcake
when it's plotting it and then your svm
goes right down the middle so it gives
it a nice split in our data and you can
see how easy it is to see based just on
sugar and flour which one's a muffin or
a
cupcake let's go ahead and create a
function to
predict muffin or
cupcake I've got my um recipe I pulled
off the um internet and I want to see
the difference between a muffin or a
cupcake so we need a function to push
that through and create a function with
DEA and let's call it muffin or cupcake
and remember we're just doing flower and
sugar today not doing all the
ingredients and that actually is a
pretty good split you really don't need
all the ingredients to know it's flour
and
sugar and let's go ahead and do an IFL
statement so if model predict
is of flour and sugar equals zero so we
take our model and we do run a predict
it's very common in skk learn where you
have a DOT predict you put the data in
and it's going to return a value in this
case if it equals zero then print you're
looking at a muffin recipe else if it's
not zero that means it's one then you're
looking at a cupcake recipe that's
pretty straightforward
for function or def for definition DF is
how you do that P on and of course if
you're going to create a function you
should run something in it and so let's
run a cupcake and we're going to send it
values 50 and 20 a muffin or a cupcake I
don't know what it is and let's run this
and just see what it gives us and it
says oh it's a muffin you're looking at
a muffin recipe so it very easily
predicts whether we're looking at a
muffin or a cupcake recipe let's plot
this there we go plot this on the graph
so we can see what that actually looks
like and I'm just going to copy and
paste it from From Below where we
plotting all the points in
there so this is nothing different than
what we did before if I run it you'll
see it has all the points and the lines
on there and what we want to do is we
want to add another point and we'll do
PLT
plot and if remember correctly we did
for our test we did 50 and
20 and then somebody went in here and
decided we'll do yo for yellow or it's
kind of a orange is yellow color is
going to come out marker size n
those are settings you can play with
somebody else played with them to come
up with the right setup so it looks good
and you can see there it is graph
clearly a
muffin in this case in cupcakes versus
muffins the muffin has won and if you'd
like to do your own muffin cupcake
Contender series you certainly can send
a note down below and the team at simply
learn will send you over the data they
use for the muffin and cupcake and
that's true of any the data um we didn't
actually run a plot on it earlier we had
men versus women you can also request
that information to run it on your data
setup so you can test that
out so to go back over our setup we went
ahead for our support Vector machine
codee we did a predict 40 Parts flour 20
Parts sugar I think it was different
than the one we did whether it's a
muffin or a cupcake hence we have built
a classifier using spvm which is able to
classify if a recipe is of a cupcake
or a muffin which wraps up our cupcake
versus muffin today in our second
tutorial we're going to cover K means
and linear regression along with going
over the quiz questions we had during
our first tutorial what's in it for you
we're going to cover clustering what is
clustering K means clustering which is
one of the most common used clustering
tools out there including a flowchart to
understand K means clustering and how it
functions and then we'll do an actual
python live demo on clustering of cars
based on Brands then we're going to
cover logistic regression what is
logistic regression logistic regression
curve and sigmoid function and then
we'll do another python code demo to
classify a tumor as malignant or benign
based on features and let's start with
clustering suppose we have a pile of
books of different genres now we divide
them into different groups like fiction
horror education and as we can see from
this young lady she definitely is into
heavy horror you can just tell by those
eyes in the maple Canadian leaf on her
shirt but we have fiction horror and
education and we want to go ahead and
divide our books up well organizing
objects into groups based on similarity
is clustering and in this case as we're
looking at the books we're talking about
clustering things with know categories
but you can also use it to explore data
so you might not know the categories you
just know that you need to divide it up
in some way to conquer the data and to
organize it better but in this case
we're going to be looking at clustering
in specific categories and let's just
take a deeper look at that we're going
to use K means clustering K means
clustering is probably the most commonly
used clustering tool in the machine
learning library K means clustering is
an example of unsupervised learning if
you remember from our previous thing it
is used when you have unlabeled data so
we don't know the answer yet we have a
bunch of data that we want to Cluster
into different groups Define clusters in
the data based on feature similarity
so we've introduced a couple terms here
we've already talked about unsupervised
learning and unlabeled data so we don't
know the answer yet we're just going to
group stuff together and see if we can
find un answer of how things connect
we've also introduced feature similarity
features being different features of the
data now with books we can easily see
fiction and horror and history books but
a lot of times with data some of that
information isn't so easy to see right
when we first look at it and so K means
is one of those tools where we can start
finding things that connect that match
with each other suppose we have these
data points and want to assign them into
a cluster now when I look at these data
points I would probably group them into
two clusters just by looking at them I'd
say two of these group of data kind of
come together but in K means we pick K
clusters and assign random centroids to
clusters where the K clusters represents
two different clusters we pick K
clusters and S random cids to the
Clusters then we compute distance from
objects to the centroids now we form new
clusters based on minimum distances and
calculate the centroids so we figure out
what the best distance is for the
centroid then we move the centroid and
recalculate those distances repeat
previous two steps iteratively till the
cluster centroids stop changing their
positions and become Static repeat
previous two steps iteratively till the
cluster centroid stop changing and the
positions become Static once the
Clusters become Static then K means
clustering algorithm is said to be
converged and there's another term we
see throughout machine learning is
converged that means whatever math we're
using to figure out the answer has come
to a solution or it's conversed on an
answer shall we see the flowchart to
understand make a little bit more sense
by putting it into a nice easy step by
step so we start we choose K we'll look
at the elbow method in just a moment we
assign random centroids to clusters and
sometimes you pick the centroids because
you might look at the dat in a graph and
say oh these are probably the central
points then we compute the distance from
the objects to the centrs we take that
and we form new clusters based on
minimum distance and calculate their
centroids then we compute the distance
from objects to the new centroids and
then we go back and repeat those last
two steps we calculate the distances so
as we're doing it it brings into the new
centroid and then we move the centroid
around and we figure out what the best
which objects are closest to each
centroid so so the objects can switch
from one centroid to the other as the
centroids are moved around and we
continue that until it is converged
let's see an example of this suppose we
have this data set of seven individuals
and their score on two topics A and B so
here's our subject in this case
referring to the person taking the test
and then we have subject a where we see
what they've scored on their first
subject and we have subject B and we can
see what they score on the second
subject now let's take two farthest
apart points as initial cluster
centroids now remember we talked about
selecting them randomly or we can also
just put them in different points and
pick the furthest one apart so they move
together either one works okay depending
on what kind of data you're working on
and what you know about it so we took
the two furthest points one and one and
five and seven and now let's take the
two farthest apart points as initial
cluster centroids each point is then
assigned to the closest cluster with
respect to the distance from the
centroids so we take each one of these
points in there we measure that distance
and you can see that if we measured each
of those distances and you use the
Pythagorean theorem for a triangle in
this case because you know the X and the
Y and you can figure out the diagonal
line from that or you just take a ruler
and put it on your monitor that'd be
kind of silly but it would work if
you're just eyeballing it you can see
how they naturally come together in
certain areas now we again calculate the
centroids of each cluster so cluster one
and then cluster two and we look at each
individual dot there's one two three
we're in one cluster uh the centroid
then moves over it becomes 1.8 comma 2.3
so remember it was at one and one well
the very center of the data we're
looking at would put it at the one point
roughly 22 but 1.8 and 2.3 and the
second one if we wanted to make the
overall mean Vector the average Vector
of all the different distances to that
centroid we come up with 4 comma 1 and
54 so we've now moved the centroids We
compare each individual's distance to
its own cluster mean and to that of the
opposite cluster and we find build a
nice chart on here that the as we move
that Cent around we now have a new
different kind of clustering of groups
and using ukian distance between the
points and the mean we get the same
formula you see new formulas coming up
so we have our individual dots distance
to the mean cent of the cluster and
distance to the mean cent of the cluster
only individual three is nearer to the
mean of the opposite cluster cluster two
then it's Stone cluster one and you can
see here in the diagram where we've kind
of circled that one in the middle so
when we've moved the clust the centroids
of the Clusters over one of the points
shifted to the other cluster because
it's closer to that group of individuals
thus individual 3 is relocated to
Cluster two resulting in a new Partition
and we regenerate all those numbers of
how close they are to the different
clusters for the new clusters we will
find the actual cluster centroids so now
we move the centroids over and you can
see that we've now formed two very
distinct clusters on here on comparing
the distance of each individual's
distance to its own cluster mean and to
that of the opposite cluster we find
that the data points are stable hence we
have our final clusters now if you
remember I brought up a concept earlier
cing on the K means algorithm choosing
the right value of K will help in less
number of iterations and to find the
appropriate number of clusters in a data
set we use the elbow method and within
sum of squares WSS is defined is the sum
of the squared distance between each
member of the cluster and its centroid
and so you see what we've done here is
we have the number of clusters and as
you do the same K means algorithm over
the different clusters and you calculate
what that centroid looks like and you
find the optimal you can actually find
the optimal number of clusters using the
elbow the graph is called as the elbow
method and on this we guessed at two
just by looking at the data but as you
can see the slope you actually just look
for right there where the elbow is in
the slope and you have a clear answer
that we want two different to start with
k means equals 2 A lot of times people
end up Computing K means equals 2 3 four
five until they find the value which
fits on the elbow joint sometimes you
can just look at the data and if you're
really good with that specific domain
remember domain I mentioned that last
time you'll know that that where to pick
those numbers or where to start guessing
at what that K value is so let's take
this and we're going to use a use case
using K means cluster ing to Cluster
cars into Brands using parameters such
as horsepower cubic inches make year Etc
so we're going to use the data set cars
data having information about three
brands of cars Toyota Honda and Nissan
we'll go back to my favorite tool the
Anaconda Navigator with the Jupiter
notebook and let's go ahead and flip
over to our Jupiter notebook and in our
Jupiter notebook I'm going to go ahead
and just paste the uh basic code that we
usually start a lot of these off with
we're not going to go too much into this
code because we've already discussed
numpy we've already discussed matplot
library and pandas and that'll be being
the number array pandas being the panda
data frame and M plot for the graphing
and don't forget uh since if you're
using the jupyter notebook you do need
the map plot library in line so that it
plots everything on the screen if you're
using a different python editor then you
probably don't need that because it'll
have a popup window on your computer and
we'll go ahead and run this just to load
our libraries and our setup into here
the next step is of course to look at
our data which I've already opened up in
a spreadsheet and you can see here we
have the miles per gallon cylinders
cubic inches horsepower weight pounds
how you know how heavy it is time it
takes to get to 60 my card is probably
on this one at about 80 or 90 what year
it is so this is you can actually see
this is kind of older cars and then the
brand Toyota Honda Nissan so the
different cars are coming from all the
way from from 1971 if we scroll down to
uh the 80s we have between the 70s and
80s a number of cars that they've put
out and let's uh when we come back here
we're going to do importing the data so
we'll go ahead and do data set equals
and we'll use pandas to read this in and
it's uh from a CSV file remember you can
always post this in the comments and
request the data files for these either
in the comments here on the YouTube
video or go to Simply learn.com and
request that the cars CSV I put it in
this same folder as the code that I've
stored so my python code is stored in
the same folder so I don't have to put
the full path if you store them in
different folders you do have to change
this and double check your name
variables and we'll go ahead and run
this and uh We've chosen data set
arbitrarily because you know it's a data
set we're importing and we've now
imported our car CSV into the data set
as you know you have to prep the data so
we're going to create the X data this is
the one that we're going to try to
figure out what's going on with and then
there is a number ways to do this but
we'll do it in a simple Loop so you can
actually see what's going on so we'll do
for i n x. columns so we're going to go
through each of the columns and a lot of
times it's important I I'll make lists
of the columns and do this because I
might remove certain columns or there
might be columns that I want to be
processed differently but for this we
can go ahead and take X of I and we want
to go fill na a and that's a panda's
command but the question is what we're
going to fill the missing data with we
definitely don't want to just put in a
number that doesn't actually mean
something and so one of the tricks you
can do with this is we can take X of I
and in addition to that we want to go
ahead and turn this into an integer
because a lot of these are integers so
we'll go ahead and keep it integers and
me add the bracket here and a lot of
editors will do this they'll think that
you're closing one bracket make sure you
get that second bracket in there if it's
a double bracket that's always something
that happens regularly so once we we
have our integer of X of Y this is going
to fill in any missing data with the
average and I was so busy closing one
set of brackets I forgot that the mean
is also has brackets in there for the
pandas so we can see here we're going to
fill in all the data with the average
value for that column so if there's
missing data is in the average of the
data it does have then once we've done
that we'll go ahead and loop through it
again and just check and see to make
sure everything is filled in correctly
and we'll print and then we take X is
null and this returns a set of the null
value or the how many lines are null and
we'll just sum that up to see what that
looks like and so when I run this and so
with the X what we want to do is we want
to remove the last column because that
had the models that's what we're trying
to see if we can cluster these things
and figure out the models there is so
many different ways to sort the X out
for one we could take the X and we could
go data set our variable we're using and
use the iocation one of the features
that's in
pandas and we could take that and then
take all the rows and all but the last
column of the data set and at this time
we could do values we just convert it to
values so that's one way to do this and
if I let me just put this down here and
print X it's a capital x we chose and I
run this you can see it's just the
values we could also take out the values
and it's not going to return anything
because there's no values connected to
it what I like to do with this is
instead of doing the iocation which does
integers more common is to come in here
and we have our data set and we're going
to do data set dot or data set. columns
and remember that list all the columns
so if I come in here let me just Mark
that as red and I print data set.
columns you can see that I have my index
here I have my MPG cylinders everything
including the brand which we don't want
so the way to get rid of the brand would
be to do data Columns of Everything But
the last one minus one so now if I print
this you'll see the brand disappears and
so I can actually just take data set
columns minus
one and I'll put it right in here for
the columns we're going to look
at and let's unmark this and unmark this
and now if I do an x. head I now have a
new data frame and you can see right
here we have all the different columns
except for the brand at the end of the
year and it turns out when you start
playing with the data set you're going
to get an error later on and it'll say
cannot convert string to uh float value
and that's because for some reason these
things the way they recorded them must
have been recorded as strings so we have
a neat feature in here on pandas to
convert
and it is simply convert
objects and for this we're going to do
convert oops convert
underscore numeric numeric equals true
and yes I did have to go look that up I
don't have it memorized the convert
numeric in there if I'm working with a
lot of these things I remember them but
um depending on where I'm at what I'm
doing I usually have to look it up and
we run that oops I must have missed
something in here let me double check my
spelling and when I double check my
spelling you'll see I missed the first
underscore in the convert objects when I
run this it now has everything converted
into a numeric value because that's what
we're going to be working with is
numeric values down
here and the next part is that we need
to go through the data and eliminate
null values most people when they're
doing small amounts you're working with
small data pools discover afterwards
that they have a null value and they
have to go back and do this so you know
be aware whenever we're formatting this
data things are going to pop up and
sometimes you go backwards to fix it and
that's fine that's just part of
exploring the data and understanding
what you
have and I should have done this earlier
but let me go ahead and increase the
size of my window one
notch there we go easier to
see so we'll do 4 I in working with x.
columns we'll page through all the
columns and we want take X of I and
we're going to change that we're going
to alter it and so with this we want to
go ahead and fill in X of I pandis Has
The Fill na a and that just fills in any
non-existent missing data I we'll put my
brackets up and there's a lot of
different ways to fill this data if you
have a really large data set some people
just void out that data because if and
then look at it later in a separate
exploration of data one of the tricks we
can do is we can take our column and we
can find the
means and the means is in our quotation
marks so when we take the columns we're
going to fill in the the non-existing
one with the means the problem is that
returns a decimal float so some of these
aren't decimals certainly we need to be
a little careful of doing this but for
this example we're just going to fill it
in with the integer version of this
keeps it on par with the other data that
is isn't a decimal
point and then what we also want to do
is we want to double check ways you can
do that is simply go in here and take
our X ofi column so it's going to go
through the x of I column it says is
null so it's going to return any any
place there's a null value it actually
goes through all the rows of each column
is null and then we want to go ahead and
sum that so we take that we add the sum
value and these are all pandas so is n's
a and a command and so is sum and if we
go through that and we go ahead and run
it and we go ahead and take and run that
you'll see that all the columns have
zero null values so we've now tested and
double checked and our data is nice and
clean we have no null values everything
is now a number value we turned it into
numeric and we've removed the last
column in our data and at this point
we're actually going to start using the
elbow method to find the optimal number
of clusters so we're now actually
getting into the SK learn part uh the K
means clustering on here I guess we'll
go ahead and zoom it up one more notot
so you can see what I'm typing in
here and then from sklearn going to or
SK
learn cluster we're going to import K
means I always forget to capitalize the
K and the M when I do this so it's
capital K capital M K
means and we'll go and create a um aray
wcss equals we make it an empty array if
you remember from the elbow method from
our
slide within the sums of squares WSS is
defined as the sum of square distance
between each member of the cluster and a
centroid so we're looking at that change
in differences as far as a squar
distance and we're going to run this
over a number of k mean
values in fact let's go for I in range
we'll do 11 of
them range 0 of
11 and the first thing we're going to do
is we're going to create the actual
we'll do it all lower
case and so we're going to create this
object from the K means that we just
imported and the variable that we want
to put into this is in clusters and
we're going to set that equals to I
that's the most important one because
we're looking at how increasing the
number of clusters changes our answer
there are a lot of settings to the K
means our guys in the back did a great
job just kind of playing with some of
them the most common ones that you see
in a lot of stuff is how you init your K
means so we have K means plus plus plus
this is just a tool to let the model
itself be smart how it picks it
centroids to start with its initial
centroids we only want to iterate no
more than 300 times we have a Max
iteration we put in there we have an the
infinite the random State equals zero
you really don't need to worry too much
about these when you're first learning
this as you start digging in deeper you
start finding that these are shortcuts
that will speed up the process as far as
a setup but the big one that we're
working with is the in clusters equals I
so we're going to literally
train our K means 11 times we're going
to do this process 11 times and if
you're working with a big data you know
the first thing you do is you run a
small sample of the data so you can test
all your stuff on it and you can already
see the problem that if I'm going to
iterate through a terabyte of data 11
times and then the K means itself is
iterating through the data multiple
times that's a heck of a process so you
got to be a little careful with this a
lot of times though you can find your l
elow using the elbow method find your
opal number on a sample of data
especially if you're working with larger
data sources so we want to go ahead and
take our K means and we're just going to
fit it if you're looking at any of the
SK learn very common you fit your model
and if you remember correctly our
variable we're using is the capital x
and once we fit this value we go back to
the um array we made and we want to go
and just depend that value on the
end and it's not the actual fit we're
pinning in there it's when it generates
it it generates the value you're looking
for is inertia so K means. inertia
will'll pull that specific value out
that we
need and let's get a visual on this
we'll do our PLT plot and what we're
plotting
here is first the xaxis which is range
01 so that will generate a nice little
plot there and the wcss for our Y
axis it's always nice to to give our
plot a
title and let's see we'll just give it
the elbow method for the title and let's
get some labels so let's go ahead and do
PLT X
label and what we'll do we'll do number
of clusters for that and PLT y
label and for that we can do oops there
we go wcss since that's what we're doing
on the plot on there and finally we want
to go ahead and display our graph which
is simply PLT do oops
do show there we go and because we have
it set to in line it'll appear in line
hopefully I didn't make a type error on
there and you can see we get a very nice
graph you can see a very nice elbow
joint there at uh two and again right
around three and four and then after
that there's not very much now as a data
scientist if I was looking at this I
would do either three or four and I'd
actually try both of them to see what
the um output look like and they've
already tried this in the back so we're
just going to use three as a setup on
here and let's go ahead and see what
that looks like when we actually use
this to show the different kinds of
cars and so let's go ahead and apply the
K means to the cars data set and
basically we're going to copy the code
that we looped through up above where K
means equals K means number of clusters
and we're just going to set that number
of clusters to three since that's what
we're going to look for you could do
three and four on this and graph them
just to see how they come up
differently' be kind of curious to look
at that but for this we're just going to
set it to three go ahead and create our
own variable y k means for our answers
and we're going to set that equal to
whoops I double equal there to K means
but we're not going to do a fit we're
going to do a fit predict is the setup
you want to use and when you're using
untrained models you'll see um a
slightly different see fit and then you
see just the predict but we want to both
fit and predict the K means on this and
that's fitore predict and then our
capital x is the data we're working
with and before we plot this data we're
going to do a little pandas trick we're
going to take our x value and we're
going to set X as Matrix so we're
converting this into a nice rows and
columns kind of setup but we want the
we're going to have columns equals none
so it's just going to be a matrix of
data in here and let's go ahead and run
that a little warning you'll see These
Warnings pop up because things are
always being updated so there's like
minor changes in the versions and future
versions instead of Matrix now that it's
more common to set it do values instead
of doing as Matrix but Mass Matrix works
just fine for right now and you'll want
to update that later on but let's go
ahead and dive in and plot this and see
what that looks like and before we dive
into plotting this data I always like to
take a look and see what I am plotting
so let's take a look at why K means I'm
just going to print that out down here
and we see we have an array of answers
we have 2 1 0 2 one two so it's
clustering these different rows of data
based on the three different spaces it
thinks it's going to
be and then let's go ahead and print X
and see what we have for x and we'll see
that X is an array it's a matrix so we
have our different values in the array
and what we're going to do it's very
hard to plot all the different values in
the array so we're only going to be
looking at the first two or positions
zero and
one and if you were doing a full
presentation in front of the board
meeting you might actually do a little
different and and dig a little deeper
into the different aspects because this
is all the different columns we looked
at but we we look at columns one and two
for this to make it easy so let's go
ahead and clear this data out of here
and let's bring up up our plot and we're
going to do a scatter plot here so PLT
scatter
and this looks a little complicated so
let's explain what's going on with this
we're going to take the X
values and we're only interested in y of
K means equals zero the first cluster
okay and then we're going to take value
zero for the xaxis and then we're going
to do the same thing here we're only
interested in K means equals zero but
we're going to take the second column so
we're only looking at the first two
columns in our answer or in the data and
then the guys in the back played with
this a little bit to make it
pretty and they discovered that it looks
good with as a size equals 100 that's
the size of the dots we're going to use
red for this one and when they were
looking at the data and what came out it
was definitely the Toyota on this so
we're just going to go ahead and label
it Toyota again that's something you
really have to explore in here as far as
playing with those numbers and see what
looks good we'll go ahead and hit enter
in there and I'm just going to paste in
the next two lines which is the next two
cars and this is our Nissa and Honda and
you'll see with our scatter plot we're
now looking at where Yore K means equals
1 and we want the zero column and y k
means equals 2 again we're looking at
just the first two columns zero and one
and each of these rows then corresponds
to Nissan in Honda
and I'll go ahead and hit enter on there
and uh finally let's take a look and put
the centroids on there again we're going
to do a scatter
plot and on the centroids you can just
pull that from our c means the uh model
we created do cluster centers and we're
going to just do
um all of them in the first number and
all of them in the second number which
is 0 one because you always start with
zero and
one and then they were playing with the
size and everything to make it look good
we'll do a size of 300 we're going to
make the color yellow and we'll label
them soor good to have some good labels
centroids and then we do want to do a
title PLT
title and pop up there PLT title you
always make want to make your graphs
look pretty we'll call it clusters of
car
make and one of the features of the plot
library is you can add a
Legend it'll automatically bring in it
since we've already labeled the
different aspects of the legend with
Toyota Nissan and
Honda and finally we want to go ahead
and show so we can actually see it and
remember it's in line uh so if you're
using a different editor this not the
Jupiter notebook you'll get a popup of
this and you should have a nice set of
clusters here so we can look at this and
we have a clusters of Honda and green
Toyota and red Nissan and purple and you
can see where they put the centroids to
to separate
them now when we're looking at this we
can also plot a lot of other different
data on here as far because we only
looked at the first two columns this is
just column one and two or 01 as as you
label them in computer scripting but you
can see here we have a nice clusters of
car Mak and we' were able to pull out
the data and you can see how just these
two columns form very distinct clusters
of data so if you were exploring new
data you might take a look and say well
what makes these different almost in
Reverse you start looking at the data
and pulling apart the columns to find
out why is the first group set up the
way it is maybe you're doing loans and
you want to go well why is this group
not defaulting on their loans and why is
the last group defaulting on their loans
and why is the middle group 50%
defaulting on their bank loans and you
start finding ways to manipulate the
data and pull out the answers you
want so now that you've seen how to use
K mean for clustering let's move on to
the next topic now let's look into
logistic regression the logistic
regression algorithm is the simplest
classification algorithm used for binary
or multiclassification problems and we
can see we have our little girl from
Canada who's into horror books is back
that's actually really scary when you
think about that with those big eyes in
the previous tutorial we learned about
linear regression dependent and
independent variables so to brush up y =
mx + C very basic algebraic function of
uh Y and X the dependent variable is the
target class variable we are going to
predict the independent variables X1 all
the way up to xn are the features or
attributes we're going to use to predict
the target class we know what a linear
regression looks like but using the
graph we cannot divide the outcome into
categories it's really hard to
categorize 1.5 3.6 9.8 uh for example a
linear regression graph can tell us that
with increase in number of hours studied
the marks of a student will increase but
it will not tell us whether the student
will pass or not in such cases where we
need the output as categorical value we
will use logistic regression and for
that we're going to use the sigmoid
function so you can see here we have our
marks 0 to 100 number of hours studied
that's going to be what they're
comparing it to in this example and we
usually form a line that says y = mx + C
and when we use the sigmoid function we
have P = 1/ 1 + eus y it generates a
sigmoid curve and so you can see right
here when you take the Ln which is the
natural logarithm I always thought it
should be n l not Ln that's just the
inverse of uh e your e to the minus y
and so we do this we get Ln of p/ 1 - p
= m * x + C that's the sigmoid curve
function we're looking for four now we
can zoom in on the function and you'll
see that the function as it derives goes
to one or to zero depending on what your
x value is and the probability if it's
greater than 0.5 the value is
automatically rounded off to one
indicating that the student will pass so
if they're doing a certain amount of
studying they will probably pass then
you have a threshold value at the0 five
it automatically puts that right in the
middle usually and your probability if
it's less than 0.5 the value rented off
to zero indicating the student will fail
so if they're not studying very hard
they're probably going to fail this of
course is ignoring the outliers of that
one student who's just a natural genius
and doesn't need any studying to
memorize everything that's not me
unfortunately have to study hard to
learn new stuff problem statement to
classify whether a tumor is malignant or
benign and this is actually one of my
favorite data sets to play with because
it has so many features and when you
look at them you really are hard to
understand you can't just look at them
and know the answer so it gives you a
chance to kind of dive into what data
looks like when you aren't able to
understand the specific domain of the
data but I also want you to remind you
that in the domain of medicine if I told
you that my probability was really good
it classify things that say 90% or 95%
and I'm classifying whether you're going
to have a malignant or a B9 tumor I'm
guessing that you're going to go get it
tested anyways so you got to remember
the domain we're working with so why
would you want to do that if you know
you're just going to go get a biopsy
because you know it's that serious this
is like an all or nothing just
referencing the domain it's important it
might help the doctor know where to look
just by understanding what kind of tumor
it is so might help them or Aid them in
something they missed from before so
let's go ahead and dive into the code
and I'll come back to the domain part of
it in just a minute so use case and
we're going to do our normal Imports
here we're importing numpy Panda Seaborn
in the matplot library and we're going
to do matplot library in line since I'm
going to switch over to Anaconda so
let's go ahead and flip over there and
get this started so I've opened up a new
window in my anaconda Jupiter
notebook by the way Jupiter notebook uh
you don't have to use Anaconda for the
Jupiter notebook I just love the
interface and all the tools that
Anaconda brings so we got our import
numpy as inp for our numpy number array
we have our Panda PD we're going to
bring in Seaborn to help us with our
graphs as SNS so many really nice Tools
in both caborn and matplot library and
we'll do our matplot library. pyplot as
PLT and then of course we want to let it
know to do it in line and let's go and
just run that so it's all set up and
we're just going to call our data data
not creative today uh equals PD and this
happens to be in a CSV file so we'll use
the pd. read CSV and I happen to name
the file renamed it data for p2.png
done the diagnosis M for malignant B for
B9 so there's two different options on
there and that's what we're going to try
to predict is the m andb and test it and
then we have like the radius mean or
average the texture average perimeter
mean area mean smoothness I don't know
about you but unless you're a doctor in
the field most of the stuff I mean you
can guess what concave means just by the
term concave but I really wouldn't know
what that means in the measurements
they're taking so they have all kind of
stuff like how smooth it is uh the
Symmetry and these are all float values
we just page through them real quick and
you'll see there's I believe 36 if I
remember correctly in this
one so there's a lot of different values
they take and all these measurements
they take when they go in there and they
take a look at the different growth the
tumorous growth so back in our data and
I put this in the same folder as a code
so I saved this code in that folder
obviously if you have it in a different
location you want to put the full path
in there there and we'll just do um
Panda's first five lines of data with
the data. head and we run that we can
see that we have pretty much what we
just looked at we have an ID we have a
diagnosis if we go all the way across
you'll see all the different columns
coming across displayed nicely for our
data and while we're exploring the data
our caborn which we referenced as SNS
makes it very easy to go in here and do
a joint plot you'll notice the very
similar to because it is sitting on top
of the um plot Library so the joint plot
does a lot of work for us and we're just
going to look at the first two columns
that we're interested in the radius mean
and the texture mean we'll just look at
those two columns and data equals data
so that tells it which two columns we're
plotting and that we're going to use the
data that we pulled in let's just run
that and it generates a really nice
graph on here and there's all kinds of
cool things on this graph to look at I
mean we have the texture mean and the
radius mean obviously the axes you can
also
see and one of the cool things on here
is you can also see the histogram they
show that for the radius mean where is
the most common radius mean come up and
where the most common texture is so
we're looking at the te the on each
growth its average texture and on each
radius its average uh radius on there
gets a little confusing because we're
talking about the individual ual objects
average and then we can also look over
here and see the the histogram showing
us the median or how common each
measurement is and that's only two
columns so let's dig a little deeper
into caborn they also have a heat map
and if you're not familiar with heat
Maps a heat map just means it's in color
that's all that means heat map I guess
the original ones we plotting heat
density on something and so ever sens
it's just called a heat map and we're
going to take our data and get our
corresponding numbers to put that into
the heat map and that's simply data. C
RR for that that's a pandas expression
remember we're working in a pandas data
frame so that's one of the Cool Tools in
pandas for our data and this just pull
that information into a heat map and see
what that looks like and you'll see that
we're now looking at all the different
features we have our ID we have our
texture we have our area our compactness
concave points and if you look down the
middle of this chart diagonal going from
the upper left to bottom right it's all
white that's because when you compare
texture to texture they're identical so
they're 100% or in this case perfect one
in their
correspondence and you'll see that when
you look at say area or right below it
it has almost a black on there when you
compare it to texture so these have
almost no corresponding data They Don't
Really form a linear graph or something
that you can look at and say how
connected they are they're very
scattered data this is really just a
really nice graph to get a quick look at
your data doesn't so much change what
you do but it changes verifying so when
you get an answer or something like that
or you start looking at some of these
individual pieces you might go hey that
doesn't match according to showing our
heat map this should not correlate with
each other and if it is you're going to
have to start asking well why what's
going on what else is coming in there
but it does show some really cool
information on here I mean we can see
from the ID there's no real one feature
that just says if you go across the top
line that lights up there's no one
feature that says hey if the area is a
certain size then it's going to be B9 or
malignant it says there's some that sort
of add up and that's a big hint in the
data that we're trying to ID this
whether it's malignant or B9 that's a
big hint to us as data scientist to go
okay we can't solve this with any one
feature it's going to be something that
includes all the features or many of the
different features to come up with the
solution for it and while we're
exploring the data let's explore one
more area and let's look at data do is
null we want to check for null values in
our data if you remember from earlier in
this tutorial we did it a little
differently where we added stuff up and
summ them up you can actually with
pandas do it really quickly data. is
null and Summit and it's going to go
across all the columns so when I run
this
you're going to see all the columns come
up with no null
data so we've just just to reash these
last few steps we've done a lot of
exploration we have looked at the first
two columns and seen how they plot with
the caborn with a joint plot which shows
both the histogram and the data plotted
on the X Y coordinates and obviously you
can do that more in detail with
different columns and see how they PL
together and then we took and did the
Seaborn heat map the SNS do heat map of
the data and you can see right here
where it did a nice job showing us some
bright spots where stuff correlates with
each other and forms a very nice
combination or points of scattering
points and you can also see areas that
don't and then finally we went ahead and
checked the data is the data null value
do we have any missing data in there
very important step because it'll crash
later on if you forget to do this step
it will remind you when you get that
nice error code that says null values
okay so not a big deal if you miss it
but it it's no fun having to go back
when you're you're in a huge process and
you've missed this step and now you're
10 steps later and you got to go
remember where you were pulling the data
in so we need to go ahead and pull out
our X and our y so we just put that down
here and we'll set the x equal to and
there's a lot of different options here
certainly we could do x equals all the
columns except for the first two because
if you remember the first two is the ID
and the diagnosis so that's certainly
would be an option but what we're going
to do is we're actually going to focus
on the worst the worst radius the worst
texture parameter area smoothness
compactness and so on one of the reasons
to start dividing your data up when
you're looking at this information is
sometimes the data will be the same data
coming in so if I have two measurements
coming into my model it might overweigh
them it might overpower the other
measurements because it's measure it's
basically taking that information in
twice that's a little bit past the scope
of this tutorial I want you to take away
from this though is that we are dividing
the data up into pieces and our team in
the back went ahead and said hey let's
just look at the worst so I'm going to
create a an array and you'll see this
array radius worst texture worst
perimeter worst we've just taken the
worst of the worst and I'm just going to
put that in my X so this x is still a
pandas data frame but it's just those
columns and our y if you remember
correctly it's going to be oops hold on
one second it's not X it's data there we
go so x equals data and then it's a list
of the different columns the worst of
the worst and if we're going to take
that then we have to have our answer for
our Y for the stuff we know and if you
remember correctly we're just going to
be looking
at the diagnosis that's all we care
about is what is it diagnosed is it B9
or malignant and since it's a single
column we can just do diagnosis oh I
forgot to put the brackets the there we
go okay so it's just diagnosis on there
and we can also real quickly do like x.
head if you want to see what that looks
like and y. head and run this and you'll
see um it only does the last one I
forgot about that if you don't do print
you can see that the the Y do head is
just Mmm ones are all malignant and if I
run this the x. head is just the first
five values of radius worst texture
worst parameter worst area worst and so
on I'll go ahead and take that out so
moving down to the next step we've built
our two data sets our answer and then
the features we want to look
at in data science it's very important
to test your model so we do that by
splitting the
data and from sklearn model selection
we're going to import train test split
so we're going to split it into two
groups there are so many ways to do this
I noticed in one of the more modern ways
they actually split it into three groups
and then you model each group and test
it against the other groups so you have
all kinds and there's reasons for that
which is past the scope of this and for
this particular example isn't necessary
for this we're just going to split it
into two groups one to train our data
and one to test our data and the sklearn
uh. model selection we have train test
split you could write your own quick
code to do this where you just randomly
divide the data up into two groups but
they do it for us
nicely and we actually can almost we can
actually do it in one statement with
this where we're going to generate four
variables capital x train capital X test
so we have our training data we're going
to use to fit the model and then we need
something to test it and then we have
our y train so we're going to train the
answer and then we have our test so this
is the stuff we want to see how good it
did on our model and we'll go ahead and
take our train test split that we just
imported and we're going to do X and our
y our two different data that's going in
for our split and then the guys in the
back came up and wanted us to go ahead
and use a test size equals three that's
testore size random State it's always
nice to kind of switch a random State
around but not that important with this
means is that the test size is we're
going to take 30% of the data and we're
going to put that into our test
variables our y test and our X test and
we're going to do 70% into the X train
and the Y train so we're going to use
70% of the data to train our model and
30% to test it let's go ahead and run
that and load those up so now we have
all our stuff split up and all our data
ready to go now we get to the actual
Logistics part we're actually going to
do our create our model so let's go
ahead and bring that in from sklearn
we're going to bring in our linear model
and we're going to import logistic
regression that's the actual model we're
using and this's we'll call it log
model oops there we go model and let's
just set this equal to our logistic
regression that we just imported so now
we have a variable log model set to that
class for us to use and with most the uh
models in the SK learn we just need to
go ahead and fix it fit do a fit on
there and we use our X train that we
separated it out with our y train and
let's go ahead and run this so once
we've run this we'll have a model that
fits this data that 70% of our training
data uh and of course it prints this out
that tells us all the different
variables that you can set on there
there's a lot of different choices you
can make but for word do we're just
going to let all the default set we
don't really need to mess with those on
this particular example and there's
nothing in here that really stands out
is super important until you start
fine-tuning it but for what we're doing
the basics will work just fine and then
let's we need to go ahead and test out
our model is it working so let's create
a variable y predict and this is going
to be equal to our log
model and we want to do a predict again
very standard uh format for the sklearn
library is taking your model and doing a
predict on it and we're going to test y
predict against the Y test so we want to
know what the model thinks it's going to
be that's what our y predict is and with
that we want the capital x x test so we
have our train set and our test set and
now we're going to do our y predict and
let's go ahead and run
that and if we uh
print y predict let me go ahead and run
that you'll see it comes up and it PR a
prints a nice array of uh B and M for B9
and malignant
for all the different test data we put
in there so it does pretty good we're
not sure exactly how good it does but we
can see that it actually works and is
functional was very easy to create
you'll always discover with our data
science that as you explore this you
spend a significant amount of time
prepping your data and making sure your
data coming in is good uh there's a
saying good data in good answers out bad
data in bad answers out that's only half
the thing that's only half of it
selecting your models becomes the next
part as far as how good your models are
and then of course fine-tuning it
depending on what model you're using so
we come in here we want to know how good
this came out so we have our y predict
here log model. predict X
test so for deciding how good our model
is we're going to go from the SK
learnmetrics we're going to import
classification report and that just
reports how good our model is doing and
then we're going to feed it the model
data and let's just print this out and
we'll take our classification
report and we're going to put into
there our test our actual data so this
is what we actually know is true and our
prediction what our model predicted for
that data on the test side and let's run
that and see what that
does so we pull that up you'll see that
we have um a Precision for B9 and
malignant B&M and we have a Precision of
93 and 91 a total of 92 so it's kind of
the average between these two of 92
there's all kinds of different
information on here your F1
score your recall your support coming
through on this and for this I'll go
ahead and just flip back to our slides
that they put together for describing it
and so here we're going to look at the
Precision using the classification
report and you see this is the same
print out I had up above some of the
numbers might be different because it
does randomly pick out which data we're
using so this model is able to predict
the type of tumor with
91% accuracy so when we look back here
that's you will see where we have uh B9
Inland it actually is 92 coming up here
but we're looking about a 92 91%
precision and remember I reminded you
about domain so we're talking about the
domain of a medical domain with a very
catastrophic outcome you know at 91 or
92% Precision you're still going to go
in there and have somebody do a biopsy
on it very different then if you're
investing money and there's a 92% chance
you're going to earn 10% and 8% chance
you're going to lose 8% you're probably
going to bet the money because at that
odds it's pretty good that you'll make
some money and in the long run if you do
that enough you definitely will make
money and also with this domain I've
actually seen them use this to identify
different forms of cancer that's one of
the things they're starting to use these
models for because then it helps the
doctor know what to investigate so that
wraps up this section we're finally
we're going to go in there and let's
discuss the anwers to the quiz asked in
machine learning tutorial part one can
you tell what's happening in the
following cases grouping documents into
different categories based on the topic
and content of each document this is an
example of clustering where K means
clustering can be used to group the
documents by topics using bag of words
approach so if You' gotten in there that
you're looking for clustering and
hopefully you had at least one or two
examples like K means that are used for
clustering different things then give
yourself a two thumbs up B identifying
handwritten digits in images correctly
this is an example of classification the
traditional approach to solving this
would be to extract digit dependent
features like curvature of different
digits Etc and then use a classifier
like svm to distinguish between images
again if you got the fact that it's a
classification example give yourself a
thumb up and if you're able to go hey
let's use svm or another model for this
give yourself those two thumbs up on it
C behavior of a website indicating that
the site is not working as designed this
is an example of anomaly detection in
this case the algorithm learns what is
normal and what is not normal usually by
observing the logs of the website give
yourself a thumbs up if you got that one
and just for a bonus can you think of
another example of a anomaly detection
one of the ones I use it for my own
business is detecting anomalies in stock
markets stock markets are very ficked
and they behave very erical so finding
those erratic areas and then finding
ways to track down why they're erratic
was something released in social media
was something released you can see where
knowing where that anomaly is can help
you to figure out what the answer is to
it in another area D predicting salary
of an individual based on his or her
years of experience this is an example
of regression this problem can be
mathematically defined as a function
between independent years of experience
and dependent variables salary of an
individual and if you guess that this
was a regression model give yourself a
thumbs up and if you're able to remember
that it was between independent and
dependent variables and that terms give
yourself two thumbs up we're going to
cover mathematics for machine learning
so today's agenda is going to cover data
and its types then we're going to dive
into linear algebra and its Concepts
calculus statistics for machine learning
probability for machine learning
Hands-On demos and of course throwing in
there in the middle is going to be your
matrixes and a few other things to go
along with all
this data and is types data denotes the
individual pieces of factual information
collected from various sources it is
stored processed and later used for
analysis and so we see here uh just a
huge grouping of information a lot of
text stuff money dollar signs
numbers uh and then you have your
performing analytics to drive insights
and hopefully you have a nice share your
shareholders gathered at the meeting and
you're able to explain it in something
they can understand so we talk about
data types of data we have in our types
of data we have a qualitative
categorical you think nominal or ordinal
and then you have your quantitative or
numerical which is discrete or
continuous and let's look a little
closer at those data type vocabulary
always people's favorite is the
vocabulary words okay not mine uh but
let's di dive into this what we mean by
nominal nominal they are used to label
various uh label our variables without
providing any measurable value uh
country gender race hair color Etc it's
something that you either mark true or
false this is a label it's on or off
either they have a red hat on or they do
not uh so a lot of times when you're
thinking nominal data labels uh think of
it as a true false kind of setup and we
look at ordinal this is categorical data
with a set order or a scale to it uh and
you can think of salary range is a great
one uh movie ratings Etc you see here
the salary R if you have 10,000 to
20,000 number of employees earning that
rate is 150 20,000 to 30,000 100 and so
forth some of the terms you'll hear is
bucket uh this is where you have 10
different buckets and you want to
separate it into something that makes
sense into those 10 buckets and so we
start talking about ordinal a lot of
times when you get down to the breast
bones again we're talking true false uh
so if you're a member of the 10 to 20K
range uh so forth those would each be
either part of that group or you're not
but now we're talking about buckets and
we want to count how many people are in
that bucket quantitative numerical data
uh falls into two classes discrete or
continuous and so data with a final set
of values which can be categorized class
strength questions answered correctly
and runs hit and Cricket a lot of times
when you see this you can think integer
uh and a very restricted integer I.E you
can only have 100 questions um on a test
so you can it's very discret I only have
a 100 different values that it can
attain so think usually you're talking
about integers but within a very small
range they don't have an open end or
anything like that uh so discreet is
very solid simple to count set number
continuous on the other hand uh
continuous data can take any numerical
value within a range so water pressure
weight of a person Etc usually we start
thinking about float values where they
can get phenomenally small in their in
what they're worth and there's a whole
series of values that falls right
between discrete and
continuous um you can think of the stock
market you have dollar amounts it's
still discreet but it starts to get
complicated enough when you have like
you know jump in the stock market from
$525 33 to
$580 67 there's a lot of Point values in
there it' still be called discreet but
you start looking at it as almost
continuous because it does have such a
variance in it now uh we talk about no
we did we went over nominal and ordinal
uh almost true false charts and we
looked at quantitative and numerical
data which we start to get into numbers
discreet you can usually a lot of times
discrete will be put into it could be
put into true false but usually it's not
uh so we want to address this stuff and
the first thing we want to look at is
the very basic which is your algebra so
we're going to take a look at linear
algebra you can remember back when your
ukian geometry uh we have a line well
let's go through this we have a linear
algebra is the domain of mathematics
concerning linear equations
and their representations in Vector
spaces and through matrixes I told you
we're going to talk about
matrixes uh so a linear equation is
simply um uh 2x + 4 y - 3 Z = 10 very
linear 10 x + 12.4 y = z and now you can
actually solve these two equations by
combining them uh and that's where we're
talking about a linear
equation in the vectors we have a plus b
equal C now we're starting to look at a
direction and these values usually think
of an XYZ plot um so each one is a
direction and the actual distance of
like a triangle AB is C and then your
Matrix can describe all kinds of things
um I find matrixes uh confuse a lot of
people not because they're particularly
difficult but because of the magnitude
and the different things they're used
for and a matrix is a a chart or a um
you know think of a spreadsheet but you
have your rows and your columns and
you'll see here we have a * Bal C very
important to know your counts uh so
depending on how the math is being done
what you're using it for making sure you
have the same rows and number of columns
or a single number there's all kinds of
things that play in that that can make
Matrix is confusing uh but really it has
a lot more to do with what domain you're
working in uh are you adding in multiple
polom where you have like uh uh ax^2
plus b y plus you know you start to see
that it can be very confusing versus a
very straightforward Matrix and let's
just go a little deeper into these
because these are such primary this is
what we're here to talk about is these
different math uh mathematical
computations that come up so when we're
looking at linear equations let's dig
deeper into that one an equation having
a maximum order of one is called a
linear equation
uh so it's linear because when you look
at this we have uh ax plus Bal C which
is a one variable we have two variable
ax plus b yal c ax plus b y + z c zal d
and so forth but all of these are to the
power of one you don't see X squ you
don't see X cubed so we're talking about
linear equations that's what we're
talking about in their addition if you
have already dived into say neural
networks you should recognize this ax
plus b y + CZ um setup plus The
Intercept uh which is basically your
your neural network each node adding up
all the different inputs and we can
drill down into that most common formula
is your y = mx plus
C so you have your uh y equals the M
which is your slope your X Value Plus C
which is your um Y intercept they kind
of labeled it wrong here uh threw me for
a loop but the the C would be your Y
intercept so when you set x equal to 0 y
equal C and that's that's your Y
intercept right there uh and that's they
they just had reversed value of y when x
equals 0 equals the Y intercept which is
C and your slope gradient line which is
your M so you get your y = 2x + 3 and
there's lots of easy ways to compute
this this why this is why we always
start with the most basic one when we're
solving one of these problems s and then
of course the one of the most important
takeaways is the slope gradient of the
line uh so the slope is very important
that M value uh in this case we went
ahead and solved this if you have y = 2x
+ 3 you can see how it has a nice line
graph here on the
right so matrixes a matrix refers to a
rectangular representation of an array
of numbers arranged in columns and
rows so we're talking M rows by in
columns here A1 is denotes the element
of the first row in the First Column
similarly a12 and it's really pronounced
a11 in this particular setup so it's a
row one column one A2 is a of Row one
column 2 uh first row and second column
and so
on and there's a lot of ways to denote
this I've seen these as like a capital
letter a smaller case a for the top row
or I mean you can see where they can go
all kinds of different different
directions as far as the
value you just take a moment to realize
there needs to be some designation as
far as what row it's in and what column
it's
in and we have our uh basic operations
we have addition so when you think about
addition you have uh two matrixes of 2x
two and you just add each individual
number in that Matrix and then when you
get to the bottom you have uh in this
case the solution is 12 10 + 2 is 12 5 +
3 is 8 and so on and the same thing with
subtraction now again you're counting
matrixes you want to check your um
dimensions of the Matrix the shape
you'll see shape come up a lot in
programming so we're talking about
Dimensions we're talking about the shape
if the two shapes are equal this is what
happens when you add them together or
subtract them and we have multiplication
when you look at the multiplication you
end up with a very slightly different
setup going now if we look at our last
one we're um uh we're like why this
always gets to me when we get to
matrixes they don't really say why you
multiply
matrixes um you know my first thought is
1 * 2 4 * 3 but if you look at this we
get 1 * 2 + 4 * 3 1 * 3 + 4 *
5 uh 6 * 2 + 3 * 3 6 * 3 + 3 * 5 if
you're looking at these matrixes uh
think of this more as an equation and so
we have uh if you remember when we back
up here for our multiple line equations
let's just go back up a couple slides
where we were looking at uh two variable
so this is a two variable equation ax
plus b yal
c um and this is a way to make it very
quick to solve these variables and
that's why you have the Matrix and
that's why you
do the multiplication the way they do
and this is the dotproduct of uh 1 * 2 +
4 * *
3 1 * 3 + 4 *
5 uh 6 * 2 + 3 * 3 6 * 3 + 3 * 5 and it
gives us a nice little U 14 23 21 and 33
over here which then can be used and
reduced down to a sample um formula as
far as solving the variables as you have
enough inputs uh and then in Matrix
operations when you're dealing with a
lot of matrixes uh now keep in mind
multiplying matrixes is different than
finding the product of two matrixes okay
so we're talking about multiplication
we're talking about solving uh for
equations when you're finding the
product you are just finding 1 time 2
keep that in mind because that does come
up I've had that come up a number of
times where I am altering data and I get
confused as to what I'm doing with it uh
transpose flipping the Matrix over its
diagonal comes up all the time where you
have you still have 12 but instead of it
being a 128 it's now 1214 821 you're
just flipping the columns and the rows
uh and then of course you can do an
inverse um changing the signs of the
values across this main diagonal and you
can see here we have the inverse a to
the minus one and ends up with uh
instead of 12 8 14 12 it's now - 22 -2
vectors uh Vector just means we
have a value and a direction and we have
down four numbers here here on our
Vector uh in mathematics a
one-dimensional matrix is called a
vector uh so if you have your X plot and
you have a single value that value is
along the x- axis and it's a single
Dimension if you have two Dimensions you
can think about putting them on a graph
you might have X and you might have y
and each value denotes a direction and
then of course the actual distance is
going to be the hypothesis of that
triangle uh and you can do that with
three dimensionals x Y and Z uh and you
can do it all the way to nth Dimensions
so when they talk about the K means uh
for categorizing and how close data is
together they will compute that based on
the Pythagorean theorem so you would
take uh the square of each value add
them all together and find the square
root and that gives you a distance as
far as where that point is where that
Vector exists or an actual point value
and then you can compare that point
value to another one and it makes a very
easy comparison inv verus comparing uh
50 or 60 different numbers and that
brings us up to ige vectors and ige
values uh igene vectors the vectors that
don't change their span while
transformation and IG values the scalar
values that are associated to the
vectors conceptually you can think of
the vector as your picture you have a
picture it's um uh two Dimensions X and
Y and so when you do those two
dimensions and those two values or
whatever that value is um that is that
point but the values change when you
skew it and so if we take and we have a
vector a and that's a set value uh B is
um your is your you have a and b which
is your igene vector two is the igene
value so we're altering all the values
by two that means we're um maybe we're
stretching it out One Direction making
it tall if you're doing picture editing
um that's one of the places this comes
in but you can see when you're
transforming uh your different
information how you transform it is then
your IG value and you can see here uh
Vector after line trans transition uh we
have 3A a is the igene vector three is
the aene value so a doesn't change
that's whatever we started with that's
your original picture and three uh is
skewing it one dire Direction and maybe
uh B is being skewed another Direction
and so you have a nice tilted picture
because you've altered it by those by
the igene
values so let's go ahead and pull up a
demo on linear algebra and to do this
I'm going to go through my trusted
Anaconda into my Jupiter notebook and
we'll create a new uh notebook called
linear
algebra since we are working in Python
uh we're going to use our numpy I always
import that as n p or numpy array
probably the most popular um module for
doing matrixes and things
in given that this is part of a series
I'm not going to go too much into numpy
uh we are going to go and create two
different variables a for a numpy array
1015 and b
29 we'll go ahead and run this and you
can see there's our two arrays 1015 29
and went didn't added a space there in
between so it's easier to read and since
it's the last line we don't have to put
the print statement on it unless you
want we can simp but we can simply do a
plus b so when I run this uh we have 10
15 29 and we get 30 24 which is what you
expect 10 + 20 15 + 9 you could almost
look at this addition as being
um just adding up the columns on here
coming down and if we wanted to do it a
different way we could also do A.T
plus B.T remember that t flips them and
so if we do that we now get them uh we
now have 3024 going the other way we
could also do something kind of fun
there's a lot of different ways to do
this uh as far as a plus b I can also do
a plus
B.T and you're going to see that that
will come out the same the 3024 whether
I transpose A and B or transpose them
both at the end
and likewise we can very easily subtract
two vectors I can go a minus B and we
run that and we get min - 10 6 now
remember this is the last line in this
particular section that's why I don't
have to put the print around it um and
just like we did before we can transpose
either the individual or we can
transpose the main setup and then we get
a - 106 going the other
way now we didn't mention this in our
notes but you can also do a scalar
multiplication let me just put down the
scaler so you can remember that uh what
we're talking about here is I have uh
this array here U and if I go a Time U
uh we'll take the value two we'll
multiply it by every value in here so 2
* 30 is 60 2 *
15 and just like we did before
um this happens a lot because when
you're doing matrixes you do need to
flip them you get 6030 coming this way
so in numpy uh we have what they call
Dot
product and uh with this this is in a
twodimensional vectors it is the
equivalent of to matrix
multiplication remember we were talking
about matrix
multiplication uh where it is the well
let's walk through
it we'll go ahead and start by defining
two um numpy arrays we'll have uh 10 20
25 6 or our U and our V uh and then
we're going to go ahead and do if we
take the values uh and if you remember
correctly an array like this would be 10
* 25 + 20 * 6 we'll go ahead and uh
print
that there we go
and then we'll go ahead and do the np.
dot of VI
comma
V and we'll find when we do this we go
and run this uh we're going to get uh
370
370 so this is a strain multiplication
where they use it to solve uh linear
algebra uh when you have multiple
numbers going across and so this could
be very complicated we could have a
whole string of different variables
going in here but for this we get a nice
uh value for our Dot
multiplication and we did um addition
earlier which was just your basic
addition uh and of course the Matrix you
can get very complicated on these or in
this case we'll go ahead and do um let's
create two complex
matrixes this one is a matrix of um you
know 1210 46431
we'll just print out a so you see what
that looks like here's print
a we print a out you can see that we
have a um
2x3 layer Matrix for a and we can also
put together always kind of fun when
you're playing with print values uh we
could do something like this we could go
in here there we go uh we could print a
we have it end with uh equals a run and
this kind of gives it a nice look uh
here's your Matrix that's all this is
comma N means it just tags it on the end
that's all all that is doing on there
and then we can simply add in what is a
plus b and you should already guess
because this is the same as what we did
before there's no difference uh when we
do a simple vector addition we have 12 +
2 is 14 10 + 8 is 18 and so on and just
like we did the uh Matrix addition we
can also do a minus B and do our Matrix
subtraction and we look at this uh we
have what 12 minus 2 is 10 10 minus 8 um
where are
we oh there we go 8 H confusing what I'm
looking at I should have reprinted out
the original numbers uh but we can see
here 12 - 2 is of course 10 10 - 8 is 2
uh 4 - 46 is - 42 and so forth so same
as a subtraction as before we just call
it Matrix sub fraction is
identical now if you remember up here we
had a scalar addition where we're adding
just one number to a matrix you can also
do scalar
multiplication uh and so simply if you
have a single value a and you have B
which is your array we can also do a * B
when we run that uh you can see here we
have 2 * 4 is 8 uh 5 * 4 is 20 and so
forth you're just multiplying the four
across each one of these values and this
is an interesting one that comes up a
little bit of a brain teaser is Matrix
and Vector
multiplication and so when we're looking
at
this uh we are just do a regular arrays
it doesn't necessarily have to be a
numpy array we have a which has our um
array of arrays and B which is a single
array and so we can from
here do the
dot
ab and this is going to return two
values and the first value is that it's
you could say it's like uh um we're
doing the this array B array first with
a and then with the second one and so it
splits it up so you have a matrix of
vector multiplication and you can mix
and match when you get into really
complicated uh backend stuff this
becomes more common because you're now
you got layers upon layers of data and
so you you'll end up with a matrix and a
set of vector matrices do you want to
multiply now keep in mind that if you're
doing data science a lot of times you're
not looking at this this is what's going
on behind the scenes so if you're in um
the scikit looking at sklearn where
you're doing linear regression models
this is some of the math that's hidden
behind the scenes that's going on other
times you might find yourself having to
do part of this and manipulate the data
around so it fits right and then you go
back in and you run it through the S kit
and if we can do um up here where we did
a uh Matrix and Vector multiplication we
can also do Matrix to matrix
multiplication and if we run this where
we have the two matrixes uh you can see
we have very complicated array that of
course comes out on there for our DOT
and just to reiterate it we have our
transpose a matrix which is yourt T and
so if we create a matrix a and we do a
transpose it you can see how it flips it
from 5 10 15 20 25 30 2 5 15 25 10 20 30
uh rows and
columns and certainly with the math uh
this comes up a lot um it also comes up
a lot with XY plotting when you put into
Pi plot you have one format where
they're looking at Pairs and numbers and
then they want all of x's and all y's so
you know the transpose is an important
tool both for your math and for plotting
and all kinds of things another tool
that we didn't discuss uh is your
identity
Matrix uh and this one is more
definition uh the identity Matrix um we
have here one where we just did uh two
so it comes down as one0 01 uh 1 0 0 1 0
it creates a diagonal of one and what
that is is when you're doing your
identities you could be comparing all
your different features to the different
features and how they correlate and of
course when you have a feature one
compared to feature one to itself it is
always one uh where usually it's between
zero one depending on how well
correlates so when we're talking about
identity Matrix that's what we're
talking about right here is that you
create this preset Matrix and then you
might adjust these numbers depending on
what you're working with and what the
domain is and then another thing we can
do uh to kind of wrap this up we'll hit
you with the most complicated uh um
piece of this puzzle here is an inverse
um a matrix and let's just go ahead and
put the um oh it's a lengthy
description let's go and put the
description this is straight out of the
uh the website for um numpy uh so given
a square Matrix a here's our Square
Matrix a which is 2 1 0 01
0121 keep in mind 3x3 it's Square it's
got to be equal it's going to return The
Matrix a inverse satisfying a um a
inverse so here's our matrix
multiplication um and then of course it
equals the dot uh yeah a inverse of a um
with an identity shape of uh a DOT
shaped zero this is just reshaping the
identity that's a little complicated
there uh so we go and have our here's
our array uh we'll go and run this and
you can see what we end up with is we
end up with uh an array 0.5 minus .5 and
so forth with our 211 going down 2 one 0
01
0121 um getting into a little deep on
the math understanding when you need
this is probably really is is what's
really important when you're doing data
science versus uh handwriting this out
and looking up the math and handwriting
all the pieces out you do need to know
about the linear algorithm inverse of a
uh so if it comes up you can easily pull
it up or at least remember where to look
it up we took a look at the algebra side
of it let's go ahead and take a look at
the calculus side of uh what's going on
here with the machine learning so
calculus oh my goodness and differential
equations you got to throw that in there
because that's all part of the bag of
tricks especially when you're doing
large neural networks but it also comes
up in many other areas the good news is
most of it's already done for you in the
back end uh so when it comes up you
really do need to understand from the
data science not data analytics data
analytics means you're digging deep into
actually solving these math equations u
and a neural network is just a giant
differential
equation uh so we talk about calculus uh
we're going to go ahead and understand
it by talking about cars versus time and
speed uh so helps to calculate the
spontaneous rate of
change uh so suppose we plot a graph
with the speed of a car with respect to
time so as you can see here going down
the highway probably merged into the
highway from an on-ramp so I had to
accelerate so my speed went way up uh
stuck in traffic merged into the traffic
traffic opens up and I accelerate again
up to the speed limit and uh maybe a
peters's off up there so you can look at
this is as um the speed versus time I'm
getting faster and faster because I'm
continually accelerating and if I hit
the brakes it' go the other way
so the rate of change of speed with
respect of time is nothing but
acceleration how fast are we
accelerating the acceleration is the
area between the start point of X and
the end point of Delta x uh so we can
calculate a simple if you had X and
Delta X we could put a line there and
that slope of the line is our
acceleration now that's pretty easy when
you're doing linear algebra but I don't
want to know it just for that line in
those two points I want to know it
across the whole of what I'm working
with that's where we get into calculus
so when we talk about the distance
between X and Delta X it has to be the
smallest possible near to zero in order
to approximate the
acceleration uh so the idea is that
instead of I mean if you ever did took a
basic calculus class they would draw
bars down here and you would divide this
area up um let's go back up a screen you
divide this area of this time period up
into maybe 10 sections and you'd use
that and you could calculate the
acceleration between each one of those
10 sections kind of thing uh and then we
just keep making that space smaller and
smaller until Delta X is almost uh
infinitism
small and so we get a function of a uh
equals a limit as H goes to zero of a
function of a plus h minus a function of
a over H and that is you're Computing
the slope of the line we're just is
Computing that slope under smaller and
smaller and smaller
samples uh and that's what calculus is
calculus is the integral you can see
down here we have our nice uh integral
sign looks like a giant s and that's
what that means is that we've taken this
down to as small as we can for that
sampling uh so we're talking about
calculus we're finding the area under
the slope is the main process in the
integration similar small intervals are
made of the smallest possible length of
X Plus Delta X where Delta X approaches
almost an infinitism small space and
then it helps to find the overall
acceleration by summing up all the
lengths together uh so we're summing up
all the accelerations from the beginning
to the end and so here's our integral we
sum of a ofx * D ofx = A + C uh that is
our basic calculus here so when we talk
about multivariate calculus uh m
multivariate calculus deals with
functions that have multiple variables
and you can see here we start getting
into some very complicated equations um
uh change in W over change of time
equals change of w over change of Z the
differential of Z to DX differential of
x to DT it gets pretty complicated uh
and it really translates into the
multivariate integration using double
integrals and so you have the the sum of
the sum of f ofx of Y of D of a equals
the sum from C to D and A to B of f ofx
y dxdy equals uh the sum of a to B sum
of C to D of fxy Dy
DX understanding the very specifics of
everything going on in here and actually
doing the math is use the calculus one
calculus 2 and differential equations uh
so you're talking about three F length
courses to dig into and solve these math
equations what we want to take from here
is we're talking about calculus uh we're
talking about summing of all these
different slopes and so we're still
solving a linear uh expression we're
still solving y = mx + b but we're doing
this for infinitism small x's and then
we want to sum them up that's what this
integral sign means the the sum of a of
x d of xal a plus
c and when you see these very
complicated uh multivariate
differentiation using the chain rule
uh when we come in here and we have the
change of w to the change of T equals
the change of w DZ uh and so forth
that's what's going on here that's what
these means we're basically looking for
the area under the curve which really
comes to how is the change changing
speed's going up how is that changing
and then you end up with a multiple
layer so if I have three layers of
neural networks how is the third layer
changing based on the second layer
changing which is based on the first
layer changing and you get the picture
here that now we have a very complicated
uh multivariate integration um with
integrals the good news is we can solve
this uh mathematically and that's what
we do when you do neural networks in
Reverse propagation uh so the nice thing
is that you don't have to solve this on
paper unless you're a data analysis and
you're working on the backend of
integrating these formulas and building
the script to actually build them so we
talk about applications of calculus uh
it provides us the tools to build an
accurate predictive model um so it's
really behind the scenes we want to
guess at what the change of the change
of the change
is that's a little goofy I I know I just
threw that out there it's kind of a
metat term but if you can guess how
things are going to change then you can
guess what the new numbers are
multivariate calculus explains the
change in our Target variable in
relation to the rate of change in the
input variables so there's are multiple
variables going in there if uh one
variable is changing how does it affect
the other
variable and then in gradient descent
calculus is used to find the local and
Global Maxima and this is really big uh
we're going actually going to have a
whole section here on gradient descent
because it is really I mean I talked
about neural networks and how you can
see how the different layers go in there
but gradient descent is one of the most
key things for trying to guess the best
answer to something so let's take a look
at the code behind gradient descent and
uh before we open up the code let's just
do real quick uh gradient
descent let's say we have a curb like
this and most common is that this is
going to represent your error
oops error there we go error uh hard to
read there and I want to make the error
as low as possible and so what I'm
looking at it is I want to find this
line here which is the minimum value so
we're looking for the minimum and it
does that by uh sampling there and then
based on this it guesses it might be
someplace here and it goes hey this is
still going down it goes here and then
goes back over here and then goes a
little bit closer and it's just playing
a high low until it gets to that spot
that bottom spot and so we want to
minimize the error and uh on the flip
note you could also want to be
maximizing something you want to get the
best output of it uh that's simply uh
minus the value uh so if you're looking
for where the peak is this is the same
as a negative for where the valley is
I'm looking for that Valley uh that's
all that is and this is a way of finding
it so the cool thing is um all the heavy
lifting is done um I actually ended up
putting together one of these a while
back is when I didn't know about
sidekick and I was just starting boy
it's a long while back and uh is playing
high low how do you play high low not
get stuck in The Valleys uh figure out
these curves and things like that well
you do that and the back end is all the
calculus and differential equations to
calculate this out the good news is you
don't have to do
those uh so instead we're going to put
together the code and let's go ahead
and see what we can do with
that so uh guys in the back put together
a nice little piece of code here which
is kind of fun uh some things we're
going to note and this is this is really
important stuff because when you start
doing your data science and digging into
your machine learning models uh you're
going to find these things are stumbling
blocks uh the first one is current X
where do we start at uh keep in mind
your model that you're working with is
very generic so whatever you use to
minimize it the first question is where
do we start um and we started at this
because the algorithm starts at x equals
3 so we arbitrarily picked five learning
rate is uh how many bars to skip going
one way or the other I'm in fact I'm
going to separate that a little bit
because these two are really important
um if we're dealing with something like
this where we're talking about um well
here's our here's the function we're
going to use our um gradient of our
function um 2 * x + 5 keep it simple so
that's a function we're going to work
with so if I'm dealing with increments
of a th 0.1 is going to be a very long
time and if I'm dealing with increments
of
0.001 uh 0.1 is going to skip over my
answer so I won't get a very good answer
uh and then we look at Precision this
tells us when to stop the algorithm so
again very specific to what you're
working on
uh if you're working with money and you
don't convert it into a float value uh
you might be dealing with 0.01 which is
a penny that might be your Precision
you're working with um and then of
course the previous step size Max
iterations uh we want something to cut
out at a certain point usually that's
built into a lot of minimization
functions and then here's our actual uh
formula we're going to be working with
and then we come in we go while previous
step size is greater than precision and
it is less than Max Max
HS say that 10 times fast um we're just
saying if it's uh if we're if we're
still greater than our Precision level
we still got to keep digging deeper um
and then we also don't want to go past a
thou or whatever this is a million or
10,000 uh running that's actually pretty
high um we almost never do Max
iterations more than like a 100 or 200
rare occasions you might go up to 4 500
if it's depending on the problem you're
working with uh so we have our previous
equals our current that way we can track
TimeWise uh the current now equals the
current minus the rate times the formula
of our previous X so now we've generated
our new version uh previous step size
equals the absolute current
previous uh so we're looking for the
change in X it equals iterations plus
one that's how we know to stop if we get
too far and then we're just going to
print the local minimum occurs at X on
here and if we go ahead and run this uh
you can see right here it gets down to
this point and it says hey um local
minimum is minus 3.3 222 for this
particular series we created uh and this
is created off of our formula here
Lambda X2 * x + 5 now when I'm running
this stuff uh you'll see this come up a
lot
in uh with the sklearn kit and and one
of the nice reasons of breaking this
down the way we did is I could go over
those top pieces uh those top pieces are
everything when you start looking at
these minimization toolkits in built-in
code and so from um we'll just do it's
actually
docs do cci.org and we're looking at the
pyit
there we go um optimize
minimize you can only minimize one value
you have the function that's going in
this function can be very complicated uh
so we used a very simple function up
here it could
be there's all kinds of things that
could be on there and there's a number
of methods to solve this as far as how
they shrink down uh and your X notot
there's your there's your start value so
your function your start value um
there's all kinds of things that come in
here that you can look at which we're
not going to um optimization
automatically creates constraints bounds
some of this it does automatically but
you really the big thing I want to point
out here is you need to have a starting
point you want to start with something
that you already know is mostly the
answer uh if you don't then it's going
to have a heck of a time trying to
calculate it
out or you can write your own little
script that does this and and does a
high low guessing and tries to find the
max value that brings us to statistics
what this is kind of all about is
figuring things out lot of vocabulary
and statistics uh so statistics well I
guess it's all relative it's definitely
not an edel class uh so a bunch of stuff
going on statistics statistics concerns
with the collection organization
analysis interpretation and presentation
of data that is a mouthful um so we have
from end to end where where does it come
from is it valid what does it mean how
do we organize it um how do we analyze
it then you got to take those analysis
and interpret it into something that uh
people can use kind of reduce it to
understandable um and nowadays you have
to be able to present it if you can't
present it then no one else is going to
understand what the heck you
did so when we look at the terminologies
uh there is a lot of terminologies
depending on what domain you're working
in so clearly if you're working in um a
domain that deals
with viruses and te- cells and and how
does you know where does it come from
and you're studying the different people
then you're going have a population if
you are working with um mechanical gear
um you know a little bit different if
you're looking for the wobbling
statistics uh to know when to replace a
rotor on a machine or something like
that uh that could be a big deal you
know we have these huge fans that turn
in our sewage processing systems and so
those fans they start to wobble and hum
and do different things that the sensors
pick up at one point do you replace them
instead of waiting for it to break in
which case it costs a lot of money
instead of replacing a bushing you're
replacing the whole fan unit uh an
interesting project that came up for our
city a while back uh so population all
objects or measurements whose properties
are being observed uh so that's your
population all the objects it's easy to
see it with people because we have our
population and large um but in the case
of the sewer fans we're talking about
how the fan units that's the population
of fans that we're working
with you have a parameter a matric uh
that is used to represent a population
or
characteristic you have your sample a
subset of the population studied you
don't want to do them all because then
you don't have a if you come up with the
conclusion for everyone you don't have a
way of testing it so you take a sample
uh sometimes you don't have a choice you
can only take a sample of what's going
on you can't U study the whole
population and a variable a metric of
interest for each person or object in a
population types of sampling we have
probabilistic approach uh selecting
samples from a larger population using a
method based on the theory of
probability and we'll go into a little
bit more deeper on these we have random
systematic stratified and then you have
a nonprobabilistic approach selecting
samples based on these subjective
Judgment of the researcher rather than
random selection uh it has to do with
convenience trying to reach a quota um
or
snowball um and they're very biased
that's one of the reasons you'll see
this big stamp on it says biased uh so
you got to be very careful on that so
probabilistic sampling uh when we talk
about a random sampling we select random
siiz samples from each group or category
so we it's as random as you can get uh
we talk about systematic sampling we're
selecting random siiz samples from each
group or category with a fixed periodic
interval uh so we kind of split it up
this would be like a Time setup or our
different categories and you might ask
your question what is a category or a
group uh if you look at I'm going to go
back of window let's say we're studying
um economics of different of an area um
we know pretty much that based on their
culture where they came from they might
need to be separated and so uh and when
I say separated I don't mean separated
from their their uh place where they
live I mean as far as the analysis we
want to look at the different groups and
make sure they're all represented so if
we had like an 80% uh of a group that is
uh say Hispanic and or Indian and also
in that same area we have 20% 20% who
are let's call our exp Patriots they
left America and they're nice and uh
your cocas group we might want to sample
a group that is representative of both
uh so we're talking about stratified
sampling and we're talking about groups
those are the groups we're talking about
and that brings us to stratified
sampling selecting approximately equal
siiz samples from each group or
category uh this way we can actually
separate the categories and give us an
insight into the different cultures and
how that might affect them in that area
uh so you can see these are very very
different kind of
depends on what you're working with um
as far as your data and what you're
studying and so we can see here just to
go a little bit more we'd have selecting
25 employees from a company of 250
employees randomly don't care anything
about them what groups are in which
office are in nothing uh and we might be
selecting one employee from every 50
unique employees in a company of 250
employees and then we have selecting one
employee from every branch in the
company office so we have all the
different branches there's our group or
categories by the branch and the
category could depend on what you're
studying so it has a lot of variation on
there you see this kind of grouping and
categorizing is also used to generate a
lot of
misinformation uh so if you only study
one group and you say this what it
is so types of Statistics uh when we
talk about statistics we're talk
about particular setup so we're talking
about solid you're describing the data
what does it look
like this
drug or 80% better survival rate than
the people um not have drug so we can
infer that that drug will work going to
affect the greater
population a measure of central
Tendencies we have your mean median and
mode and then we have a major spread
range uh but one of you can think of is
um how the data difference differences
you know what's the max in range all
that stuff is andal Tendencies maor of
central Tendencies so we talk about the
mean the average of the vales Med the
higher half and the lower half of
data uh so where's the center point of
all your different data points so your
mean might have a couple really big
numbers that skew it uh so the average
is much
higher might give you a much lower
number median because you have some
outliers why is it so much lower
and then the mode is the most frequent
appear value uh this is really
interesting you're economics and how
people um income like in the US was uh
1. 124,000 a year where the average was
close 80,000 and wow that way up so the
average person is not making money the
average soate a very interesting way of
looking at the data again these are all
uh Central tendenc single numbers you
can look at for the whole spread of the
dates the mean is the average marks of a
students in a classroom so here we have
the mean some of Mark of the students
total number of students and as we
talked about the median have Z 10 half
the numbers on the other side of the
line uh we end five in the middle mode
in a test you know simple case where
most people scored like an
82% not so easy when you have different
areas where like you have
economy and a slightly bigger group that
makes 26,000 so what do you put
thee
variations uh range what's the
difference between the highest and the
lowest
value got 100% or maybe 60 to could not
get
100% rank ordered data set into four
equal parts very common thing to do all
the basic
pesing in Scala whether you're working
in R you'll see this come up Max and
then you'll have your interquartile
range how does it look like in each
quarter of data variance measures how
far each number in the set is from the
mean and therefore from every other
number Set uh so you have like how much
Tri is going on in this
data values from the mean and you'll
usually see if I'm doing a graph I might
have the value graphed um then based on
the the error on the graph as a
background so you can see how far off it
is uh so standard deviations used a lot
so me of spread uh marks of a student
out of 100 we here from 50 to 63 or 50
to 90 so range maximum marks minimum
marks we have 90 to 45 and the spread of
that is 45 90 minus 45 and then we have
the interc cortile range using the same
marks over there you can see here where
the median is and there
and find out the mean so here you know
calculating the average there we end up
at approximately 66 for the average and
then we look at that the variance what
know the means we can do equal Markus
the mean squared Why is it squared uh
because one you want to make sure it's
you don't have like if you if you're
putting all this stuff together you end
up with error as far as one negative
one's positive one's higher lower uh so
you always see the squared value and
over the total observations and so the
standard deviation equals the square
root of the variance which is
approximately
16 model you be looking at the deviation
B again really important to know if
you're if you're predicting being way
off a little bit off at the tools let's
go ahead and pull up a little demo like
in Python some hands on here for that go
back our almost all of this you can do
in
numpy and if from here this basically a
print DF
head you can see we have the name Jane
Michael Willam Rosie Hannah and their
salaries on here and of course instead
of having to do all calculations and use
the command mean in pandas and so if I
go and do this print DF check our column
salary because we want to find the means
of that color we want to find the means
of that column uh and we go and print
this out and you can see that the
average
in and let's just go ahead and do this
we'll go ahead and put in
means and if we're going to do that we
also might want to find the is very
similar
Med we're used to means and average
interesting that those are they use the
two different words uh there can be in
some computation slight differences but
for the most part the means is the
average uh in the median oops let's put
a median here DF salary that way it
displays a little better you see the
median is
54
189,000 darn you Rosie for throwing off
our numbers uh but that's something you
want to notice this is this is the
difference between is huge looking at uh
the different data coming in and of
course we also want to find out hey
what's the most uh income that people
make example into the mode you can see
50,000 so this is this is very telling
that most people are making 50,000 the
middle point is at 54,000 so half the
people are making more than that what
that tells me is that if the most common
income is way is below the median then
there's a few there's a you know there's
a lot of high salaries going up but
there's some really low salaries in
there and so this trend which is very
common in statistic you when you're
analyzing the economy and different
people's income is pretty common and the
bigger difference between these is also
very important when we're studying
statistics uh and when you hear someone
just say hey the average income was you
might start asking questions at that
point why aren't you talking about the
median income why aren't you're talking
about the mode the most common income
what are you hiding uh and if you're
doing these analysis you should be
looking at these saying hey why why this
discrepancies why are these so different
and of course with any uh analysis it's
important to find out the
minimum and the maximum so we'll go
ahead it's just simply uh um
Min straightforward on as far as um you
know what the your lowest value what
your highest value is here later on and
real quick on no mode uh note that it
puts mode zero like I said there's a
couple different ways you can compute
mode range which is your max minus your
men so now we have a
r feature in pandas
and this you can see have our standard
deviation which we didn't compute
compute it it looks for axes and things
like that uh we have our minimum
value our maximum value and of course a
name salary uh these are these are the
basic statistics you can pull them up in
like just describe this is a dictionary
so I could actually do something like um
in here I could actually go uh count and
run and now it just prints the count uh
so because this is a dictionary you can
pull any one of these values out of here
it's kind of a quick and dirty way to
pull all the different information and
then split it up depending on what you
need now if I just walked in and gave
you this information um in a meeting at
some point you would just kind of fall
asleep that's what I would do anyway um
so we want to go ahead and and see about
graphing it here we'll go ahead and put
it into a history gram and plot that
graph on it of the salaries and let's
just go ahead and put that in here so we
do our map plot inline remember that's a
Jupiter's notebook thing uh a lot of the
new version of the map plot Library does
it automatically but just in case I
always put it in there uh import map
plot Library pip plot is PLT that's my
plot
plotting and then we have our data frame
uh I don't I guess I really don't need
to respell the data frame maybe we could
just remind yourself what's in it so
we'll go ahead and just uh
print DF that way we still have it and
then we have our salary DF salary
salary. plot history title salary
distribution color
gray uh plot axv line salary the mean
value so we're going to take the mean
value um color violet line style Dash
this is just all making it pretty uh
what color dash line line width of two
that kind of thing and the median and
let's go ahead and run this just so you
can see what we're talking
about and so up here we
are see it with the salaries we're
looking at the salary distribution and
just
look case we did Let's see we had red
for the median we have
Violet for our
aage here's our outlier here's our
person who makes a lot of money here's
the um average Med um based on the
average it really doesn't tell you much
about what people are really taking home
all it does is tell you how
much it's very easy to plot um an axv
line these are these markers and
the do a histogram and pictur thousand
words do a basic cribe which pulls all
this information from the describe uh
because this is a
dictionary and so if we want to go ahead
and look up um the mean value describe
being able
to doesn't have the print on there so
it's only going to print you easily
reference any one of these and then you
can also you don't need just the basics
you can come through and pull any one of
the
individual references from the from the
pandas on here so now we've had a chance
to describe our data uh let's get into
inferential statistics inferential
statistics allows you to make
predictions or inferences from data and
you can see here we have a nice little
picture movie ratings and and said hey
how many people like the movie dislike
it can't say and then you movie who
hasn't been in the study you can infer
that 55% chance of saying liked 35%
chance of saying disliked or so that's
real basics of what we're talking about
is you're going to infer that the next
person uh so let's look at point value
for a populations
parameter and flu bugs all that it's a
pretty big thing of how do you test
these out and make sure they're
going are generalized for the whole
population so here's a prot's small grp
here them results work for the
population nice diagram with the arrows
going back and forth in the very scary
of inferential statistics
very Central is what they call
hypothesis
testing uh and the confidence interval
which go with that and then as we get
into probability we get into our
binomial theorem our normal distribution
and Central limit theorem hypothesis
testing hypothesis testing is used to
measure the plausibility of a hypothesis
assumption by using sample data now when
we talk about theorems Theory
hypothesis uh keep in mind that if you
are in a philosophy class theory is the
same as hypothesis where theorem is a
scientific uh statement that is
something that has been proven although
it is always up for debate because in
science we always want to make sure
things are up to debate so hypothesis is
the same as a Phil philosophical class
calling a theory where theory in science
is not the same Theory and science says
this has been well proven gravity is a
theory uh so if you want to debate the
theory of gravity try jumping up and
down if you want to have a theory about
why the economy is collap collapsing in
your area that is a philosophical debate
very important I've heard people mix
those up and it is a pet peeve of mine
when we talk about hypothesis testing
the steps involved in hypothesis testing
is first we formulate a hypothesis we
figure out the right test to test our
hypothesis and we make a decision and so
when you're talking about hypothesis
you're usually trying to disprove it if
you can't disprove it and it works for
all the facts then you might call that a
theorem at some point so in a use case
uh let's consider an example we have
four students who were given a task to
clean a room every day sounds like
working with my kids they decided to
distribute the job of cleaning the room
among themselves they did so by making
four chits which has their names on it
and the name that gets picked up has to
do the cleaning for that day Rob took
the opportunity to make chits and wrote
everyone's name on it so here's our four
people Nick Rob emia emia and
summer now Rick emia and summer are
asking us to decide whether Rob has done
some Mischief in preparing the chits I.E
whether Rob has written his name on one
of the chit for that we will find out
the probability of Rob getting the
cleaning job on first day second day
third day and so on till 12 days the
probability of Rob getting the job
decreases every day I.E his turn never
comes up then definitely he has done
some Mischief while making the chits so
the probability of not doing work on day
one is uh three out of four there's a 75
chance that he didn't do uh two days 34
* 34 =
.56 3 days you have 34 34 34 which
equals
42 uh when you get to day 12 it's 032
Which is less than 0.05
remember this 0.005 uh that comes up a
lot when we're talking about um certain
values when we're looking at statistics
Rob is cheating as he wasn't chosen for
12 consecutive days that's a
verythis A
stomen or no association among the
groups alternative
hypothesis whenever something is
happening go hand in hand uh so you're
no
this is always interesting in in we're
talking about data science and the math
behind it it's about proving that the
things have no correlation no
hypothesis P value the P value is the
probability of finding the observed or
question is and in units of standard
error greater the magnitude of T the
greater the evidence against the null
hypothesis and you look at the T to the
test you're doing where the P
value 05 showing that it has a high
correlation so digging in deeper
that you don't have any drug then no
value here than the existing drug now if
we get that uh that says our null there
is no correlation and new Dr the
alternative
hypothesis positive results which will
reject the N
hypothesis all the positive test results
and finding means of different samples
in order to test they have and this
leads us to the confidence intervals
values of
observations how many cans of food do
you buy for your per on an average
around 95% of the people bought around
200 hence we can say that we have a
confidence interval of 2300 where 95% of
our values lie in that a lot so you can
start seeing what you're looking at here
where you have the
95% curve equal on both sides it's not
asymmetrical then you have your outliers
the 2.5% going each way so we t upon
hypothesis uh we're going to move to
probability uh so you have your
hypothesis once youve generated your
hypothesis we want to know the
probability of something occurring prob
occur any event can be predicted with
total certainty only ofur so any event
cannot be predicted with total certainty
be predicted as a likelihood of its
occurrence uh score prediction how good
you're going to do in whatever sport
you're in weather prediction physics in
Chaos Theory even the location of the
chair you're sitting on has
under one in trillions upon trillions so
it's probability it's going to happen
there are some things that have such a
low probability that we don't see them
so we talk about a random variable uh
random variable is a variable whose
possible vales outcomes random phenomena
so uh we have the coin toss how many
heads will occur in the series of 20
coin flips probably you know the on
average they're 10 but you really can't
know cuz it's very random how many times
a red ball is picked from a bag of balls
if there's equal number of of of red
balls and blue balls and green balls in
there how many times do sum of digits on
two dice uh result or five each um so
you know there's how often you going to
roll two fives on your pair of dice so
in a use case uh let's consider the
example of rolling two dice we have a
random variable outcome equals y you can
take values 2 3 4 5 6 7 8 9 10 11 12 so
we have a random variable in a
combination of dice and instead of
looking at how many times
um both dice for rle five let's go ahead
and look at a total sum of five and you
have in as far as your random variables
you can have a 1 4al 5 4 1 2 3
32 so four of those roles can be four if
you look at all the different options
you have for those random roles can be a
five and if we look at the total
number which happens to be 36 different
options uh you can see that we have four
out of 36 chance every time you roll the
dice that you're going to roll a total
of five you're going to have an outcome
of five and uh we'll look a little
deeper as to what that means uh but you
could think of that at what point if
someone never rolls a five or they
always roll a five can you say hey that
person's probably cheating uh we'll look
a little closer at the math behind that
but let's just consider this is one of
the cases is rolling two dice and
gambling there's also a binomial
distribution it is a probability of
getting success or failure as an outcome
in an experiment or trial that is
repeated multiple times and the key is
is by meaning two binomial uh so passing
or failing an exam winning or losing a
game and getting either head or tails so
if you ever see binomial distribution is
based on a um true false kind of setup
you win or lose let's consider a uh use
case and let's consider the game of
football between two clubs Barcelona and
Dortmund the teams will have to play a
total of four matches and we have to
find out the chances of Barcelona
winning the
series so we look at the total games and
we're looking at five different games or
matches let's say that the winning
chance for Barcelona is 75% or 75 that
means at each game they have a 75%
chance that they're going to win that
game and losing chances are 25% or .25
clearly 75 +25 equal 1 so that and
you'll see this as B theorem and this is
read I mean you have these funky looking
little p brackets AB this is the
probability of a being true while B is
already true
and probability of a being true divided
by the probability of B being true and
we talk about B theem which occurred
back in the an important formula and
it's really it's not if you actually do
the math you could just kind of do um
um XY equals JK and then you divide them
out and you're going to see the same
math but it works with prob have uh
eight or nine different studies going on
in done the studies together umus the
virus spread certainly the is done in
China us dat is different in each of
those studies but if you can find a
place together you can then compute the
changes that you need to make in one
study to make them equal and this um one
group and you want to find out more more
about it so this formula is very
powerful uh it really has to do with the
data collection part of the math and
data science and understanding where
your data is coming from and how you're
going to combine different studies in
different
groups and we'll go and go into a use
case uh let's find out the chance of a
person getting lung disease due to
smoking uh and this is kind of
interesting the way they word this um
let's say that according to medical
report provided by the hospital patients
they treated suffered long so have kind
of a generic medical report they further
found out by a survey that 15% of the
patients visit them smoke so we have 10%
that are lung disease and um 15% of the
patients smoke and finally 5% of the
people continue smoke even when they had
lung disease uh not the brightest Choice
um but you know it is an addiction so it
can be really difficult to kick and so
we can look at the probability of a uh
prior probability of 10% people having
having lung disease and then probability
B probability that a patient smokes is
15% uh and the probability of b um if B
then a the probability of a patient
smokes even though they have lung
disease is
5% and probability of a is B probability
of the patient will have lung disease if
they smoke and then when you put the
formulas together you can't create a 47
comma 4 it'll delete the four out so
it's only values and if you use
dictionaries quick reminder this should
look familiar because it is a dictionary
uh where you have a value and that value
is assigned to or that key is assigned
to a value uh so you could have a key
value setup is a dictionary so it's like
a dictionary without the value it's just
the keys and they all have to be
unique and if we run this we have a set
of 47
we can also take a list a regular um
setup and I'm going to go ahead and just
throw in another number in here four and
run it uh and you can see here if I take
my list 1 2 3 4 4 and I convert it to a
set and here it is my set from list
equals set my
list the result is 1 2 3 4 so it just
deletes that last four right out of
there and with the sets you can also go
in there and um print here is my set my
Set uh three is in the set and then if
you do three in my
set that's going to be a logic function
uh and one in my set six is not in the
set and so forth if we run
this we get uh three is in the set true
one is in the set false cuz 357 is
another one six is in the Set uh six is
not in the set so not in my set you can
also use this with the list we could
have just used 357 and it would to have
um the same response on there is three
in usually do if three is in but three
in my set is still works on a just a
regular list we'll go ahead and do a
little iteration we're going to do kind
of the dice one remember um uh 1 2 3 4 5
6 and so we're going to bring in an
iteration tool and import product this
product and I'll show you what that
means in just a second so we have our
two dice we have dice a and it's going
to be a set of values uh you can only
have one value for each one that's why
they put it in a set and if you remember
from range it is up to seven so this is
going to be 1 2 3 4 5 six it will not
include the seven and the same thing for
our dice
B and then we're going to do is we're
going to create a list with which is the
product of A and B so what's U A plus b
and if we go ahead and run this uh it'll
print that out and you'll see um in this
case when they say product because it's
an iteration
tool we're talking about creating a
tupal of the two so we've now created a
tupal of all possible outcomes of the
dice where dice a is 1 two 3 1 to six
and dice B is 1 to six and you can see
1: one 1: two 1: 3 and so forth you
remember we had a slide on this
earlier we can do in dice equals two div
dice faces one two 56 uh another way of
doing before and then we space where we
have set which is the product of
faes here just again put it through all
the different possible variables we can
have like we had before uh can go
through for outcome in event space
outcome
in versus going through and putting them
in a nice line
go do something let's go print since we
have the in printing with a comma that
just means it's not going to hit the
return going down the next line uh and
we'll go ahead and do the
link of our event space variable we're
going to want to know in a minute and of
course I get carried away with my typing
of
L and we might want to calculate
something like um what about the three
what if we want to have probility of the
three in our setup and so uh we can put
together the code for the outcome event
space of X yals outcome come if x +
y remainder three so we're going to
divide by three and look at the
remainder and it equals
zero then it's a faval outcome and we're
going to pop that outcome on the end
there and we'll turn it into a set so
the favor outcome equals a
set but just in case we'll go ahead and
do
that and if we want to print out the
outcome we can go ahead and see what
that looks like and you can see here the
these are all U multiples of three uh 1
+ 2 is 3 5 + 4 is 9 which divided by 3
is three and so
forth and just like we looked up the
length uh of the one before let's go
ahead and print the
length of our F outcome so we can see
what that looks
like there we
go and of course I did forget to add the
print in the mid because We're looping
through PR in there and if I run this
you can
see we end up with 12 so we have 36 to
we have 12 that are multiple that add up
to a multiple of three and we can easily
comp the probability of this our faval
outcome over the length of the event
space with a 3333
chance the probability of getting the
sum which is a mul is
3333 and go run it you see we just have
a huge amount of choices go on down here
and we look at
the
7,776 choices that's a lot of
choices uh what is the S where the sum
is a multiple of five but not a multiple
of three we can go through all these
different options and then you can see
here D1 D2 d35 outcome and if and the
division by five does not have a
remainder of zero but the remainder also
of a division by three is not equal to
Zer so the multiple of five is equal to
zero but the multiple of three is not we
can just Ain that on here and then we
can look at that uh favorable outcome
we'll go ahead and set that and we'll
just take a look at this what's our
length of our favorable
outcome
e
e e
it's always good to see what we're
working with and so we have 904 out of
770 six and then of course we can just
do a simple division to get the
probability on here what's the
probability that we're going to roll a
multiple of five when you add them
together but not positive true positive
um and then we have false positive you
think of this as your predicted model
what does that mean that means if you
divided your data and
you third to see how well it comes out
how many times was it uh true positive
versus uh false positive it gave a false
positive response and you can imagine in
medical uh situations you might adjust
your model accordingly so you don't have
a false positive say better a false
negative and they go back and get
retested than to have 30% false positive
where then the test is pretty much
invalid so in a used case uh like cancer
prediction let's consider an example
where a cancer prediction model is put
to the test for its accuracy and
precision actually result of a person's
medical
report and so you can see here here's
our actual predicted whether cancer not
you know cancer you don't want to have a
false positive I mean a false negative
in other words you don't want to have it
tell you that you don't have cancer when
you do so that would be something you'd
really be looking for in this particular
domain you don't want a false
negative uh and this is again you know
you've created a model you have hundreds
of people or thousands of pieces of data
that come in there's a real famous case
study where they have the imagery and
all the measurements they take and
there's about 36 different measurements
they take and then if you run the a
basic model you want to know just how
accurate is how many um negative results
do you have that are either telling
people they have cancer that don't or
telling people that don't have cancer
that they do and then we can take these
numbers
and e
we can feed them into our accuracy our
precision and our
recall uh so accuracy precision and
recall accuracy metric to measure how
accurately the result results are
predicted and this is your um total um
true where you got the right results you
add them together the true positive the
true negative over all the results so
what percentage positive plus the false
negative on there and we'll want to go
ahead and do a demo on the naive Bas
classifier before I get too far into uh
naive Bas class going to pull it from SK
learn or the pyit um let's go ahead kind
of an interesting
classif I'll just zoom up in here so you
can see some of titles uh everything the
nearest neighbor linear um but we're
going to be focusing on the naive base
over here and this is a sample dat set
that they put together these naive Bas
remember is set up as probably most
simplified calc and so with true false
and stuff that between the features
where the features assume to have some
then we can go ahead and use that for
the prediction and so that's what we're
using as an Bas classifier versus many
of the other classifiers that are out
there for this we're going to use uh The
Social Network ads it's a little data
set on here and let me go and just open
that up the file uh here we go it has
user ID gender age estimated salary uh
purchased and so we you see the user ID
mail 19 uh estimated salary
19,000 and purchase zero uh so it's
either going to make a purchase or not
so look at that last one 01 we should be
thinking of binomials we should be
thinking of uh simple naive based
classifier kind of
setup so if we close this out we're
going to go ahead and import our numpy
as
NP we're going nice to have a a good
visual of our data so we'll put in our
map plot Library here's our pandas our
data
frame uh and then we're going to go
ahead and import the data set and the
data set's going to be we're going to
read it from The Social Network ads. CSV
then we're going to print the head just
so you can see it again uh even though I
showed you it in the file and x equals
the data set ication uh two three values
and Y is going to be the four uh column
four let me just run this it's a little
easier to go over that um you can see
right here we're going to be looking at
uh 012 is age and estimated salary so 2
three and that's what iocation just
means um that we're looking at the
number versus a regular location uh
regular location you'd actually say age
and estimated salary
and then column four is did they make a
purchase they purchased something uh so
those are the three columns we're going
to be looking at when we do this and
we've G ahead and imported these
and
e e
imported the data so now our data set is
all set with this information in
it and we'll need to go ahead and split
the data up uh so we need our from the
SK learn model selection we can import
train test split uh this does a nice job
we can set the random state so it
randomly picks the data and we're just
going to take uh 25% of it it's going to
go into the test our X test and our our
y test and the 75% will go to XT train
and Y train that way once we create our
model we can then have data to see just
how accurate or how well it has
performed with our um
prediction the next step in
pre-processing our data is to go ahead
and do feature scaling now a lot of this
start to look familiar if you've done a
number of the other modules and setup
you should start noticing that we bring
in our data we take a look at what we're
working with um we go ahead and split it
up into training and testing uh in this
case we're going to go ahead and scal it
skelet means we're putting it between a
value of minus one and one uh or
someplace in that middle ground there
this way if you have any huge set you
don't have this huge um setup if we go
back up to here where salary the salary
is well there's a good chance with a lot
of the backend math that 20,000
properly and finally we get to actually
create our naive B model um and we're
going to go ahead and import the gazan
naive
Bay and the gazan is is the most basic
one that's what we're looking at now it
turns out though if you go to the SK um
learn kit uh they have a number of
different ones you can pull in there
there's a um beri I I've never used that
oneaz
uh so there's a number of different
options you can look at commonly used uh
so we're talking about the naive B us
what people about they pulling this in
and one of the nice things about the
gazan if you go to their website um to
sklearn the naive Bay gazan there's a
lot of cool features one of them is you
can do partial fit on here um that means
if you have a huge amount of data you
don't have to process it all at want you
once you can batch it into the gajian
with it as far as fitting the data and
how you
we're just doing the basics so we're
going to go ahead and create our
classifier we're going to equal the
gazan
INB and then we're going to do a fit
we're going to fit our training data and
our training Solutions so X train y
train and we'll go ahead and run this uh
it's going to tell us that it it ran the
code right
there and now we have our trained
classifier model so the next step is we
need to go ahead and run a prediction
we're going to do our predict
equ the classifier predict X test so
here we fit the data and now now we're
going to go ahead and
predict and now we get to our confusion
Matrix uh so from the SK learn Matrix
metrix you can import your confusion
Matrix just as saves you from doing all
the simple math that does it all for you
and then we'll go ahead and create our
confusion metric with the Y test and the
Y predict so we have our actual and we
have our predicted
value and you can see from here this is
the chart we look that here's predicted
so true positive false positive false
negative true
negative and if we go ahead and run this
there we have it 65
3725 and in this particular uh
prediction we had 65 uh where predicted
the truth as far as a a purchase they're
going to make a purchase and we guessed
three wrong and we had 25 we predicted
would not purchase
and at this point if you were with your
shareholders or a board meeting um you
would start to hear some snoozing they
looking at the numbers and say hey
here's my confusion M
Matrix so let's go ahead and visualize
the
results we're going to pull from the map
plot Library colors import listed color
map um and this is actually my machine's
going to throw an error because this is
being um because of the way the setup is
I have a newer version on here than when
they put together the demo and we need
our x set and our y y set which is our X
train and Y train and then we'll create
our X1 X2 and we'll put that into a grid
uh and we set our exet minimum stop and
our ex set max stop and if we come all
the way over here we're going to step
0.01 this is going
to give us a nice line uh is what that's
doing and then we're going to plot the
Contour uh plot the X limit plot the Y
limit and put the scatter plot in there
let's go ahead and run this uh to be
honest when I'm doing these graphs
there's so many different ways to do
that there's so many different ways to
put this code together to show you what
we're doing it's uh a lot easier to pull
up the graph and then go back up and
explain it so the first thing we want to
note here when we're looking at the
data is this is the training
set and so we have those who didn't make
a purchase we've drawn a nice area for
that it's defined by the naive Bay setup
and then we have those who did make a
purchase the green and you can see that
some of the green drts fall into the red
area and some of the red dots fall into
the green so even our training set isn't
going to be 100% uh we couldn't do that
and so we're looking at our different
data coming down uh we can kind of
arrange our X1 X2 so we have a nice plot
going on and then we're going to create
the the um
Contour that's that nice line that's
drawn down the middle on here with the
red green um that's where that's what
this is doing right here with the
reshape and notice that we had to uh do
the dot T if you remember from numpy um
if you did the numpy module um you end
up with pairs you know x uh X1 X2 X1 X2
next row and so forth you have to flip
it so it's all one row you have all your
x1's and all your x2s um so that's what
we're kind of looking for right here on
this
setup uh and then the scatter plot is of
course um your scattered data across
there we're just going through all the
points that puts these nice little dots
onto our setup on here and we have our
estimated salary and our H and then of
course the dots are did they make a
purchase or not and just a quick note
this is kind of funny you can see up
here where it says X set yasset equals
uh XT train y train which seems kind of
a little weird to do
um this is because this was probably
originally a definition uh so it's its
own module that could be called over and
over again and which is really a good
way to do it because the next thing
we're going to want to do is do the
exact same thing when we're going to
visualize the test set results uh that
way we can see what happened with our
test group our
25% and you can see down here we have um
the test Set uh and it if you look at
the two graphs next to each other this
one obviously has um 75% of the data so
it's going to show a lot more this is
only 25% of the data you can see that
there's a number that are kind of on the
edge as to whether they could guess by
age and income they're going to make a
purchase or not uh but that said it
still is pretty clear it's pretty good
as far as how much the estimate is and
how good it
does now graphs are really effective for
showing people what's going on but you
also need to have the numbers and so
we're going to do from SK learn we're
going to import metrics and then we're
going to print our metrics
classification Port from the Y test and
the Y
predict and you can see here we have
Precision uh Precision of zeros is 90
there's our recall
96 we have an F1 score and a support and
we have our Precision the recall on
getting it right uh and then we can do
our accuracy the macro average and the
weighted average uh so you can see that
it pulls in pretty good as far as um how
accurate it is you could say it's going
to be about 90% is going to guess
correctly um that that they're not going
to purchase and we had an 89% chance
that they are going to purchase um and
then the other numbers as you get down
have a little bit different meaning but
it's pretty straightforward on here
here's our accuracy and here's our micro
average and the weighted average and
everything else you might need and if
you forgot the exact definition of
accuracy see it is the true positive
true negative over all of the different
setups Precision is your true positive
over all positives true and false and
recall is a true positive over true
positive plus false negative and we can
just real quick flip back there so you
can see those numbers on here uh here's
our Precision here's our
recall and here's our accuracy on this
multiple linear regression let's take a
brief look at what happens when you have
multiple inputs so in multiple linear
regression we have uh well we'll start
with the simple linear regression where
we had y = m + x + C and we're trying to
find the value of y now with multiple
linear regression we have multiple
variables coming in so instead of having
just X we have X1 X2 X3 and instead of
having just one slope each variable has
its own slope attached to it as you can
see here we have M1 M2 m three and we
still just have the single coefficient
so when you're dealing with multiple
linear regression you basically take
your single linear regression and you
spread it out so you have y = M1 * X1 +
M2 * X2 so on all the way to m to the n
x to the nth and then you add your
coefficient on there implementation of
linear regression now we get into my
favorite part let's understand how
multiple linear regression works by
implementing it in Python if you
remember before before we were looking
at a company and just based on its R&D
trying to figure out its profit we're
going to start looking at the
expenditure of the company we're going
to go back to that we're going to
predict this profit but instead of
predicting it just on the R&D we're
going to look at other factors like
Administration cost marketing costs and
so on and from there we're going to see
if we can figure out what the profit of
that company is going to be to start our
coding we're going to begin by importing
some basic libraries and we're going to
be looking through the data before we do
any kind of linear regression we're
going to take a look at the data see
what we're playing with then we'll go
ahead and format the data to the format
we need to be able to run it in the
linear regression model and then from
there we'll go ahead and solve it and
just see how valid our solution is so
let's start with importing the basic
libraries now I'm going to be doing this
in Anaconda Jupiter notebook a very
popular IDE I enjoy it because it's such
a visual to look at and so easy to use
um just any ID for python will work just
fine for this so break out your favorite
python IDE so here we are in our Jupiter
notebook let me go ahead and paste our
first piece of code in there and let's
walk through what libraries we're
importing first we're going
to these are very common tools most your
line regression the whichs for number
python it does such a wonderful job
importing data setting into a DAT frame
so use P when we can and I'll show you
what that looks like the other three
lines are R to this and take a look so
we're going to import matplot library.
pyplot as p
M plot Library so you have to always
import matplot library and then caborn
sits on top of it and we'll take a look
at what that looks like you could use
any of your own plotting libraries you
want there's all kinds of ways to look
at the data these are just very common
ones and the caborn is so easy to use it
just looks beautiful is the Amber signed
map plot Library inline that is only
because I'm doing inline IDE my
interface in the Anaconda Jupiter
notebook requires I put that in there or
you're not going to see the graph when
it comes up let's go ahead and run this
it's not going to be that interesting so
we're just setting up variables in fact
it's not going to do anything that we
can see but it is importing these
different libraries and setup the next
step is load the data set and extract
independent and dependent variables now
here in the slide you'll see companies
equals pd. read CSV and it has a long
line there with the file at the end
1,000 companies. CSV you're going to
have to change this to fit
whatever e
setup you have and the file itself you
can request just go down to the
commentary below this video and put a
note in there and simply learn we'll try
to get in contact with you and Supply
you with that file so you can try this
coding yourself so we're going to add
this code in here and we're going to see
that I have companies equals pd. reader
CSV and I've changed this path to match
my computer c/s simplylearn
1000 companies. CSV and then below there
we're going to set the x equals to
companies under the I location and
because this is companies is a PD data
set I can use this nice notation that
says take every row that's what the
colon the first colon is comma except
for the last column that's what the
second part is where we have a colon
minus one and we want the values set
into there so X is no longer a data set
a pandas data set but we can easily
extract the data from our pandis data
set with this notation and then y we're
going to set equal to the last row well
the question is going to be what are we
actually looking at so let's go ahead
and take a look at that and we're going
to look at the companies. head which
lists the first five rows of data and
I'll open up the file in just a second
so you can see where that's coming from
but let's look at the data in here as
far as the way the panda sees it when I
hit run you'll see it breaks it out into
a nice setup this is what pandas one of
the things pandas is really good about
is it looks just like an Excel
spreadsheet you have your rows and
remember when we're programming we
always start with zero we don't start
with one so it shows the first five rows
0 1 2 3 4 and then it shows your
different columns R&D spend
Administration marketing spend State
profit it even notes that the top are
column names it was never told that but
pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
open it up as a text editor and you can
see at the top we have R&D spin comma
Administration comma marketing spin
comma State comma profit carriage return
I don't know about you but I go crazy
trying to read files like this that's
why we use the pandas you could also
open this up in an Excel and it would
separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
$165,300
compared to the administration cost of
$1
13689 780 so on so on helps to create
the profit of
$2,261 83 C that makes no sense to me
whatsoever no pun intended so let's flip
back here and take a look at our next
set of code where we're going to graph
it so we can get a better understanding
of our data and what it means so at this
point we're going to use a single line
of code to get a lot of information so
we can see where we're going with this
let's go ahead and paste that into our
notebook and see what we got going and
so we have the visualization and again
we're using SNS which is pandas as you
can see we imported the map plot
library. pyplot as PLT which then the
caborn uses and we imported the caborn
as SNS and then that final line of code
helps us show this in our um inline
coding without this it wouldn't display
and you could display it to a F and
other means and that's the matap plot
library in line with the Amber sign at
the beginning so here we come down to
the single line of code caborn is great
cuz it actually recognizes the panda
data frame so I can just take the
companies. core for coordinates and I
can put that right into the cbor and
when we run this we get this beautiful
plot and let's just take a look at what
this plot means if you look at this plot
on mine the colors are probably a little
bit more purplish and blue than the
original one uh we have the columns and
the row we have R and D spending we have
Administration we have marketing
spending and profit and if you cross
index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so R&D spending is going
to be the same as R&D spending and the
same thing with Administration Coster
right down the middle you get this dark
row or dark um diagonal row that shows
that this is the highest corresponding
data that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with R&D spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so now that we
have a nice look at the data let's go
ahead and dig in and create some actual
useful l regression model so that we can
predict values and have a better profit
now that we've taken a look at the
visualization of this data we're going
to move on to the next step instead of
just having a pretty picture we need to
generate some hard data some hard values
so let's see what that looks like we're
going to set up our linear regression
model in two steps the first one is we
need to prepare some of our data so it
fits correctly and let's go ahead and
paste this code into our Jupiter
notebook and what we're bringing in is
we're going to bring in the SK learn
pre-processing where we're going to
import the label encoder and the one hot
encoder to use the label encoder we're
going to create a variable called label
encoder and set it equal to capital L
label capital E encoder this creates a
class that we can reuse for transferring
the labels back and forth now about now
you should ask what labels are we
talking about let's go take a look at
the data we processed before and see
what I'm talking about here if you
remember when we did the companies. head
and we printed the top five rows of data
we have our columns going across we have
column zero which is R&D spending column
one which is Administration column two
which is marketing spending and column
three is State and you'll see under
State we have New York California
Florida now to do a linear regression
model it doesn't know how to process New
York it knows how to process a number so
the first thing we're going to do is
we're going to change that New York
California and Florida and we're going
to change those to numbers that's what
this line of code does here x equals and
then it has the colon comma 3 in
Brackets the first part the colon comma
means that we're going to look at all
the different rows so we're going to
keep them all together but the only row
we're going to edit is the third row and
in there we're going to take the label
coder and we're going to fit and
transform the X also the third row so
we're going to take that third row we're
going to set it equal to a
transformation and that transformation
basically tells it that instead of
having a uh New York it has a zero or
one or a two and then finally we need do
a one hot encoder which equals one hot
encoder categorical features equals
three and then we take the X and we go
ahead and do that equal to one hot
encoder fit transform X to array this
final transformation preps our data
Force so it's completely set the way we
need it as just a row of numbers even
though it's not in here let's go ahead
and print X and just take a look at what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just do row zero you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers next on setting up our
data we have avoiding dummy variable
trap this is very important why because
the computer has automatically
transformed our header into the setup
and it's automatically transformed all
these different variables so when we did
the encoder the encoder created two
columns and what we need to do is just
have the one because it has both the
variable and the name that's what this
piece of code does here let's go ahead
and paste this in here and we have xal X
colon comma 1 colon all this is doing is
removing that one extra column we put in
there when we did our one hot encoder
and our label encoding let's go ahead
and run that and now we get to create
our linear regression model and let's
see what that looks like here and we're
going to do that in two steps the first
step is going to be in splitting the
data now whenever we create a uh
predictive model of data we always want
to split it up so we have a training set
and we have a testing set that's very
important otherwise we'd be very
unethical without testing it to see how
good our fit is and then we'll go ahead
and create our multiple linear
regression model and train it and set it
up let's go ahead and paste this next
piece of code in here and I'll go ahead
and shrink it down a size or two so it
all fits on one line so from the sklearn
module selection we're going to import
train test split and you'll see that
we've created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually referenes when
we're doing different uh models you
usually see that a capital x and you see
the train and the test and the lowercase
Y what this is is X is our data going in
that's our R&D spin our Administration
our marketing and then Y which we're
training is the answer that's the profit
because we want to know the profit of an
unknown entity that's what we're going
to shoot for in this tutorial the next
part train test split we take X and we
take y we've already created those X has
the columns with the data in it and Y
has a column with profit in it and then
we're going to set the test size equals
0.2 that basically means 20% So 20% of
the rows are going to be tested we're
going to put them off to the side so
since we're using a th000 lines of data
that means that 200 of those lines we're
going to hold off to the side to test
for later and then the random State
equals zero we're going to randomize
which ones it picks to hold off to the
side we'll go ahead and run this it's
not overly exciting it's setting up our
variables but the next step is the next
step we actually create our linear
regression model now that we got to the
linear regression model we get that next
piece of the puzzle let's go ahead and
put that code in there and walk through
it so here we go we're going to paste it
in there and let's go ahead and since
this is a shorter line of code let's
zoom up there so we can get a good luck
and we have from the SK learn. linear
model we're going to import linear
regression now I don't know if you
recall from earlier when we were doing
all the math let's go ahead and flip
back there and take a look at that do
you remember this or we had this long
formula on the bottom and we were doing
all this suiz and then we also looked at
setting it up with the different lines
and then we also looked all the way down
to multiple linear regression where
we're adding all those formulas together
all of that is wrapped up in this one
section so what's going on here is I'm
going to create a variable called
regressor and the regressor equals the
linear regression that's a linear
regression model that has all that math
built in so we don't have to have it all
memorized or have to compute it
individually and then we do the
regressor do fet in this case we do X
train and Y train because we're using
the training data X being the data in
and Y being profit what we're looking at
and this does all that math for us so
within one click and one line we've
created the whole linear regression
model and we fit the data to the linear
regression model and you can see that
when I run the regressor it gives an
output linear regression it says copy x
equals True Fit intercept equals true in
jobs equal one normalize equals false
it's just giving you some general
information on what's going on with that
regressor model now that we've created
our linear regression model let's go
ahead and use it and if you remember we
kept a bunch of data aside so we're
going to do a y predict variable and
we're going to put in the X test and
let's see what that looks like scroll up
a little bit paste that in here
predicting the test set results so here
we have y predict equals regressor do
predict X test going in and this gives
us y predict now because I'm in Jupiter
in line I can just put the variable up
there and when I hit the Run button
it'll print that array out I could have
just as easily done print y predict so
if you're in a different IDE that's not
an inline set up like the Jupiter
notebook you can do it this way print y
predict and you'll see that for the 200
different test variables we kept off to
the side it's going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a quick flash what's going on
behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficients for us and we can simply
just print regressor do coefficient
uncore when I run this you'll see our
coefficient here and if we can do the
regressor coefficient we can also do The
regressor Intercept let's run that and
take a look at that this all came from
the multiple regression model and we'll
flip over so you can remember where this
is going into and where it's coming from
you can see the formula down here where
y = M1 * x1+ M2 * X2 and so on and so on
plus C the coefficient ient so these
variables fit right into this formula y
= slope 1 * column 1 variable plus slope
2 * column 2 variable all the way to the
m into the n and x to the N plus C the
coefficient or in this case you have Min
-
8.89 to the power of 2 etc etc times the
First Column and the second column and
the third column and then our intercept
is the minus
10309 Point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the R squ value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from SK K learnmetrics we're going
to import R2 score that's the R squar
value we're looking at the error so in
the R2 score we take our y test versus
our y predict y test is the actual
values we're testing that was the one
that was given to us that we know are
true the Y predict of those 200 values
is what we think it was true and when we
go ahead and run this we see we get a
9352 that's the R2 score now it's not
exactly a straight perent percentage so
it's not saying it's
93 correct but you do want that in the
upper 90s oh and higher shows that this
is a very valid prediction based on the
R2 score and if r squ value of 91 or 92
as we got on our model remember it does
have a random generation involved this
proves the model is a good model which
means success yay we successfully
trained our model with certain
predictors and estimated the profit of
the companies using linear regression
let's take an example and see how we can
apply logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into Jupiter notebook and um show
the code but before that let me take you
through a couple of slides to explain
what we are trying to do so let's say
you have an 8x8 image and the the image
has a number 1 2 3 4 and you need to
train your model to predict what this
number is so how do we do this so the
first thing is obviously in any machine
learning process you train your model so
in this case we are using logistic
regression so and then we provide a
training set to train the model and then
we test how accurate our model is with
the test data which means that like any
machine learning process we split our
set and then
then so that is typical methology of
training testing and mod so let's take a
look at the code and uh see what we are
doing so I'll not go line by line but
just take you through some of the blocks
so first thing we do is import all the
libraries and then we basically take a
look at the images and
see mention earlier and we can
and use heat map for visualize this and
I show you in the code what exactly is
can use for finding the accuracy in our
example we get
accuracy so what is confus Matrix this
is an example of confus and for
identifying the accuracy of
classification mod so the most important
part in a confusion Matrix is that first
of all this as you can see this is a
matrix and the size of the Matrix
depends on how many outputs we are
expecting right
should have maximum numbers and other
should have very few numbers so here
that's what is happening so there's a
two here there are there's a one here
but most of them are along the diagonal
this what does mean this means that the
number that has been fed is zero and the
number that has been detected is also
zero so the predicted value and the
actual value are the same so along the
diagonals that is true which means that
let's let's take this diagonal right if
the maximum number is here that me that
is4 which
that4 four and out of which 34 have been
predicted correctly as number four and
one has been predicted as number eight
and another one has been predicted as
number nine so these are two
misclassifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100% accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like a
100% accurate model okay so that's uh
gist of how to use this Matrix how to
use this uh confusion Matrix I know the
name uh is a little funny sounding
confusion Matrix but actually it is not
very confusing it's very straightforward
so you are just plotting what has been
predicted and what is the labeled
information or what is the actual data
that's also known as the ground truth
sometimes okay these are some fancy
terms that matal so predicted label and
the actual label that's all it is okay
yeah so we are showing a little bit more
information here so 38 have been
predicted and here you will see that all
of them have been predicted correctly
there have been 38 zeros and the
predicted value and actual 37 + 5 yeah
42 have been fed the images 42 images
are of Digit three and uh the accuracy
is only 37 of them have been accurately
predicted three of them have been
predicted as number seven and two of
them have been predicted as number eight
and so on and so forth okay so this is
the code in in Jupiter notebook for
logistic regression in this particular
demo what we are going to do is train
our model to recognize digits which are
the images which have digits from let's
say 0 to 5 or 0 to 9 and um and then we
will see how well it is trained and
whether it is able to predict these
numbers correctly or not so let's get
started so the first part is as usual we
are importing some libraries that are
required and uh then the last line in
this block is to load the digits so
let's go ahead and run this code then
here we will visualize the shape of
these uh digits so we can see here okay
we take a look this is how the shape is
7097 by 64 these are like 8 by8 images
so that's that's what is reflected in
this shape now from here onwards we are
basically once again importing some of
the libraries that are required like
numai and map plot and we will take a
look at uh some of the sample images
that that we have un loaded so this one
for example creates a figure uh and then
we go ahead and take a few sample images
to see how they look so let
me
e e
run this code and so that it becomes
easy to understand so these are about
five images sample images that we are
looking at 0 1 2 3 4 so this is how the
images this is how the data is okay and
uh based on this we will actually train
our logistic regression model and then
we will test it and see how well it is
able to recognize so the way it works is
the pixel information so as you can see
here this is an 8 by 8 pixel kind of a
image and uh the each pixel whether it
is activated or not activated that is
the information available for each pixel
now based on the pattern of this
activation and non-activation of the
various pixels this will be identified
as a zero for example right similarly as
you can see so overall each of these
numbers actually has a different pattern
of the pixel activation and that's
pretty much that our model needs to
learn uh for which number what is the
pattern of the activation of the pixels
right so that is what we are going to
train our model okay so the first thing
we need to do is to split our data into
training and test data set right so
whenever we perform any training we
split the data into training and test so
that the training data set is used to
train the system so we pass this
probably multiple times times uh and
then we test it with the test data set
and the split is usually in the form of
there and there are various ways in
which you can split this data it is up
to the individual preferences in our
case here we are splitting in the form
of 23 and 77 so when we say test size as
20 23 that means 23% of the entire data
is used for testing and the remaining
77% is used for training so there is a
readily available function which is uh
called train test split so we don't have
to write any special code for the
splitting it will automatically split
the data based on the proportion that we
give here which is test size so we just
give the test size automatically
training size will be determined and uh
we pass the data that we want to split
and the the results will be stored in
xcore train and Yore train for the
training data set and what is xcore
train this are these are the features
right which is like the independent
variable and Yore train is the label
right so in this case what happens is we
have the input value which is or the
features value which is in xcore train
and since this is labeled data for each
of them each of the observations we
already have the label information
saying whether this digit is a zero or a
one or a two so that this this is what
will be used for comparison to find out
whether the the system is able to
recognize it correctly or there is an
error for each observation it will
compare with this right so this is the
label so the same way xcore train Yore
train is for the training data set xcore
test Yore test is for the test data set
okay so let me go ahead and execute this
code as well and then we can go and
check quickly what is the how many
entries are there and in each of this so
xcore train the shape is
1383 by 64 and Yore train has 1383
because there is uh nothing like the
second part is not required here and
then xcore test shape we see is 414 so
actually there are 414 observations in
test and 1383 observations in train so
that's basically what these four lines
of code are are saying okay then we
import the logistic regression library
and uh which is a part of psyit learn so
we we don't have to implement the
logistic regression process itself we
just call these the function and uh let
me go ahead and execute that so that uh
we have the logistic regression Library
imported now we create an instance of
logistic regression right so logistic RR
is a is an instance of logistic
regression and then we use the that for
training our model so let me first
execute this code so these two lines so
the first line basically creates an
instance of logistic regression model
and then the second line where is where
we are passing our data the training
data set right this is our the the
predictors and uh this is our Target we
are passing this data set to train our
model all right so once we do this in
this case the data is not large but by
and large as the training is what takes
usually a lot of time so we spend in
machine learning activities in machine
learning projects we spend a lot of time
for the training part of it okay so here
the data set is relatively small so it
was pretty quick so all right so now our
model has been trained using the
training data set and uh we want to see
how accurate this is so what we'll do is
we will test it out in probably faces so
let me first try out how well this is
working for one image okay I will just
try it out with one image my the first
entry in my test data set and see
whether it is uh correctly predicting or
not so and in order to test it so for
training purpose we use the fit method
there is a method called fit which is
for training the model and once the
training is done if you want to test for
uh a particular value new input you use
the predict method okay so let's run the
predict method and we pass this
particular image and uh we see that the
shape is or the prediction is four so
let's try a few more let me see for the
next 10 uh seems to be fine so let me
just go ahead and test the entire data
set okay that's basically what we will
do so now we want to find out how
accurately this has um performed so we
use the score method to find what is the
percentage of accuracy and we see here
that it has performed up to 94% Accurate
okay so that's uh on this part now what
we can also do is we can um also see
this accuracy using what is known as a
confusion Matrix so let us go ahead and
try that as well uh so that we can also
visualize how well uh this model has uh
done so let me execute this piece of
code which will basically import some of
the libraries that are required and uh
um we we basically create a confusion
Matrix an instance of confusion matrix
by running confusion Matrix and passing
these uh values so we have so this
confusion underscore Matrix method takes
two parameters one is the Yore test and
the other is the prediction so what is
the Yore test these are the labeled
values which we already know for the
test data set and predictions are what
the system has predicted for the test
data set okay so this is known to us and
this is what the system has uh the model
has generated so we kind of create the
confusion Matrix and we will print it
and uh this is how the confusion Matrix
looks as the name suggests it is a
matrix and um the key point out here is
that the accuracy of the model is
determined by how many numbers are there
in the diagonal the more the number
numbers in the diagonal the better the
accuracy is okay and first of all the
total sum of all the numbers in this
whole Matrix is equal to the number of
observations in the test data set that
is the first thing right so if you add
up all these numbers that will be equal
to the number of observations in the
test data set and then out of that the
maximum number of them should be in the
diagonal that means the accuracy is
predy good if the the numbers in the
diagonal are less than in all other
places there are a lot of numbers uh
which means the accuracy is very low the
diagonal indicates a correct prediction
that this means that the actual value is
same as the predicted value here again
actual value is same as the predicted
value and so on right so the moment you
see a number here that means the actual
value is something and the predicted
value is something else right similarly
here the actual value is something and
the predicted value is something else so
that is basically how we read the the
confusion Matrix now how do we find the
accuracy you can actually add up the
total values in the diagonal so it's
like 38 + 44 + 43 and so on and divide
that by the total number of test
observations that will give you the
percentage accuracy using a confusion
Matrix now let us visualize this
confusion Matrix in a slightly more
sophisticated way uh using a heat map so
we will create a heat map with some
we'll add some colors as well it's uh
it's like a more visually visually more
appealing so that's the whole idea so if
we let me run this piece of code and
this is how the heat map uh looks uh and
as you can see here the diagonals again
are are all the values are here most of
the values so which means reasonably
this seems to be reasonably accurate and
yeah basically the accuracy score is 94%
this is calculated as I mentioned by
adding all all these numbers divided by
the total test values or the total
number of observations in test data set
okay so this is the confusion Matrix for
logistic
regression all right so now that we have
seen the confusion Matrix let's take a
quick sample and see how well uh the
system has classified and we will take
uh a few examples of the data so if we
see here we we picked up randomly a few
of them so this is uh number four which
is the actual value and also the
predicted value both are four this is an
image of zero so the predicted value is
also zero actual value is of course zero
then this is the image of nine so this
is also been predicted correctly N9 and
actual value is nine and this is a image
of one and again this has been predicted
correctly as like the actual value okay
so this was a quick demo of logic istic
regression how to use logistic
regression to identify images need for
confusion matrixes classification models
have multiple output categories most
error measures will tell us the total
error in our model but we cannot use it
to find out individual instances of
errors in our models so you have your
input coming in you have your classifier
uh it measures the error and it says oh
53 of these are correct but we don't
know which 5 3% are correct is it 53%
correct uh on guessing on the spam is it
23% guessing on spam and 27% guessing on
what's not spam this is where the
confusion Matrix comes in so during the
classification we also have to overcome
the limitations of accuracy accuracy can
be misleading for classification
problems if there is a significant class
imbalance a model might predict the
majority class for all all cases and
have a high accuracy score and so you
can see here we have our emao coming in
and there's two spams the classifier
comes in and goes hey it only catches
one of those spams and it misclassifies
one that's not spam so our model
predicted eight out of 10 instuments and
will have an accuracy of 80% but is it
classifying
correctly a confusion Matrix represents
a table layout of the different outcomes
of prediction and results of a
classification problem and helps
visualize its
outcomes and so you see here we have our
uh simple chart predicted and actual the
confusion Matrix helps us identify the
correct predictions of a model for
different individual classes as well as
the errors so you'll see here that the
values predicted by our classifier are
along the rows this is what we're going
to guess it is or our our model is
guessing what this is based on its
training so we've already trained the
model to um guess whether it's spam or
not spam or whatever it is you're
working on and then the actual values of
our data set are along the
columns so this is the actual value it's
supposed to be people who can speak
English will be classified as positives
so because they have a remember 01 do
you speak English yes no and you could
extend this that they might have do you
speak uh French do you speak whatever
languages and so you might have a whole
lot of classifiers that you would look
at each one of these people who cannot
speak English will be classified as
negatives so they'll be a zero so you
know zero ones the number of times are
actual positive values are equal to
predicted positive values gives us true
positive TP the number of times our
actual negative values are equal to
predictive negative values gives us true
negative
TN the number of times our model wrongly
predicts negative values as
positives gives us a false
positive and FP and you'll see when
you're working with these a lot you know
memorizing that is false positive you
can easily figure out what that is and
pretty you're just looking at the FP or
the TP depending on what you're working
on and the number times our model
wrongly predicts positive values as
negatives gives us a false negative
FP now I'm going to do a quick step out
here let's say you're working in the
medical and we're talking about
cancer uh do you really want a bunch of
false negatives you want zero under
false negative uh so when we look at
this confusion Matrix if you have 5%
false positives and 5% false negatives
it'd be much better to even have 20%
false positives because they go in and
test it and zero false negatives the
same might be true if you're working on
uh uh say uh a car driving is this a
safe place for the car to go so what is
KF NN what is the K&N algorithm K
nearest neighbors is what that stands
for it's one of the simplest supervised
machine learning algorithms mostly used
for classification so we want to know is
this a dog or it's not a dog is it a cat
or not a cat it classifies a data point
based on how its neighbors are
classified KNN stores all available
cases and classifies new cases based on
a similarity measure and here we gone
from cats and dogs right into wine and
another favorite of mine K&N stores all
available cases and classifies new cases
based on a similarity measure and here
you see we have a measurement of sulfur
dioxide versus the chloride level and
then the different wines they've tested
and where they fall on that graph based
on how much sulfur dioxide and how much
chloride K and KN andn is a perimeter
that refers to the number of nearest
neighbors to include in the majority of
the voting process and so if we add a
new glass of wine there red or white we
want to know what the neighbors are in
this case we're going to put k 5 we'll
talk about K in just a minute a data
point is classified by the majority of
votes from its five nearest neighbors
here the unknown point would be
classified as red since four out of five
neighbors are red so how do we choose K
how do we know k equals 5 I mean that's
was the value we put in there so we're
going to talk about it how do we choose
the factor k k andn algorithm is based
on feature similarity choosing the right
value of K is a process called parameter
tuning and is important for better
accuracy so at k equal 3 we can classify
we have a question mark in the middle as
either a as a square or not is it a
square or is it in this case a triangle
and so if we set k equals to three we're
going to look at the three nearest
neighbors we're going to say this is a
square and if we put k equals to seven
we classify as a triangle depending on
what the other data is around and you
can see as the K changes depending on
where that point is that drastically
changes your answer and uh we jump here
we go how do we choose the factor of K
you'll find this in all machine learning
choosing these factors that's the face
you get it's like oh my gosh did I
choose the right K did I set it right my
values in whatever machine learning tool
you're looking at so that you don't have
a huge bias in One Direction or the
other and in terms of K andn the number
of K if you choose it too low the bias
is based on it's just too noisy it's
it's right next to a couple things and
it's going to pick those things and you
might get a skewed answer and if your K
is too big then it's going to take
forever to process so you're going to
run into processing issues and resource
issues so what we do the most common use
and there's other options for choosing K
is to use the square root of n so N is a
total number of values you have you take
the square root of it and most cases you
also if it's an even number so if you're
using uh like in this case squares and
triangles if it's even you want to make
your K value odd that helps it select
better so in other words you're not
going to have a balance between two
different factors that are equal so
usually take the square root of N and if
it's even you add one to it or subtract
one from it and that's where you get the
K value from that is the most common use
and it's pretty solid it works very well
when do we use KNN we can use KNN when
data is labeled so you need a label on
it we know we have a group of pictures
with dogs dogs cats cats data is Noise
free and so you can see here when we
have a class and we have like
underweight 140 23 Hello Kitty normal
that's pretty confusing we have a high
variety of data coming in so it's very
noisy and that would cause an issue data
set is small so we're usually working
with smaller data sets where you might
get into gig of data if it's really
clean doesn't have a lot of noise
because KNN is a lazy learner I.E it
doesn't learn a discriminative function
from the training set so it's very lazy
so if you have very complicated data and
you have a large amount of it you're not
going to use the KNN but it's really
great to get a place to start even with
large data you can sort out a small
sample and get an idea of what that
looks like using the KNN and also just
using for smaller data sets KNN works
really good how does a KNN algorithm
work consider a data set having two
variables height in centimeters and
weight in kilograms and each point is
classified as normal or underweight so
we can see right here we have two
variables you know true false they're
either normal or they're not their
underweight on the basis of the given
data we have to classify the below set
as normal or underwe weight using KNN so
if we have new data coming in that says
57 kilg and 177 cm is that going to be
normal or underweight to find the
nearest neighbors we'll calculate the
ukian distance according to the ukan
distance formula the distance between
two points in the plane with the
coordinates XY and ab is given by
distance D equals the square root of x -
a^ 2 + y - b^ 2 and you can remember
that from the 2 of a triangle we're
Computing the third Edge since we know
the X side and the yide let's calculate
it to understand clearly so we have our
unknown point and we placed it there in
red and we have our other points where
the data is scattered around the
distance D1 is a square OT of 170 - 167
2ar + 57 - 51 SAR which is about 6.7 and
distance two is about 13 and distance
three is about 13. four similarly we
will calculate the ukian distance of
unknown data point from all the points
in the data set and because we're
dealing with small amount of data that's
not that hard to do and it's actually
pretty quick for a computer and it's not
a really complicated Mass you can just
see how close is the data based on the
idian distance hence we have calculated
the ukian distance of unknown data point
from all the points as showing where X1
and y1 equal 57 and 170 whose class we
have to classify so now we're looking at
that we're saying well here here's the
ukian distance who's going to be their
closest neighbors now let's calculate
the nearest neighbor at k equals 3 and
we can see the three closest neighbors
puts them at normal and that's pretty
self-evident when you look at this graph
it's pretty easy to say okay what you
know we're just voting normal normal
normal three votes for normal this is
going to be a normal weight so majority
of neighbors are pointing towards normal
hence as per K&N algorithm the class of
57170 should be normal so recap of knnn
positive integer K is specified along
with a new sample we select the K
entries in our database which are
closest to the new sample we find the
most common classification of these
entries this is the classification we
give to the new sample so as you can see
it's pretty straightforward we're just
looking for the closest things that
match what we got so let's take a look
and see what that looks like in a use
case in Python so let's dive into the
predict diabetes use case so use case
predict diabetes the objective predict
whether a person will be diagnosed with
diabetes or not we have a data set of
768 people who were or were not
diagnosed with diabetes and let's go
ahead and open that file and just take a
look at that data and this is in a
simple spreadsheet format the data
itself is comma separated very common
set of data and it's also a very common
way to get the data and you can see here
we have columns a through I that's what
1 2 3 4 5 6 78 um eight columns with a
particular tribute and then the ninth
column which is the outcome is whether
they have diabetes as a data scientist
the first thing you should be looking at
is insulin well you know if someone has
insulin they have diabetes because
that's why they're taking it and that
could cause issue on some of the machine
learning packages but for very basic
setup this works fine for uh doing the
KNN and the next thing you notice is it
it didn't take very much to open it up
um I can scroll down to the bottom of
the data there's
768 it's pretty much a small data set
you know at 769 I can e fit this into my
ram on my computer I can look at it I
can manipulate it and it's not going to
really tax just a regular desktop
computer you don't even need an
Enterprise version to run a lot of this
so let's start with importing all the
tools we need and before that of course
we need to discuss what IDE I'm using
certainly you can use any particular
editor for python but I like to use for
doing uh very basic visual stuff the
Anaconda which is great for doing demos
with the Jupiter notebook and just a
quick view of the Anaconda Navigator
which is the new release out there which
is really nice you can see under home I
can choose my application we're going to
be using python 36 I have a couple
different uh versions on this particular
machine if I go under environments I can
create a unique environment for each one
which is nice and there's even a little
button there where I can install
different packages so if I click on that
button and open the terminal I can then
use a simple pip install to install
different packages I'm working with
let's go ahead and go back under home
and we're going to launch our note book
and I've already you know kind of like
uh the old cooking shows I've already
prepared a lot of my stuff so we don't
have to wait for it to launch because it
takes a few minutes for it to open up a
browser window in this case I'm it's
going to open up Chrome because that's
my default that I use and since the
script is pre-done you'll see I have a
number of windows open up at the top the
one we're working in and uh since we're
working on the KNN predict whether a
person will have diabetes or not let's
go and put that title in there and I'm
also going to go up here and click on
sell actually we going to go ahead and
first insert a cell below and then I'm
going to go back up to the top cell and
I'm going to change the cell type to
markdown that means this is not going to
run as python it's a markdown language
so if I run this first one it comes up
in nice big letters which is kind of
nice remind us what we're working on and
by now you should be familiar with doing
all of our Imports we're going to import
the pandas as PD import numpy as inp
pandas is the pandas data frame and
numpy is a number array very powerful
tools to use in here so we have our
Imports so we've brought in our pandas
our numpy our two general python tools
and then you can see over here we have
our train test split by now youed should
be familiar with splitting the data we
want to split part of it for training
our thing and then training our
particular model and then we want to go
ahead and test the remaining data just
see how good it is pre-processing a
standard scaler pre-processor so we
don't have a bias of really large
numbers remember in the data we had like
number pregnancies isn't going to get
very large where the amount of insulin
they take can get up to 256 so 256
versus 6 that will skew results so we
want to go ahead and change that so
they're all uniform between minus one
and one and then the actual tool this is
the K neighbors classifier we're going
to use and finally the last three are
three tools to test all about testing
our model how good is it me just put
down test on there and we have our
confusion Matrix our F1 score and our
accuracy so we have our two General
python modules we're importing and then
we have our six modules specific from
the sklearn setup and then we do need to
go ahead and run this so these are
actually imported there we go and then
move on to the next step and so in this
set we're going to go ahead and load the
database we're going to use pandas
remember pandas is PD and we'll take a
look at the data in Python we looked at
it in a simple spread sheet but usually
I like to also pull it up so that we can
see what we're doing so here's our data
set equals pd. read CSV that's a pandas
command and the diabetes folder I just
put in the same folder where my IPython
script is if you put in a different
folder youd need the full length on
there we can also do a quick length of
uh the data set that is a simple python
command Len for length we might even
let's go ahead and print that we'll go
print and if you do it on its own line
link. data set in the jupyter notebook
it'll automatically print it but when
you're in most of your different setups
you want to do the print in front of
there and then we want to take a look at
the actual data set and since we're in
pandas we can simply do data set head
and again let's go ahead and add the
print in there if you put a bunch of
these in a row you know the data set one
head data set two head it only prints
out the last one so I usually always
like to keep the print statement in
there but because most projects only use
one data frame Panda data frame doing it
this way doesn't really matter the other
way works just fine and you can see when
we hit the Run button we have the 768
lines which we knew and we have our
pregnancies it's automatically given a
label on the left remember the head only
shows a first five lines so we have zero
through four and just a quick look at
the data you can see it matches what we
looked at before we have pregnancy
glucose blood pressure all the way to
age and then the outcome on the end and
we're going to do a couple things in
this next step we're going to create a
list of columns where we can't have zero
there's no such thing as zero skin
thickness or zero blood pressure zero
glucose uh any of those you'd be dead so
not a really good Factor if they don't
if they have a zero in there because
they didn't have the data and we'll take
a look at that because we're going to
start replacing that information with a
couple of different things and let's see
what that looks like so first we create
a nice list as you can see we have the
values we talked about glucose blood
pressure skin thickness uh and this is a
nice way when you're working with
columns is to list the columns you need
to do some kind of transformation on uh
very common thing to do and then for
this particular setup we certainly could
use the there's some Panda tools that
will do a lot of this where we can
replace the na but we're going to go
ahead and do it as a data set column
equals data set column. replace this is
this is still pandas you can do a direct
there's also one that that you look for
your nan a lot of different options in
here but the N nump Nan is what that
stands for is is non doesn't exist so
the first thing we're doing here is
we're replacing the zero with a numpy
none there's no data there that's what
that says that's what this is saying
right here so put the zero in and we're
going to replace zeros with no data so
if it's a zero that means the person's
well hopefully not dead hopefully they
just didn't get the data the next thing
we want to do is we're going to create
the mean which is the in integer from
the data set from the column do mean
where we skip Nas we can do that that is
a panda's command there the skip na so
we're going to figure out the mean of
that data set and then we're going to
take that data set column and we're
going to replace all the
npnn with the means why did we do that
and we could have actually just uh taken
this step and gone right down here and
just replaced zero and Skip anything
where except you could actually there's
a way to skip zeros and then just
replace all the zeros but in this case
we want to go ahead and do it this way
so you can see that we're switching this
to a non-existent value then we're going
to create the mean well this is the
average person so if we don't know what
it is if they did not get the the data
and the data is missing one of the
tricks is you replace it with the
average what is the most common data for
that this way you can still use the rest
of those values to do your computation
and it kind of just brings that
particular value those missing values
out of the equation let's go ahead and
take this and we'll go ahead and run it
doesn't actually do anything so we're
still preparing our data if you want to
see what that looks like we don't have
anything in the first few lines so it's
not going to show up but we certainly
could look at a row let's do that let's
go into our data set with printed data
set and let's pick in this case let's
just do glucose and if I run this this
is going to print all the different
glucose levels going down and we
thankfully don't see anything in here
that looks like missing data at least on
the ones it shows you can see it skipped
a bunch in the middle because that's
what it does if you have too many lines
in Jupiter notebook it'll skip a few and
and go onto the next in a data set let
me go and remove this and we'll just
zero out that and of course before we do
any processing before proceeding any
further we need to split the data set
into our train and testing data that way
we have something to train it with and
something to test it on and you're going
to notice we did a little something here
with the uh Panda's database code there
we go my drawing tool we've added in
this right here off the data set and
what this says is that the first one in
pandas this is from the PD pandas it's
going to say within the data set we want
to look at the iocation and it is all
rows that's what that says so we're
going to keep all the rows but we're
only looking at 0er column 0 to 8
remember column 9 here it is right up
here we printed in in here is outcome
well that's not part of the training
data that's part of the answer yes
column nine but it's listed as eight
number eight so 0 to eight is nine
columns so uh eight is the value and
when you see it in here zero this is
actually 0 to 7even it doesn't include
the last one and then we go down here to
Y which is our answer and we want just
the last one just column 8 and you can
do it this way with this particular
notation and then if you remember we
imported the train test split that's
part of the SK learn right there and we
simply put in our X and our y we're
going to do random State equals zero you
don't have to necessarily seed it that's
a seed number I think the default is one
when you SE it I'd have to look that up
and then the test size test size is 0.2
that simply means we're going to take
20% of the data and put it aside so that
we can test it later that's all that is
and again we're going to run it not very
very exciting so far we haven't had any
print out other than to look at the data
but that is a lot of this is prepping
this data once you prep it the actual
lines of code are quick and easy and
we're almost there with the actual
writing of our KNN we need to go ahead
and do a scale the data if you remember
correctly we're fitting the data in a
standard scaler which means instead of
the data being from you know 5 to 303 in
one column and the next column is 1 to
six we're going to set that all so that
all the data is between minus1 and one
that's what that standard scaler does
keeps it standardized and we only want
to fit the scaler with the training set
but we want to make sure the testing set
is the X test going in is also
transformed so it's processing it the
same so here we go with our standard
scaler we're going to call it scor X for
the scaler and we're going to import the
standard scaler into this variable and
then our X train equals score x. fit
transform so we're creating this scaler
on the X train variable and then our X
test we're also going to transform it so
we've trained and transformed the XT
train and then the X test isn't part of
that training it isn't part of that of
training the Transformer it just gets
transformed that's all it does and again
we're going to go and run this if you
look at this we've now gone through
these steps all three of them we've
taken care of replacing our zeros for
key columns that shouldn't be zero and
we replace that with the means of those
columns that way that they fit right in
with our data models we've come down
here and we split the data so now we
have our test data and our training data
and then we've taken and we' scaled the
data so all of our data going in now no
we don't tra we don't train the Y part
the Y train and Y test that never has to
be trained it's only the data going in
that's what we want to train in there
then Define the model using K neighbors
classifier and fit the train data in the
model so we do do all that data prep and
you can see down here we're only going
to have a couple lines of code where
we're actually building our model and
training it that's one of the cool
things about Python and how far we've
come it's such an exciting time to be in
machine learning because there's so many
automated tools let's see before we do
this let's do a quick length of and
let's do y we want let's just do length
of Y and we get 768 and if we import
math we do
math. root let's do y train there we go
it's actually supposed to be X train
before we do this let's go ahead and do
import math and do math square root
length of Y test and when I run that we
get
12.49 I want to see show you where this
number comes from we're about to use 12
is an even number so if you know if
you're ever voting on things remember
the neighbors all vote don't want to
have an even number of neighbors voting
so we want to do something odd and let's
just take one away we'll make it 11 let
me delete this out of here that's one of
the reasons I love jupyter notebook cuz
you can flip around and do all kinds of
things on the fly so we'll go ahead and
put in our classifier we're creating our
classifier now and it's going to be the
K neighbors classifier n neighbors equal
11 remember we did 12 minus 1 for 11 so
we have an odd number of neighbors P
equals 2 because we're looking for is it
are they diabetic or not and we're using
the ukian metric there are other means
of measuring the distance you could do
like square square means value there's
all kinds of measure this but the ukian
is the most common one and it works
quite well it's important to evaluate
the model let's use the confusion Matrix
to do that and we're going to use the
confusion Matrix wonderful tool and then
we'll jump into the F1 score and finally
accuracy score which is probably the
most commonly used quoted number when
you go into a meeting or something like
that so let's go ahead and paste that in
there and we'll set the cm equal to
confusion Matrix y test y predict so
those are the two values we going to put
in there and let me go ahead and run
that and print it out and the way you
interpret this is you have the Y
predicted which would be your title up
here we could do uh let's just do p d
predicted across the top and actual
going down actual it's always hard to to
write in here actual that means that
this column here down the middle that's
the important column and it means that
our prediction said 94 and pred and the
actual agreed on 94 and 32 this number
here the 13 and the 15 those are what
was wrong so you could have like three
different if you're looking at this
across three different variables instead
of just two you'd end up with the third
row down here and the column going down
the middle so in the first case we have
the the and I believe the zero is a 94
people who don't have diabetes the
prediction said that 13 of those people
did have diabetes and were at high risk
and the 32 that had diabetes it had
correct but our prediction said another
15 out of that 15 it classified as
incorrect so you can see where that
classification comes in and how that
works on the confusion Matrix then we're
going to go ahead and print the F1 score
let me just run that and you see we get
a 69 in our F1 score the F1 takes into
account both sides of the balance of
false positives where if we go ahead and
just do the accuracy account and that's
what most people think of is it looks at
just how many we got right out of how
many we got wrong so a lot of people
when you're a data scientist and you're
talking to other data scientists they're
going to ask you what the F1 score the F
score is if you're talking to the
general public or the U decision makers
in the business they're going to ask
what the accuracy is and the accuracy is
always better than the F1 score but the
F1 score is more telling it lets us know
that there's more false positives than
we would like on here but 82% not too
bad for a quick flash look at people's
different statistics and running an SK
learn and running the knnn the K nearest
neighbor on it why reinforcement
learning training a machine learning
model requires a lot of data which might
not always be available to us further
the data provided might not be reliable
learning from a small subset of actions
will not help expand the vast realm of
solutions that may work for a particular
problem
and you can see here we have the robot
learning to walk um very complicated
setup when you're learning how to walk
and you'll start asking questions like
if I'm taking one step forward and left
what happens if I pick up a 50 pound
object how does that change how a robot
would walk these things are very
difficult to program because there's no
actual information on it until it's
actually tried out learning from a small
subset of actions will not help expand
the vast realm of solutions that may
work for a particular problem
and we'll see here it learned how to
walk this is going to slow the growth
that technology is capable of machines
need to learn to perform actions by
themselves and not just learn off
humans and you see the objective clima
Mountain real interesting point here is
that as human beings we can go into a
very unknown environment and we can
adjust for it and kind of explore and
play with it most the models the
non-reinforcement models in computer U
machine learning aren't able to do that
very well uh there's a couple of them
that can be used or integrated see how
it goes is what we're talking about with
reinforcement learning so what is
reinforcement learning reinforcement
learning is a subbranch of machine
learning that trains a model to return
an Optimum solution for a problem by
taking a sequence of decisions by itself
consider a robot learning to go from one
place to another the robot is given a
scenario and must arrive at a solution
by itself the robot can take different
paths to reach the
destination it will know the best path
by the time taken on each path it might
even come up with a unique solution all
by itself and that's really important is
we're looking for Unique Solutions uh we
want the best solution but you can't
find it unless you try it so we're
looking at uh our different systems our
different model we have supervised
versus unsupervised versus reinforcement
learning and with the supervised
learning that is probably the most
controlled environment uh we have a lot
of different supervised learning models
whether it's linear regression neural
networks um there's all kinds of things
in between decision trees the data
provided is labeled data with output
values specified and this is important
because when we talk about supervised
learning you already know the answer for
all this information you already know
the picture has a motorcycle in it so
you're supervised learning you already
know that um the outcome for tomorrow
for you know going back a week you're
looking at stock you can already have
like the graph of what the next day
looks like so you have an answer for
it and you have labeled data which is
used you have an external supervision
and solves Problems by mapping labeled
input to know one output so very
controlled unsupervised learning and un
supered learning is really interesting
because it's now taking part in many
other models they start with an you can
actually insert an unsupervised learning
model um in almost either supervised or
reinforcement learning as part of the
system which is really cool uh data
provided is unlabeled data the outputs
are not specified machine makes its own
predictions used to solve association
with clustering problems unlabeled data
is used no supervision solves Problems
by understanding patterns and
discovering
output uh so you can look at this and
you can
think um some of these things go with
each other they belong together so it's
looking for what connects in different
ways and there's a lot of different
algorithms that look at this um when you
start getting into those there's some
really cool images that come up of what
unsupervised learning is how we can pick
out say uh the area of a donnut one
model will see the area of the dnut and
the other one will divide it into three
sections based on its location versus
what's next to it so there's a lot of
stuff that goes in with unsupervised
learning
and then we're looking at reinforcement
learning probably the biggest industry
in today's market uh in machine learning
or growing Market it's very in its very
infant stage uh as far as how it works
and what it's going to be capable of the
machine learns from its environment
using rewards and errors used to solve
reward-based problems no predefined data
is used no supervision follows Trail and
error problem solving approach uh so
again we have a random at first you
start with a random I try this it works
and this is my reward doesn't work very
well maybe or maybe it doesn't even get
you where you're trying to get it to do
and you get your reward back and then it
looks at that and says well let's try
something else and it starts to play
with these different things finding the
best route so let's take a look at
important terms in today's reinforcement
model and this has become pretty
standardized over the last uh few years
so these are really good to know we have
the agent uh agent is the model that is
being trained via reinforcement learning
so this is your actual U entity that has
however you're doing it whether you're
using a neural network or a q table or
whatever combination thereof this is the
actual agent that you're using this is
the
model and you have your environment uh
the training situation that the model
must optimize to is called its
environment uh and you can see here I
guess we have a robot who's trying to
get a chest full of gems or whatever and
that's the output and then you have your
action this is all possible steps that
can be taken by the model and it picks
one action and you can see here it's
picked three different uh routes to get
to the chest of diamonds and
gems we have a state the current
position condition returned by the
model and you could look at this uh if
you're playing like a video game this is
the screen you're looking at uh so when
you go back here uh the environment is a
whole game board so if you're playing
one of those those Mobius games you
might have the whole game board going on
uh but then you have your current
position where are you on that game
board what's around that what's around
you um if you were talking about a robot
the environment might be moving around
the yard where it is in the yard and
what it can see what input it has in
that location that would be the current
position condition returned by the model
and then the reward uh to help the model
move in the right direction it is
rewarded points are given to it to
appraise some kind of action so yeah you
did good or if uh didn't do as good
trying to maximize the reward and have
the best reward
possible and then policy policy
determines how an agent will behave at
any time it acts as a mapping between
action and present State this is part of
the model what what is your action that
you're you're going to take what's the
policy you're using to have an output
from your agent one of the reasons they
separate uh policy
as its own entity is that you usually
have a prediction um of a different
options and then the policy well how am
I going to pick the best based on those
predictions I'm going to guess at
different options and we'll actually
weigh those options in and find the best
option we think will work uh so it's a
little tricky but the policy thing is
actually pretty cool how it works let's
go and take a look at a reinforcement
learning example and just in looking at
this we're going to take a look uh
consider what a dog um that we want to
train uh so the dog would be like the
agent so you have your your puppy or
whatever uh and then your environment is
going to be the whole house or whatever
it is where you're training them and
then you have an action we want to teach
the dog to
fetch so action equals
fetching uh and then we have a little
biscuit so we can get the dog to perform
various actions by offering incentives
such as a dog biscuit as a
reward the dog will followup policy to
maximize this reward and hence will
follow every command and might even
learn new actions like begging by itself
uh so you have B you know so we start
off with fetching it goes oh I get a
biscuit for that it tries something else
and you get a handshake or begging or
something like that and goes oh this is
also reward-based and so it kind of
explores things to find out what will
bring it as biscuit and that's very much
like how a reinforced model goes is it
uh looks for different rewards how do I
find can I try different things and find
a reward that
works the dog also will want to run
around and play an explorers environment
uh this quality of model is called
exploration so there's a little
Randomness going on in
Exploration and explores new parts of
the house climbing on the sofa doesn't
get a reward in fact it usually gets
kicked off the
sofa so let's talk a little bit about
markov's decision process uh markov's
decision process is a reinforcement
learning policy used to map a current
state to an action where the agent
continuously interacts with the
environment to produce new Solutions and
receive rewards and you'll see here's
all of our different uh uh vocabulary we
just went over we have a reward our
state or agent or environment
interaction and so even though the
environment kind of contains everything
um that you you really when you're
actually writing the program your
environment's going to put out a reward
in state that goes into the agent uh the
agent then looks at this uh state or it
looks at the reward usually um first and
it says okay I got rewarded for whatever
I just did or I didn't get rewarded and
then it looks at the state then it comes
back and if you remember from policy the
policy comes in um and then we have a
reward the policy is that part that's
connected at the bottom and so it looks
at that policy and it says hey what's a
good action
that will probably be similar to what I
did or um uh sometimes they're
completely random but what's a good
action that's going to bring me a
different
reward so taking the time to just
understand these different pieces as
they go is pretty important in most of
the models today um and so a lot of them
actually have templates based on this
you can pull in and start using um
pretty straightforward as far as once
you start seeing how it works uh you can
see your environment send it says hey
this is the agent did this if you're a
character in a game this happened and it
shoots out a reward in a state the agent
looks at the reward looks at the new
state and then takes a little guess and
says I'm going to try this action and
then that action goes back into the
environment it affects the environment
the environment then changes depending
on what the action was and then it has a
new state and a new reward that goes
back to the agent so in the diagram
shown we need to find the shortest path
between node A and D each path has a
reward associated with it and the path
with a maximum reward is what we want to
choose the nodes a b c d denote the
nodes to travel from node uh A to B is
an action reward is the cost of each
path and policy is each path
taken and you can see here a can go uh
to b or a can go to C right off the bat
or it can go right to D and if explored
all three of these uh you would find
that a going to D was a zero reward um a
going to C and D would generate a
different reward or you could go AC b d
there's a lot of options here um and so
when we start looking at this diagram
you start to
realize that even though uh today's
reinforced learning models do really
good at um finding an answer they end up
trying almost all the different
directions you see and so they take up a
lot of work uh or a lot of processing
time for reinforcement learning they're
right now in their infant stage and
they're really good at solving simple
problems and we'll take a look at one of
those in just a minute in a tic tac toe
game uh but you can see here uh once
it's gone through these and it's
explored it's going to find the
ACD is the best reward it gets a full 30
points for it so let's go ahead and take
a look at a reinforcement learning
demo uh in this demo we're going to use
reinforcement learning to make a tic tac
toe game you will be playing this game
Against the Machine learning
model and we'll go ahead and we're doing
it in Python so let's go ahead and go
through um I always uh not always
actually have a lot of python tools
let's go through um Anaconda which will
open up a Jupiter notebook seems like a
lot of steps but it's worth it to keep
all my stuff separate and it's also has
a nice display when you're in the
jupyter notebook for doing
python so here's our Anaconda Navigator
I open up the note book which is going
to take me to a web page and I've gone
in here and created a new uh python
folder in this case I've already done it
and enabled it to change the name to
tic-tac-toe uh and then for this example
uh we're going to go ahead
and import a couple things we're going
to um import numpy as NP we'll go ahead
and import pickle numpy of course is our
number array and then uh pickle is just
a nice way sometimes for storing uh
different information uh different
states that we're going to go through on
here
uh and so we're going to create a class
called State we're going to start with
that and there's a lot of lines of code
to this uh class that we're going to put
in here don't let that scare you too
much there's not as much here um it
looks like there's going to be a lie
here but there really is just a lot of
setup going on in the in our class date
and so we have up here we're going to
initialize it um we have our board um
it's a TI Tech toe board so we're only
dealing with nine spots on the board uh
we have player one player
two uh is end we're going to create a
board hash uh we'll look at that in just
a minute we're just going to store some
information in there symbol of player
equals one um so there's a few things
going on as far as the
initialization uh then something simple
we're just going to get the hash um of
the board we going to get the
information from the board on there
which is uh columns and rows we want to
know when winner occurs uh so if you get
three in a row that's what this whole
section here is for um me go ahead and
scroll up a little bit and you can get a
copy of this code if you send a note
over to Simply learn we'll send you over
um this particular file and you can play
with it yourself and see how it's put
together I don't want to spend a huge
amount of time on this uh because this
is just some real General python
coding uh but you can see here we're
just going through um all the rows and
you add them together and if it equals
three three in a row same thing with
columns um diagonal so you got to check
the diagonal that's what all this stuff
does here is it just goes through the
different areas actually let me go ahead
and
put there we
go um and then it comes down here and we
do our sum and it says true uh minus
three just says did somebody win or is
it a tie so you got to add up all the
numbers on there anyway just in case
they're all filled up up and next we
also need to know available positions um
these are ones that don't no one's ever
used before this way when you try
something or the computer tries
something uh it's not going to give it
an illegal move that's what the
available positions is doing uh then we
want to update our state and so you have
your position going in we're just
sending in the position that you just
chose and you'll see there's a little
user interface we put in there we P pick
the row and column in there
and again I mean this is a lot of code
uh so really it's kind of a thing you'd
want to go through and play with a
little bit and just read through it get
a copy of it uh great way to understand
how this works and here is a given
reward um so we're going to give a
reward result equals self winner this is
one of the hearts of what's going on
here uh is we have a result self. winner
so if there's a winner then we have a
result if the result result equals one
here's our
feedback uh if it doesn't equal one then
it gets a zero so it only gets a reward
in this particular case if it
wins and that's important to know
because different uh systems of
reinforced learning do rewarding a lot
differently depending on what you're
trying to do this is a very simple
example with a uh 3x3 board imagine if
you're playing a video game uh certainly
you only have so many actions but your
environment is huge you have a lot going
on in the environment and suddenly a
reward system like this is going to be
just um it's going to have to change a
little bit it's going to have to have
different rewards and different setup
and there's all kinds of advanced ways
to do that as far as weighing you add
weights to it and so they can add the
weights up depending on where the reward
comes in so it might be that you
actually get a reward in this case you
get the reward at the end of the game
and I'm spending just a little bit of
time on this because this is an
important thing to note but there's
different ways to add up those rewards
it might have like if you take a certain
path um the first reward is going to be
weighed a little bit less than the last
reward because the last reward is
actually winning the game or scoring or
whatever it is so this reward system
gets really complicated on some of the
more advanced uh
setups um in this case though you can
see right here that they give a a a 0.1
and a 05
reward um just for getting a picking the
right value and something that's
actually valid instead of picking an
invalid value so rewards again that's
like key it's huge how do you feed the
rewards back in uh then we have a board
reset that's pretty straightforward it
just goes back and resets the board to
the beginning because it's going to try
out all these different things while
it's learning it's going to do it by
trial and error so you have to keep
resetting it and then of course there's
the play we want to go ahead and play uh
rounds equals 100 depends on what you
want to do on here um you can set this
different you obviously set that to a
higher level but this is just going to
go through and you'll see in here uh
that we have player one and player two
this is this is the computer playing
itself uh one of the more powerful ways
to learn to play a game or even learn
something that isn't a game is to have
two of these models that are basically
trying to beat each other and so they
they keep finding explore new things
this one works for this one so this one
tries new things it beats this we've
seen this in um chess I think was a big
one where they had the two players in
chess with reinforcement learning uh was
one of the ways they train one of the
top um computer chess playing
algorithms uh so this is just what this
is it's going to choose an action it's
going to try something and the more it
tries stuff um the more we're going to
record the hash we actually have a board
hash where they s get the hash setup on
here where it stores all the
information and then once you get to a
win one of them wins it gets the reward
uh then we go back and reset and try
again and then kind of the fun part we
actually get down here is uh we're going
to play with a human so we'll get a
chance to come in here and see what that
looks like when you put your own
information in and then it just comes in
here does the same thing it did above it
gives it a reward for its things um or
sees if it wins or ties um looks at
available positions all that kind of fun
stuff and then finally we want to show
the board uh so it's going to print the
board out each
time really um as an integration is not
that exciting what's exciting uh in here
is one looking at this reward system
whoops Play One More up the reward
system is really the heart of this how
do you reward the different uh setup and
the other one is when it's playing it's
got to take an action and so what it
chooses for an action is also the heart
of reinforcement learning how do we
choose that action and those are really
key to right now where reinforcement
learning is um in today's uh technology
is uh figuring this out how do we reward
it and how do we guess the next best
action so we have our uh environment and
you can see the environment is we're
going to be or the state uh which is
kind of like what's going on we're going
to return the state depending on what
happens we want to go ahead and create
our agent uh in this case our player so
each one is let me go and grab that and
so we look at a class player
um this is where a lot of the magic is
really going on is what how is this
player figuring out how to maneuver
around the board and then the board of
course returns a state uh that it can
look at and a reward
uh so we want to take a look at this we
have uh name uh self State this is class
player and when you say class player
we're not talking about a human player
we're talking about um just a uh the
computer players and this is kind of
interesting so remember I told you
depending on what you're doing there's
going to be a Decay gamma um explor rate
uh these are what I'm talking about is
how do we train it
um as you try different moves it gets to
the end and the first move is important
but it's not as important as the last
one and so you could say that the last
one has the heaviest weight and then as
you as you get there the first one let
see the first move gives you a five
reward the second gives you a two reward
and the third one gives you a 10 reward
because that's the final ending you got
it the 10's going to count more than the
first step uh and here's our uh we're
going to you know get the board
information coming in and then choose an
action this was the second part that I
was talking about that was so important
uh so once you have your training going
on we have to do a little Randomness and
you can see right here is our NP random
um uniform so it's picking out a random
number take a random action this is
going to just pick which row and which
column it is um and so choosing the
action this one you can see we're just
doing random States uh Choice length of
positions action position and then it
skip in there and takes a look at the
board uh for p and positions you it's
actually storing the different boards
each time you go through so it has a
record of what it did so it can properly
weigh the values and this simply just
depins a hash date what's the last date
pin it to the to our states on here
here's our
feedback reward so the reward comes in
and it's going to take a look at this
and say is it none uh what is the reward
and here is that formula remember I was
telling you about up here um that was
important because it has Decay gamma
times a reward this is where as it goes
through each step and this is really
important this is this is kind of the
heart of this of what I was talking
about earlier uh you have step
one and this might have a a reward of
two you have step two I should probably
should have done ABC this has a step
three uh step four
so on till you get to step in and this
might have a reward of
10 uh so reward of
10 we're going to add that but we're not
adding uh let's say this one right here
uh let's say this reward here right
before 10 was um let's say it's also 10
that just makes the the math easy so we
had 10 and 10 we had 10 this is 10 and
10 n whatever it is but it's time of
0.9 uh so instead of putting a full 10
here we only do nine that's a 0. n
times
10 and so this
formula um as far as the Decay times the
reward minus the cell State value uh it
basically adds in it says here's one or
here's two I'm sorry I should have done
this ABC it would have been easier uh so
the first move goes in here and it puts
two in
here uh then we have our s uh setup on
here you can see how this gets pretty
complicated in the math but this is
really the key is how do we train our
states and we want the the final State
the win to get the most points if you
win you get most points um and the first
step gets the least amount of points so
you're really training this almost in
Reverse you're training you're training
it from the last place where you have
like it's says okay this is now where I
where need to sum up my rewards and I
want to sum them up going in reverse and
I want to find the answer in Reverse
kind of an interesting uh uh play on the
mind when you're trying to figure this
stuff
out and of course we want to go ahead
and reset the board down here uh save
the policy load
policy these are the different things
that are going in between the agent and
the state to figure out what's going on
let's go ahead and load that up and then
finally we want to go ahead and create a
human
player and the human player is going to
be a little different uh in that uh you
choose an action row and column here's
your action uh if action is if action in
positions meaning positions that are
available uh you return the action if
not it just keeps asking you until you
get the action that actually works and
then we're going to go ahead and append
to the hash state which uh we don't need
to worry about it Returns the action up
here and feed forward uh again this is
because it's a
human um at the end of the game bat
propagate and update State values this
part isn't being done because it's not
programming uh the model uh the model is
getting its own rewards so we've gone
ahead and loaded this in here uh so
here's all our pieces and the first
thing we want to
do is set up up uh P1 player one uh P2
player two and then we're going to send
our players to our state so now it has
P1 P2 and it's going to play and it's
going to play 50,000 rounds now we can
probably do a lot less than this and
it's not going to get the full results
in fact you know what uh let's go ahead
and just do five um just to play with it
because I want to show you something
here oops somewhere in there I forgot to
load
something there we go I must have start
forgot to run this
run oops forgot a reference there for
the board rows and columns
3x3 um there is actually in the state it
references that we just tack it on on
the end it was supposed to be at the
beginning uh so now I've only set this
up with um see where are we going here
I've only set this up to
train five times and the reason I did
that is we're going to uh come in and
actually play it and then I'm going to
change that and we can see how it
differs on
there there we go I didn't make it
through a run and we're going to go
ahead and save the
policy um so now we have our player one
and our player two policy uh the way we
set it up it has two separate policies
loaded up in
there and then we're going to come in
here and we're going to do uh player one
is going to be the computer experience
rate zero load policy one human player
human and we're going to go ahead and
play this I remember I only went through
it um uh just one round of training in
fact minimal training and so it puts an
X there and I'm going to go ahead and do
row zero column one you can see this is
very uh basic on here and so I put in my
zero and then I'm going to go zero block
it z z and you can see here it let me
win uh just like that I was able to win
zero two and woo human winds so I only
trained it five times we're going to run
this again and this time uh instead of
five let's do 5,000 or 50,000 I think
that's what the guys in the back had and
this takes a while to train it this is
where reinforcement learning really
Falls apart look how simple this game is
we're talking about uh a 3X3 set of
columns and so for me to train it on
this um I could do a q table which would
take which would go much quicker um you
could build a quick Q table with almost
all the different options on there and
uh you would probably get a the same
result much quicker we're just using
this as an example so when we look at
reinforcement learning you need to be
very very careful what you apply it to
it sounds like a good deal until you do
like a large neural network where you're
doing um you set the neural network to a
learning increment of one so every time
it goes through it
learns and then you do your action so
you pick from the learning uh setup and
you actually try actions on the learning
setup until you get the what you think
is going to be the best action so you
actually feed what you think is right
back through the neural network there's
a whole layer there which is really fun
to play with and then it has an output
well think of all those processes I mean
that is just a huge amount of work it's
going to do uh let's go ahead and Skip
ahead here give it a moment it's going
to take a a minute or two to go ahead
and
run now to train it uh we went ahead and
let it run and it took a while this this
took um I got a pretty powerful
processor and it took about five minutes
plus to run it and we'll go ahead and
run our player setup on here oops I
brought in the last whoops I brought in
the last round so give me just a moment
to redo the policy save there we go I
forgot to save the policy back in
there and then go ahead and run our
player again so we we've saved the
policy and then we want to go ahead and
load the policy for P1 as the computer
and we can see the computer's gone in
the bottom right corner I'm going to go
ahead and go uh one one which is the
center and it's gone right up the top
and if you have ever played tic tactoe
you know the computer has me uh but
we'll go ahead and play it out row zero
column
two there it is and then it's gone here
and so I'm going to go ahead and go row
01 two no 01 there we go and column zero
that's where I wanted oh and it says I
okay your action there we go boom uh so
you can see here we've got a didn't
catch the win on this it said Tai um
kind of funny that didn't catch the win
on
there but if we play this a bunch of
times you'll find it's going to win more
and more the more we train it the more
the reinforcement
happens this lengthy training process uh
is really the stopper on reinforcement
learning as this changes reinforcement
learning will be one of the more
powerful uh packages evolving over the
next next decade or two in fact I would
even go as far as to say it is the most
important uh machine learning tool and
artificial intelligence tool out there
as it learns not only a simple Tic Tac
Toe board but we start learning
environments and the environment would
be like in language if you're
translating a language or something from
one language to the other so much of it
is lost if you don't know the context
that's in what's the environments it's
in and so being able to attach
environment and context and all those
things together is going to require
reinforcement learning to
do so again if you want to get a copy of
the Tic Tac Toe board it's kind of fun
to play with uh run it you can test it
out you can do um you know test it for
different uh uh values you can switch
from P1
computer um where we loaded the policy
one to load the policy 2 and just see
how it varies there's all kinds of
things you can do on there so what is
q-learning q-learning is reinforcement
learning policy which will fill the next
best action given a current state it
chooses this action at random and aims
to maximize the reward and so you can
see here's our standard reinforcement
learning graph um by now if you're doing
any reinforcement learning you should be
familiar with this where you have your
agent your agent takes an action the
action affects the environment and then
the environment sends back the reward or
the feedback and the state the new state
the agents in where is it at on the
chessboard where's it at in the video
game um if your robots out there picking
trash up off the side of the road where
is it at on the road consider an ad
recommendation system usually when you
look up a product
online you get ads which will suggest
the same product over and over
again using Q learning we can make an
add recommendation system which will
suggest related products to our previous
purchase the reward will be if user
clicks on the suggested
product and and again you can see um you
might have a lot of products on uh your
web advertisement or your pages but it's
still not a float number it's still a
set number and that's something to be
aware of when you're using Q learning
and you can see here that if you have
100 people clicking on ads and you click
on one of the ads it might go in there
and say okay this person clicked on this
ad what is the best set of ads based on
clicking on this ad or these two ads
afterwards based on where they are
browsing so let let's go and take a look
at some important terms when we talk
about Q learning uh we have States the
state s represents the current position
of an agent in an
environment um the action the action a
is the step taken by the agent when it
is particular State rewards for every
action the agent will get a positive or
negative
[Music]
reward and again uh when we talk about
States we're usually not with when
you're using a q table you're not
usually talking about float variables
you're talking about true false um and
we'll take a closer look at that in a
second and episodes when an agent ends
up in a terminating State and can't take
a new
action uh this might be if you're
playing a video game your character
stepped in and is now dead or whatever
uh Q values used to determine how good
an action a taken at a particular State
s is QA of s and temporal difference a
formul used to find the Q value by using
the value of the current state and
action and previous state and action and
very I mean there's bellman's equation
which basically is the equation that
kind of uh covers what we just looked at
in all those different terms the Bellman
equation is used to determine the values
of a particular State and deduce how
good it is to be in take that state the
optimal the optimal state will give us
the highest optimal value Factor
influencing Q Val valuse the current
state and action that's your essay so
your current state in your
action uh then you have your previous
state in action which is your s um I
guess Prime I'm not sure how they how
they reference that S Prime a prime so
this is what happened before uh then you
have a reward for Action so you have
your R reward and you have your maximum
expected future
reward and you can see there's also a
learning rate put in there and a
discount rate
uh so we're looking at these just like
any other model we don't want to have an
absolute um final value on here we don't
want it to if you do absolute values
instead of taking smaller steps you
don't really have that approach to the
solution you just have it jump and then
pretty soon if you jump one solution out
that's what's going to be the new
solution whichever one jumps up really
high first um kind of ruining the whole
idea of doing a random selection and
I'll go into the random selection just a
second steps in Q
learning step one create an initial Q
table with all values initialized to
zero again we're looking at 0 one uh so
are you you know here's our action we
start we're an idol we took a wrong
action we took a correct action and int
and then we have our um actions fetching
sitting and running of course we're just
using the dog example and choose an
action and perform it update values in
the table and of course when we're
choosing an action we're going to kind
of do something random and just randomly
pick one so you start out and you sit
and you have then a um then depending on
that um um action you took you can now
update the value for sitting after you
start from start to
sitting get the value of the reward and
calculate the Val the value Q value
using the Bellman equation and so now we
attach a reward to
sitting and when we attach all those
rewards we continue the same until the
table's filled with or an episode
ends and and M's going to come back to
the random side of this and there's a
few different formulas they use for the
random um setup to pick it I usually let
whatever Q model I'm using do their
standard one because someone's usually
gone in and done the math uh for the
optimal uh spread uh but you can look at
this if I have running has a reward of
10 sitting has a reward of seven
fetching has a reward of five um just
kind of without doing like a a a means
you know using the bell curve for the
means value and like I said there's some
math you can put in there to pick um so
that you're more like so that running
has even a higher chance um but even if
you were just going to do an average on
this you could do an average a random
number by adding them all together uh so
you get 10 + 7 + 5 is 22 you could do 0
to 22 and or zero to 21 but 1 to 22 one
to five would be fetching uh and so
forth you know the last 10 so you can
just look at this as what percentage are
you going to go for that particular
option um and then that gets your random
setup in there and then as you slowly
increment these up uh you see that uh um
if you're idle um where's one here we go
sitting at the end if you're at the end
of wherever you're at sitting gets a
reward of one um where's a good one on
here oh wrong action running for a wrong
action gets almost no reward so that
becomes very very less likely to happen
but it still might happen it still might
have a percentage of coming up and
that's where the random programming and
Q learning comes in the below table
gives us an idea of how many times an
action has been taken and how positively
correct action or negatively wrong
action it is going to affect the next
state so let's go ahead and dive in and
pull up a little piece of code and see
what this looks like like um in
Python uh in this demo we'll use
q-learning to find the shortest path
between two given points if getting your
learning started is half the battle what
if you could do that for free visit
skillup by simply learn click on the
link in the description to know more if
you've seen my videos before um I like
to do it in the Anaconda Jupiter
notebook um setup just because it's
really easy to see it's a nice demo uh
and so here's my anaconda this one I'm
actually using uh python 36 environment
that I set up in here and we'll go ahead
and launch the Jupiter Notebook on this
and once we in our Jupiter notebook uh
which has the kernel loaded with Python
3 we'll go ahead and create a new Python
3 uh folder in
here and we'll call this uh
Q
learning and to start this demo
let's go ahead and import our uh numpy
array we'll just run that so it's
imported and like a lot of these uh
model programs when you're building them
you spend a lot of time putting it all
together um and then you end up with
this really short answer at the
end uh and we'll we'll take a look at
that as we come into it so we we go
ahead and start with our location to
State uh so we have um L1 L2 these are
our nine locations one to nine and then
of course this state is going to be 0 1
2 3 4 it's just a mapping of our
location to a integer on there and then
we have our actions our actions are
simply uh moving
from um One
location to another so I can go to I can
go to location zero I can go to location
1 2 3 4 5 6 7 8 uh so these are my
actions I can choose these are the
locations of our
state and if you remember earlier I
mentioned uh uh
that the limitation is that you you
don't want to put in um a continually
growing table because you can actually
create a dynamic Q table where you
continually add in new values as they
arise because um if you have float
values is just becomes infinite and then
your memory and your computer's gone or
you know does it's not going to work at
the same time you might think well that
kind of really limits the the Q uh T
learning setup but there are ways to use
it in conjunction with other systems and
so you might look
at uh well I do um I've been doing some
work in stock um and one of the
questions that comes out is to buy or
sell the stock and the state coming in
might be um you might take it and create
what we call
buckets um where anything that you
predict is going to return more than a
certain amount of money um the error for
that stock that you've had in the past
you put those in buckets and suddenly as
you start putting the creating these
buckets you realize you do have a
limited amount of information coming in
you no longer have a float number you
now have um bucket one two three and
four and then you can take those buckets
put them through a a q learning table
and come up with the best action which
stock should I buy it's like gambling
stock is pretty much gambling if you're
doing day trading you're not doing
long-term
um Investments and so you can start
looking at it like that a lot of the um
current feeds say that the best
algorithms used for day Traders where
you're doing it on your own is really to
ask the question do I want to trade the
stock yes or no and now you have it in a
q learning table and now you can take it
to that next level and you can see where
that can be a really powerful tool at
the end of doing a basic linear
regression model or something um what is
the best investment and you start
getting the best reward on there
uh and so if we're going to have rewards
these rewards we just create um it says
uh if basically if you're uh this should
match our Q table because it's going to
be uh you have your state and you have
your action across the top if you
remember from the dog and so we have
whatever state we're in going down and
then the next action and what the reward
is for it um and of course if you were
actually doing a um something more
connected your reward would be based on
on U the actual environment it's in and
then we want to go ahead and create a
state to location uh so we can map the
indexes so just like we defined our
rewards uh we're going to go and do
state to location um and you can see
here it's a a dictionary setup for
location State and location to state
with
items and we also need to
um Define what we want for learning
rates uh you remember we had our two
different rates um as far as like
learning from the past and learning from
the current so we'll go ahead and set
those to uh 75 and the alpha set to 0.9
and we'll see that when we do the
formula and of course any of this code
uh send a note to our simply learn team
they'll get you a copy of this code on
here let's go ahead and
pull there we
go the next two set c s um since we're
going to keep it short and
sweet here we go so let's go ahead and
create our agent um so our agent is
going to have our initialization where
we send it all the information uh we'll
Define our self gamma equals gamma we
could have just set the gamma rate down
here instead of uh submitting it it's
kind of nice to keep them separate
because you can play with these numbers
uh our self Alpha um then we have our
location State we'll set that in here uh
we have our choice of actions um we're
going to go ahead and just embed the
rewards right into the agent so
obviously this would be coming from
somewhere else uh instead of from uh
self-generated and then a self state to
location equals our state to location uh
dictionary and we go ahead and create a
QQ learning table and I went ahead and
just set the Q learning table up to um
uh 0 to zero what what what the setup is
location to State how many of them are
there and this just creates an array of
zero to zero setup on
there and then the big part is the
training we have our rewards new equals
a copy of self.
rewards ending State equals the self
location state in location so this is
whatever we end up at rewards new equals
ending State plus ending State equals
999 just kind of goes to a dead end and
we start going through
iterations and we'll go ahead um let's
do this uh so this we're going to come
back and we're going to call call it on
here uh let me just erase that switch it
to an
arrow there we go uh so what we're doing
is we're going to send in here to train
it we're going to say hey um I want to
iterate through this a thousand times
and see what happens now this part would
actually be
instead of iterating you might have your
external environment and they're going
back and forth and you iterate through
outside of here uh but just for ease of
use our agent's going to come in here
and iterate through this sometimes I'll
put this iteration in here and I'll have
it call the environment and say hey this
is what I did what's the next state and
the environment does its thing right in
here as I iterate through
it uh and then we want to go ahead and
pick a random state to start with that's
what's going on here you have to start
somewhere um and then you have your
playable actions we're going to start
with just an empty thing for playable
actions and we'll fill that up so that's
what choices I have and so we're going
to iterate through the rewards Matrix to
get the states uh directly reachable
from the randomly chosen current state
assign those states to a list named
playable
actions and so you can see here we have
uh range nine I usually use length of
whatever I'm looking at uh which is our
locations or States as they are uh we
have a reward so we want to look at the
current the rewards uh the new
reward is our uh is in our chart here of
rewards uncore new uh current state um
plus J uh J being what is the next state
we want to try and so we go Ahad and do
our playable actions and we append
J and so we're doing is we're randomly
trying different things in here to see
what's going to generate a better
reward and then of course we go ahead
and choose our next State uh so we have
our random Choice playable actions and
if you remember I mentioned on this let
me just go ahead and uh oops do a free
form when we were talking about the next
State uh this right here just does a
random selection instead of a random uh
selection you might do something where
uh whatever the best selection is which
might be option three here and then so
you can see that it might use a bell
curve and then option two over here
might have a bell curve like this oops
and we start looking at these averages
and these spreads um or we can just add
them all together and pick the one that
kind of goes in all of those uh so those
are some of the options we have in here
we just go with a random Choice uh
that's usually where you start play with
it um and then we have our reward
section down here and so we want to go
ahead and find well in this case a
temporal difference uh so you have your
rewards new plus the self gamma and this
is the formula we were looking at this
is bellman's equation here uh so we have
our current value our learning rate our
discount rate involved in there the
reward system coming in for that um and
we can add it all together this is of
course our uh maximum expected future
setup in here uh so this is all of our
our bellman's equation that we're
looking at here and then we come up in
here and we update our Q table that's
all this is on this one that's right
here we have um self Q current state
next state and we add in our um Alpha
because we don't want to we don't want
to train all of it at once in case
there's slight differenes coming in
there we want to slowly approach the
answer uh and then we have our route
equals the start location
and next location equals start location
so we're just incrementing we took a
step forward and then finally remember I
was telling you how uh we're going to do
all this and just have some simple thing
at the end or it just generates a simple
path we're going to go ahead and and get
the optimal route we want to find the
best route in here and so we've created
a definition for the optimal route down
here just scroll down for that and we
get the optimal route we go ahead and
put the information in including the Q
TBL self uh start location in location
next location route q and it says while
next location is not equal to in
location so while we can still go our
start location equals self location to
State start location so we already have
our best value for the start
location uh the next state looks at the
Q table and says hey what's H the next
one with the best value and then the
next location we go ahead and pull that
in and we just append it that's what's
going on down here
and then our start location equals the
next location and we just go through all
the steps and we'll go ahead and run
this and now that we have our Q table
our um Q agent loaded we're going to go
ahead and uh take our Q agent load them
up with our Alpha Gamma that we set up
above um along with the location step
action reward state to location and uh
our goal is to plot a course between L9
and L1
and we're going to go through a 100 a
thousand iterations on here and so when
I run that it runs pretty quick uh why
is this so fast um if youve been running
neural networks and you've been doing
all these other models you sit here and
wait a long time well we're very small
amount of data these are all integers
these aren't float values there's not a
the math is not heavy on the on the
processing end and this is where Q
tables are so powerful if you have a
small amount of information coming in
you very quickly uh get an answer off of
this even though we went through it a
thousand times to train it and you'll
see here we have l985 2 and one and
that's based on our reward table we had
set up on there and this is the shortest
path going between these different uh
setups in here and if you remember on
our reward table uh you can see that if
you start here you can go to here
there's places you can't go that's how
this reward table was set up so I can
only go to certain places
uh so kind of a little maze setup in
there and you can play with it this is
really fun uh setup to play with uh and
you can see how you can take this whole
code and you can like I was saying
earlier you can embed it into another
setup in model and predictions where you
put things into buckets and you're
trying to guess the best investment the
best course of action long as you can
take that course into of action and and
uh uh reduce it down to a yes no um or
if you're using text you can use a one
hot encoder which word is next there's
all kinds of things you can do with a Q
table uh depending on just how much
information you're putting in there so
that wraps up our demo in this demo
we've uh found the shortest distance
between two paths based on whatever
rules or state rewards we have to get
from point A to point B and what
available actions there are hello and
welcome to this tutorial on deep
learning my name is moan and in the next
about 1 one and a half hours I will take
you through what is deep learning and
into tensorflow environment to show you
an example of deep learning now there
are several applications of deep
learning really very interesting and
Innovative applications and one of them
is identifying the geographic location
based on a picture and how does this
work the way it works is pretty much we
train an artificial neural network with
millions of images which are tagged
their geolocation is tagged and then
when we feed a new picture it will be
able to identify the geolocation of this
new image for example you have all these
images especially with maybe some
significant monuments or or U
significant locations and you train with
millions of such images and then when
you feed another image it need not be
exactly one of those that you have
climed it can be completely different
that is the whole idea of exping it will
be able to recognize for example that
this is a picture from Paris because it
is able to recognize the eiar so the way
it works internally if we have to look a
little bit under the H is these images
are nothing but this is digital
information in the form of pixels so
each image could be a certain size it
can be 256x 256 pixel kind of a
resolution and then each pixel is either
are having a certain grade of color and
all that is fed into the neural network
and it then gets trained in and it's
able to based on these pixels pixel
information it is able to get trained
and able to recognize the features and
extract the features and thereby it is
able to identify these images and the
location of these images and then when
you feed a new image it kind of based on
the training it will be able to figure
out where this image is from so that's
the way a little bit under the hood how
it works so what are we going to do in
this tutorial we will see what is deep
learning and what do we need for deep
learning and one of the main components
of deep learning is neural network so we
will see what is neural network what is
a perceptron and how to implement logic
gates like and or nor and so on using
perceptrons the different types of
neural networks and then applications of
deep learning and we will also see how
neural networks works so how do we do
the training of neural networks and at
the end we will end up with a small demo
code which will take you through
intensive flow now in order to implement
deep learning code there are multiple
libraries or development environments
that are available and tensor flow is
one of them so the focus at the end of
this would be on how to use tensorflow
to write a piece of code using python as
a programming language and we will take
up a an example which is a very common
one which is like the hollow world of
deep learning the handwriting number
recognition which is a Mist commonly
known as Mist database so we will take a
look at Mist database and how we can
train a neural network to recognize
handwritten numbers so that's what you
will see in this particular video so
let's get started what is deep learning
deep learning is like a subset of what
is known as a high level concept called
artificial intelligence you must be
already familiar must have heard about
this term artificial intelligence so
artificial intelligence is like the high
level concept if you will and in order
to implement artificial intelligence
applications we use what is known as
machine learning and within machine
learning a subset of machine learning is
deep learning machine learning is a
little bit more generic concept and deep
learning is one type of machine learning
if you will and we will see a little
later in maybe the following slides
little bit more in detail how deep
learning is different from traditional
machine learning but to start with we
can mention here that deep learning uses
one of the differentiators between deep
learning and traditional machine
learning is that deep learning uses
neural networks and we will talk about
what are neural networks and how we can
Implement neural networks and so on and
so forth as a part of this tutorial so a
little deeper into deep learning deep
learning primarily involves working with
complicated unstructured data compared
to traditional machine learning with
where we normally use structured data in
deep learning the data would be
primarily images or Voice or maybe text
file so and it is large amount of data
as well and deep learning can handle
complex operations it involves complex
operations and the other difference
between traditional machine learning and
deep learning is that the feature
extraction happens pretty much
automatically in traditional machine
learning feature engineering is done
manually the data scientists we data
scientists have to do feature
engineering feature extraction but in
deep learning that happens automatically
and of course deep learning for Lar
amounts of data complicated unstructured
data deep learning gives very good
performance now as I mentioned one of
the secret sources of deep learning is
neural networks let's see what neural
networks is neural networks is based on
our biological neurons the whole concept
of deep learning and artificial
intelligence is based on human brain and
human brain consists of billions of tiny
stuff called neurons and this is how a
biological neuron looks and this is how
an artificial neuron look so neural
networks is like a simulation of our
human brain human brain has billions of
biological neurons and we are trying to
simulate the human brain using
artificial neurons this is how a
biological neuron looks it has tendres
and and the corresponding component with
an artificial neural network is or an
artificial neuron are the inputs they
receive the inputs through ddes and then
there is the cell nucleus which is
basically the processing unit in a way
so in artificial neuron also there is a
piece which is an equivalent of this
cell nucleus and based on the weights
and biases we will see what exactly
weights and biases are as we move the
input get processed and that results in
an output in a biological neuron the
output is sent through a synapse and in
an artificial neuron there is an
equivalent of that in the form of an
output and biological neurons are also
interconnected so there are billions of
neurons which are interconnected in the
same way artificial neurons are also
interconnected so this output of this
neuron will be fed as an input to
another neuron and so on now in neural
network one of the very basic units is a
perceptron so what is a perceptron A
perceptron can be considered as one of
the fundamental units of neural networks
it can consist at least one neuron but
sometimes it can be more than one neuron
but you can create a perceptron with a
single neuron and it can be used to
perform certain functions it can be used
as a basic binary classifier it can be
trained to do some basic binary
classification and this is how a basic
perceptron looks like and this is
nothing but a neuron you have inputs X1
X2 X to xn and there is a summation
function and then there is what is known
as an activation function and based on
this input what is known as the weighted
sum the activation function either gets
gives an output like a zero or a one so
we say the neuron is either activated or
not so that's the way it works so you
get the inputs these inputs are each of
the inputs are multiplied by a weight
and there is a bias that gets added and
that whole thing is fed to an activation
function and then that results in an
output and if the output is correct it
is accepted if it is wrong if there is
an error then that error is fed back and
the neuron then adjust the weights and
biases to give a new output and so on
and so forth so that's what is known as
the training process of a neuron or a
neural network there's a concept called
perceptron learning so perceptron
learning is again one of the very basic
learning processes the way it works is
somewhat like this so you have all these
inputs like X1 to xn and each of these
inputs is multiplied by a weight and
then that sum this is the formula of the
equation so that sum wi XI Sigma of that
which is the sum of all these product of
X and w is added up and then a bias is
added to that the bias is not dependent
on the input but or the input values but
the bias is common for one neuron
however the bias value keeps changing
during the training process once the
training is completed the values of
these weights W1 W2 and so on and the
value of the bias gets fixed so that is
basically the whole training process and
that is what is known as the perceptron
train remaining so the weights and
biases keep changing till you get the
accurate output and the summation is of
course passed through the activation
function as you see here this wixi
summation plus b is passed through
activation function and then the neuron
gets either fired or not and based on
that there will be an output that output
is compared with the actual or expected
value which is also known as as labeled
information so this is the process of
supervised learning so the output is
already known and um that is compared
and thereby we know if there is an error
or not and if there is an error the
error is fed back and the weights and
biases are updated accordingly till the
error is reduced to the minimum so this
iterative process is known as perceptron
learning or perceptron learning Rule and
this error needs to be minimized so till
the error is minimized this iteratively
the weights and biases keep changing and
that is what is the training process so
the whole idea is to update the weights
and the bias of the perceptron till the
error is minimized the error need not be
zero the error may not ever reach zero
but the idea is to keep changing these
weights and bias so that the error is is
minimum the minimum possible that it can
have so this whole process is an
iterative process and this is the
iteration continues till either the
error is zero which is uh unlikely
situation or it is the minimum possible
Within These given conditions now in
1943 two scientists Warren mik and
Walter pittz came up with an experiment
where they were able to implement the
logical functions like and or and nor
using neurons and that was a significant
breakthrough in a sense so they were
able to come up with the most common
logical Gates they were able to
implement some of the most common
logical Gates which could take two
inputs Like A and B and then give a
corresponding result so for example in
case of an and gate A and B and then the
output is a in case of an orgate it is a
plus b and so on and so forth and they
were able to do this using a single
layer perceptron now most of these GS it
was possible to use single layer
perceptron except for XR and we will see
why that is in a little bit so this is
how an endgate works the inputs A and B
the output should be fired or the neuron
should be fired only when both the
inputs are one so if you have 0 0 output
should be zero for 0 1 it is again 0 1 0
again 0 and 1 1 the output should be 1
so how do we implement this with a
neuron so it was found that by changing
the values of Weights it is possible to
achieve this logic so for example if we
have equal weights like 7 7 and then if
we take the sum of the weighted product
so for example 7 into 0 and then 7 into
0 will give you 0 and so on and so forth
and in the last case when both the
inputs are one you get a value which is
greater than one which is the threshold
so only in this case the neuron gets
activated and the output is there is an
output in all the other cases there is
no output because the threshold value is
one so this is implementation of an and
gate using a single perceptron or a
single neuron similarly an orgate in
order to implement an orgate in case of
an orgate the output will be one if
either of
these inputs is one so for example 01
will result in one or other in all the
cases it is one except for 0 0 so how do
we implement this using a perceptron
once again if you have a perceptron with
weights for example 1.2 now if you see
here if in the first case when both are
zero the output is zero in the second
case when it is 0 and 1 1.2 into 0 is0
and then 1.2 into 1 is 1 and in the
second case similarly the output is 1 .2
in this last case when both the inputs
are one the output is 2.4 so during the
training process these weights will keep
changing and then at one point where the
weights are equal to W1 is equal to 1.2
and W2 is equal to 1.2 the system learns
that it gives the correct output so that
is implementation of orgate using a
single neuron or a single layer
perceptron now exor gate this was one of
the challenging ones they tried to
implement an XR gate with a single level
perceptron but it was not possible and
therefore in order to implement an XR so
this was like a a roadblock in the
progress of U neural network however
subsequently they realized that this can
be implemented an XR gate can be
implemented using a multi-level
perceptron or MLP so in this case there
are two layers instead of a single layer
and this is how you can implement El an
XR gate so you will see that X1 and X2
are the inputs and there is a hidden
layer and that's why it is denoted as H3
and H4 and then you take the output of
that and feed it to the output at 05 and
provide a threshold here so we will see
here that this is the numerical
calculation so the weights are in this
case for X1 it is 20 and minus 20 and
once again 20 and minus 20 so these
inputs are fed into H3 and H4 so you'll
see here for H3 the input is 01 1 1 and
for H4 it is 1011 and if you now look at
the output final output where the
threshold is taken as one if you use a
sigmoid with the threshold one you will
see that in these two cases it is zero
and in the last two cases it is one so
this is a implementation of XR in case
of XR or only when one of the inputs is
one you will get an output so that is
what we are seeing here if we have
either both the inputs are one or both
the inputs are zero then the output
should be zero so that is what is an
exclusive or gate so it is exclusive
because only one of the inputs should be
one and then only you'll get an output
of one which is Satisfied by this
condition so this is a special
implementation XR gate is a special
implementation of perceptron now that we
got a good idea about per perceptron
let's take a look at what is the neural
network so we have seen what is a
perceptron we have seen what is a neuron
so we will see what exactly is a neural
network so neural network is nothing but
a network of these neurons and they are
different types of neural networks there
are about five of them these are
artificial neural network convolutional
neural network then recursive neural
network or recurrent neural network deep
neural network and deep belief Network
so and each of these types of neural
networks have a special you know they
can solve special kind of problems for
example convolutional neural networks
are very good at performing image
processing and image recognition and so
on whereas RNN are very good for speech
recognition and also text analysis and
so on so each type has some special
characteristics and they can they're
good at performing certain special kind
of tasks what are some of the
applications of deep learning deep
learning is today used extensively in
gaming you must have heard about alphao
which is a game created by a startup
called Deep Mind which got acquired by
Google and alphago is an AI which
defeated the human world champion Le at
all in this game of Go so gaming is an
area where deep learning is being
extensively used and a lot of research
happens in the area of gaming as well in
addition to that nowadays there are
neural networks a special type called
generative adversarial networks which
can be used for synthesizing either
images or music or text and so on and
they can be used to compose music so the
neural network can be trained to compose
a certain kind of music and autonomous
cars you must be familiar with Google
Google's self-driving car and today a
lot of Automotive companies are
investing in this space and uh deep
learning is a core component of this
autonomous Cars the cars are trained to
recognize for example the road the the
lane markings on the road signals any
objects that are in front any
obstruction and so on and so forth so
all this involves deep learning so
that's another major application and uh
Ro robots we have seen several robots
including Sofia you may be familiar with
sopia who was given a citizenship by
Saudi Arabia and there are several such
robots which are very humanlike and the
underlying technology in many of these
robots is deep learning medical
Diagnostics and Health Care is another
major area of where deep learning is
being used and within Healthcare
Diagnostics again there are multiple
areas where deep learning and image
recognition image processing can be used
for example for cancer detection as you
may be aware if cancer is detected early
on it can be cured and one of the
challenges is in the availability of
Specialists who can diagnose cancer
using these diagnostic images and
various scans and and so on and so forth
so the idea is to train neural network
to perform some of these activities so
that the load on the cancer specialist
doctors or oncologists comes down and
there is a lot of research happening
here and there are already quite a few
applications that are claimed to be
performing better than human beings in
this space can be lung cancer it can be
breast cancer and so on and so forth so
Healthcare is a major area where deep
learning is being applied let's take a
look at the inner working of a neural
network so how does an arcial neural
network let's say identify can we train
a neural network to identify the shapes
like squares and circles and triangles
when these images are fed so this is how
it works any image is nothing but it is
a digital information of the pixels so
in this particular case let's say this
is an image of 28x 28 pixel and this is
an image of a square there's a certain
way in which the pixels are are lit up
and so these pixels have a certain value
maybe from 0 to 256 and 0 indicates that
it is black or it is dark and 256
indicates it is completely it is white
or lit up so that is like an indication
or a measure of the how the pixels are
lit up and so this is an image is let's
say consisting of information of 784
pixels so all the inform what is inside
this image can be kind of compressed
into this 784 pixels the way each of
these pixels is lit up provides
information about what exactly is the
image so we can train neural networks to
use that information and identify the
images so let's take a look how this
works so each neuron the value if it is
close to one that means it is one white
whereas if it is close to zero that
means it is black now this is a an
animation of how this whole thing works
so these pixels one of the ways of doing
it is we can flatten this image and take
this complete 784 pixels and feed that
as input to our neural network the
neural network can consist of probably
several layers there can be a few hidden
layers and then there is an input layer
and an output layer now the input layer
take these 784 pixels as input the
values of each of these pixels and then
you get an output which can be of three
types or three classes one can be a
square a circle or a triangle now during
the training process there will be
initially obviously you feed this image
and it will probably say it's a circle
or it will say it's a triangle so as a
part of the training process we then
send that error back and the weights and
the biases of these neurons are adjusted
till it correctly identifies that this
is a square that is the whole training
mechanism that happens out
here now let's take a look at a circle
same way so you feed these 784 pixels
there is a certain pattern in which the
pixels are lit up and the neural network
is trained to identify that pattern and
during the training process once again
it would probably initially identify it
incorrectly saying this is a square or a
triangle and then that error is fed back
and the weights and biases are adjusted
finally till it finally gets the image
correct so that is the training process
so now we will take a look at same way a
triangle so now if you feed another
image which is consisting of triangles
so this is the training process now we
have trained our neural network to
classify these images into a triangle or
a circle and a square so now this neural
network can identify these three types
of objects now if you feed another image
and it will be able to identify whether
it's a square or a triangle or a circle
now what is important to be observed is
that when you feed a new image it is not
necessary that the image or the the
triangle is exactly in this position now
the neural network actually identifies
the patterns so even if the triangle is
let's say positioned here not exactly in
the middle but maybe at the corner or in
the side it would still identify that it
is a triangle and that is the whole idea
behind pattern recognition so how does
this straining process work this is a
quick
view of how the training process works
so we have seen that a neuron consists
of inputs it receives inputs and then
there is a weighted sum which is nothing
but this XI wi summation of that plus
the bias and this is then fed to the
activation function and that in turn
gives us a output now during the
training process initially obviously
when you feed these images when you send
maybe a square it will identify it as a
triangle and when you maybe feed a
triangle it will identify as a square
and so on so that error information is
fed back and initially these weights can
be random maybe all of them have zero
values and then it will slowly keep
changing so the as a part of the
training process the values of these
weights W1 W2 up to WN keep changing in
such a way that towards the end of the
training process it should be able to
identify these images correctly so till
then the weights are adjusted and that
is known as the training process so and
these weights are numeric values could
be.
52535 and so on it could be positive or
it could be negative and the value that
is coming here is the pixel value as we
have seen it can be anything between 0
to 1 you can scale it between 0 to 1 or
0 to 256 whichever way Z being black and
256 being white and then all the other
colors in between so that is the input
so this these are numerical values this
multiplication or the product W ixi is a
numerical value and the bias is also a
numerical value we need to keep in mind
that the bias is fixed for a neuron it
doesn't change with the inputs whereas
the weights are one per input so that is
one important point to be noted so but
the bias also keeps changing initially
it will again have a random value but as
a part of the training process the
weights the values of the weights W1 W2
WN and the value of B which will change
and ultimately once the training process
is complete these values are fixed for
this particular neuron W1 W2 up to WN
and plus the value of the B is also
fixed for this particular neuron and in
this way there will be multiple neurons
and each there may be multiple levels of
neurons here and that's the way the
training process works so this is
another example of multi-layer so there
are two hidden layers in between and
then you have the input layer values
coming from the input layer then it goes
through multiple layers hidden layers
and then there is an output layer and as
you can see there are weights and biases
for each of these neurons in each layer
and all of them gets keeps changing
during the training process and at the
end of the training process all these
weights have a certain value and that is
a trained model and those values will be
fixed once the training is completed all
right then there is something known as
activation function neural networks
consists of one of the components in
neural networks is activation function
and every neuron has an activation
function and there are different types
of activation functions that are used it
could be a relu it could be sigmoid and
so on and so forth and the activation
function is what decides whether a
neuron should be fired or not so whether
the output should be zero or one is
decided by the activation function and
the activation function in turn takes
the input which is the weighted sum
remember we talked about wixi + B that
weighted sum is fed as a input to the
activation function and then the output
can be either a zero or a one and there
are different types of activation
functions which are covered in an
earlier video you might want to watch
all right so as a part of the training
process we feed the inputs the labeled
data or the training data and then it
gives an output which is the predicted
output by the network which we indicate
as y hat and then there is a labeled
data because we for supervised learning
we already know what should be the
output so that is the actual output and
in the initial process before the
training is complete obviously there
will be error so that is measured by
what is known as the cost function so
the difference between the predicted
output and the actual output is the
error and U the cost function can be
defined in different ways there are
different types of cost functions so in
this case it is like the average of the
squares of the error so and then all the
errors are added which can sometimes be
called as sum of squares sum of square
errors or ssse and that is then fed as a
feedback in what is known as backward
propagation or back propagation and that
helps in the network adjusting the
weights and biases and so the weights
and biases get updated till this value
the error value or the cost function is
minimum now there is a optimization
technique which is used here called
gradient descent optimization and this
algorithm Works in a way that the error
which is the cost function needs to be
minimized so there's a lot of
mathematics that goes behind this for
example they find the uh local Minima
the global Minima using the
differentiation and so on and so forth
but the idea is this so as a training
process as the as the part of training
the whole idea is to bring down the
error which is like let's say this is
the function the cost function at
certain levels it is very high the cost
value of the cost function the output of
the cost function is very high so the
weights have to be adjusted in such a
way and also the bias of course that the
cost function is minimized so there is
this optimization technique called
gradient descent that is used and this
is known as the learning rate now
gradient descent you need to specify
what should be the learning rate and the
learning rate should be optimal because
if you have a very high learning rate
then the optimization will not converge
because at some point it will cross over
to the side on the other hand if you
have very low learning rate then it
might take forever to convert so you
need to come up with the optimum value
of the learning rate and once that is
done using the gradient descent
optimization the error function is
reduced and that's like the end of the
training process all right so this is
another view of gradient descent so this
is how it looks this is your cost
function the output of the cost function
and that has to be minimized using
gradient descent Al gorithm and these
are like the parameters and weight could
be one of them so initially we start
with certain random values so cost will
be high and then the weights keep
changing and in such a way that the cost
function needs to come down and at some
point it may reach the minimum value and
then it may increase so that is where
the gradient descent algorithm decides
that okay it has reached the minimum
value and it will kind of try to stay
here this is known as the GL Global
Minima now sometimes these curves may
not be just for explanation purpose this
has been drawn in a nice way but
sometimes these curves can be pretty
erratic there can be some local Minima
here and then there is a peak and then
and so on so the whole idea of gradient
descent optimization is to identify the
global Minima and to find the weights
and the bias at that particular point so
that's what is gradient descent and then
this is another example so you can have
these multiple local Minima so as you
can see at this point when it is coming
down it may appear like this is a
minimum value but then it is not this is
actually the global minimum value and
the gradient desent algorithm will make
an effort to reach this level and not
get stuck at this point so the algorithm
is already there and it knows how to
identify this Global minimum and that's
what it does during the training process
now in order to implement de learning
there are multiple platforms and
languages that are available but the
most common platform nowadays is tensor
flow and so that's the reason we have uh
this tutorial we have created this
tutorial for tensorflow so we will take
you through a quick demo of how to write
a tensorflow code using Python and
tensorflow is uh an open source platform
created by Google so let's just take a
look at the details of tens oflow and so
this is a
a library a python Library so you can
use python or any other languages it's
also supported in other languages like
Java and R and so on but python is the
most common language that is used so it
is a library for developing deep
learning applications especially using
neural networks and it consists of
primarily two parts if you will so one
is the tensors and then the other is the
graphs or the flow that's the way the
name that's the reason for this kind of
a name called tensorflow so what are
tensors tensors are like
multi-dimensional arrays if you will
that's one way of looking at it so
usually you have a one-dimensional array
so first of all you can have what is
known as a scalar which means a number
and then you have a onedimensional array
something like this which means this is
like a set of numbers so that is a
onedimensional array then you can have a
two-dimensional array which is like a
matrix and and beyond that sometimes it
gets difficult so this is a
three-dimensional array but tens oflow
can handle many more Dimensions so it
can have multi-dimensional arrays that
is the strength of tensor flow and which
makes computation deep learning
computation much faster and that's the
reason why tensor flow is used for
developing deep learning applications so
tensorflow is a deep learning tool and
this is the way it works so the data
basically flows in the the form of
tensors and the way the programming
works as well is that you first create a
graph of how to execute it and then you
actually execute that particular graph
in the form of what is known as a
session we will see this in the tensor
flow code as we move forward so all the
data is managed or manipulated in
tensors and then the processing happens
using these graphs there are certain
terms called like for example ranks of a
tensor the rank of a tensor is like a
dimensional dimensionality in a way so
for example if it is scalar so there is
just a number just one number the rank
is supposed to be zero and then it can
be a
one-dimensional vector in which case the
rank is supposed to be one and then you
can have a
two-dimensional Vector typically like a
matrix then in that case we say the rank
is two and then if it is a
threedimensional array then it rank is
three and so on so it can have more than
three as well so it is possible that you
can store multi-dimensional arrays in
the form of tensors so what are some of
the properties of tensor flow I think
today it is one of the most popular
platform torf flow is the most popular
deep learning platform or Library it is
open source it's developed by Google
developed and maintained by Google but
it is open source one of the most
important things about tensorflow is
that it can run on CPUs as well as gpus
GPU is a graphical Processing Unit just
like CPU is central processing unit now
in earlier days GPU was used for
primarily for graphics and that's how
the name has come and one of the reasons
is that it cannot perform generic
activities very efficiently like CPU but
it can perform iterative actions or
computations extreme extremely fast and
much faster than a CPU so they are
really good for computational activities
and in deep learning there is a lot of
iterative computation that happens so in
the form of matrix multiplication and so
on so gpus are very well suited for this
kind of computation and tensorflow
supports both GPU as well as CPU and
there's a certain way of writing code in
tensorflow we will see as we go into the
code and of course tensorflow can be
used for traditional machine learning as
well
but then that would be an Overkill but
just for understanding it may be a good
idea to start writing code for a normal
machine learning use case so that you
get a hang of how tensorflow code works
and then you can move into neural
networks so that is um just a suggestion
but if you're already familiar with how
tensor flow works then probably yeah you
can go straight into the neural networks
part so in this tutorial we will take
the use case of of recognizing
handwritten digits this is like a hollow
world of deep learning and this is a
nice little Ms database is a nice little
database that has images of handwritten
digits nicely formatted because very
often in deep learning and neural
networks we end up spending a lot of
time in preparing the data for training
and with amness database we can avoid
that you already already have the data
in the right format which can be
directly used for training and amnest
also offers a bunch of buil-in utility
functions that we can straight away use
and call those functions without
worrying about writing our own functions
and that's one of the reasons why mes
database is very popular for training
purposes initially when people want to
learn about deep learning and tensor
flow this is the database that is used
and it has a collection of 70,000
handwritten digits and a large part of
them are for training then you have test
just like in any machine learning
process and then you have validation and
all of them are labeled so you have the
images and their label and these images
they look somewhat like this so they are
handwritten images collected from a lot
of individuals people have these are
samples written by human beings they
have handwritten these numbers these
numbers going from 0 to 9 so people have
written these numbers and then the
images of those have been taken and
formatted in such a way that it is very
easy to handle so that is Ms database
and the way we are going to implement
this in our tensorflow is we will feed
this data especially the training data
along with the label information and uh
the data is basically these images are
stored in the form of the pixel
information as we have seen in one of
the previous slides all the images are
nothing but these are pixels so an image
is nothing but an arrangement of pixels
and the value of the pixel either it is
lit up or it is not or in somewhere in
between that's how the images are stored
and that is how they are fed into the
neural network and for training once the
network is trained when you provide a
new image it will be able to identify
within a certain error of course and for
this we will use one of the simpler
neural network configurations called
softmax and for Simplicity what we will
do is we will flatten these pixels so
instead of taking them in a
two-dimensional arrangement we just
flatten them out so for example it
starts from here it is a 28 by 28 so
there are 784 pixels so pixel number one
starts here it goes all the way up to 28
then 29 starts here and goes up to 56
and so on and the pixel number 784 is
here so we take all these pixels flatten
them out and feed them like one single
line into our neural network and this is
a what is known as a softmax layer what
it does is once it is trained it will be
able to identify what digit this is so
there are in this output layer there are
10 neurons each signifying a digit and
at any given point of time when you feed
an image only one of these 10 neurons
gets activated so for example if this is
strained properly and if you feed a
number nine like this then this
particular neuron gets activated so you
get an output from this neuron let me
just use uh a pen or a laser to show you
here okay so you're feeding a number
nine let's say this has been trained and
now if you're feeding a number nine this
will get activated now let's say you
feed one to the trained Network then
this neuron will get activated if you
feed two this neuron will get activated
and so on I hope you get the idea so
this is one type of a neural network or
an activation function known as softmax
layer so that's what we will be using
here this is one of the simpler ones for
quick and easy understanding so this is
how the code would look we will go into
our lab environment in the cloud and uh
we will show you there directly but very
quickly this is how the code looks and
uh let me run you through briefly here
and then we will go into the Jupiter
notebook where the actual code is and we
will run that as well so as a first step
first of all we are using python here
and that's why the syntax of the
language is Python and the first step is
to import the tensor flow Library so and
we do this by using this line of code
saying import tensor flow as TF TF is
just for convenience so you can name
give any name and once you do this TF is
tens flow is available as an object in
the name of TF and then you can run its
uh methods and accesses its attributes
and so on and so forth and Ms data this
is actually an integral part of
tensorflow and that's again another
reason why we as a first step we always
use this example Mist database example
so you just simply import mnist database
as well using this line of code and you
slightly modify this so that the labels
are in this format what is known as one
hot true which means that the label
information is stored like an array and
uh let me just uh use the pen to show
what exactly it is so when you do this
one hot true what happens is each label
is stored in the form of an array of 10
digits and let's say the number is uh 8
okay so in this case all the remaining
values there will be a bunch of zeros so
this is like array at position zero this
is at position one position two and so
on and so forth let's say this is
position 7 then this is position 8 that
will be one because our input is eight
and again position 9 will be zero okay
so one hot encoding this one hot
encoding true will kind of load the data
in such a way that the labels are in
such a way that only one of the digits
has a value of one and that indicate So
based on which digit is one we know what
is the label so in this case the eighth
position is one therefore we know this
sample data the value is eight similarly
if you have a two here let's say then
the labeled information will be somewhat
like this so you have your labels so you
have this as zero the zeroth position
the first position is also zero the
second position is one because this
indicates number two and then you have
third as zero and so on okay so that is
the significance of this one hot true
all right and then we can check how the
data is uh looking by displaying the the
data and as I mentioned earlier this is
pretty much in the form of digital form
like numbers so all these are like pixel
Valu so you will not really see an image
in this format but there is a way to
visualize that image I will show you in
a bit and uh this tells you how many
images are there in each set so the
training there are 55,000 images in
training and in the test set there are
10,000 and then validation there are
5,000 so altogether there are 70,000
images all right so let's uh move on and
we can view the actual image by uh using
the matplot flip library and this is how
you can view this is the code for
viewing the images and you can view them
in color or you can view them in Gray
scale so the cmap is what tells in what
way we want to view it and what are the
maximum values and the minimum values of
the pixel values so these are the Max
and minimum values so of the pixel
values so maximum is one because this is
a scaled value so one means it is uh
White and zero means it is black and in
between is it can be anywhere in between
black and white and the way to train the
model there is a certain way in which
you write your tensorflow code and um
the first step is to create some
placeholders and then you create a model
in this case we will use the softmax
model one of the simplest ones and um
placeholders are primarily to get the
data from outside into the neural
network so this is a very common
mechanism that is used and uh then of
course you will have variables which are
your you remember these are your weights
and biases so for in our case there are
10 neurons and and uh each neuron
actually has
784 because each neuron takes all the
inputs if we go back to our slide here
actually every neuron takes all the 784
inputs right this is the first neuron it
has it receives all the 784 there is a
second neuron this also receives all the
78 so each of these inputs needs to be
multiplied with the weight and that's
what we are talking about here so these
are this is a a m matx of
784 values for each of the neurons and
uh so it is like a 10 by 784 Matrix
because there are 10 neurons and uh
similarly there are biases now remember
I mentioned bias is only one per neuron
so it is not one per input unlike the
weights so therefore there are only 10
biases because there are only 10 neurons
in this case so that is what we are
creating a variable for biases so this
is uh something little new in tensor
flow you will see unlike our regular
programming languages where everything
is a variable here the variables can be
of three different types you have
placeholders which are primarily used
for feeding data you have variables
which can change during the course of
computation and then a third type which
is not shown here are constants so these
are like fixed numbers all right so in a
regular programming language you may
have everything as variables or at the
most variables and constants but in tens
oflow you have three different types
placeholders variables and constants and
then you create what is known as a graph
so tensorflow programming consists of
graphs and tensors as I mentioned
earlier so this can be considered
ultimately as a tensor and then the
graph tells how to execute the whole
implementation so that the execution is
stored in the form of a graph and in
this case what we are doing is we are
doing a multiplication TF you remember
this TF was created as a tensorflow
object here one more level one more so
TF is available here now tensor flow has
what is known as a matrix multiplication
or mmal function so that is what is
being used here in this case so we are
using the matrix multiplication of
tensor flow so that you multiply your
input values x with W right this this is
what we were doing xw + B you're just
adding B and this is in very similar to
one of the earlier slides where we saw
Sigma XI wi so that's what we are doing
here matrix multiplication is
multiplying all the input values with
the corresponding weights and then
adding the bias so that is the graph we
created and then we need to Define what
is our loss function and what is our
Optimizer so in this case we again use
the tensor flows apis so tf. NN softmax
cross entropy with logits is the uh API
that we will use and reduce mean is what
is like the mechanism whereby which says
that you reduce the error and Optimizer
for doing deduction of the error what
Optimizer are we using so we are using
gradient descent Optimizer we discussed
about this in couple of slides uh
earlier and for that you need to specify
the learning rate you remember we saw
that there was a a slide somewhat like
this and then you define what should be
the learning rate how fast you need to
come down that is the learning rate and
this again needs to be tested and tried
and to find out the optimum level of
this learning rate it shouldn't be very
high in which case it will not converge
or shouldn't be very low because it will
in that case it will take very long so
you define the optimizer and then you
call the method minimize for that
Optimizer and that will Kickstart the
training process and so far we've been
creating the graph and in order to
actually execute that graph we create
what is known as a session and then we
run that session and once the training
is completed we specify how many times
how many iterations we want it to run so
for example in this case we are saying
Thousand Steps so that is a exit
strategy in a way so you to specify the
exit condition so a training will run
for th000 iterations and once that is
done we can then evaluate the model
using some of the techniques shown here
so let us get into the code quickly and
see how it works so this is our Cloud
environment now you can install
tensorflow on your local machine as well
I'm showing this demo on our existing
Cloud but you can also install denlow on
your local machine and uh there is a
separate video on how to set up your Tor
flow environment you can watch that if
you want to install your local
environment or you can go for other any
cloud service like for example Google
Cloud Amazon or Cloud Labs any of these
you can use and U run and try the code
okay so it has got
started we will log in
all right so this is our deep learning
tutorial uh
code and uh this is our tensorflow
environment and uh so let's get started
the first we have seen a little bit of a
code walk through uh in the slides as
well now you will see the actual code in
action so the first thing we need to do
is import tensorflow and then we will
import the data and we need to to adjust
the data in such a way that the one hot
is encoding is set to True one hot
encoding right as I explained earlier so
in this case the label values will be
shown appropriately and if we just check
what is the type of the data so you can
see that this is a data sets python data
sets and if we check the number of
images the way it looks so this is how
it looks it is an array of type float 32
similarly the number if you want to see
what is the number
of training images there are 55,000 then
there are test images 10,000 and then
validation images 5,000 now let's take a
quick look at the data itself
visualization so we will use um matte
plot lip for this and um if we take a
look at the shape now shape gives us
like the dimension the tensors or or or
the arrays if you will so in this case
the training data set if we see the size
of the training data set using the
method shape it says there are 55,000
and 55,000 by 784 so remember the 784 is
nothing but the 28 by 28 28 into 28 so
that is equal to 784 so that's what it
is uh showing now we can take just uh
one image and just see what is the the
first image and see what is the shape so
again size obviously it is only 784
similarly you can look at the image
itself the data of the first image
itself so this is how it it shows so
large part of it will probably be zeros
because as you can imagine in the image
only certain areas are written rest is U
blank so that's why you will mostly see
zeros either it is black or white but
then there are these values are so the
values are actually they are scaled so
their values are between 0er and one
okay so this is what you're seeing so
certain locations there are some values
and then other locations there are zeros
so that is how the data is stored and
loaded if we want to actually see what
is the value of the handwritten image if
you want to view it this is how you view
it so you create like do this reshape
and um matplot lib has this um feature
to show you these images so we will
actually use the function called um IM
am show and then if you pass this
parameters appropriately you will be
able to see the different images now I
can change the values in this position
so which image we are looking at right
so we can say if I want to see what is
there in maybe
5,000 right
so 5,000 as three similarly you can just
say five what is in five five as eight
what is in
[Music]
50 again H so basically by the way if
you're wondering uh how I'm executing
this code shift enter in case you're not
familiar with Jupiter notebooks shift
enter is how you execute each cell
individual cell and if you want to
execute the entire program you can go
here and say run all so that is
how this code gets executed and um here
again we can check what is the maximum
value and what is the minimum value of
this pixel values as I mentioned this is
it is scaled so therefore it is between
the values lie between one and zero now
this is where we create our
model the first thing is to create the
required placeholders and variables and
that's what we are doing here as we have
seen in the slides so we create one
placeholder and we create two variables
which is for the weights and biases
these two variables are actually
matrices so each variable has 784 by 10
values okay so one for this 10 is for
each neuron there are 10 neurons and 784
is for the pixel values inputs that are
given which is 28 into 28 and the biases
as I mentioned one for each neuron so
there will be 10 biases they are stored
in a variable by the name b and this is
the graph which is basically the
multiplication of these matrix
multiplication of X into W and then the
bias is added for each of the neurons
and the whole idea is to minimize the
error so let me just execute I think
this code is executed then we Define
what is our the Y value is basically the
label value so this is another
placeholder we had X as one placeholder
and Yore true as a second placeholder
and this will have values in the form of
uh 10 digigit 10 digigit uh arrays and
uh since we said one hot encoded the
position which has a one value indicates
what is the label for that particular
number all right then we have cross
entropy which which is nothing but the
loss loss function and we have the
optimizer we have chosen gradient
descent as our Optimizer then the
training process itself so the training
process is nothing but to minimize the
cross entropy which is again nothing but
the loss function so we Define all of
this in the form of a graph so the up to
here remember what we have done is we
have not exactly executed any any tens
oflow code till now we are just
preparing the graph the execution plan
that's how the tens oflow code works so
the whole structure and format of this
code will be completely different from
how we normally do programming so even
with people with programming experience
may find this a little difficult to
understand it and it needs quite a bit
of practice so you may want to view this
uh video also maybe a couple of times to
understand this flow because the way
tensor flow programming is done is
slightly different from the normal
programming some of you who let's say
have done uh maybe spark programming to
some extent will be able to easily
understand this uh but even in spark the
the programming the code itself is
pretty straightforward behind the scenes
the execution happens slightly
differently but in tens oflow even the
code has to be written in a completely
different way so the code doesn't get
executed in the same way as you have
written so that that's something you
need to understand and a little bit of
practice is needed for this so so far
what we have done up to here is creating
the variables and feeding the variables
and um or rather not feeding but setting
up the variables and uh the graph that's
all defining maybe the uh what kind of a
network you want to use for example
example we want to use softmax and so on
so you have created the variables have
to load the data loaded the data viewed
the data and prepared everything but you
have not yet executed anything in tens
of flow now the next step is the
execution in tens of flow so the first
step for doing any execution in tensor
flow is to initialize the variables so
anytime you have any variables defined
in your code you have to wrun R this
piece of code always so you need to
basically create what is known as a a
node for initializing so this is a node
you still are not yet executing anything
here you just created a node for the
initialization so let us go ahead and
create that and here onwards is where
you will actually execute your code uh
intensive flow and in order to execute
the code what you will need is a session
tensor flow session so TF do session
will give you a session and there are a
couple of different ways in which you
can do this but one of the most common
methods of doing this is with what is
known as a WID Loop so you have a withd
tf. session as SS and with a uh colon
here and this is like a block starting
of the block and these indentations tell
how far this block goes and this session
is valid till this block gets executed
so that is the purpose of creating this
width block this is known as a width
block so with tf. session as CS you say
cs. run in it now cs. run will execute a
node that is specified here so for
example here we are saying SS do run
sess is basically an instance of the
session right so here we are saying tf.
session so an instance of the session
gets created and we are calling that
sess and then we run a node within that
one of the nodes in the graph so one of
the nodes here is in it so we say run
that particular node and that is when
the initialization of the variables
happens now what this does is if you
have any variables in your code in our
case we have W is a variable and B is a
variable so any variables that we
created you have to run this code you
have to run the initialization of these
variables otherwise you will get an
error okay so that is the that's what
this is doing then we within this width
block we specify a for Loop and we are
saying we want the system to iterate for
thousand steps and perform the
training that's what this for Loop does
run training for,
iterations and what it is doing
basically is it is fetching the data or
these images remember there are about
50,000 images but it cannot get all the
images in one shot because it will take
up a lot of memory and performance
issues will be there so this is a very
common way of Performing deep learning
training you always do in batches so we
have maybe 50,000 images but you always
do it in batches of 100 or maybe 500
depending on the size of your system and
so on and so forth so in this case we
are saying okay get me 100 uh images at
a time and get me only the training
images remember we use only the training
data for training purpose and then we
use test data for test purpose you must
be familiar with machine learning so you
must be aware of this but in case you
are not in machine learning also not
this is not specific to deep learning
but in machine learning in general you
have what is known as training data set
and test data set your available data
typically you will be splitting into two
parts and using the training data set
for training purpose and then to see how
well the model has been trained you use
the test data set to check or test the
validity or the accuracy of the model so
that's what we are doing here and You
observe here that we are actually
calling an mest function here so we are
saying mnist train m. next batch right
so this is the advantage of using mes
database because they have provided some
very nice helper functions which are
readily available otherwise this
activity itself we would have had to
write a piece of code to fetch this data
in batches that itself is a a lengthy
exercise so we can avoid all that if we
are using amness database and that's why
we use this for the initial learning
phase okay so when we say fetch what it
will do is it will fetch the images into
X and the labels into Y and then you use
this batch of 100 images and you run the
training so cs. run basically what we
are doing here is we are running the
training mechanism which is nothing but
it passes this through the neural
network passes the images through the
neural network finds out what is the
output and if the output obviously the
initially it will be wrong so all that
feedback is given back to the neural
network and thereby all the W's and Bs
get updated till it reaches th000
iterations in this case the exit
criteria is th000 but you can also
specify probably accuracy rate or
something like that for the as an exit
criteria so here it is it just says that
okay this particular image was wrongly
predicted so you need to update your
weights and biases that's the feedback
given to each neur on and that is run
for thousand iterations and typically by
the end of this thousand iterations the
model would have learned to recognize
these handwritten images obviously it
will not be 100% accurate okay so once
that is done after so this happens for
thousand iterations once that is done
you then test the accuracy of these
models by using the test data set right
so this is what we are trying to do here
the code may appear a little complicated
because if you're seeing this for the
first time you need to understand uh the
various methods of tensor flow and so on
but it is basically comparing the output
with what has been what is actually
there that's all it is doing so you have
your test data and uh you're trying to
find out what is the actual value and
what is the predicted value and seeing
whether they are equal or not TF do
equal right and how many of them are
correct and so on and so forth and based
on that the accuracy is uh calculated as
well so this is the accuracy and uh that
is what we are trying to see how
accurate the model is in predicting
these uh numbers or these digits okay so
let us run this this entire thing is in
one cell so we will have to just run it
in one shot it may take a little while
let us see and uh not bad so it has
finished the thousand iterations and
what we we see here as an output is the
accuracy so we see that the accuracy of
this model is around
91% okay now which is pretty good for
such a short exercise within such a
short time we got 90% accuracy however
in real life this is probably not
sufficient so there are other ways in to
increase the accuracy we will see
probably in some of the later tutorial
how to improve this accuracy how to
change maybe the hyper parameters like
number of neurons or number of layers
and so on and so forth and uh so that
this accuracy can be increased Beyond
90% so what are confusion metrics and
why do we need them so in machine
learning classification is used to
divide data into different categories
but after we have clean prepared the
data and trained our model how we can
tell if the model is performing well so
there is where confusion Matrix come in
handy so a confusion metric is a tool
that helps measure the performance of a
classifier in detail it goes beyond just
giving an overall accuracy score it
shows exactly where the model is making
mistake and how it is classifying each
category for example if there is an
imbalance in the data where one category
has many more instances than other the
model might predict the majority
category most of the time and still show
a high accuracy but this wouldn't be
helpful because the model is not
accurately predicting the minority
categories the confusion matrics help
visualize these outcome by showing a
table of all the predicted and the
actual values allowing you to see where
the model is performing well and where
it needs Improvement so moving forward
let's see confusion matrices parts or
how to create 2x two Matrix so we can
get four different outcomes when
comparing the predict actual values from
a classifier the first one is true
positive
this is when the model correctly
predicts a positive outcome you
predicted positive and it was actually
positive the second one is false
positive this happens when the model
incorrectly predicts a positive outcome
for something that is actually negative
or a predictive positive but it was
actually negative the third one true
negative this is when the model
correctly predicts a negative outcome
you predicted negative and it was
actually negative and the fourth one
false negative
this occurs when the model incorrectly
predicts a negative outcome for
something that is actually positive C
predictive negative and but it was
actually positive so moving forward
let's see some matrices of confusion
Matrix so this is a graph so we know
there are four parts true positive false
positive false negative and true
negative so this one is true positive
this one is false positive this one is
false negative this one is true negative
so just consider a confusion Matrix made
for a classifier that classify People
based on whether they speaks English or
they speak Spanish okay from this
diagram you can see that fine so these
are some values TP FN FP TN there's
nothing two positive two negative like
this just from looking at the Matrix the
performance of our model is not very
clear so to find out how accurate our
model is we use the following matrices
the first one is
accuracy so this is the formula of
accuracy TP means true positive means
this 86 value so the accuracy is used to
find the portion of correctly classified
values it tells us how often our
classifier is right it is the sum of all
the True Values divided by total values
so in this case this is
88% okay 86 + 79 2 positive + 2 negative
divided by this it will come
8.23 okay and the second one is
precision so Precision is used to
calculate the model ability to classify
positive value correctly it is the true
positive divided by the total number of
predicted positive value so this is the
formula it is true positive divided by
true positive plus false positive so in
this case the value will come
87.7% okay and the third one is recall
so recall is used to calculate the model
ability to predict positive values how
often does the model predict the currect
positive values so it is true positives
divided by the total number of actual
positive values so in this case the
value is
88.9 okay and the last one is F1 score
so it is the harmonic mean of recall and
precision it is useful when you need to
take both precision and recall into an
account so in this case the value is
88.7% so now let's jump into the demo
part and see how we can get these values
accuracy precision recall and F1 score
from our data set using handsome so I'm
performing this in Google collab so
first I will rename it to
confusion
Matrix okay so yes let's start so first
we will import some basic libraries of
python like numpy and the pandas so here
I like import numpy as
NP and
import pandas aspd why I'm writing this
NP and PD because I don't want to write
again and again nay and pandas okay this
is a short form NP so everyone knows I
guess npay and pandas because these are
the basic Library so napai is used for
mostly linear algebra stuff and pandas
is used for the creating data frames
data processing or if you want to read
files CSP files like that okay yeah so
now let's import our data okay so data
equals to PD do
Dore CSV then file name data. CSV so I
will upload this file on the description
box below you guys can check out from
there okay yes so let okay what is
saying no suchar file or directory on
data or CSV
I guess it's
there okay it's not let me import it
okay
yeah okay now file is there yeah it's
working fine so now let's
check okay F
Ros yeah so this is our data you can see
we have columns withit data dot
columns okay so we have this 32 columns
in our data set ID diagnosis JS means
texture means parameter mean so I just
simply you know talk this data there is
nothing related to confusion Matrix and
all okay we have to just we can perform
you know confusion Matrix to you know
any data so head is used for displaying
top five rows of of our data set okay if
I will write data simply you can see the
full data okay with 33 columns and all
the rows right so uh now we don't need
this ID
and one is this unnamed data because
there is no
values okay so what I will do I will
write data Dot
drop
ID comma okay let me copy it because I
don't want any spelling
mistake
yeah copy paste here okay let me write
XIs equal to
1 then comma in place equals to
True okay that mean run it okay if you
will check data do head
now I guess there will be no idea all
see there is now no ID and all okay this
we remove that LGE column as well un
name
32 okay so now what I have to
do I will write here data
dot
diagnosis = to 1 if each was equals to
M okay
lse
zero
for each in
data
dot
diagnosis
diagnosis yeah so if you can see there
is mmm value and there is you know other
values as well so for M I'm putting one
for for the rest I will put
zero okay let's check
again yeah okay you can see it's done
replacement is done
basically okay so let's check the info
of the
data so these are some basic uh what to
the functions of python so these are the
column these are the non Nal values
these are the D values okay float we
have or int we have okay so we have only
two t type of data sets so now what we
will do we will do normalization okay y
equals to data do
diagonosis
diagnosis dot
values okay so here I will write xcore
data equals to
data.
drop
diagnosis
comma
XIs = to
1 okay here I will write xal
to
xcore data
minus NP dot minimum value and the
xcore
data and divided by so I put using that
formula if you guys know okay divided
by
NP so I'm basically what I'm finding is
accuracy okay I'm using that formula NP
Dot
Max of X for
data then minus
NP dot main value then xcore
data
okay okay some okay access Valu is wrong
sorry spelling is wrong
now fine yes it's fine so now we will do
train test split so what is stain test
split okay so first let me
route underscore
selection
import
train test split okay so what is train
test split so what we do
we you know split our data set into
training and the testing mostly we train
and the some part of data we test okay
so here I will set the ratio so xcore
train comma
xcore
test comma ycore
train comma ycore
test equals to
train test
split then X comma y comma
testore
size equals to
0.15
means 85% we are on the training and the
15% we are on the testing part okay
comma
random State equals to 42 okay let me
run it it's working fine yeah it's
working fine so now let's see the
accuracy from the random Forest so I
will write
from skar
dot
emble
import random
random Forest
classifier okay then RF equals
to random for is classifier and
ncore
estimators equals to
100 and here I will set random state
to one okay then RF this is a short form
of om Forest do
fit
fit xcore
train comma Yore
train then
print random Forest
score comma RF
doore then xcore test comma y underscore
test okay now let's check how much the
accuracy so accuracy is coming
95% okay which is very good so now what
I will do I will create confus confusion
Matrix now
Yore
prediction equals to RF dot
predict xcore
test okay then y underscore true equals
to
Yore
test okay so here now let's create the
array
skar
dot
matrix import
confusion
Matrix okay then I will write CM short
form of confusion
metric here I will write y
true comma y
prediction then confusion
Matrix so this is our array okay true
positive and this is
false negative and this is false
positive like this okay so now let's
create the visualization of confusion
Matrix okay so here I will write
import
cbor as
SNS and
import M plot
lip do
pyplot as PLT
PLT means plot okay the short form F
comma ax =
to PLT do
subplots let's give the figure
size equals to 5A
5 then SNS do heat map
then confusion Matrix comma n equals to
true then comma line
width equal to
0.5 comma line
color equals to let's give
red
then
fmt equal
to I don't need any
any you know uh that decimal
values comma AXS to
X okay then PLT
dot X
label y underscore
prediction then PLT
do y
label then y underscore
true add the okay forgot to add
this Braes then PLT do
show yeah so as you can see this is the
Y True Values and this is the Y
prediction values okay this is the heat
map we have created so this is how you
can visualize your confusion Matrix fine
[Music]
learning objectives welcome to math
refresher probability and
statistics in this lesson we are going
to explain the concepts of statistics
and
probability describe conditional
probability Define the chain rule of
probability discuss the measure of
variance identify the types of gsan
distribution
basic of statistics and
probability probability and statistics
data science relies heavily on estimates
and predictions a significant portion of
data science is made up of evaluations
and
forecast statistical methods are used to
make estimates for further analysis
probability theory is helpful for making
predictions statistical methods are
highly dependent on probability Theory
and all probability in statistics are
dependent on data data is information
acquired for reference or research via
observations facts and measurements data
is a set of facts structured in the form
that computers can interpret such as
numbers words estimations and Views
importance of data data AIDS in seeing
more about the information by
identifying possible connections between
two features data assists in the
detection of distortion by uncovering
hidden patterns based on prior
information patterns data may be
utilized to anticipate the future or
predict the current state of affairs
also data AIDS in determining whether
two pieces of information have any
instance in common or not types of data
data might be quantitative that is data
that can be measured or counted in
numbers or it may be qualitative which
is data which is generally divided into
groups or in simpler Words which cannot
be counted or measured in numbers let's
consider an example a customer
information data of a bank may contain
quantitative and qualitative data
consider this snapshot where we have
customer ID surname geography gender age
balance has CR card is active member
amongst these variables we can see
surname is mostly quality ative as it
cannot be counted and measured in
numbers geography and gender are also
qualitative as they cannot be counted in
numbers and are mostly groups has Seer
card that is has credit card and his
active member although are containing
numerical in form but these are
categorical that means these have been
divided into groups of one and zero that
represent yes and no as an answer hence
these two variables are also
qualitative customer ID is again
although a numerical data however the
significance or intuition behind
customer ID is
categorical hence it may be kept in the
qualitative data also however age and
balance these are numerical information
which have been measured or counted and
numerical operations can be performed on
them hence these are Under quantitative
data categories introduction to
descriptive statistics descriptive
statistics a descriptive measurement is
summary measure that quantitatively
portrays the most important features of
a set of data allowing for a better
comprehens of the information data can
be measured as different levels the
levels of measurement describe the
nature of information stored in the data
assigned to the variables qualitative
data can be measured as nominal or
ordinal quantitative data can be
measured in terms of interval and ratio
type nominal data the data is
categorized using names labels or
qualities for example brand name zip
code and gender ordinal data can be
arranged in order or ranked and can be
compared examples include grades Star
reviews position and race and date
interval data is the data that is
ordered and has meaningful differences
between the data points example
temperature in celsius and year of birth
ratio data is similar to the interval
level with the added property of
inherent zero mathematical calculations
can be performed on both interval as
well as ratio data for example height
age and weight population versus sample
before analyzing the data it's important
to figure out if it's from a population
or a sample population is a collection
of all available items as well as each
unit in our study sample is a subset of
the population that contains only a few
units of the
population population data is used for
study when the data pool is very small
and can give all the required
information samples are collected
randomly and represent the entire
population in the best possible way
measures of central tendency
the central tendency is a single value
that aids in the description of the data
by determining its Center position
measures of central tendency are
sometimes known as summary statistics or
measures of central
location the most popular measurements
of central tendency are mean median and
mode the normal distribution is a
bell-shaped symmetrical distribution in
which mean median and mode all are equal
the curve over here shows the
bell-shaped curve or the normal
distribution of variable X the point
over here that is X1 is the point which
represents the mean median and mode of
this distribution mean mean is
calculated by dividing the sum of all
data values by the total number of data
values it gets affected when there are
unusual or extreme values it is
sensitive to the outliers mean can be
calculated as summation over all the
values of X in a collection divided by
the size of the
collection for example we have a
collection where we have values as 7 3 4
1 6 and
7 we find out the sum of these values
which is 28 and there are total of six
values so 28 / 6 gives us a mean value
of
4.66
median it is the middle value in the set
of the data that has been sorted in
ascending
order it is a better alternative to mean
since it is less impacted by outliers
and
skewness it is closer to the actual
Central
value median is calculated differently
for different sizes of
data differentiated as if the total
number of values is odd or if the total
number of values is even
if the size of the data is odd for
example in this case we have five
elements after sorting whatever middle
value we
get that means n + 1 by twoth term in
this
case 5 + 1 /
2 that is the third term which is four
is the median
value in case when the total number of
values is is even like here there are
six values the average or the mean of
the two Central values is considered as
the median in this case the median is
the mean of 6 and four which is five
mode mode represents the most common
value in the data set it is not at all
affected by extreme
observations it is the best measure of
central tendency for highly skewed or
non normal
distribution mode for categorical data
is determined by estimating the
frequencies for each
categories and then the category with
the highest frequency is considered to
be
mode like in this case s has the highest
frequency hence seven becomes the mode
value however in case of continuous data
or quantitative data the calculation of
mode is slightly different the first
step in calculation of mode
is dividing the data into classes which
are equal with then getting the
frequency of data points lying in within
that range of classes and finally
selecting the class with the highest
frequency using the range of that class
and the frequencies we can get the final
mode
value using the formula
l+ fmus fub1 multiplied to h / by fmus
fub1 Plus FM minus
F2 here L is the lower limit or the
lower observation of the mode class H is
the size of the mode
class FM is the frequency of the mode
class F1 is the frequency of the class
proceeding to mode and F2 is the
frequency of the class succeeding to
mode this gives us the final mode
value mean versus expectation
now let's talk about mean versus
expectation so in general we use the
expected value or expectation when we
want to calculate the mean of a
probability distribution that represents
the average value we expect to occur
before collecting any data and mean on
the other hand mean is basically used
when we want to calculate the average
value of a given sample this represents
the average value of raw data that we
may have already
collected we can understand this by
using a simple
example now to calculate the expected
value of this probability distribution
we can use a specific formula from the
previous
discussion this is going to be the
expected value where X is going to be
the data value and this PX is the
probability of
value for example we could calculate the
expected value for this probability
distribution to be his
shown so here it will be 1.45
goals so this represents the expected
number of goals that the team will score
in any given
game and then if you talk about
calculating mean so we typically
calculate the mean after we have
actually collected raw
data for example suppose we record the
number of goals that a soccer team will
score in 15 different
games now to calculate the mean number
of goals scored per
game we can use the following
formula where sum of X is basically the
sum of all the goals divided by n and
the number of Records or we can save the
sample
size it is as shown on the screen
so this represents the mean number of
goals scored per game by the
team measures of
asymmetry the difference between the
three distinct curves can be studied in
this
image the central curve is the normal or
no skewness curve here mean median and
mode all lie on the same point this
normal curve is symmetrical about its
mean median and
mode that means the left hand side of
the curve is a mirror image of the right
hand side of the
curve however in case of negatively
skewed data the tail is elongated on the
left hand
side and the mean is smaller than the
mode and the median values or is on the
left hand side of the
mode hence indicating that the outliers
are in the negative
Direction on the other hand in case of
positively skewed the data is
concentrated on the left hand side of
the
curve while the tail is elongated or
longer on the right hand side of the
curve the mean is greater than the mode
and median
or is on the right hand side of the mode
and median indicating that the outliers
are in the positive
direction let's consider an
example the graph here shows the global
income distribution for the year 2003
2013 and a projection for
2035 if we see the global income
distribution statistics for 2003 it is
highly right
skewed we can observe in the previous
graph that in
2003 the mean of
$3,451 was higher than the median of
$10.90 the global income is definitely
not evenly distributed the majority of
people make less than $22,000 each year
while only a small percentage of the
population earns more than
$144,000 measures of
variability measures of
variability dispersion the measure of
central Tendencies provide a single
value that addresses the full worth
however the central tendency cannot
depict the Viewpoint entirely the metric
of dispersion helps us focus on the
inconsistency in the data spread
measures of dispersion describe the
spread of the
data the range interqual range standard
deviation and variance are examples of
dispersion
measures
range the range of distribution is the
difference between the largest and the
smallest amount of
data the range for example does not
include all of a series positive aspects
it concentrates on the most shocking
aspects and ignores that aren't
considered critical for example for a
set 13 33 45 67
70 the range is
57 that is the maximum of this which is
70 minus the minimum over here which is
13
variance variance is the average of all
square
deviations it is defined as the sum of
square distance between each point and
the mean or the dispersion around the
mean the standard deviation is used as
variance suffers from a unit
difference variance can be computed as
Sigma Square summation over x - mu^
s divided by
n where mu is the mean of the data X is
the individual data
point and N is the size of the
data this representation is for a
population
data for a sample data variance can be
computed as
xus xar whole Square
summation over it divided by n minus
one here xar is the mean of these sample
data and N is the sample size
size the units of values and variance
are not equal so another variability
measure is
used standard
deviation standard deviation is a
statistical term used to measure the
amount of variability or dispersion
around a
mean the standard deviation is
calculated as the square root of
variance it depicts the con conentration
of the data around the mean of the data
set standard deviation as indicated
previously can be computed as square
root of
variance for a population data standard
deviation Sigma can be computed as
square root of summation over X IUS mule
s /
n where mu is the mean of the data XI
are the data points and N is the size
let's consider an
example let's find out the mean variance
and standard deviation for this data the
data values are 3 5 6 9 and 10 to find
out the mean we first find the sum of
all these data
values that is 33 and divide it by the
count which is
five we get the mean of 6 6 to compute
the variance we start by Computing the
deviation that is x minus the mean of
X here three is one of the values of the
data and 6.6 is the
mean so 3 - 6.6 squ and we do
that to find out some of all the
deviations divided by the
count which is
five we end up getting an overall
variance of
6.64 standard deviation as we know is
measured at square root of variance that
is square root of
6.64 which amounts to
2576 measures of
relationship measures of relationship
covariance covariance is the measure of
joint variability of two
variables it measures the direction of
the relationship between the variables
it determines if one variable will cause
the other to alter in the same
way Co variance between variable X and Y
can be computed as summation over the
product of x i -
xar and y i - Y Bar the whole / by n
minus1 here xar and Y Bar are the mean
of X and Y
respectively the value of covariance can
range from minus infinity to a plus
infinity
correlation correlation is normalized
covariance it measures the strength of
association between two variables the
most common measure for correlation is
the Pearson correlation
coefficient correlation between two
variables X and Y can be measured with
respect to covariance as covariance
between
X and Y divided by the standard
deviation of X and standard deviation of
Y the value of correlation ranges from a
negative 1 to positive
one types of
correlation correlation can be either a
positive
correlation zero correlation or a
negative correlation
the first picture over here represents a
perfect positive
correlation we in a straight line with a
positive
slope is representing the relationship
between the two
variables zero correlation means that
the line representing the relationship
between the two variables is horizontal
to the X
AIS perfect negative correlation can can
be represented by a straight line with a
negative
slope correlation equals to 1 implies a
positive
relationship that is when one variable
increases the other variable also
increases a correlation value of
negative one implies a negative
relationship that is when one variable
increases the other
decreases the correlation coefficient of
zero shows that the variables are
completely independent of each
other let's consider an
example here we have two variables
height and
weight to compute the correlation
between height and
weight we use the correlation formula
asarian of
X and Y / standard deviation of X and
standard deviation of
Y here height is the X variable and
weight is the Y
variable first to compute covariant we
compute the x - xar and Y - y values and
then the product of
them we then compute x - xar
squ and Y - Y Bar Square values to
compute the standard deviations of
height and weight respectively
correlation as we know has been been
defined as covariant of X and I and Y
divided by standard deviations of X and
Y this can also be represented as
summation over x - xar multiplied to y -
Y
Bar divided square root of summation
over sum of squared
deviations that is x - xar squ
multiplied to square root of summation
over y - Y Bar square that is sum of
square deviations for
y now let's find out values to put into
this
formula first we find out the overall
sum of height to get the mean of height
which is
5.14 similarly we get the sum of weight
to get the mean of weight as 50 we now
get the summation over x - xar
multiplied to y - Y Bar to get the
numerator for the
formula then we compute x - xar s
summation and Y - Y Bar s that is sum of
squared deviation of X and Y
respectively now we put in the values in
this final correlation formula to get a
correlation value of
0.889 this indicates that height and
weight have a positive relationship
it is evident that as height grows
weight also
increases in this module we will be
talking about expectation and
variance so the expected value or we can
say mean of a given variable that we can
denote by X is a discrete random
variable where it is a weighted average
of the possible values that X can take
and each value is going to be according
to the probability of that specific
event occurring
so usually the expected value of x is
denoted by a simple formula where we can
Define the expectation based on the X
parameter which is going to be the sum
of each possible outcome multiplied by
the probability of the outcome
occurring so in more concrete terms the
expectation is what we would expect the
outcome of an experiment to be on
average
we can take an example for the coin if a
coin is being tossed 10 times then one
is most likely to get five heads and
five
tails same logic can be discussed if we
talk about another example of rolling a
die so there are six possible outcomes
when you roll a die 1 2 3 4 5 6 and each
of these has a probability of 1 by six
of occurring
so we can say that the expectation is
going to be 1 multiplied by the
probability of that happening which is
going to be 1X 6 + 2x 6 + 3x 6 + 4x 6 +
5x 6 + 6x 6 and that is going to give us
3.5 as an output the expected value is
3.5 so if you think about it 3.5 is
halfway between the possible values that
I in take and this is what we should
have
expected next we talk about the concept
of variance so variance of a random
variable allows us to know something
about the spread of the possible values
of the variable so for a discrete random
variable X the variances of X is going
to be denoted by using a simple formula
that is going to be varx equal e x - M
the whole Square where m is basically
the expected value of the expectation of
X so this is more like a standard
deviation of X which can also be
represented by using this formula so the
variance does not behave in the same way
as expectation when we multiply and add
constants to random
variables so now there are two different
type of variants that we can have a fair
understanding on first of all we have
low variant and then we have high
variance
so low variance simply means that there
is a small variation in the production
of the target function with changes in
the trading data set and at the same
time high variance as we can see here
High variance shows a large variation in
prediction of the target function with
changes in the trading data set so a
model that shows High variance learns a
lot and perform well with the trading
data set and it does not generalize with
the Unseen data set and that's why as a
result such a model gives good results
with training data set but shows High
error rates on the test data set and
since the high variance a model learns
Too Much from the data set it leads to
an overfitting of the model so model
with high variance will be having couple
of issues like it may lead to
overfitting or it may also lead to
increase in model
complexities next next we have
skewness so skewness in simple terms is
basically a measure of asymmetry of a
distribution so distribution is
asymmetrical when it's left and right
sides are not the mirror
images right now this is a mirrored
image and a distribution can have right
positive or we can say negative or it
can have zero
skewness so right skewed in this
scenario is B basically the distribution
is longer on the right side of its
peak and a left skew distribution is
going to be we can say where it is
longer on the left
side so we can see we have this one as a
part of right side it is more elongated
towards the right side and this one is
more elongated towards the left side so
we can think of skewness in terms of
Tails a tail is long tampering and the
end of a distribution so it simply
indicates that they are observations at
one end of the distribution but that
they are relatively infrequent so a
right skew distribution has a long tail
on the right side as you can see here so
the number supports observed let's say
we have a data on a perear basis so
again we can have a more skewness
towards the right side where data is
being dropping as we continue to
increase the number of years for example
we may have a high sales towards the
beginning of year suppose in
2022 but again as we proceed to
2023 second half we are seeing the dip
in performance so that is rightly skewed
and same way let's suppose if we started
with the sales figure it was really Less
in suppose
2002 but again as we proceeded to 2023
now our sales have been gradually
increasing so it's more like skew
towards the left side section as a part
of negative skew next we have
curtosis so curtosis is basically a
measure of the tailedness of a
distribution so tailedness is how often
the outliers occur and access curtes is
the tailedness of the distribution
related to a normal distribution so a
distribution with medium curtosis is
called as mesokurtic a distribution with
low curtosis like this one
this is called as the platic kurtic and
then distribution with high curtosis
like this one this is called as the
lepto
kurtic so Tails here they are tapering
ends on either side of a distribution
like this so they represent the
probability or the frequency of values
that are extremely high or extremely low
to the
mean in other words tals here represents
how often the outliers
occur so there are three type of
curtois we have platic kurtic which is
negative leptokurtic which is a positive
towards the upper end and then we have
mesokurtic which is a normal
distribution so mesokurtic is the medium
tail so normal distributions they have a
curtois of three so any distribution
with a curtois of a prox value of three
is going to be mesokurtic and curtosis
is described in terms of excess curtosis
which is curtosis minus 3 and since
normal distribution they have a curtosis
of three axis cures makes comparing a
distribution curtois to a normal
distribution even easier introduction to
probability probability Theory
probability is a measure of the
likelihood that an event will
occur let's consider an example of coin
toss where the chances of of getting
heads on a coin are 1 by two or
50% the probability of each given event
is between zero and 1 both inclusive sum
of an events cumulative probability
cannot be greater than
one hence the probability of an event X
lies between 0o and 1 this means that
the integral of probability of
distribution over xal to 1
conditional
probability conditional probability of
any event a is defined as the
probability of occurrence of a given
that event B has previously
occurred condition probability of event
a given B can be estimated as
probability of a intersection B that is
probability of both A and B happening
together divided by the probability of B
it is also written as that probability
of a intersection b equals to
probability of a given B multiplied to
probability of
B let's consider an
example in a coin we are doing a two
coin flip coin one gets heads Tails
heads and tails in subsequent
flips while coin 2 gets Tails heads
heads and tails in the subsequent
flips now the probability that coin one
will get a head is two out of four while
the probability that coin 2 will get
heads is again two out of
four the probability that both coin one
and coin 2 will have a heads is just one
out of the four
flips hence the probability that coin
one will get heads given that coin 2 is
already heads can be computed as
probability of coin 1 Edge intersection
coin 2
Edge that is 1x4 divided by probability
of coin 2
Edge that's a given that is 2x4 which is
going to be 0.5 or 50%
based base theorem base theorem
calculates the conditional probability
of an event based on its prior
probabilities
basically base theorem incorporates the
prior probability distribution to
predict the posterior probabilities base
theorem for conditional
probability can be expressed as
probability of a given b equals
probability of B given a divided by
probability of B multiplied to
probability of
a base theorem allows updating the
probability values by using new
information or evidence here probability
of a is known as prior probability that
is the probability of event before any
new data is collected probability of a
given B is known as the posterior
probability it is the revised
probability of an event occurring after
taking into consideration the new
information probability of B given a is
known as the likelihood and probability
of B is probability of observing an
Evidence Bay model an example consider
an example for calculating the
likelihood of having diabetes based on
frequency of fast food consumption here
is the observed data let's say the fast
food audience is 20% diabetes prevalence
is 10% and 5% is fast food and
diabetes the chances of diabetes given
fast food that is the conditional
probability of D given be
can be calculated as probability of
diabetes and fast food together divided
by probability of fast food that means
5% ided
20% that equals
25% Define an analysis can State eating
fast food increases the chance of having
diabetes by
25% the multiplication rule of
probability if events A and B are
statistically independent and
probability of a intersection B can be
given as probability of a given B
multiplied to probability of B however
probability of a intersection B is also
given as probability of a multiply to
probability of B here probability of a
given b equals to probability of a when
we assume that probability of B is non
zero similarly probability of B equals
probability of B given a assuming
probability of a is non zero chain rule
of probability joint probability
distributions over many random variables
can be reduced into conditional
distributions over a single variable it
can be expressed as probability of X1 X2
so on until xn equals probability of X1
intersection probability of x i given
probability of X1 1 till x i minus
one for example The Joint probability of
a b and c can be given as probability of
a given b c multiply to probability of B
given C multiply to probability of C
logistic
sigmoid the logistics function is a type
of sigmoid function that aims to predict
the class to which a particular sample
belongs
its outcome is discrete binary value a
probability between 0 and 1 the
logistics sigmoid is a useful function
that follows the yes curve it saturates
when the input is very large or very
small logistic sigmoid is expressed as
Sigma of x = 1 upon 1 + e to the^ -
x the logistic sigmoid can be expressed
as sigmoid function of X is given as one
upon 1 + e to^ - x where e is the ool's
number gsan
distribution the gossing distribution is
a type of distribution in which data
tends to Cluster around a central value
with little or no bias to the left or
right it is often referred to as normal
distribution in absence of Prior
information the normal distribution is
frequently a fair assumption in machine
learning
equation the formula for calculating
gsan distribution is described as the
normal distribution of
X that is the function of x given mean
as Mu and variance is Sigma Square can
be calculated as 1 upon Sigma Square <
TK of 2 piun e to^ minus half x - mu /
Sigma whole
squ where mu is the mean or Peak value
which also is the expected value of
x Sigma is the standard deviation Sigma
square is the
variance a standard normal distribution
has a mean of zero and a standard
deviation of
one gsh and distribution can be
univariate which describes the
distribution of a single variable
X it can also be multivariate where it
can just used to describe the
distribution of several
variables it is represented in 3D of ND
formats law of large
numbers now let's talk about law of
large numbers the law of large numbers
states that an observed sample average
from a large sample will be close to the
true population average and that it will
get closer in the larger sample so so
the law of large number does not
guarantee that a given sample spatially
a small sample will reflect the true
population characteristics or that a
sample does not reflect the true
population will be balanced by a
subsequent sample this is for the law of
large numbers to express the
relationship between scale and growth
rate so there are multiple examples
through which we can
understand and it is widely used in
statistical analysis in working with the
central limit theorem in terms of the
business growth so there are multiple
real times set up in which these are
going to be used so if you talk about
tossing a coin so tossing a coin in a
number of times will give us two
different type of
outcomes the result will spread evenly
between head and Tails and the expected
average value is going to be
half that means 50 * nails and 30 times
heads but again if you toss a coin 1,000
times then the result can be in
different manners because out of 1,000
let's say 850 times it has been head and
only 150 times it has been tails and so
on so that's why the possibility of one
event occurring is going to be changed
in large sample sets as compared to a
small sample sets as in let's say 10
times so the number of heads and tails
unbalanced for lower number of Trials so
we can see it is
unbalanced but again as soon as we toss
more number of coins more leans towards
the balance value or we can see the
observed
averages next we have P
value so P value is basically a number
calculated from the statistical test
that describes How likely we are to have
found a particular set of observations
if the null hypothesis were true so P
values are used in hypothesis testing to
help decide whether to reject the null
hypothesis and the smaller the P value
the more likely we are to reject the
null
hypothesis so we have a term called as
null hypothesis so all statistical tests
they have null hypothesis so for most
tests the null hypothesis is that there
is no relationship between our variables
of INF first or that there is no
difference among groups for example in a
two-tail T Test the non-hypothesis is
that the difference between two groups
is going to be
zero so P value is going to tell us how
likely it is that our data could have
occurred under the null
hypothesis it is done by calculating the
likelihood of a test
statistic which is the number calculated
by a statistical test using our data so
P value tell us how often we would
expect to see a test statistic as
extreme or more
extreme than one calculated by a
statistical test if the null hypothesis
of the test was
true so there are multiple limitations
as well so first one is the results can
be significant but again they are they
may not be practical as we have compared
it can be based on multiple hypothesis
for a game for the healthcare test if if
the test is going to be positive or not
it may show even values of the effect of
a variable but not the magnitude in real
life what exactly is going to be the
application of a drug test being failed
in Pharma company therefore it is
recommended to use confidence and levels
in addition to the P values to quantify
or we can say to give a solid figure to
the reserve which we are going to get
the P values they are interpreted as
supporting or we can say refuting the
alternative
hypothesis so P value can only tell you
whether or not the null hypothesis is
supported it cannot tell us whether our
alternative hypothesis is true or why so
the risk of rejecting the null
hypothesis is often higher than the P
value so especially when we are looking
at a single study or when using small
sample sizes so this is because the
smaller frame of reference the greater
greater are the chance that as we
stumble across a statistically
significant pattern completely by
accident key
takeaways key takeaways probability and
statistics structure the premise of the
data the data helps in anticipating the
future or gauging in view of the past
patterns of
information the central tendency is a
single value that helps to describe the
data by identifying these Central
position
the mean median and mode are the
measures of central
Tendencies the distribution where the
data tends to be around a central value
with a lack of bias or minimal bias
towards the left or right is called as
gshi and distribution so now what is RNN
RNN are a type of neural network that
are designed to process sequential data
they can analyze data with temporal
Dimensions such as time series speech
and text RN can do this by using a
hidden State pass from one time stem to
the next the next hidden state is
updated at each other time step based on
the input and the previous hiden state
RNN are able to capture short-term
dependencies in sequential data but they
struggle with capturing long-term
dependencies why the lstms are made so
moving forward let's discuss types of
lstm gates so lstm models have three
types of gates the input gate the
forgate gate and the output gate so
let's first discuss the input gate the
input gate controls the flow of
information into the memory cell
deciding what to store the input gate
determines which values from the input
should be updated in the memory set it
uses a sigmoid activation function to
scale the values between 0o and one and
then applies pointwise multiplication to
decide what information to store next is
forget gate controls the flow of
information out of the memory cell
deciding what to discard the forget gate
decides what information should be
discarded from the memory cell it also
uses a zmo ination function to scale the
values between 0o and one followed by
pointwise multiplication to determine
what information to forget the last one
is output gate controls the flow of
information out of the lstm deciding
what to use for the output the output
gate determines the output of the LST DM
unit it uses a sigmoid activation
function to scale the values from 0 to
one then applies point5 multiplication
to produce the output of the lstm unit
so these GS implemented using sigmoid
function are trained using back
propagation they open and close based on
the input and the previous hidden State
allowing the lstm to selectively retain
or discard information effectively
capturing long-term dependencies so now
let's discuss application of LSM lstm
models are highly effective and used in
various application including video
analysis analyzing video frames to
identify action object and scenes the
second is language simulation tasks like
language modeling machine translation
and text summarization the third one is
time series prediction so predicting
future values in a Time series the
fourth is voice recognition tasks such
as speech to text transcription and
command
recognition the last one is sentiment
analysis classifying text sentiment as
positive negative or neural so there are
many more examples of LSD so now let's
move forward and understand LSM model
and how it works with example let's
consider the task of predicting the next
word in a sentence this is a common
application of lstm networks in natural
language processing so I will break it
down step by step using the analogy of
remembering a story and deciding what
comes next based on the context so
imagine you are reading a story as you
read you need to remember what has
happened so far to predict what might
happen next so let's illustrate with the
simple example sentence the cat set on
the dash so you want to predict the next
word which could be mat or roof or
something else an lstm Network helps
this make prediction by remembering
important parts of the story and
forgetting irrelevant details so now
let's dive into step by- step process so
step bystep exclamation using LM the
first one is reading the story input
sequence as you read each word in the
word sentence you process it and store
relevant information for example the you
understand it's determiner CAD you know
it's a noun and the subject of the
sentence set indicates the action
performed by the subject on preposition
indicating the relationship between the
cat and the next noun so this sequence
diagram showing the words being read and
processed so second comes forget G As
you move through the sentence you might
decide that some details are no longer
important for instance you might decide
that knowing the is less important now
that you have get and said so the word
forget get held discard this less
important information so this sequence
diagram you can see on the screen
showing how relevant information is
discarded so the third one input gate
when you read on you need to decide how
relevant this information is so this
sequence diagram in the screen is
showing how new information is
integrated with the last one okay the
fourth one is the cell state memory part
so this is like your memory of the story
so far it carries the information about
the subject cat and the action set on so
it updates the new information as you
read each word okay the cat set on the
retaining the important context so this
sequence diagram showing how the memory
is updated with the new information so
the last one is output gate when you
need to predict the next word the the
output get helps you decide based on the
current memory Cate so it uses the
context the cat set on the so the
predict the next word might be mat
because cat and mat are often associated
with the context so it can predict
anything the cat sat on the table or on
the sofa anything but Matt why I'm
saying Matt because cat and mat are
often associated in the same context so
this diagram is showing the prediction
of the next word based on the current
memory so there are many applications
where you can use lstm in predicting
time series or next word in the sentence
so by the using of LSM Gates input gate
forget gate and output gate and updating
the cell State the network can predict
the next word in a sequence by
maintaining relevant context and
discarding unnecessary information this
is step by-step process allow LSM
Network to effectively handle sequence
and make accurate prediction based on
the content so here's the open a
documentation and you could see the new
features introduced with the chat GPD 40
so these are the improvements uh one is
the updated and interactive bar graphs
or pie charts that you can create and
these are the features that you could
see here you could change the color you
could download it and what we have is
you could update the latest file
versions directly from Google Drive and
Microsoft One drive and we have the
interaction with tables and charts in a
new expandable view that I showed you
here that is here you can expand it in
the new window and you can customize and
download charts for presentations and
documents moreover you can create the
presentation also that we'll see in
further and here we have how data
analysis Works in chat
jbt you could directly upload the files
from Google Drive and Microsoft One
Drive I'll will show you guys how we can
do that and where this option is and we
can work on tables in real
time and there we have customized
presentation ready charts that is you
can create a presentation with all the
charts based on a data provided by you
and moreover a comprehensive security
and privacy feature so with that guys
we'll move to chat GPT and here we have
the chat GPT 40 version so this is the
PIN section or the insert section where
you can have the options to connect to
Google Drive connect to Microsoft One
drive and you can upload it from the
computer this option was already there
that is upload from computer and you can
upload at least or at Max the 10 files
that could be around Excel files or
documents so the max limit is 10 and if
you have connected to Google Drive I'll
show you guys uh I'm not connecting
you but you guys can connect it to and
you could upload it from there also and
there's another pool update that is
ability to code directly in your chat uh
so while chatting with chat gbt I'll
show you guys how we can do that and you
could find some new changes that is in
the layout so this is the profile
section it used to be at the left bottom
but now it's mve to the top right and
making it more accessible than ever so
let's start with the data analysis part
and the first thing we need is data so
you can find it on kager or you could
ask chat GPT for to provide the data
I'll will show you guys so this is the
kagle website you can sign in here and
click on data sets you can find all the
data sets here that would be around
Computer Science Education
classification computer vision or else
you could move back to chat
GPD and you could ask the chat GP for
model to generate a data and provide it
in Excel format so we'll ask him we'll
not ask him can you we'll just ask
provide a data
set that I can use for data
analysis
and provide in CSV
format so you could see that it has
responded that I can provide a sample
data set and he has started generating
the data set
here so you could see that he has
provided only 10 rows and he is saying
that I will now generate this data set
in CSV format first he has provided the
visual presentation on the screen and
now he generating the CSV format so if
you want more data like if you want 100
rows or th rows you could specify in the
prompt and chat jpt will generate that
for
you so we already have the data I will
import that data you could import it
from here or else you can import it from
your Google Drive so we have a sales
data here we will open it
so we have the sales data here so the
first step we need to do is data
cleaning so this is the crucial step to
ensure that the accuracy of file
analysis is at its best so we can do
that by handling missing values that is
missing values can distort our analysis
and here chat gb4 can suggest methods to
impute these values such as using the
mean median or a sophisticated approach
Based on data patterns and after
handling the missing values we will
remove duplicate and outlier detection
so we'll ask CH
jpt clean the
data if
needed so we can just write a simple
prompt that would be clean the data if
needed and this is also a new feature
you can see the visual presentation of
the data here that we have 100 rows here
and the columns provided that is sales
ID date product category quantity and
price per unit and total sales so this
is also a new feature that
okay uh we just head it
back we'll move back to our chat GPT
chat
here okay so here we are so you could
see that CHT has cleaned the data and he
has provided that it has checked for
missing values checked for duplicates
and ensure consistent formatting and
he's
saying okay okay so now we will ask him
that
execute these
steps
and provide the clean
data as CHT has provided that these
would the steps to clean the data and
let's
see so he has provided a new CSV file
with the clean sales data we will
download
it and ask him to use the same file
only use
this new
cleaned sales data CSV
file for further analysis
so you could see that he is providing
what analysis we can do further but once
our data is clean the next step is
visualization so visualizations help us
understand the data better by providing
a graphical representation so the first
thing we will do is we will create a
prompt for generating the histograms and
we'll do that for the age distribution
part so we'll write a prompt that
generate a histogram
generator histogram to visualize the
distribution of customer
ages to
visualize the
distribution of customer
ages and what I was telling you guys is
this code button if you just select the
text and you would find this reply
section just click on that and you could
see that it has selected the text or you
want to get all the prompts started with
chat jpd so
we'll make it cross and you could see
that it has provided the histogram
here and these are the new features here
and we could see that he's providing a
notification that interactive charts of
this type are not yet supported that is
histogram don't have the color Change
option I will show you the color Change
option in the bar chart section so these
features are also new you can download
the chart from here only and this is the
expand chart if we click on that you
could see that you could expand the
chart here and continue chat with chat
GPD here so this is the interactive
section so you could see that he has
provided the histogram that is showing
the distribution of customer ages and
the age range are from 18 to 70 years
with the distribution visualized in 15
bins that he has created 15 bins here
and now moving to another visualization
that we'll do by sales by region so
before that I will open the CSV file
that is provided by the chat GPT so you
guys can also see what data he has
provided so this is the clean sales data
and you could see that we have columns
sales ID date product category quantity
price per item total sales region and
sales
person so now moving back to chat jity
so now we will create a bar chart
showing total sales by region so we'll
enter this prompt that create a bar
chart showing total
sales by
region so what we are doing here is we
are creating bar charts or histogram
charts but we can do that for only two
columns if we want to create these data
visualization charts we need two columns
to do so so you could see that he has
provided the response and created the
bar chart here and this is the
interactive section you could see that
here's an option to switch to static
chart if we click on that we can't like
we are not getting any information we
scroll on that and if I enable this
option you could see that I can visually
see how many numbers this bar is
indicating and after that we have the
change color
section you can change the color of the
data set
provided so we can change it to any
color that is provided here or you could
just write the color code
here and similarly we have other two
options that is download and is the
expand chart
section and if you need uh what code it
has done to figure out this bar graph so
this is the code you could use any ID to
do so
if you don't want the presentations or
the visualizations of the bar charts
here you could use your ID and use the
Python language and he will provide the
code for you just take your data set and
read it through pandas and generate the
bar
charts so moving to next section that is
category wise sales section so here we
will generate a pie chart showing the
proportion of sales for each product
category
so for that we'll write a
prompt generate a pie
chart showing the proportion of
sales for each product
category so you could see that it has
started generating the P chart and this
is also an interactive
section if you click on that you would
be seeing a static pie chart and if you
want to change the color you can change
for any section that could be clothing
Electronics furniture or
kitchen and similarly we have the
download section and the expand chart
section so this is how this new chat jpd
4 model is better than chat
jp4 that you could use a more
interactive pie charts you could change
the colors for that and you can just H
over these bar charts and found all the
information according to them so after
this data visualization now we'll move
to statistical analysis so this will
help us uncover patterns and
relationships in the data so the first
thing we'll do is correlation analysis
and for that we'll write the prompt
analyze the correlation between age and
purchase amount so this correlation
analysis help us understand the relation
a ship between two variables so this can
indicate if older customers tend to
spend more or less so we will find out
that by analyzing the data and we
provide a prom to chat jyy that analyze
the correlation between age and purchase
amount so let's see what it provides
uh so here's the response by CH gbt you
could see a scatter plot that shows the
relationship between customer age and
total sales that is with a calculated
correlation coefficient of approximately
0.16 so this indicates a weak positive
correlation between age and purchase
amount suggesting that as customer age
increases there's a slight tendency for
total sales to increase as well so you
could just see the scatter PL here that
if the age increases so it is not
correlated to sales as you would see an
empty graph here so till 40 to 50 years
of age or the 70 years of age you could
find what amount they have spent here
that is the total sales accumulated by
these
ages so now moving to sales Trend so
here we will perform a Time series
analysis of purchase amount or the given
dates so what does this do is time
series analysis allows us to examine how
sales amount changes over time helping
us identify Trends and seasonal patterns
so for that we'll write a
prompt perform
a Time series
analysis of purchase
amount so give
dates so you could see that chat gbt has
provided us the response and here is the
time series plot showing total sales or
the given dates and each point on the
plot represents the total sales for a
particular
day so through this you can find out and
the businesses find out which is the
seasonal part of the year and we to
stock up their stocks for these kind of
dates and after that you could also do
customer segmentation so what does this
do is so we can use clustering here to
segment customers based on age income
and purchase amount so clustering groups
customers into segments based on
similarities this is useful for targeted
marketing and personalized
services and after that we have the
advanced usage for data analysis here we
can draw predictive modeling table and
do the Market Basket analysis and
perform a customer lifetime value
analysis so we will see one of those and
what we'll do is we'll perform a Market
Basket analysis and perform an
association rule mining to find
frequently bought together
products so the theory behind this is
the association rule mining helps
identify patterns of products that are
often purchased together aing an invent
management and cross selling strategies
so for that we'll write a prompt that so
perform an association rule mining to
find frequently bought to together
products so for that we'll write a
prompt here perform an association rle
mining to find frequently
bought products
together so let's see for this prompt
what does char4 respond to
us uh so you could see that he is
providing a code here but we don't need
a code here we need the
analysis don't provide
code do the market pket
analysis and provide visualizations
so you could see that uh Chad jpt has
provided the response that given the
limitations in this environment so he is
not able to do the Market Basket
analysis here so but he can help us how
we can perform this in an ID so he
providing you can install the required
Library is then prepare the data and
here is providing the example code so
you could see there are some limitations
to chat GPT 4 also that he can't do
Advanced Data
analysis so you could use the code in
your ID and do the Market Basket
analysis there so there are some
limitations to chat GT4 also and now we
will ask chat GPT can you create a
presentation based on the data set and
we'll provide a data set to it
also so we will provide a sample sales
data and we'll ask him can
you create a
presentation or PowerPoint
presentation based on this data
set and only
provide data
visualization
graphs so you can see that J GPT 4 has
started analyzing the data and he is
stating that and he will start by
creating a data visualization from the
provided data set and compile them into
PowerPoint presentation
so you could see that CH4 has provided
us the response and these are all the
presentations or the bar graphs that he
has created and now we have downloaded
the presentation here we will open
that and here's the presentation that is
created by chat
jp40 today in this video we are diving
into a powerful machine learning
algorithm that is made waves in the
world of data science adab boost whether
you are a beginner or experienced data
scientist understanding adab boost is
crucial for building robust and accurate
predictive models but what exactly is
adaboost adaboost short for adaptive
boosting is an unstable learning
technique that combines multiple we
classifiers to form a strong classifier
think of it as a term of experts working
together where each expert focuses on
their areas where others might have made
mistakes so by the end end of this video
you will be clearly understand how adab
boost work why it is so effective and
how you can implement it in your
projects so we will start by breaking
down the concept of boosting and how
adab boost fits into this category then
we will dive into how the algorithm
adapts by assigning different weights to
each stance focusing more on mistakes
made by a previous model this unique
approach help us to reduce errors and
improve model accuracy significantly to
make things even clearer we will walk
you through a practical example showing
you step by step how adab boost can be
applied to solve real wealth problems
whether you are working on
classification problem or trying to
improve your model performance adab
boost is a technique you will want in
your toolkit so stay updated and by end
of this video you will not only
understand that a boost but also feel
confident in using it to boost your own
models without any further Ado let's get
started so what is boosting in machine
learning boosting refers to the process
of creating a strong learner from a
collection of weak Learners a weak
learner is a model that performs only
slightly better than random guessing on
the training data by iteratively
adjusting the weights of the training
instances boosting algorithm assign
higher importance to misclassified
instances forcing subsequent weak
Learners to focus on these challenges
samples the final prediction is
determined by aggregating the prediction
of all week Learners with higher
emphasis placed on those that
demonstrate Superior performance per so
now move forward and see types of
boosting in machine learning types of
boosting algorithm the first one is adab
Boost adaptive boosting adab boost is
the most popular boosting algorithm it
assign ways to train instances and
adjust these weights based on the
performance of weak Learners it focuses
on misclassified instances allowing
subsequent weak Learners to concentrate
on these samples and the final
prediction is determined by aggregating
the prediction of all week Learners
through a weighted me V the second one
is gradient boosting gradient boosting
is a widely used boosting algorithm that
builds an enable of decision learning
the third one is XG boost algorithm
which is also known as extreme gradient
boosting XG boost is an advanced
boosting algorithm that combines
gradient boosting with regularization
techniques the fourth one is light GBM
light gradient boosting machine light
GBM is a high performance boosting
algorithm that uses a Lea wise approach
to construct decent Tre so now let's see
what is adaboost algorithm so there are
several machine learning algorithms
available to address your problem
statement and adab boost is one of these
powerful productive modeling techniques
known as adaptive boosting so adab boost
is an anible method in machine learning
it commonly uses desent Tre with just
one split often called diffusion stumps
as its base estimator in this approach
the model initially assign equal ways to
all the data points it then then
increases the weight of the incorrectly
classified points in subsequent models
these points with higher weight are
given more attention the process
continues training new models until the
overall error is minimized so now let's
see working of atab boost algorithm so
this image illustrate an example of aab
boost algorithm using the data set
provided okay so this is a
classification problem since the
targeted column is binary initially each
data point will be assigned an equal
weight as you can see 1X 5 okay so the
sample weights are calculated using this
following formula and here n denotes the
total number of data points okay in step
two we will first assess how well gender
classifies the samples followed by
evaluation of how the variables age and
income perform in classifying the
samples for each feature we will create
a decent stump and calculate its guine
index the tree with the data lowest
guine index will be selected as a first
ter let's assume that in our data set
gender has the lowest guine index making
it our first St so in step three using
this approach we will now determine the
amount of say or importance or influence
for this classifier and categories da
data points using this formula okay here
the total error is just the sum of
misclassified data Point sample weights
if there is one incorrect output in our
data set thus our total error is is 1x 5
and the alpha performance of the St okay
using this so now we will find out the
performance of the stem so using this
formula here zero represent a Flawless
stem while one represent a bad stem so
according to this graph where zero is
misclassification there is no error
hence the amount of say Alpha you can
say will be a huge value okay when the
classifier predicts half correctly and
half incorrect the total error is 0.5
and the classifier signifant amount of
equals to0 okay and if all the samples
were improperly categorized the error
will be quite large about to one and our
Alpha value will be negative integer
Okay negative integer you can say minus
two okay in step four you might be
wondering why it's necessary to
calculate as STS total error and
performance the reason is
straightforward we need to update the
weights because if the same weights are
used in the next model it will yield the
same result as previous one the weights
of incorrectly predicted points will be
increased while those of the correctly
predicted points will be decreased okay
more emphasis will be placed on the
points with higher weights so after
assessing the classifier significance
the total error we update weights using
this formula new sample weight equals to
Old weight multip by amount of say Alpha
okay when the sample is is successfully
identified the amount of say you know
Alpha will be negative and then when the
sample is misclassified the amount of
alpha will be positive you can see here
plus and minus there are four correctly
categorized samples and one incorrectly
classified sample so in this case the
sample weight of the data point is 1x5
and the quantity of sa performance of
the gender stump is
0.69 okay so these are the following new
sample weights Okay so
this is the adjusted weight for
incorrectly categorized okay 0.398 so
here the total sum of the weights must
equal one but the updated weights add up
to
0.84 to normalize them we divide each
weight by
0.84 making the total sum equals to one
after normalization the DAT weights now
sum to one okay see now sum to one so in
step five we we must know how to create
a fresh data set to see whether or not
the mistake have decreased to do this we
will delete the same weights and new
sample weights column and then split our
data points into buckets based on new
sample weights okay and in Step six we
are almost done here the method now
select random values between 0 to one
since misclassified records have higher
sample weights so they are more likely
to be select let's say the algorithm
random select the number 0.38 or 0.26 or
anything next we will see where the
these random number Falls within the Ved
range and create our new data set so
this is our new data set so in Step
seven the the new data set will be used
to repeat the previous steps start by
assigning equal ways to each point
identifying this term the best classify
the sample of calculating their guine
index and selecting the one with the
lowest value so assume we have sequently
built three decis entri dt1 dt2 and dt3
using our data set okay so when we run
our data set through these trees the
class with the majority will be
determined and we will make prediction
based on that welcome to the world of
machine learn as we embrace the
Innovations of tomorrow here are the top
10 projects that will Skyrocket your
skills in the ever evolving landscape of
machine learning and artificial
intelligence the project number one is
image recognition with convolutional
neural network or CNN delve into the
domain of image recognition with a focus
on convolutional neural networks this
project serves as an introduction to the
intricacies of CNN architectures aiding
in image classification and feature
extraction offering a foundational
understanding of visual data processing
tools needed to work on this project
includes Python tensorflow and cars
skills that you will learn our image
processing CNN architecture tensorflow
usage and much more moving on to project
number two we have sentiment analysis
with natural language processing or NLP
engage in the analysis of sentiment
using natural language processing tools
this project aims to instill the skills
necessary for text pre-processing
feature extraction and sentiment
analysis contributing to a comprehensive
understanding of emotional contexts with
textual data tools needed to work on
this project are Python nltk and Sigler
the skills you will acquire includes
text pre-processing feature extraction
and sentiment anal is moving on to
project number three we have
reinforcement learning with open ay
Master the principles of reinforcement
learning through the utilization of the
open AI gym environment this project
focuses on Q learning policy iteration
and the Practical implementation of
reinforcement learning algorithms
facilitating a deeper comprehension of
decision- making systems tools needed to
work on these projects include python
open a skills you will acquire includes
Q learning deep Q networks and policy
iteration moving on to project number
four we have generative adversarial
networks or Gans for image generation
explore the creative possibilities of
generative adversarial networks in
crafting synthetic images this Advanced
project dels into the architecture of G
offering insight into image generation
and adversarial Network Frameworks tools
needed to work on this project includes
Python tensorflow and caral skills that
you will acquire include G and
architecture image generation and
adversarial networks moving on to
project number five
we have time series forecasting with
long short-term memory or lstm networks
enhance predictive capabilities through
the study of long short-term memory
networks this project addresses time
series data handling lstm architectures
and sequence prediction enabling
adeptness in forecasting sequential data
tools needed includes Python kasas and
Paras the skills you will acquire
includes time series data handling LSM
architecture sequence prediction moving
on to project number six we have object
detection using y look you only look
once examine realtime object detection
techniques with you only look once model
this intermediate level project focuses
on object recognition YOLO architecture
and computer vision culminating in the
ability to detect and categorize objects
swiftly tools needed includes python
open CV and darket skills you will learn
includes object detection YOLO
architecture and computer vision moving
on to the next project we have
Transformer models for language
translation embark on a linguistic
Journey exploring Transformer models for
language translation dive into the
mechanics of attention mechanisms
enabling a comprehensive understanding
of language translation and
transformative capabilities of those
models tools needed includes python
hugging face Transformers the skills you
will acquire includes transform
architectures attention mechanisms and
language translation moving on to
project number eight we have Federated
learning for privacy preserving machine
learning explore the paradig time of
privacy preserving machine learning with
a focus on Federated learning this
Advanced project emphasizes secure
Collaborative Learning techniques
facilitating the enhancement of models
while ensuring data privacy tools needed
to work on this project are Python
tensorflow and pis skills that you will
acquire includes Federated learning
secure and privacy preserving ml moving
on to project number nine we have
reinforcement learning for real-time
strategy use develop strategic decision-
making skills through rein enement
learning with a realtime strategy games
this project focuses on empowering AI
agents with strategic Pros thereby
owning decision-making capabilities in
game environments the tools needed
includes Python and
PC2 moving on to the skills you will
learn out of this project includes
strategic decision making RL in game
environments moving on to the last
project for the video we have
explainable ai for model
interpretability unveil Integra of model
interpretability to explainable AI
techniques the skills you will acquire
include Model interpretability H Techni
so That's all folks for this top 10
machine learning project ideas 2022 in
today's video we are diving deep into
the world of machine learning interview
preparation as we GE up for 2024 it's
crucial to be well prepared for the
questions that can come your way in any
machine learning interview we have
compiled 30 essential interview
questions and answers thoughtfully
categorized into beginner intermediate
and advanced levels so let's start with
beginner level questions and number one
is what is machine learning so machine
learning is a subset of artificial
intelligence that involves the use of
algorithms and statistical models to
enable computers to perform task without
explicit instructions that is by relying
on patterns and interference and now
moving to number second question that is
what are the different types of machine
learning so the three main types of
machine learning are number one is
supervised learning and then comes
unsupervised learning and then there is
reinforcement learning now moving to
next question that is third that is what
is supervised learning so supervised
learning involves training a model on a
label data set which means each training
example is paired with an output level
the model learns to predict the output
from the input data now moving to the
fourth question that is what is
unsupervised learning so unsupervised
involve training a model on data that
does not have labeled responses the
model tries to learn the patterns and
the structure from the input data so
Guys these are the beginner level
questions and now we'll move to the
fifth question that is what is
reinforcement learning so reinforcement
learning is a type of machine learning
where an agent learns to make decisions
by performing actions and receiving
Rewards or penalties the goal is to
maximize the cumulative reward so now
moving to the sixth question that is
what is a model in machine learning so a
model in machine learning is a
mathematical representation of a real
well process it is trained on data to
recognize patterns and make predictions
or decisions based on new data so now
moving to seventh question that is what
is overfitting so overfitting occurs
when a machine learning model performs
well on the training data but poorly on
new unseen data it indicates that the
model has learned the noise and details
in the training data instead of the
actual patterns so now coming to
question number eight that is what is
underfitting so underfitting occurs when
a machine learning model is too simple
to capture the underlying patterns in
the data it performs poorly on both the
training data and new data now move to
the next question that is ninth question
and the question is what is a confusion
Matrix so confusion Matrix is a table
used to evaluate the performance of a
classification model it summarizes the
number of correct and incorrect
predictions made by the model and that
is categorized by each class now moving
to the 10th question that is what is
cross validation so cross validation is
a technique for assessing how the
results of a statistical analysis will
generalize to an independent data set it
involves partitioning the data into
subsets training the model on some
subsets and validating it on the
remaining subsets so this was all about
that is the 10th question or the overall
1 to 10 questions for beginner level Now
we move to intermediate level and here
we will cover 10 questions so we'll
start with 11th question that is what is
a Roc curve so Roc that is receiver
operating characteristic curve it is a
graphical representation of a
classifier's performance across
different thresholds it plots the true
positive rate that is tpr against a
false positive rate that is fpr now
moving to 12th question that is what is
precision and record so Precision is the
ratio of correctly predicted posit itive
observations to the total predicted
positives and recall is the ratio of
correctly predicted positive
observations to all actual positives so
the formula is precision equal to
tp/ tp+ FP and the recall is tp/ TP + FN
so now we'll move to the 13th question
that is what is the F1 score so the F1
score is the harmonic mean of precision
and recall it provides a b balance
between the two metrics and is useful
when you need to balance precision and
recall fub1 score is equal to twice into
Precision into recoil and that is
divided by Precision plus recoil now
we'll move to 14th question and here we
will cover
regularization so the question is what
is regularization so it is a technique
used to prevent overfitting by adding a
penalty to the model's complexity and
the common types of regularization
include L1 that is lasso and L2 Ridge
regularization now we'll move to the
15th question that is what is the bias
variance tradeoff so the bias variance
tradeoff is a fundamental issue in
machine learning that involves balancing
the error introduced by the model's
assumptions and the error due to model
complexity so a good model should have
low bias and low variance now we'll move
to the question number 16 that is what
is feature engineering so feature
engineering is the process of creating
new features or modifying existing ones
to improve the performance of a machine
learning model it involves techniques
like normalization and coding
categorical variables and creating
interaction terms so now we'll move to
question number 17 and that is about
gradient descent so the question is what
is gradient descent and your answer is
gradient descent is an optimization
algorithm used to minimize the cost
function in machine learning models and
it I atively adjust the model parameters
in the direction of the steepest Descent
of the coast function so with this we
move to the 18th question and that will
cover with the difference between
bagging and boosting so the question is
what is difference between bagging and
boosting and you could answer this with
starting with bagging that is bootstrap
aggregating that involves training
multiple models on different subsets of
the data and averaging their predictions
then comes boosting that inv mod
training models sequentially with each
new model focusing on correcting the
errors of the previous ones and then we
have the question number 19 that is what
is a decision tree so a decision tree is
a known parametric supervised learning
algorithm used for classification and
regression it splits the data into
subsets based on the value of input
features resulting in a tree like
structure of decisions Now we move to
question number 20 that is what is a
random Forest So Random Forest is an
ensemble learning method that combines
multiple decision trees to improve the
accuracy and robustness of the model it
builds each tree using a random subset
of features and data points and then
averages their predictions so these were
the questions that are for the
intermediate level and these are just
the basic questions or I will just say
the theoretical questions that can be
asked in an interview so be prepared for
that now we'll move to the advanced
level interview questions and we'll
start with question number 21 and here
also we'll cover the 10 questions so
number one question or that is 21th
question and the question is what is a
support Vector machine so support Vector
machine is a supervised learning
algorithm used for classification and
regression it finds the optimal hyper
plane that maximizes the margin between
different classes in the feature space
and then comes question number 22 that
is what is principal component analysis
so principal component analysis is a
dimensionality reduction technique that
transforms High dimensional data into a
lower dimensional Space by finding the
directions that is principal components
that maximize the variance in the data
and then comes the question number 23
that is what is a neural network so a
neural network is a series of algorithms
that attempt to recognize underlying
relationships in a set of data through a
process that mimics the way the human
brain operates it consist of layer of
interconnected nodes or neurons and then
comes the question number 24 that is
what is deep learning so deep learning
is a subset of machine learning that
involves neural networks with many
layers that is deep neural networks and
it is particularly effective for task
like image and speech recognition now
I'll move to question number 25 that is
what is convolutional neural network
that is CNN so we will start the answer
by answering the interor that a
convolutional neural network is a type
of deep learning model specifically
designed for for processing structured
grid data like images it uses
convolutional layers to extract special
features or the spal features and
patterns from the input data now we'll
move to the question number 26 that is
what is a recurrent neural network or
RNN so a recurent neural network is a
type of neural network designed for
sequential data and it has connections
that form directed Cycles allowing it to
maintain a memory of previous inputs and
process sequences of data so this is all
about question number 26 and now we will
cover the question number 27 that is
what is the difference between badge
gradient descent and stochastic gradient
descent so batch gradient descent
computes the gradient of the cost
function using the entire training data
set while stochastic gradient descent
that is SGD computes the gradient using
only one training example at a time so
SGD is faster but noisier now we'll move
to question number 28 that is what is
Dropout in neural networks so Dropout is
a regularization technique used in
neural networks to prevent overfitting
and it involves randomly setting a
fraction of the neur Rons to zero during
training forcing the network to learn
more robust features and now we'll move
to question number 29 and that will be
about transfer learning and your
question is what is transfer learning so
we'll answer this to the interviewer by
starting that transfer learning is a
technique in machine Mach linning where
a model developed for one task is reused
as the starting point for a model on a
second related task it is particularly
useful when there is limited data
available for the second task now we'll
move to the last question and the 30th
question so that is what is a generative
adversor that is g so you can start
answering this so generative adial
network is a type of deep learning model
consisting of two neural networks a
generator and a discriminator that are
trained simultaneously the generator
creates fake data while the
discriminator tries to distinguish
between real and fake data leading to
the generator producing increasingly
realistic data and these questions and
answers are over and these covers a wide
range of topics in machine learning and
should help prepare for interviews at
thank you for joining SAR course on
machine learning if you have any doubt
please reach out to us on the comment
section below thank you and keep
learning with simply learn staying ahead
in your career career requires
continuous learning and upscaling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in cuttingedge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know
more hi there if you like this video
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to ner up and get certified click
here