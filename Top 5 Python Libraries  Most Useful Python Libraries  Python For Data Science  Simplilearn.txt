hello everyone and welcome to this video
on top 5 python libraries by simply
learn
over the past five years
python has become one of the most
preferred programming languages for
carrying out tasks related to data
science data analytics statistical
analysis and machine learning
then is easy to learn and supports
english like syntax that is simple to
understand
python programming has a wide collection
of libraries for numerical computation
data manipulation data visualization and
is heavily used for building machine
learning and deep learning models
python's inbuilt data structures and
functions make it easier for users to
solve complex problems
so in this video
we will cover the best
five python libraries that are
frequently used for solving business
problems
so we look at
numpy then we'll see how you can
clean filter sort and manipulate data
using pandas next you will learn how to
create machine learning models using
psychic learn and after that we will
cover two of the most popular deep
learning libraries that is tensorflow
and keras so let's get started
to start with this chart doesn't even do
the filled um justice because it's just
exploded
these are just some of the major
frameworks out there there's a cross
which happens to sit on tensorflow so
they're very integrated there's
tensorflow
pie torches out there cafe piano
dl4j
and chainer these are just a few of the
deep learning frameworks we're talking
about neural networks if you're just
starting out never seen a neural network
you can go into python in the
scikit and do the neural network in
there which is probably the most
simplest version i know but the most
robust version out there the most top of
the ladder as far as the technology
right now is tensorflow and that of
course is changing from day to day and
some of these
are
better for different purposes
so let's dive into tensorflow let's see
what is tensorflow what is tensorflow
tensorflow is a popular open source
library released in 2015 by google brain
team for building machine learning and
deep learning models
it is based on python programming
language and performs numerical
computations using data flow graphs to
build models so let's take a look at
some of the features of tensorflow it
works efficiently with multi-dimensional
arrays
if you've ever played with any of the
simpler packages of neural networks
you're going to find that you have to
pretty much flatten them and make sure
your your stuff is set in a flat model
tensorflow works really good so we're
talking pictures here where you have
x and y coordinates where the picture is
and then each pixel has three or four
different channels that's a very
complicated array very multi-dimensional
array it provides scalability of
computation across machines and large
data sets
this is so new right now
and you might think that's a minor thing
but when python is operating on one
computer and it has a float value
and it truncates it differently on each
computer you don't get the same results
and so your training model might work on
one machine and then another it doesn't
this is one of the things that
tensorflow
addresses and does a very good job on
it supports fast debugging and model
building
this is why i love tensorflow
i can go in there and i can build a
model with different layers each layer
might have different properties
they have like the convolutional neural
network which you can then sit on top of
a regular neural network with reverse
propagation there's a lot of tools in
here and a lot of options and each layer
that it goes through can utilize those
different options and stack differently
and it has a large community and
provides tensorboard to visualize the
model tensorboard is pretty
recent but it's a really nice tool to
have so you when you're working with
other people or showing your
clients or the
shareholders in the company
you can give them a nice visual model so
they know what's going on what are they
paying for and let's take a glance at
some of the different uses or
applications for tensorflow when we talk
about tensorflow applications
clearly this is data analytics we're
getting into the data science i like to
use data science as probably a better
term this is the programming side
and it's really the sky is a limit
we can look at face detection language
translation fraud detection video
detection
there are so many different things out
there that tensorflow can be used for
when you think of neural networks
because tensorflow is a neural network
think of complicated chaotic data this
is very different than if you have a set
numbers like you're looking at the stock
market you can use this on the stock
market but if you're doing something
where the numbers are very clear
and not so chaotic as you have in a
picture then you're talking more about
linear regression models
and different regression models when
you're looking at that when you're
talking about these really complicated
data patterns
then you're talking neural networks in
tensorflow and if we're going to talk
about tensorflow we should talk about
what tensors are after all that is what
tensor
that's what this is named after so we
talk about tensors in tensorflow
tensorflow is derived from its core
component known as a tensor a tensor is
a vector or a matrix of n dimensions
that represent all types of data and you
can see here we have the scalar which is
just a single
number you have your vector which is two
numbers
might be a number in a direction you
have a simple matrix and then we get
into the tensor
i mentioned how a picture is a very
complicated tensor because it has your x
y coordinates and then each one of those
pixels
has three to four channels for your
different colors
and so each image coming in would be its
own tensor
and
in tensorflow tensors are defined by a
unit of dimensionality called as rank
and you can see here we have our
scalar which is a single number that has
a rank of zero because it has no real
dimensions to it other than it's just a
single point
and then you have your vector which
would be a single list of numbers
uh so it's a rank one
matrix would have rank two and then as
you can see right here as we get into
the full tensor it has a rank three
and so
the next step is to understand how a
tensorflow works
and if you haven't looked at
the basics of a neural network in
reverse propagation that is the basics
of tensorflow and then it goes through a
lot of different options and properties
that you can build into your different
tensors
so a tensorflow performs computations
with the help of data flow graphs it has
nodes that represent the operations in
your model and if you
look at this you should see a
neural network going on here we have our
inputs bc and d
and you might have x equals b plus c y
equals d minus 4
a equals x times y and then you have an
output
and so
even though this isn't a neural network
here it's just a simple set of
computations going across
you can see how the more complicated it
gets the more because you can actually
one of the tensors is a neural network
with reverse propagation
but it's not limited to that there's so
much more you can do with it and this
here is just a basic uh flow of
computations of the data going across
and you can see we can plug in the
numbers uh b equals four c equals three
d equals six and you get x equals four
plus three
so x equals seven y equals six minus
four so y equals two and finally a
equals seven times two or a equals
fourteen like i said this is a very
simplified version of how tensorflow
works each one of these layers can get
very complicated
but tensorflow does such a nice job that
you can spin different setups up very
easily and test them out so you can test
out these different models to see how
they work now tensorflow has gone
through two major stages
we had the original tensorflow release
of 1.0 and then they came out with the
2.0 version and the 2.0 addressed so
many things out there that the 1.0
really needed so we start talking about
tensorflow 1.0 versus 2.0 i guess you
would need to know this for
a legacy programming job if you're
pulling apart somebody else's code the
first thing is that tensorflow 2.0
supports eager execution by default it
allows you to build your models and run
them instantly and you can see here from
tensorflow 1 to tensorflow 2
we have
almost double the code to do the same
thing so if i want to do with tf.session
or tensorflow session as a session the
session run you have your variables your
session run you have your tables
initializer and then you do your model
fit
x train y train and then your validation
data your x value y value and your epics
and your batch size all that goes into
the fit
and you can see here where that was all
just compressed to make it run easier
you can just create a model and do a fit
on it uh and you only have like that
last set of code on there so it's
automatic that's what they mean by the
eager so if you see the first part
you're like what the heck is all the
session thing going on that's tensorflow
1.0 and then when you get into 2.0 it's
just nice and clean
if you remember from the beginning i
said cross
on our list up there
and cross is the high level api in
tensorflow 2.0
cross is the official high level api of
tensorflow 2.0 it has incorporated cross
as tf.caras cross provides a number of
model building apis such as sequential
functional and subclassing so you can
choose the right level of abstraction
for your project and uh we'll hopefully
touch base a little bit more on this
sequential being the most common uh form
that is your your
layers are going from one side to the
other so everything's going in a
sequential order
functional
is where you can split the layers so you
might have your input coming on one side
it splits into two completely mod
different models and then they come back
together
and one of them might be doing
classification the other one might be
doing just linear regression kind of
stuff or a neural basic
reverse propagation neural network and
then those all come together into
another layer which is your
neural network reverse propagation setup
subclassing is the most complicated as
you're building your own models and you
can subclass your own models into cross
so very powerful tools here this is all
the stuff that's been coming out
currently in the tensorflow cross setup
a third big change we're going to look
at is it in tensorflow 1.0
uh in order to use tf layers as
variables you would have to write tf
variable block so you'd have to
pre-define that
in tensorflow 2 you just add your layers
in under the sequential and it
automatically defines them as long as
they're flat layers
of course this changes a little bit as
the more complicated
tensor you have coming in but all of
it's very easy to do and that's what 2.0
does a really good job of and here we
have
a little bit more on the scope of this
and you can see how tensorflow 1 asks
you to do
these different layers and values if you
look at the scope and the default name
you start looking at all the different
code in there to create the variable
scope that's not even necessary in
tensorf 2.0 so you'd have to do one
before you do do what you see the code
in 2.0 in 2.0 you just create your model
it's a sequential model then you can add
all your layers in you don't have to
pre-create the
variable scope so if you ever see the
variable scope you know that came from
an older version and then we have the
last two which is our api cleanup and
the autograph
in the api cleanup tensorflow 1 you
could build models using tf gans tf app
tf contrib tf flags etc in tensorflow 2
a lot of apis have been removed and this
is just they just cleaned them up
because people weren't using them and
they've simplified them and that's your
tf app your tf flags your tf logging are
all gone
so there's those are three legacy
features that are not in 2.0 and then we
have our tf function and autograph
feature
in the old version tensorflow 1 0 the
python functions were limited and could
not be compiled or exported re-imported
so you were continually having to redo
your code you couldn't very easily just
put a pointer to it and say hey let's
reuse this
in tensorflow 2 you can write a python
function using the tf function to mark
it for the jit compilation for the
python jit so that tensorflow runs it as
a single graph autograph feature of tf
function helps to write graph code using
natural python syntax
now we just threw in a new word in you
graph a graph is not a picture of a
person
you'll hear graph x and some other
things
graph is what are all those lines that
are connecting different objects so if
you remember from before where we had
the different layers going through
sequentially each one of those white
lined arrows would be a graph x that's
where that computation is taken care of
and that's what they're talking about
and so if you had your own special code
or python way that you're sending that
information forward you can now put your
own function in there instead of using
whatever function they're using
in neural networks this would be your
activation function although it could be
almost anything out there depending on
what you're doing next let's go for
hierarchy and architecture and then
we'll cover three basic tools in
tensorflow before we roll up our sleeves
and dive into the example so let's just
take a quick look at tensorflow toolkits
in their hierarchy at the high level we
have our object oriented api so this is
what you're working with you have your
tf cross you have your estimators this
sits on top of your tf layers tf losses
tf metrics so you have your reusable
libraries for model building this is
really where tensorflow shines is
between the karass
running your estimators and then being
able to swap in different layers you can
your losses your metrics all of that is
so built into tensorflow makes it really
easy to use and then you can get down to
your low level tf api
you have extensive control over this you
can put your own formulas in there your
own procedures or models in there
you could have it split we talked about
that earlier so with the 2.0 you can now
have it split one direction we do a
linear regression model and then go to
the other where it does a
neural network and maybe each neural
network has a different activation set
on it and then it comes together into
another layer which is another neural
network so you can build these really
complicated models and at the low level
you can put in your own apis you can
move that stuff around and most recently
we have the tf code can run on multiple
platforms
and so you have your cpu
which is
basically like on the computer i'm
running on i have uh eight cores and 16
dedicated threads i hear they now have
one out there that has over 100 cores
uh so you have your cpu running and then
you have your gpu which is your graphics
card
and most recently they also include the
tpu setup which is specifically for
tensorflow models uh neural network kind
of setup
so now you can export the tf code and it
can run on all kinds of different
platforms for the most
diverse setup out there and moving on
from the hierarchy to the architecture
in the tensorflow 2.0 architecture uh we
have uh you can see on the left this is
usually where you start out with and 80
of your time in data science is spent
pre-processing data making sure it's
loaded correctly and everything looks
right
so the first level in tensorflow is
going to be your read and pre-processed
data your tf data feature columns
this is going to feed into your tf cross
or your pre-made estimators
and kind of you have your tensorflow hub
that sits on top of there so you can see
what's going on
once you have all that set up you have
your distribution strategy where are you
going to run it you're going to be
running it on just your regular cpu are
you gonna be running it uh with the gpu
added in um like i have a pretty
high-end graphics card so it actually
grabs that gpu processor and uses it or
do you have a specialized tpu setup in
there that you paid extra money for
it could be if you're in later on when
you're distributing the package you
might need to run this on some really
high processors because you're
processing at a server level for
let's say net you might be processing
this at a
you're distributing it not the
distribution strategy but you're
distributing it into a server where that
server might be analyzing thousands and
thousands of purchases done
every minute
and so you need that higher speed to
give them a
to give them a recommendation or a
suggestion so they can buy more stuff
off your website or maybe you're looking
for
data fraud analysis working with the
banks you want to be able to run this at
a high speed so that when you have
hundreds of people
sending their transactions in it says
hey this doesn't look right someone's
scamming this person and probably has
their credit card so when we're talking
about all those fun things we're talking
about saved model this is we were
talking about that earlier where it used
to be when you did one of these models
it wouldn't truncate the float numbers
the same and so a model going from one
you build the model on your
machine in the office and then you need
to distribute it and so we have our
tensorflow serving cloud on premium
that's what i was talking about if
you're like a banking or something like
that
now they have tensorflow lite so you can
actually run a tensorflow on an android
or an ios or raspberry pi a little
breakout board there in fact they just
came out with a new one that has a
built-in there's just a little mini tpu
with the camera on it so it can
pre-process a video so you can load your
tensorflow model on to that
talking about an affordable way to beta
test a new product you have the
tensorflow js which is for browser and
node server so you can get that out on
the browser for some simple computations
that don't require a lot of heavy
lifting but you want to distribute to a
lot of endpoints and now they also have
other language bindings so you can now
create your tensorflow backend save it
and have it accessed from c java go
c sharp rust r or from whatever package
you're working on so we kind of have an
overview of the architecture and what's
going on behind the scenes and in this
case what's going on as far as
distributing it let's go ahead and take
a look at uh three specific pieces of
tensorflow
and those are going to be constants
variables and sessions
so very basic things you need to know
and understand when you're working with
the tensorflow
setup so constants in tensorflow in
tensorflow constants are created using
the function constant in other words
they're going to stay static the whole
time whatever you're working with the
syntax for constant
value d type 9 shape equals none name
constant verify shape equals false
that's kind of the syntax you're looking
at and we'll explore this with our hands
on a little more in depth
and you can see here we do z equals
tf.constant 5.2 name equals x
d type is a float that means that we're
never going to change that 5.2 it's
going to be a constant value and then we
have our variables in tensorflow
variables in tensorflow are in memory
buffers that store tensors
and so we can declare a two by three
tensor populated by ones you could also
do constants this way by the way so you
can create a an array of ones for your
constants i'm not sure why you'd do that
but you know you might need that for
some reason
in here we have v equals tf.variables
and then in tensorflow you have
tf.ones and you have the shape which is
2 3 which is then going to create a nice
uh 2x3
array that's filled with ones and then
of course you can go in there and
they're variables so you can change them
it's a tensor so you have full control
over that and then you of course have
sessions in tensorflow a session in
tensorflow is used to run a
computational graph to evaluate the
nodes
and remember when we're talking a graph
or graph x we're talking about all that
information then goes through all those
arrows and whatever computations they
have that take it to the next node and
you can see down here where we have
import tensorflow as tf if we do x
equals a tf.constant of 10
we do y equals a tf constant of 2.0 or
20.0 and then you can do z equals
tf.variable
and it's a tf dot add x comma y
uh and then once you have that set up in
there you go ahead and knit your tf
global variables initializer with tf
session as session you can do a session
run init
and then you print the session run y
and so when you run this you're going to
end up with of course the 10 plus 20 is
30. and we'll be looking at this a lot
more closely as we actually roll up our
sleeves and put some code together
so let's go ahead and take a look at
that and for my coding today i'm going
to go ahead and go through anaconda and
then i'll use specifically the jupiter
notebook on there and of course this
code is going to work
whatever platform you choose whether
you're in a notebook
the jupiter lab which is just the
jupiter notebook but with tabs for
larger projects we're going to stick
with jupiter notebook
pycharm uh whatever it is you're going
to use in here uh you have your spider
and your qt console for different
programming environments the thing to
note
it's kind of hard to see but i have my
main
pi 3 6.
right now when i was writing this
tensorflow works in python version 3.6
if you have python version 37 or 3 8
you're probably going to get some errors
in there
might be that they've already updated it
and i don't know it now you have an
older version
but you want to make sure you're in
python version 3 6 in your environment
and of course in anaconda i can easily
set that environment up make sure you go
ahead and
pip in your
tensorflow or if you're in anaconda you
can do a conda install tensorflow to
make sure it's in your package
so let's just go ahead and dive in and
bring that up this will open up a nice
browser window i just love the fact i
can zoom in and zoom out depending on
what i'm working on making it really
easy to adjust
a demo for the right size go under new
and let's go ahead and create a new
python and once we're in our new python
window this is going to leave it
untitled
uh let's go ahead and import import
tensorflow as tf
at this point we'll go ahead and just
run it real quick
no errors yay no errors i
i do that whenever i do my imports
because i unbearably will have opened up
a new environment and forgotten to
install tensorflow into that environment
uh or something along those lines so
it's always good to double check
and if we're going to double check that
we also it's also good to know
what version we're working with and we
can do that simply by
using the version command in tensorflow
which you should know is is probably
intuitively the tf
dot underscore underscore version
underscore underscore
and
you know it always confuses me because
sometimes you do tf.version for one
thing you do tf dot underscore version
underscore for another thing
this is a double underscore in
tensorflow for pulling your version out
and it's good to know what you're
working with we're going to be working
in tensorflow version 2.1.0 and i did
tell you the the um we were going to dig
a little deeper into our constants and
you can do an array of constants and
we'll just create this nice array
a equals tf.constant
and we're just going to put the array
right in there 4 3 6 1.
we can run this and now that is what a
is equal to and if we want to just
double check that remember we're in
jupiter notebook where i can just put
the letter a
and it knows that that's going to be
print
otherwise you round you surround it in
print and you can see it's a tf tensor
it has the shape the type and the and
the array on here it's a two by two
array and just like we can create a
constant we can go and create a variable
and this is also going to be a two by
two array and if we go ahead and print
the v out we'll run that
and sure enough there's our tf variable
in here
then we can also let's just go back up
here and add this in here
i could create another tensor and we'll
make it a constant this time
and we're going to put that in over here
we'll have b tf constant
and if we go and print out v and b
we're going to run that
and this is an interesting thing that
always that happens in here you'll see
right here when i print them both out
what happens it only prints the last one
unless you use print commands so
important to remember that in jupyter
notebooks we can easily fix that by go
ahead and print and surround v with
brackets and now we can see with the two
different variables we have uh we have
the 3152 which is a variable and this is
just a flat
constant so it comes up as a tf tensor
shape 2 kind of two and that's
interesting to note
that this label is a tf.tensor and this
is a tf variable
so that's how it's looking in the back
end when you're talking about the
difference between a variable and a
constant
the other thing i want you to notice is
that in variable we capitalize the v and
with the constant we have a lowercase c
little things like that can lose you
when you're programming and you're
trying to find out hey why doesn't this
work
so those are a couple of things to note
in here
and just like any other array in math
we can do like a concatenate or
concatenate the different values here
and you can see we can take a b
concatenated you just do a tf.concat
values and there's our a b axes on one
hopefully you're familiar with axes and
how that works when you're dealing with
matrixes and if we go ahead and print
this out
you'll see right here we end up with a
tensor so let's put it in as a constant
not as a variable
and you have your array four three seven
eight and six one four five it's
concatenated the two together and again
i wanna highlight a couple things on
this our axes equals one this means
we're doing the columns um so if you had
a longer array like right now we have an
array that is like you know has a shape
one whatever it is two comma two
um axis
zero
is going to be your first one and axes
one is going to be your second one and
it translates as columns and rows if we
had a shape let me just put the word
shape here
um
so you know what i'm talking about is
very clear and this is i'll tell you
what i spend a lot of time
looking at these shapes and trying to
figure out which direction i'm going in
and whether to flip it or whatever
so you can get lost in which way your
matrix is going which is column which is
rows are you dealing with the third axes
or the second axis
axis one you know zero one two that's
going to be our columns and if you can
do columns then we also can do rows and
that is simply just changing the
concatenate
we'll just grab this one here and copy
it we'll do the whole thing over
ctrl copy
ctrl v and changes from axis 1
to axis 0 and if we run that
you'll see that now we concatenate by
row as opposed to column
and you have four three six one seven
eight four seven so it just brings it
right down and turns it into rows versus
columns you can see the difference there
your output this really you want to look
at the output sometimes just to make
sure your eyes are looking at it
correctly and it's in the format i find
visually looking at it is almost more
important than
understanding what's going on because
conceptually your mind just just too
many dimensions sometimes
the second thing i want you to notice is
this says a numpy array
so tensorflow is utilizing numpy as part
of their format as far as python is
concerned
and so you can treat you can treat this
output like a numpy array because it is
just that it's going to be a numpy array
another thing that comes up uh more than
you would think is filling uh one of
these with zeros or ones and so you can
see here we just create a tensor
tf.zeros
and we give it a shape we tell it what
kind of data type it is in this case
we're doing an integer
and then if we
print out our tensor again we're in
jupiter so i just type out tensor and i
run this you can see i have a nice
array of with shape three comma four of
zeros one of the things i want to
highlight here is integer 32. if i go to
the
tensorflow data types i want you to
notice how
we have float 16 float 32 float 64
complex if we scroll down you'll see the
integer down here 32 the reason for this
is that we want to control how many bits
are used in the precision
this is for exporting it to another
platform
so what would happen is i might run it
on this computer where python goes does
a float to indefinite however long it
wants to
and then we can take it but we want to
actually say hey we don't want that high
precision we want to be able to run this
on any computer and so we need to
control whether it's a tf float 16 in
this case we did an integer 32
we could also do this as a float
so if i run this as a float 32
that means this has a 32-bit precision
you'll see zero point whatever and then
to go with zeroes
we have ones if we're going from the
opposite side and so we can easily just
create a tensorflow with ones
and you might ask yourself why would i
want zeros and ones and your first
thought might be to initiate a new
tensor
usually we initiate a lot of this stuff
with random numbers because it does a
better job solving it if you start with
a uniform
set of ones or zeros you're dealing with
a lot of bias so be very careful about
starting a neural network
for one of your rows or something like
that with ones and zeros
on the other hand uh i use this for
masking you can do a lot of work with
masking you can also have uh it might be
that
one tenser row is masked
you know zero is is false one is true or
whatever you want to do it
and so in that case you do want to use
the zeros and ones and there are cases
where you do want to initialize it with
all zeros or all ones and then swap in
different numbers as
the tensor learns so it's another form
of control
but in general
you see zeros and ones you usually are
talking about a mask over another array
and just like in numpy you can also do
reshapes so if we take our remember this
is shaped three comma four maybe we
wanna swap that to four comma three
and if we print this out
you will see let me just go and do that
control v
let me run that
and you'll see that the the order of
these is now switched instead of four
across now we have three across and four
down
and just for fun let's go back up here
where we did the ones
and i'm going to change the ones to
tf.random
uniform
and we'll go ahead and just take off
well we'll go and leave that we're going
to run this
and you'll see now we have 0.0441
and this way you can actually see how
the reshape looks a lot different
0.041.15.7
and then instead of having this one it
rolls down here to the 0.14
and this is what i was talking about
sometimes you fill a lot of times you
fill these with random numbers and so
this is the random dot uniform is one of
the ways to do that now i just talked a
little bit about this float 32 and all
these data types
one of the things that comes up of
course is
recasting your data
so if we have a d type float 32 we might
want to convert these two integers
because of the project we're working on
i know one of the projects i've worked
on
ended up wanting to do a lot of round
off so that it would take a dollar
amount or a float value and then have to
round it off to a dollar amount so we
only wanted two decimal points um in
which case you have a lot of different
options you can multiply by 100 and then
round it off or whatever you want to do
there's a lot or then converted to an
integer was one way to round it off
uh kind of
a cheap and dirty trick
so we can take this and we can take this
same
tensor and we'll go ahead and create a
as an integer and so we're going to take
this tensor we're going to tf.cast it
and if we print
tensor
and then we're going to go ahead and
print
our
tensor
let me just do a quick copy and paste
and when i'm actually programming i
usually type out a lot of my stuff just
to double check it uh in doing a demo
copy and paste works fine but sometimes
be aware that copy and paste can copy
the wrong code over
personal choice depends on what i'm
working on and you can see here we took
a float 32 4.6 4.2 and so on and it just
converts it right down to a integer
value
that's our integer 32 set up and
remember we talked about
a little bit about reshape
um as far as flipping it and i just did
uh four comma three on the reshape up
here and we talked about axes zero axis
one
uh one of the things that is important
to be able to do is to take one of these
variables we'll just take this last one
tensor as integer
and i want to go ahead and transpose it
and so i can do
we'll do a
equals tf.transpose
and we'll do our tensor integer in there
and then if i print the a out and we run
this
you'll see that's the same array but
we've flipped it so that our columns and
rows are flipped this is
the same as reshaping so when you
transpose you're just doing a reshape
what's nice about this is if you look at
the numbers the columns
when we went up here and we did the
reshape they kind of roll down to the
next row so you're not maintaining the
structure of your matrix so when we do a
reshape up here they're similar but
they're not quite the same and you can
actually go in here and there's settings
in the reshape that would allow you to
turn it into a transform
uh so we come down here it's all done
for you and so there are so many times
you have to transpose
your digits that this is important to
know that you can just do that you can
flip your rows and columns rather
quickly here and just like numpy you can
also do multiple your different math
functions we'll look at multiplication
and so we're going to take matrix
multiplication of tensors
we'll go ahead and create a as a
constant five eight three nine and we'll
put in a vector v four comma two and we
could have done this where they matched
where this was a two by two array um but
instead
we're going to do just a two by one
array and the code for that is your tf
dot mat mole uh so matrix multiplier and
we have a times v and if we go ahead and
run this up let's
make sure we print out our av on there
and if we go ahead and run this
you'll see that we end up with 36 by 30.
and if it's been a while since you've
seen the matrix math
this is
5 times 4 plus eight times two
three times four plus nine times two
and that's where we get the 36 and 30.
now i know we're covering a lot really
quickly as far as the basic
functionality
so the matrix or your matrix multiplier
is a very commonly used back-end tool as
far as computing
different models or linear regression
stuff like that
one of the things
is to note is that just like in
numpy you have all of your different
math so we have our tf math
and if we go in here we have
functions we have our cosines absolute
angle all of that's in here so all of
these
are available for you to use in the
tensorflow model
and if we go back to our example and
let's go ahead and pull
oh let's do some multiplication that's
always good we'll stick with our
av
our
constant a and our vector v
and we'll go ahead and do some bit wise
multiplication and we'll create an av
which is a times b let's go and print
that out
and you can see coming across here uh we
have the four two and the five eight
three nine and it produces uh 2032 618
and that's pretty straightforward if you
look at you have four times
five is twenty four times eight is
uh 32 that's where those numbers come
from
now we can also quickly create an
identity matrix
which is basically
your main values on the diagonal being
ones and zeros across the other side
let's go ahead and take a look and see
what that
looks like and we can do let's do this
so we're going to get the shape this is
a simple way very similar to your numpy
you can do a dot shape and it's going to
return a tuple in this case our rows and
columns and so we can do a quick
print
we'll do rows
and we'll do columns
and if we run this
you can see we have three rows
two columns
and then if we go ahead and create an
identity matrix
whoops
the script for that
got a wrong button there the script for
that looks like this
where we have the number of rows equals
rows the number of columns equals
columns and d type is a 32
and then if we go ahead and just print
out our identity
you can see we have a nice identity
column with our ones going across here
now clearly we're not going to go
through every math module
available but we do want to start
looking at this as a prediction model
and seeing how it functions so we're
going to move on to a more of a
direct setup we can actually see the
full tensorflow in use for that let's go
back and create a
new setup
and we'll go in here new python 3 module
there we go
bring this out so it takes up the whole
window because i like to do that
hopefully you made it through that first
part and you have a basic understanding
of tensorflow as far as being
a series of numpy arrays you've got your
math equations and different things that
go into them
we're going to start building a
full setup as far as the numpy so you
can see how
kara sits on top of it and the different
aspects of how it works
the first thing we want to do is we're
going to go ahead and do a lot of
imports
date times warning scipy
pi is your
math so the back end scientific math
warnings because
whenever we do a lot of this you have
older versions newer versions
and so sometimes when you get warnings
you want to go ahead and just suppress
them
we'll talk about that if it comes up on
this particular setup and of course date
time
pandas again is your data frame think
rows and columns we import it as pd
numpy is your numbers array
which of course tensorflow is integrated
heavily with
seaborn for our graphics and the seaborn
as sns is going to be set on top of our
matplot library which we import as mpl
and then of course we're going to import
our matplot library pi plot as plt and
right off the bat we're going to set
some graphic colors um patch force edge
color equals true
the style we're going to use the 538
style you can look this all up there's
when you get into matplot library and to
seaborn there are so many options in
here it's just kind of nice to make it
look pretty when we start the um
when we start up that way we don't have
to think about it later on
uh and then we're going to take we have
our
mplrc we're going to put a patch ed
color dim gray line width again this is
all part of our graphics here
in our setup we'll go ahead and do an
interactive shell
node interactivity equals last
expression
here we are pd for pandas options
display max columns so we don't want to
display more than 50.
and then our matplot library is going to
be inline this is a jupiter notebook
thing the matplot library in line then
warnings we're going to filter our
warnings and we're just going to ignore
warnings that way when they come up we
don't have to worry about them
not really what you want to do when
you're working on a major project you
want to make sure you know those
warnings and then
filter them out and ignore them later on
and if we run this it's just going to be
loading all that into the background
so that's a little back end kind of
stuff then we want to go ahead and do is
we want to go ahead and import
our
specific packages
that we're going to be working with
which is under keras now remember cross
kind of sits on tensorflow so when we're
importing cross and the sequential model
we are in effect importing
tensorflow underneath of it
we just brought in the math probably
should have put that up above
and then we have our cross models we're
going to import sequential
now if you remember from our
slide there was three different options
let me just flip back over there so we
can have a quick
recall on that and so in keras
we have sequential functional and
subclassing so remember those three
different setups in here we talked about
earlier and if you remember from here we
have a sequential where it's going
one tensorflow layer at a time you go
kind of look at think of it as going
from left to right or top to bottom or
whatever direction it's going in but it
goes in one direction all the time where
functional can have a very complicated
graph of directions you can have the
data split into two separate
tensors and then it comes back together
into another tensor
all those kinds of things and then
subclassing is really the really
complicated one where now you're adding
your own subclasses into the tensor to
do external computations right in the
middle of like a huge flow of data
but we're going to stick with sequential
it's not a big jump to go from
sequential to functional
but we're running a sequential
tensorflow and that's what this first
import is here we want to bring in our
sequential and then we have our layers
and let's talk a little bit about these
layers this is where cross and
tensorflow
really are happening this is what makes
them so nice to work with is all these
layers are pre-built
so from cross we have layers import
dense
from cross layers import lstm
when we talk about these layers
cross has so many built in layers you
can do your own layers
the dense layer is your standard neural
network
by default it uses relu for its
activation
and then the lstm is a long short term
memory layer since we're going to be
looking probably at sequential data
we want to go ahead and do the lstm and
if we go into
um cross and we look at their layers
this is across website you can see as we
scroll down for the cross layers that
are built in
we can get down here and we can look at
let's see here we have our layer
activation our base layers um activation
layer weight layer waste there's a lot
of stuff in here we had the relu which
is the basic activation that was listed
up here for layer activations you can
change those
and here we have our core layers
and our dense layers we have an input
layer a dense layer
and then we've added a more customized
one with the long term short term memory
layer and of course you can even do your
own custom layers in cross there's a
whole functionality in there if you're
doing your own thing
what's really nice about this is it's
all built in even the convolutional
layers this is for processing graphics
there's a lot of cool things in here you
can do
this is white cross is so popular it's
open source and you have all these tools
right at your fingertips so from cross
we're just going to import a couple
layers the dense layer
and the long short term memory layer
and then of course from
sk learn our psi kit
we want to go ahead and do our min max
scalar standard scalar for pre editing
our
data and then metrics just so we can
take a look at the errors and compute
those let me go ahead and run this and
that just loads it up we're not
expecting anything from the output and
our file coming in is going to be air
quality dot csv and let's go ahead and
take a quick look at that this is in
open office it's just a standard you
know like we do excel whatever you're
using for your spreadsheet and you can
see here we have a number of columns uh
number of rows it actually goes down to
like 8 000.
the first thing we want to notice is
that the first row
is kind of just a random number put in
going down probably not something we're
going to work with
the second row
is ben dung i'm guessing that's a
reference for the profile
if we scroll to the bottom which i'm not
going to do because it takes forever to
get back up they're all the same uh the
same thing with the status the status is
the same
we have a date so we have a sequential
order here
um here is the jam which i'm going to
guess is the time stamp on there so we
have a date and time
we have our o3 co no2 reading so2 no co2
voc
and then some other numbers here pm1 pm
2.5
pm 10
10 without actually
looking through the data i mean some of
this i can guess is like temperature
humidity i'm not sure what the pms are
but we have a whole slew of data here so
we're looking at air quality as far as
an area in a region and what's going on
with our date time stamps on there and
so code wise we're going to read this
into a pandas data frame so our data
frame df is a nice abbreviation commonly
used for data frames equals pd.read csv
and then our the path to it just happens
to be on my d drive uh separated by
spaces and so if we go ahead and run
this
we'll print out the head of our data and
again this looks very similar to what we
were just looking at
being in jupiter i can take this and go
the other way
make it real small so you can see all
the columns going across and we get a
full view of it
or we can bring it back up in size
that's pretty small on there overshot
but you can see it's the same data we
were just looking at we're looking at
the number we're looking at the profile
which is the bandung the
date we have a timestamp
our o3 count co and so forth on here
and this is just your basic
pandas printing out the top five rows we
could easily have done
three rows
five rows ten whatever you want to put
in there by default that's five for
pandas now i talk about this all the
time so i know i've already said it at
least once or twice during this video
most of our work is in pre-formatting
data what are we looking at how do we
bring it together
so we want to go ahead and start with
our date time it's come in in two
columns
we have our date here and we have our
time
and we want to go ahead and combine that
and then we have this is just a simple
script in there that says combined date
time that's our formula we're building
our we're going to submit our
pandas data frame
and the tab name when we go ahead and do
this
that's all of our information that we
want to go ahead and create
and then goes for i iron range df uh
shape zero
so we're going to go through
the whole setup and we're going to list
tab append df location i
and here is our date going in there and
then return the numpy array list tab d
types date time 64. that's all we're
doing we're just switching this to a
date time stamp and if we go ahead and
do df date time equals combined date
time
and then i always like to
print we'll do df head just we can see
what that looks like
and so when we come out of this uh we
now have our setup on here and of course
it's added it on to the far right here's
our date time
you can see the format's changed
so there's our we've added in the date
time column and we've brought the date
over and we've taken this format here
and it's an actual variable with a 0 0 0
on here well that doesn't look good so
we need to also include the time part of
this and we want to convert it into
hourly data
so let's go ahead and do that
to do that to finish combining our date
time let's go ahead and create a
little
script here to combine the time in there
same thing we just did we're just
creating a numpy array returning a numpy
array and cr forcing this into a date
time format and we can actually spend
hours just going through
these conversions how do you
pull it from the panda's data frame how
do you set it up
so i'm kind of skipping through it a
little fast because i want to stay
focused on tensorflow and cross
keep in mind this is like 80 percent of
your coding when you're doing a lot of
this stuff is going to be reformatting
these things resetting them back up
uh so that it looks right on here and
you know it just takes time to get
through all that but that is usually
what the companies are paying you for
that's what the big
bucks are for
and we want to go ahead and a couple
things going on here is we're going to
go ahead and do our date time we're
going to reorganize some of our setup in
here convert into hourly data we just
put a pause in there
now remember we can select from df are
different columns we're going to be
working with and you're going to see
that we actually dropped
a couple of the columns those ones i
showed you earlier they're just
repetitive data so there's nothing in
there that exciting
and then we want to go ahead and we'll
create a
second
data frame here let me just get rid of
the df head
and df2 is we're going to group by date
time and we're looking at the mean value
and then we'll print that out so you can
see what we're talking about
uh we have now reorganized this so we
put in date time 03 co
so now this is in the same order
as it was before
and you'll see the date time now has our
0 0
same date 1 2 3 and so on so let's group
the data together
so there's a lot more manageable and in
the format we want and in the right
sequential order
and if we go back to
there we go our air quality uh you can
see right here we're looking at um
these columns going across we really
don't need since we're going to create
our own
date time column we can get rid of those
these are the different columns of
information we want and that should
reflect right here in the columns we
picked coming across so this is all the
same columns on there that's all we've
done is reformatted our data
grouped it together by date and then you
can see the different data coming out
set up on there and then as a data
scientist
first thing i want to do is get a
description what am i looking at
and so we can go ahead and do the df2
describe and this gives us our you know
describe gives us our basic data
analytics information we might be
looking for like what is the mean
standard deviation
uh minimum amount maximum amount we have
our first quarter second quarter and
third quarter
numbers also in there
so you can get a quick look at a glance
describing the data or descriptive
analysis
and even though we have our quantile
information in here we're going to dig a
little deeper into that
we're going to calculate the quantile
for each variable
we're going to look at a number of
things for each variable and we'll see
right here q1 we can simply do the
quantile 0.25 percent
which should match
our 25 up here and we'll be looking at
the min the max
and we're just going to do this it's
basically we're breaking this down for
each uh different
variable in there
one of the things that's kind of fun to
do we're going to look at that in just a
second let me get put the next piece of
code in here
clean out some of our
we're going to drop a couple thing our
last rows and first row because those
have
usually have a lot of null values and
the first row is just our titles so
that's important it's important to drop
those rows in here and so this right
here as we look at our different
quantiles
again it's the same you know we're still
looking at the 25
quantile here
we're going to do a little bit more with
this
so now that we've cleared off our first
and last rows we're going to go ahead
and go through
all of our columns and this way we can
look at each
column individually and so we'll just
create a q1 q3 min max min iqr max iqr
and calculate the quantile of i of df2
we're basically doing
the math that they did up here but we're
splitting it apart that's all this is
and
this happens a lot because you might
want to look at individual if this was
my own project i would probably spend
days and days going through
what these different values mean
one of the biggest
data science
things we can look at that's important
is uh use your use your common sense
you know if you're looking at this data
and it doesn't make sense and you go
back in there and you're like wait a
minute what the heck did i just do
at that point you probably should go
back and double check what you have
going on
now uh we're looking at this and you can
see right here here's our attribute for
our o3 so we've broken it down uh we
have our q1 5.88 q3 10.37 if we go back
up here here's our 5.8 we've rounded it
off
10.37 is in there
so we've basically done the same math
just split it up we have our minimum and
our max iqr and that's computed let's
see where is it here we go uh q1 minus
1.5 times iqr and the iqr is your q3
minus q1 so that's the difference
between our two different quarters and
this is all
data science
as far as the hard math
we're really not we're actually trying
to focus on cross and tensorflow you
still got to go through all this stuff i
told you eighty percent of your
programming is going through and
understanding what the heck
uh happened here
what's going on what does this data mean
and so when we're looking in that we're
going to go ahead and say hey um
we've computed these numbers and the
reason we've computed these numbers is
if you take the minimum value and it's
less than your minimum iqr
that means something's going wrong there
and they usually in this case is going
to show us an outlier
so we want to go ahead and find the
minimum value if it's less than the min
minimum iqr it's an outlier and if the
max value is greater than the
max iqr we have an outlier and that's
all this is doing low outliers found
minimum value high outliers found
really important actually outliers are
almost everything in data sometimes
sometimes you do this project just to
find the outliers because you want to
know
crime detection what are we looking for
we're looking for the outliers what
doesn't fit a normal business deal and
then we'll go ahead and throw in
just threw in a lot of code oh my
goodness
so we have if your max is greater than
iqr print outlier is found what we want
to do is we want to start cleaning up
these outliers and so we want to convert
we'll do create a convert nand
x max iqr equals max
underscore iqr min iqr equals mini qr so
this is just saying this is the data
we're going to send that's all that is
in python and if x is greater than the
max iqr and x is less than the min iq
are x equals
null we're going to set it to null why
because we want to clear
these outliers out of the data now again
if you're doing fraud detection you
would do the opposite you would be
cleaning everything else that's not in
that series so that you can look at just
the outlier uh and then we're going to
convert the nand hum again we have x
max iqr is 100
min iqr is men iqr
if x is greater than max iqr and x is
less than min iqr again we're going to
return
null value otherwise it's going to
remain the same value x x equals x
and you can see as we go through the
code if i equals
our hum
then we go ahead and do this
that's a column specific to humidity
that's your hum column
uh then we're going to go ahead and
convert do the
run a map on there and convert the non
hum
uh you can see here it's this is just
cleanup uh we run we found out that
humidity probably has some weird values
in it uh we have our outliers um
that's all this is
and so when we go ahead and finish this
and we take a look at our outliers
and we run this code here
we have a low outlier 2.04 we have a
high outlier
99.06
outliers have been interpolated
that means we've given them a new value
chances are these days when you're
looking at something like
these sensors coming in
they probably have a failed sensor in
there something went wrong
that's the kind of thing that you really
don't want to do your data analysis on
so that's what we're doing is we're
pulling that out and then
converting it over and setting it up
method linear
so we interpolate
the linears it's going to fill that data
in
based on a linear regression model of
similar data
same thing with this up here with the
df2i interpolate that's what we're doing
again this is all data prep
we're not actually talking about
tensorflow we're just trying to get all
our data
set up correctly so that when we run it
it's not going to cause problems or have
a huge bias
so we've dealt with outliers
specifically in humidity
and again this is one of these things
where when we start running
we run through this you can see down
here that we have our outliers found
high low outliers
migrated them in
we also know there's other issues going
on with this data
how do we know that
some of it's just looking at the data
playing with it until you start
understanding what's going on let's take
the temp value
and we're going to go ahead and and use
a logarithmic function on the temp value
and
it's interesting because it's like how
do you how do you heck do you even know
to use logarithmic on the temp value
that's domain specific
we're talking about being an expert in
air care i'm not an expert in air care
you know it's not what i go look at i
don't look at air care data in fact this
is probably the first air care data set
up i've looked at but the experts come
in there and they come to you and say
hey in data science this is a
exponentially variable variable on here
so we need to go ahead and do
transform it
and use a logarithmic scale on that
so at that point that would be coming
from your
data here we go data science programmer
overview does a lot of stuff connecting
the database and connecting in with the
experts
data analytics a lot of times you're
talking about somebody who is a data
analysis might be all the way usually a
phd level
data science programming level
interfaces database manager that's going
to be the person who's your admin
working on it
so when we're looking at this we're
looking at something they've sent to me
and they said hey
domain air care
this needs to be this is a skew because
the data just goes up exponentially and
affects everything else and we'll go
ahead and take that data let me just go
ahead and run this
just for another quick look at it
we have our
uh we'll do a distribution df
we'll create another data frame from the
temp values and then from a data set
from the log temp so we can put them
side by side and we'll just go ahead and
do a quick histogram and this is kind of
nice plot a figure figure size here's
our plt from matplot library
and then we'll just do a distribution
underscore df there's our data frames
this is nice because it just integrates
the histogram right into pandas love
pandas
and this is a chart you would send back
to your data analysis and say hey is
this what you wanted this is how the
data is converting on here as a data
sciences scientist the first thing i
note is we've gone from a 10
20 30 scale to 2.5 3.0 3.5 scale
and the data itself has kind of been uh
adjusted a little bit based on some kind
of a skew on there so let's jump into
we're getting a little closer to
actually doing our
cross on here we'll go ahead and split
our data up
and this of course is any good data
scientists
you want to have a training set and a
test set uh
and we'll go ahead and do the train size
we're going to use 0.75 of the data make
sure it's an integer we don't want to
take a slice as a float value give you a
nice error
uh and we'll have our train size of 75
percent and the test size is going to be
of course the
train size minus the length of the data
set and then we can simply do train
comma test
here's our data set
which is going to be the train size the
test size
and then if we go and print this let me
just go ahead and run this we can see
how these values
split it's a nice split of 1298 and then
433 points of value
that are going to be for our
setup on here and if you remember we're
specifically looking at the data set
where did we create that data set from
that was from up here that's what we
called the uh logarithmic uh value of
the temp
that's where the data set came from so
we're looking at just that column with
this train size and the test with the
train and test data set here and let's
go ahead and do uh converted an array of
values into a data set matrix we're
going to create a little um
set up in here we create our data set
our data set's going to come in we're
going to do a look back of one so we're
going to look back one piece of data
going backward
and we have our data x and our data y
for i and range length of data set look
back -1
this is creating let me just go ahead
and run this actually the best way to do
this
is to go ahead and create this data and
take a look at the shape of it let me go
ahead and just put that code in here
so we're going to do the look back one
here's our train x our train y
and it's going to be adding the data on
there and then when we come up here
and we take a look at the shape
there we go um and we run this piece of
code here
we look at the shape on this and we have
a new slightly different change on here
but we have a shape of x 1296 comma 1
shape of y train y
test x text y
and so what we're looking at is that
the x comes in
and we're only having a single value at
out
we want to predict what the next one is
that's what this little piece of code is
here for what are we looking for well we
want to look back one that's the um what
we're going to train the data with is
yesterday's data yesterday says hey the
humidity was at
97 percent what should today's humidity
be at if it's 97 yesterday is it going
to go up or is it going to go down today
if 97 does it go up to 100 what's going
on there uh and so our we're looking
forward to the next piece of data which
says hey tomorrow's is going to be
today's humidity is this this is what
tomorrow's humidity is going to be
that's all that is all that is is
stacking our data so that
our y is basically
x plus 1 or x could be y minus 1.
and then a couple things to note is our
x data
we're only dealing with the one column
but you need to have it in a shape that
has it by the columns so you have the
two different numbers and since we're
doing just a single point of data
we have and you'll see with the train y
we don't need to have the extra shape on
here
now this is going to run into a problem
and the reason is is that we have what
they call a time step
and the time step is that long term
short term memory layer
so we're going to add another reshape on
here let me just go down here and put it
into the next cell
and so we want to reshape the input
array in the form of sample time step
features
we're only looking at one feature
and i mean this is one of those things
when you're playing with this you're
like why am i getting an error in the
numpy array why is this giving me
something weird going on
so we're going to do is we're going to
add one more
level on here instead of being 1299.1 we
want to go one more
and when they put the code together in
the back you can see we kept the same
shape the 1299
we added the one dimension and then we
have our train x shape one
and this could have depends again on how
far back in the long short term memory
you want to go
that is what that piece of code is for
and that reshape is and you can see the
new shape is now one
1299 one one
versus the 1299 one and then the other
part of the shape 432 one one again this
is our tr our x in and of course our
test x and then our y is just a single
column because we're just doing one
output that we're looking for
so now we've done our 80 percent um you
know that's all the the
writing all the code reformatting our
data
um
bringing it in now we want to go ahead
and do the fun part which is we're going
to go ahead and create and fit
the lstm neural network
and if we're going to do that the first
thing we need is we're going to need to
go ahead and create a model and we'll do
this sequential model
and if you remember sequential means it
just goes in order that means we have if
you have two layers the layers go from
layer one to layer two or layer zero to
layer one
this is different than functional
functional allows you to split the data
and run two completely separate models
and then bring them back together
we're doing just sequential on here and
then we decided to do the long short
term memory
and we have our input shape which it
comes in again this is what all this
switching was we could have easily made
this one two three or four going back as
far as the end number on there we just
stuck to going back one
and it's always a good idea when you get
to this point where the heck
is this model coming from um what kind
of models do we have available
and uh there's let me go and put the
next model in there uh because we're
gonna do two models and the next model
is going to go ahead and we're going to
do dent so we have model equal
sequential
and then we're going to add the lstm
model and then we're going to add a
dense model and if you remember from the
very top of our code
when we did the imports oops there we go
our cross
this is it right here here's our
importing a dense model and here's our
importing an lstm now just about every
tensorflow model uses dents uh your
dense model is your basic forward
propagation reverse propagation error
or it does reverse propagation to
program the model
so any of your neural networks you've
already looked at that
luxon says here's the error and sends
the error backwards that's what this is
the long short term memory is a little
different the real question that we want
to look at right now is where do you
find these models what kind of models do
you have available and so for that let's
go to the cross website
which is the cross dot io
if you go under api
layers and i always have to do a search
just search for caras api layers it'll
open up and you can see we have
your base layers right here class
trainable weights all kinds of stuff
like there your activation uh so a lot
of your layers you can switch how it
activates uh relu which is like your
smaller arrays or if you're doing
convolutional neural networks the
convolution usually uses a relu
your sigmoid all the way up to soft mac
soft plus all these different choices as
far as how those
are set up and what we want to do is we
want to go ahead and if you scroll down
here you'll see your core layers and
here is your dense layer
so you have an input object your dense
layer your activation layer embedding
layer
this is your your kind of your one set
up on there that's most common
convolutional neural networks or
convolutional layers these are like for
doing uh image categorizing so trying to
find objects in a picture that kind of
thing
we have pooling layers so as you have
the layers come together
usually bring them down into
a single layer although you can still do
like global max pulling 3d and there is
just i mean this list just goes on and
on there's all kinds of different things
hidden in here as far as what you can do
and it changes you know you go in here
and you just have to do a search for
what you're looking for
and figure out what's going to work best
for you
as far as which project you're working
on
long short term memory is a big one
because this is when we start talking
about text
what if someone says the what comes
after the
the cat in the hat a little kid's book
there
starts programming it and so you really
want to know not only
what's going on but it's going to be
something that has a history the history
behind it tells you what the next one
coming up is now once we've built all
our different you know we built our
model we've added our different layers
we went in there
play with it remember if you're in
functional you can actually link these
layers together and they branch out and
come back together if you do a
the sub
setup then you can create your own
different model you can embed a model in
there that might become linear
regression you can embed a linear
regression model
as part of your functional split and
then have that come back together with
other things
so we're going to go ahead and compile
your model this brings everything
together we're going to put in what the
loss is which will use the mean squared
error and we'll go ahead and use the
atom optimizer clearly there's a lot of
choices on here depending on what you're
doing and just like any of these
different prediction models if you've
been doing any
scikit from python
you'll recognize that we have to then
fit the model
so what are we doing in here we're going
to send in our train x our train y
um we're going to decide how many epochs
we're going to run it through
500 is probably a lot for this i'm
guessing it'd probably be about two or
three hundred probably do just fine
our batch size
so how many different when you process
it this is the math behind it
if you're in data analytics
you might try know what this number is
as a data scientist where i haven't had
the phd level math that says this is why
you want to use this particular batch
size you kind of play with this number a
little bit
you can dig deeper into the math
see how it affects the results depending
what you're doing and there's a number
of other settings on here we did verbose
2. i'd have to actually look that up to
tell you what verbose means i think
that's actually the default on there if
i remember correctly
there's a lot of different settings when
you go to fit it
the big ones are your epic and your
batch size those are what we're looking
for
and so we're going to go ahead and run
this
and this is going to take a few minutes
to run because it's going through
500 times
through all the data so if you have a
huge data set this is the point where
you're kind of wondering oh my gosh is
this going to finish tomorrow um if i'm
running this on a single machine and i
have a tera terabyte of data
going into it
if this is my personal computer and i'm
running a terabyte of data into this um
you know this is running rather quickly
through all 500 iterations
but yeah a terabyte of data we're
talking something closer to days week
you know even with a
3.5 gigahertz machine in in eight cores
it's still going to take a long time to
go through a full terabyte of data
and then we want to start looking at
putting it into some other framework
like spark or something that will print
the process on there more across
multiple
processors and multiple computers
and if we scroll all the way down to the
bottom you're going to see here's our
square mean error of 0.0088
if we scroll way up you'll see it kind
of oscillates between 0.0888 and 0.8089
it's right around 2 to 250 where you
start seeing that oscillation which
really not going anywhere so we really
didn't need to go through a full 500
epics
you know if you're retraining the stuff
over and over again it's kind of good to
know where that
error zone is so you don't have to do
all the extra processing of course if
you're going to build a model
we want to go ahead and run a prediction
on it
so let's go ahead and make our
prediction remember we have our training
test set
and our test set or that
we have the
train x and the train y for training it
or train predict and then we have our
test x and our test y going in there uh
so we can test to see how good it did
uh and we come in here we have
you'll see right here we go ahead and do
our train predict equals
model predict train x
and test predict model predict test x
why would we want to run the prediction
on train x well it's not a hundred
percent on its prediction we know it has
a certain amount of error and we want to
compare the error we have on what we
programmed it with with the error we get
when we run it on new data that's never
seen the model's never seen before and
one of the things we can do we go ahead
and invert the predictions uh this helps
us
level it off a little bit more um
get rid of some of our bias we have
train predict equals and np
exponential m1 the train predict and
then train y equals the exponential m1
for train y and then we do the again
that with train test predict and test y
um
again reformatting the data so that we
can it all matches and then we want to
go ahead and calculate the root mean
square error
so we have our train score uh which is
your math square root times the mean
square root error train
y and train predict and again we're just
um
this is just feeding the data through so
we can compare it and the same thing
with the test
and let's take a look at that because
really
the code makes sense if you're going
through it line by line you can see
we're doing but the answer really helps
to zoom in
so we have a train score which is 2.40
of our root mean square error
and we have a test score of 3.16 of the
root mean square error
if these were reversed if our test score
is better than our training score
then we've over trained something's
really wrong at that point you got to go
back and figure out what you did wrong
because you should never have a better
result on your test data than you do
when you're training data and that's why
we put them both through that's why we
look at the error for both the training
and the testing
when you're going out and quoting you're
publishing this you're saying hey how
good is my model it's the test score
that you're showing people this is what
it did on my test data that the model
had never seen before this is how good
my model is
and a lot of times you actually want to
put together like a little formal code
where we actually want to print that out
and if we print that out you can see
down here
test prediction standard deviation of
data set 3.16 is less than 4.40
i'd have to go back
and we're
up here if you remember we did the
square means error this is standard
deviation that's why these numbers are
different
it's saying the same thing that we just
talked about
uh
3.16 is less than 4.40 model is good
enough we're saying hey this is this
model is valid we have a valid model
here so we can go ahead and go with that
uh and along with
putting a formal print out of there
we want to go ahead and plot what's
going on
uh and this we just want to pretty graft
here so that people can see what's going
on when i walk into a meeting and i'm
dealing with a number of people
they really don't want these numbers
they don't want to say hey what's i mean
standard deviation unless you know what
statistics are
um you might be dealing with a number of
different departments head of cells
might not work with standard deviation
or have any idea what that really means
number wise
and so at this point we really want to
put it in a graph so we have something
to display and with displaying you
remember that we're looking at uh the
data
today going into it and what's going to
happen tomorrow so let's take a quick
look at this uh we're going to go ahead
and shift the train predictions for
plotting uh we have our train predict
plot
np empty like data set
train predict plot
set it up with null values
you know it's just kind of it's kind of
a weird thing where we're creating the
um
the
data groups as we like them
and then putting the data in there is
what's going on here
so we have our train predict plot
which is going to be our look back our
length
plus look back
we're just is going to equal train
train predict so we're creating this
basically we're taking this and we're
dumping the train predict into it so now
we have our nice train predict plot
and then we have the shift test
predictions for the plotting uh we're
going to continue more of that oops
looks like i put it in here double no
it's just uh yeah they put it in here
double
didn't mean to do that
we really only need to do it once oh
here we go
this is where the problem was is because
this is the test predict
so we have our training prediction we're
doing the shift on here and then the
test predict we're going to look at that
same thing we're just creating those two
data sets
uh test
predict plot length prediction
setup on there and then we're going to
go through the plotting the original
data set and the predictions so we have
a time axis always nice to have your
time set on there
set that to the time array
time axes lap
all this is setting up the time variable
for the bottom and then we have a lot of
stuff going on here as far as setting up
our figure
let's go ahead and run that and then
we'll break it down
we have on here
our main plot we have two different
plots going on here
the ispu going up and the data and the
ispu here with all these different
settings on it
and so we look at this we have our ax1
that's the main plot i mean our ax
that's the main plot and we have our ax1
which is the secondary plot over here so
we're doing a figure plt
or plt.figure
and we're going to dump those two graphs
on there
and so we take
and if you
go through the code piece by piece
which we're not going to do we're going
to do the
the
data set here
exponential reverse exponential so it
looks correctly we're going to label it
the original data set
um we're going to plot the train predict
plot that's what we just created
we're going to make that orange and
we'll label it train prediction uh test
prediction plot we're gonna make that
red and label it test prediction and so
forth um set our ticks up this actually
just put ticks time axes gets its ticks
the little little marks there going
along the axes that kind of thing and
let's take a look and see what these
graphs look like
and these are just kind of fun you know
when you show up into a meeting and this
is the final output you say hey this is
what we're looking at
here's our original data in blue
here's our training prediction um you
can see that it trains pretty close to
what the data is up there
i would also probably put a um like a
little little time stamp and do just
right before and right after where we go
from train to test prediction
and you can see with the test prediction
the data comes in in red
and then you can also see what the
original data set look like behind it
and how it differs
and then we can just isolate that here
on the right that's all this is
is just the
test prediction on the right uh and it's
you know there's you'll see with the
original data set there's a lot of peaks
were missing and a lot of lows are
missing
but as far as the actual test prediction
it's pretty does pretty good it's pretty
right on you can get a good idea what to
expect for your ispu
and so from this we would probably
publish it and say hey this is
what you expect and this is our area of
this is a range of error that's the kind
of thing i put out
on a daily basis maybe we predict the
cells are going to be this or maybe
weekly so you kind of get a nice kind of
flatten the
data coming out and you say hey this is
what we're looking at the big takeaway
from this is that we're working with
let me go back up here oops oh too far
there we go
is this model here this is what this is
all about we worked through all of those
pieces all the tensorflows and that is
to build this sequential model and we're
only putting in the two layers
this can get pretty complicated if you
get too complicated it never
it never verges into a usable model
so if you have like 30 different layers
in here there's a good chance you might
crash it kind of thing
so don't go too haywire on that and that
you kind of learn as you go again it's
domain knowledge
and also starting to understand all
these different layers and what they
mean
the data
analytics behind those layers
is something that your data analysis
professional will come in and say this
is what we want to try
but i tell you as a data scientist
um a lot of these basic setups are
common and i don't know how many times
uh
working with somebody and they're like
oh my gosh if i only did a tangent h
instead of a relu activation
i worked for two weeks to figure that
out well as a data science i can run it
through the model in you know five
minutes instead of spending two weeks
doing the the math behind it
so that's one of the advantages of data
scientists is we do it from programming
side and a data analytics is going to
look for it how does it work in math
and this is really the core right here
of tensorflow and cross is being able to
build your data model quickly and
efficiently and of course uh with any
data science putting out a pretty graph
so that your shareholders again we want
to take and
reduce the information down to something
people can look at and say oh that's
what's going on they can see stuff
what's going on as far as the dates and
the change in the ispu
so what is the site kit learn it's
simple and efficient tool for data
mining and data analysis it's built on
numpy scipy and matplot library so it
interfaces very well with these other
modules and it's an open source
commercially usable bsd license bsd
originally stood for berkeley software
distribution license but it means it's
open source with very few restrictions
as far as what you can do with it
another reason to really like the site
kit learn setup so you don't have to pay
for it as a commercial license versus
many other copyrighted platforms out
there what we can achieve using the
scikit-learn we use class the two main
things are classification and regression
models classification identifying which
category an object belongs to for one
application very commonly used is spam
detection so is it a spam or is it not a
spam yes no in banking it might be is
this a good loan bad loan today we'll be
looking at wine is it going to be a good
wine or a bad wine and regression is
predicting an attribute associated with
an object one example is stock prices
prediction what is going to be the next
value if the stock today sold for
23 dollars and five cents a share what
do you think it's going to sell for
tomorrow and the next day and the next
day so that would be a regression model
same thing with weather weather
forecasting any of these are regression
models we're looking at one specific
prediction on one attribute today we
will be doing classification like i said
we're gonna be looking at whether a wine
is good or bad but certainly the
regression model which is in many cases
more useful because you're looking for
an actual value is also a little harder
to follow sometimes so classification is
a really good place to start we can also
do clustering and model selection
clustering is taking an automatic
grouping of similar objects into sets
customer segmentation is an example so
we have these customers like this
they'll probably also like this or if
you like this particular kind of
features on your objects maybe you like
these other objects so it's a referral
is a good one especially on amazon.com
or any of your shopping networks model
selection comparing validating and
choosing parameters and models now this
is actually a little bit deeper as far
as a site kit learn we're looking at
different models for predicting the
right course or the best course or
what's the best solution today like i
said we're looking at wines it's going
to be how do you get the best wine out
of this so we can compare different
models and we'll look a little bit at
that and improve the model's accuracy
via different parameters and fine-tuning
now this is only part one so we're not
gonna do too much tuning on the models
we're looking at but i'll point them out
as we go two other features
dimensionality reduction and
pre-processing dimensionality reduction
is we're reducing the number of random
variables to consider this increases the
model efficiency we won't touch that in
today's tutorial but be aware if you
have you know thousands of columns of
data coming in thousands of features
some of those are going to be duplicated
or some of them you can combine to form
a new column and by reducing all those
different features into a smaller amount
you can have a you can increase the
efficiency of your model it can process
faster and in some cases you'll be less
biased because if you're weighing it on
the same feature over and over again
it's going to be biased to that feature
and pre-processing these are both
pre-processing but pre-processing is
feature extraction and normalization so
we're going to be transforming input
data such as text for use with machine
learning algorithms we'll be doing a
simple scaling in this one for our
pre-processing and i'll point that out
when we get to that and we can discuss
pre-processing at that point with that
let's go ahead and roll up our sleeves
and dive in and see what we got here now
i like to use the jupiter notebook and i
use it out of the anaconda navigator so
if you install the anaconda navigator by
default it will come with the jupiter
notebook or you can install the jupiter
notebook by itself this code will work
in any of your python setups i believe
i'm running an environment of 3.7 set up
on there i'd have to go in here in
environments and look it up for the
python setup was one of the three x's
and we go and launch this and this will
open it up in a web browser so it's kind
of nice it keeps everything separate and
in this anaconda you can actually have
different environments different
versions of python different modules
installed in each environment so it's a
very powerful tool if you're doing a lot
of development and jupiter notebook is
just a wonderful visual display
certainly you can use i know spyder is
another one which is installed with the
anaconda i actually use a simple notepad
plus plus when i'm doing some of my
python script any of your ides will work
fine jupiter notebook is iron python
because it's designed for the interface
but it's good to be aware of these
different tools
and when i launch the jupyter notebook
it'll open up like i said a web page in
here and we'll go over here to new and
create a new python setup like i said i
believe this is python37 but any of the
three this the scikit learn works with
any of the three x's there's even two
seven versions so it's been around a
long time so it's very big on the
development side and then the
guys in the back guys and gals developed
they went ahead and put this together
for me and let's go ahead and import our
different packages now if you've been
reading some of our other tutorials
you'll recognize pandas as pd pandas
library is pretty widely used it's a
data frame set up so it's just like
columns and rows in a spreadsheet with a
lot of different features for looking
stuff up seaborne sits on top of map
plot libraries this is for our graphing
and we'll see that how quick it is to
throw a graph out there to view in the
jupiter notebook for demos and showing
people what's going on and then we're
going to use the random forest the svc
or support vector classifier
and also the neural network so we're
going to look at this we're actually
going to go through and look at three
different classifiers that are most
common some of the most common
classifiers and let's show how those
work in the scikit-learn setup and how
they're different and then if you're
going to do your setup on here you'll
want to go ahead and import some metrics
so the
sklearn.metrics on here and we'll use
the confusion metrics and the
classification report out of that and
then we're going to use from the sklearn
pre-processing the standard scalar and
label encoder standard scalar is
probably the most commonly used
pre-processing there's a lot of
different pre-processing packages in the
sklearn and then model selection for
splitting our data up it's one of the
many ways we can split data into
different sections and the last line
here is our percentage map plot library
in line some of the seaboard and map
plot library will go ahead and display
perfectly inline without this and some
won't it's good to always include this
when you're in the jupiter notebook this
is jupiter notebook so if you're in ide
when you run this it'll actually open up
a new window and display the graphics
that way so you only need this if you're
running it in a editor like this one
with the specifically jupiter notebook
i'm not even familiar with other editors
that are like this but i'm sure they're
out there i'm sure there's a firefox
version or something jupiter notebook
just happens to be the most widely used
out there and we can go ahead and hit
the run button and this now has saved
all this underneath the packages so my
packages are now all loaded i've run
them whether you run it on top we run it
to the left and all the packages are up
there so we now have them all available
to us for our project we're working on
and i'm just gonna make a little side
note on that when you're playing with
these and you delete something out and
add something in even if i went back and
deleted this cell and just hit the
scissors up here these are still loaded
in this kernel so until i go under
kernel and restart or restart and clear
or restart and run all i'll still have
access to pandas
important to know because i've done that
before i've loaded up maybe not a module
here but i've loaded up my own code and
then changed my mind and wondering why
is it keep putting out the wrong output
and then i realized it's still loaded in
the kernel and you have to restart the
kernel just a quick side note for
working with a jupiter notebook and one
of the troubleshooting things that comes
up and we're going to go and load up our
data set we're using the pandas so if
you haven't yet go look at our pandas
tutorial a simple read the csv with the
separation on here let me go ahead and
run that and that's now loaded into the
variable wine and let's take a quick
look at the actual file i always like to
look at the actual data i'm working with
in this case we have wine quality dash
red i'll just open that up i have it in
my open office set up separated by
semicolons that's important to notice
and we open that up you'll see we have
go all the way down here it looks like
1600 lines of data minus the first one
so 15
1599 lines and we have a number of
features going across the last one is
quality and right off the bat we see the
quality is uh has different numbers in
it five six seven it's not really i'm
not sure how high of a level it goes but
i don't see anything over a seven so
it's kind of five through seven is what
i see here five six and seven four five
six and seven looking to see if there's
any other values in there looking
through the demo to begin with i didn't
realize the setup on this so you can see
there's a different quality values in
there alcohol sulfates ph density total
sulfur dioxide and so on those are all
the features we're going to be looking
at
and since this is a pandas we'll just do
wine head and that prints our first five
rolls rows of data that's of course a
pandas command and we can see that looks
very similar to what we're looking at
before we have everything across here
it's automatically assigned an index on
the left that's what pandas does if you
don't give it an index and for the
column names it has assigned the first
row so we have our first row of data
pulled off the our comma separated
variable file in this case uh semicolon
separated and it shows the different
features going across and we have what
one two three four five six seven eight
nine ten eleven features twelve
including quality but that's the one we
wanna work on and understand and then
because we're in uh panda's data frame
we can also do wine.info and let's go
ahead and run that this tells us a lot
about our variables we're working with
you'll see here that there is
1599 that's what i said from the
spreadsheet so that looks correct
non-null float 64. this is very
important information especially the
non-null so there's no null values in
here that can really trip us up in
pre-processing and there's a number of
ways to process non-null values one is
just to delete that data out of there so
if you have enough data in there you
might just delete your non-null values
another one is to fill that information
in with like the average or the most
common values or other such means but
we're not gonna have to worry about that
but we'll look at another way because we
can also do wine is null and sum it up
and this will give us a similar it won't
tell us that these are float values but
it will give us a summation oops there
we go let me run that it'll give us a
summation on here how many null values
in each one so if you wanted to you know
from here you would be able to say okay
this is a null value but it doesn't tell
you how many are null values this one
would clearly tell you that you have
maybe five null values here two null
values here and you might just if you
had only seven null values and all that
different data you'd probably just
delete them out where if ninety percent
of the data was null values you might
rethink either a different data
collection setup
or find a different way to deal with the
null values we'll talk about that just a
little bit in the models too because the
models themselves have some built-in
features especially the forest model
which we're going to look at at this
point we need to make a choice and to
keep it simple we're going to do a
little pre-processing of the data and
we're going to create some bins and bins
we're going to do is 2 comma 6.5 comma
8. what this means is that we're going
to take those values if you remember up
here let me scroll back up here we had
our quality the quality comes out
between two and eight basically or one
and eight we have five five five six you
can see just in the just in the first
five lines of variation in quality
we're going to separate that into just
two
bends of quality and so we've decided to
create two bins and we have bad and good
it's going to be the labels on those two
bins we have a spread of 6.5 and an
exact index of 8. the exact index is
because we're doing 0 to 8 on there the
6.5 we can change we could actually make
this smaller or greater but we're only
looking for the really good wine we're
not looking for the zero one two three
four five
six we're looking for wines with seven
or eight on them so high quality you
know like this is what i want to put on
my dinner table at night
i might taste the good wine not the
semi-good wine or mediocre wine and then
this is a panda so pd remember stands
for pandas pandas cut means we're
cutting out the wine quality and we're
replacing it and then we have our bins
equals bins that's the command bins is
the actual command and then our variable
bins to comma 6.58 so two different bins
and our labels bad and good and we can
also do
let me just do it this way wine quality
since that's what we're working on and
let's look at unique another pandas
command and we'll run this and i get
this lovely error why did i get an error
well because i replaced wine quality and
i did this cut here which changes things
on here so i literally altered one of
the variables is saved in the memory so
we'll go up here to the kernel restart
and run all that starts it from the very
beginning and we can see here that that
fixes the error because i'm not cutting
something that's already been cut we
have our wine quality unique and the
wine quality unique is a bad or good so
we have two qualities objects bad is
less than good meaning bad's going to be
zero and good's going to be one and to
make that happen we need to actually
encode it so we'll use the label quality
equals label encoder and the label
encoder let me just go back there so
this is part of sklearn that was one of
the things we imported was a label
encoder you can see that right here from
the
sklearn.processing import standard
scalar which we're going to use in a
minute and label encoder and that's what
tells it to use bad equals 0 and good
equals 1. and we'll go ahead and run
that and then we need to apply it to the
data and when we do that we take our
wine quality that we had before and
we're going to set that equal to label
quality which is our encoder and let's
look at this line right here we have dot
fit transform and you'll see this in the
pre-processing these are the most common
used is fit transform and fit transform
because they're so often that you're
also transforming the data when you fit
it they just combined them into one
command and we're just going to take the
wine quality feed it back into there and
put that back in our wine quality setup
and run that and now when we do
the wine
and the head of the first five values
and we go ahead and run this you can see
right here underneath quality zero zero
zero i have to go down a little further
to look at the better wines
let's see if we have some that are ones
yeah there we go there's some ones down
here so when you look at 10 of them you
can see all the way down to zero or one
that's our quality and again we're
looking at high quality we're looking at
the seven and the eights or six point
five and up and uh let's go ahead and
grab our or was it here we go wine
quality let's take another look at what
else more information about the wine
quality itself and we can do a simple
pandas thing value
counts hopefully i typed that in there
correctly and we can see that we only
have 217
of our wines which are going to be the
higher quality so 217 and the rest of
them fall into the bad bucket and the
zero which is uh 1382 so again we're
just looking for the top percentage of
these the top what is that it's probably
about a little under 20
on there so we're looking for our top
wines our seven and eights and let's use
our let's plot this on a graph so we
take a look at this and the sns if you
remember correctly that is let me just
go back to the top that's our seabourn
seaborn sits on top of matplot library
it has a lot of added features plus all
the features of the matplot library and
also makes it quick and easy to put out
a graph we'll do a simple bar graph and
they actually call it count plot and
then we want to just do count plot the
wine quality so let's put our wine
quality in there and let's go ahead and
run this and see what that looks like
and nice inline remember this is why we
did the inline so make sure it appears
in here and you can see the blue space
or the first space represents low
quality wine and our second bar is a
high quality line and you can see that
we're just looking at the top quality
wine here most of wine we want to just
give it away to the neighbors no maybe
if you don't like your neighbors maybe
give them the good quality wine and i
don't know what to do with the bad
quality wine i guess use it for cooking
there we go but you can see here it
forms a nice little graph for us with
the seaboard on there and you can see
our setup on that so now we've looked at
we've done some pre-processing we've
described our data a little bit we have
a picture of how much of the wine what
we expect it to be high quality low
quality checked out the fact that
there's none we don't have any null
values to contend with or any odd values
some of the other things you sometimes
look at these is if you have like some
values that are just way off the chart
so the measurement might be off or
miscalibrated equipment if you're in the
scientific field so the next step we
want to go ahead and do is we want to go
ahead and separate our data set or
reformat our data set and we usually use
capital x and that denotes the features
we're working with and we usually use a
lowercase y that denotes what uh in this
case quality what we're looking for and
we can take this we can go wine it's
going to be our full thing of wine
dropping what are we dropping we're
dropping the quality so these are all
the features minus quality and make sure
we have our axes equals one if you left
it out it would still come out correctly
just because of the way it processes uh
on the defaults and then our y if we're
going to remove quality for our x that's
just going to be wine and it is just the
quality that we're looking at for y so
we put that in there and we'll go ahead
and run this so now we've separated the
features that we want to use to predict
the quality of the wine and the quality
itself the next step is if you're going
to create a data set in a model we got
to know how good our model is so we're
going to split the data train and test
splitting data and this is one of the
packages we imported from sklearn and
the actual package was train test split
and we're going to do x y test size 0.2
random state 42. and this returns four
variables and most common you'll see is
capital x train so we're going to train
our set with capital x test that's the
data we're going to keep on the side to
test it with y train y remember stands
for the quality or the answer we're
looking for so when we train it we're
going to use x train and y train and
then y test to see how good our x test
does and the train test split let me
just go back up to the top that was part
of the sklearn model selection import
train test split there is a lot of ways
to split data up this is when you're
first starting you do your first model
you probably start with the basics on
here you have one test for training one
for test our test size is point two or
twenty percent and random state just
means we just start with a it's like a
random seed number so it's not too
important back there we're randomly
selecting which ones we're going to use
since this is the most common way this
is what we're going to use today there
is and it's not even an sklearn package
yet so someone's still putting it in
there one of the new things they do is
they split the data into thirds and then
they'll run the model on each of they
combine each of those thirds into two
thirds for training and one for testing
and so you actually go through all the
data and you come up with three
different test results from it which is
pretty cool that's a pretty cool way of
doing it you could actually do that with
this by just splitting this into thirds
and then or you have a test size one
test set third and then split the
training set also into thirds and also
do that and get three different data
sets this works fine for most projects
especially when you're starting out it
works great so we have our x train our x
test our y train and our y test and then
we need to go ahead and do the scaler
and let's talk about this because this
is really important some models do not
need to have scaling going on most
models do and so we create our scalar
variable we'll call it sc standard
scalar
and if you remember correctly we
imported that here wrong with the label
encoder the standard scalar setup so
there's our scalar and this is going to
convert the values instead of having
some values that go from zero if you
remember up here we had some values are
54 60 40 59 102. so our total sulfur
dioxide would have these huge values
coming into our model and some models
would look at that and they'd become
very biased to sulfur dioxide it'd have
the hugest impact and then a value that
had
0.076 0.098 or chlorides would have very
little impact because it's such a small
number so we take the scalar we kind of
level the playing field and depending on
our scalar it sets it up between zero
and one a lot of times is what it does
let's go ahead and take a look at that
and we'll go ahead and start with our x
train and our x train equals sc fit
transform we talked about that earlier
that's an sk learn setup that's going to
both fit and transform our x train into
our x train variable and if we have an x
train we also need to do that to our
test and this is important because you
need to note that you don't want to
refit the data we want to use the same
fit we used on the training is on the
testing otherwise you get different
results and so we'll do just oops not
fit
transform we're only going to transform
the test side of the data so here's our
x test that we want to transform and
let's go ahead and run that and just so
we have an idea let's go ahead and take
and just print out our x train oh let's
do
first 10 variables very similar to the
way you do the head on a data frame you
can see here our variables are now much
more uniform and they've scaled them to
the same scale so they're between
certain numbers and with the basic
scalar you can fine tune it i just let
it do its defaults on this and that's
fine for what we're doing in most cases
you don't really need to mess with it
too much it does look like it goes
between like minus probably minus 2 to 2
or something like that that's just
looking at the train variable i'll go
ahead and cut that one out of there so
before we actually build the models and
start discussing the sk-learn models
we're going to use we covered a lot of
ground here most of when you're working
with these models you put a lot of work
into pre-prepping the data so we looked
at the data notice that it's uh
separated loaded it up we went in there
we found out there's no null values
that's hard to say no nodal values we
have there's none there's none nobody i
can't say it
and of course we sum it up if you had a
lot of null values this would be really
important coming in here so is there a
null summary we looked at pre-processing
the data as far as the quality and we're
looking at the bins so this would be
something you might start playing with
maybe you don't want super fine wine you
don't want the seven and eights maybe
you want to split this differently so
certainly you can play with the bins and
get different values and make the bins
smaller or lean more towards the lower
quality so you then have like medium to
high quality and we went ahead and gave
it labels again this is all pandas we're
doing in here setting up with unique
labels and group names bad good badass
lesson good that could be so important
you don't know how many times people go
through these models and they have them
reversed or something and then they go
back they're like why is this data not
looking correct so it's important to
remember what you're doing up here and
double check it and we used our label
encoder so that was
to set that up as quality zero one good
in this case we have bad good zero one
and we just double check that to make
sure that's what came up in the quality
there and we threw it into a graph
because people like to see graphs i
don't know about you but you start
looking all these numbers and all this
text and you get down here and you say
oh yes you know here this is how much of
the wine we're going to label as subpar
not good this is how much we're going to
label as good and then we go down here
to finally separating out our data so
it's ready to go into the models and the
models take x and a y in this case x is
all of our features minus the one we're
looking for and then y is the features
we're looking for so in this case we
dropped quality and in the y case we
added quality and then because we need
to have a training set and a test set so
we can see how good our models do we
went ahead and split the models up x
train x test y train y test and that's
using the train test split which is part
of the sk learn package and we did as
far as our testing size point two or
twenty percent the default is twenty
five percent so if you leave that out
it'll do default setup and we did a
random state equals 42. if you leave
that out it'll use a random state i
believe it's default one i'd have to
look that back up and then finally we
scaled the data this is so important to
scale the data going back up to here if
you have something that's coming out as
a hundred is going to really outweigh
something that's 0.071
that's not in all the models different
models handle it differently and as we
look at the different models i'll talk a
little bit about that we're going to
look at three models today three the top
models used for this and see how they
compare and how the numbers come out
between them so we're going to look at
three different setups let me change my
cell here to mark down there we go and
we're going to start with the random
forest classifier so the three sets
we're looking at is the random forced
classifier support vector classifier and
then a neural network now we start with
the random force classifier because it
has the least amount of
parts moving parts to fine-tune and
let's go ahead and put this in here so
we're going to call it rfc for random
force classifier and if you remember we
imported that so let me go back up here
to the top real quick and we did an
import of the random force classifier
from sk learn ensemble and then we'll
all we also let me just point this out
here's our svm where we imported our
support vector classifier so svm is
support vector model support vector
classifier and then we also have our
neural network and we're going to from
there the multi-layered
perceptron classifier kind of a mouthful
for the p perceptron don't worry too
much about that name it's just it's a
neural network there's a lot of
different options on there and setups
which is where they came up with the
perceptron but so we have our three
different models we're going to go
through on here and then we're going to
weigh them here's our metrics we're
going to use a confusion metrics also
from the sk learn package to see how
good our model does
with our split so let's go back down
there and take a look at that
and we have our rfs equals random forest
classifier and we have n estimators
equals 200. this is the only value you
play with with a random forest
classifier how many forests do you need
or how many trees in the forest so how
many models are in here that makes it
pretty good as a startup model because
we're only playing with one number and
it's pretty clear what it is and you can
lower this number or raise it usually
start up with a higher number and then
bring it down to see if it keeps the
same value so you have less you know the
smaller the model the better the fit and
it's easier to send out to somebody else
if you're going to distribute it now the
random forest classifier
everything i read says it's used for
kind of a medium-sized data set so you
can run it in on big data you can run it
on smaller data obviously but tends to
work best in the mid-range and we'll go
ahead and take our rfc
and i just copied this from the other
side dot fit x train comma y train so
we're sending it our features and then
the quality in the y train what we want
to predict in there and we just do a
simple fit now remember this is sk
learned so everything is fit or
transform another one is predict which
we'll do in just a second here let's do
that now predict
rfc equals and it's our rfc model
predict and what are we predicting on
well we trained it with our train values
so now we need our test our x test so
this has done it this is going to do
this is the three lines of code we need
to create our random force variable fit
our training data to it so we're
programming it to fit in this case it's
got 200 different trees it's going to
build and then we're going to predict on
here let me go ahead and just run that
and we can actually do something like oh
let's do predict rf
c just real quick we'll look at the
first 20 variables of it let's go ahead
and run that and in our first 20
variables we have three wines that make
the cut and the other 17 don't so the
other 17 are bad quality and three of
them are good quality in our predicted
values and if you remember correctly
we'll go ahead and take this out of here
this is based on our test so these are
the first
20 values in our test and this has as
you can see all the different features
listed in there and they've been skilled
so when you look at these they're a
little bit confusing to look at and hard
to read but we have there's a minus 01
so this is 0.36 minus 01 so 0.164 minus
0.09 or no it's still minus 1. so minus
0.9 all between 0 and 1 on here i think
i was confused earlier and i said 0
between 2 negative 2. but between -1 and
1 which is what it should be in the
scale and we'll go ahead and just cut
that out of there run this we have our
setup on here so now that we've run the
prediction and we have predicted values
well one you could uh publish them but
what do we do with them well we want to
do with them is we want to see how well
our model model performed that's the
whole reason for splitting it between a
training and testing model and for that
remember we imported the classification
report
that was again from the sklearn there's
our confusion matrix and classification
report and the classification report
actually sits on the confusion matrix so
it uses that information and our
classification report we want to know
how good are y tests that's the actual
values versus our predicted rfc so we'll
go ahead and print this report out and
let's take a look and we can see here we
have a precision out of the zero we had
about point 92 that were labeled as uh
bad that were actually bad and out of
precision for the quality wines we're
running about 78 percent so you kind of
give us an overall 90
and you can see our f1 score our support
set up on there our recall you could
also do the confusion matrix on here
which gives you a little bit more
information but for this this is going
to be good enough for right now we're
just going to look at how good this
model was because we want to compare the
random fourth classifier with the other
two models and you know what let's go
ahead and put in the confusion matrix
just so you can see that on there with y
test and prediction rfc so in the
confusion matrix we can see here that we
had
266 correct and seven wrong these are
the missed labels for bad wine and we
had a lot of missed labels for good wine
so our quality labels aren't that good
we're good at predicting bad wine not so
good at predicting whether it's a good
quality wine important to note on there
so that is our basic random forest
classifier and let me go ahead upsell
and change cell type to markdown and run
that so we have a nice label let's look
at our svm classifier our support vector
model and this should look familiar we
have our clf we're going to create
what's we'll call it just like we call
this an rfc and then we'll have our clf
dot fit and this should be identical to
up above x train comma y train and just
like we did before let's go ahead and do
the prediction and here is our clf
predict and it's going to equal the clf
dot predict and we want to go ahead and
use x underscore test and right about
now you can realize that you can create
these different models and actually just
create a loop to go through your
different models and put the data in and
that's how they designed it they
designed it to have that ability let's
go ahead and run this and then let's go
ahead and do our classification report
and i'm just going to copy this right
off of here
they say you shouldn't copy and paste
your code and the reason is is when you
go in here and edit it
you unbearably will miss something we
only have two lines so i think i'm safe
to do it today and let's go ahead and
run this
and let's take a look how the svm
classifier came out so up here we had a
90 percent and down here we're running
about an 86
so it's not doing as good now remember
we randomly split the data so if i run
this a bunch of times you'll see some
changes down here so these numbers this
size of data if i read it a hundred
times it would probably be within plus
or minus three or four on here in fact
if i ran this 100 times you'd probably
see these come out almost the same as
far as how well they do in
classification and then on the confusion
matrix let's take a look at this one
this had 22 by 25 this one has 35 by 12.
so it's it's doing not quite as good
that shows up here 71 percent versus 78
percent and then if we're going to do a
svm classifier we also want to show you
one more and before i do that kind of
tease you a little bit here before we
jump into neural networks the big save
all deep learning because everything
else must be shallow learning that's a
joke let's just talk a little bit about
the svm versus the random forest
classifier the svm tends to work better
on smaller numbers
it also works really good on
a lot of times you convert things into
numbers and bins and things like that
the random forest tends to do better
with those at least that's my brief
experience with it where if you have
just a lot of raw data coming in the svm
is usually the fastest and easiest to
apply model on there so they they each
have their own benefits you'll find
though again that when you run these
like 100 times difference between these
two on a data set like this is going to
just go away there's randomness involved
depending on which data we took and how
they classify them the big one is the
neural networks and this is what makes
the neural networks nice is they can do
they can look into huge amounts of data
so for a project like this you probably
don't need a neural network on this but
it's important to see how they work
differently and how they come up
differently so you can work with huge
amounts of data you can also many
respects they work really good with text
analysis especially if it's time
sensitive more and more you have an
order of text and they've just come out
with different ways of feeding that data
in where the series in the order of the
words is really important same thing
with starting to predict in the stock
market if you have tons of data coming
in from different sources the neural
network can really process that in a
powerful way to pull up things that
aren't seen before when i say lots of
data coming in i'm not talking about
just the high lows that you can run an
svm on real easily i'm talking about the
data that comes in where you have maybe
you pulled off the twitter feeds and
have word counts going on and you've
pulled off the uh the different news
feeds that business are looking at and
the different releases when they release
the different reports so you have all
this different data coming in and the
neural network does really good with
that pictures picture processing now is
really moving heavily into the neural
network if you have a pixel 2 or pixel 3
phone put out by google it has a neural
network for doing it's kind of goofy but
you can put little star wars androids
dancing around your pictures and things
like that that's all done with the
neural network so it has a lot of
different uses but it's also requires a
lot of data and is a little heavy-handed
for something like this and this should
now look familiar because we've done it
twice before we have our multi-layered
perceptron classifier we'll call it an
mlpc and it's this is what we imported
mlpc classifier there's a lot of
settings in here the first one is the
hidden layers you have to have the
hidden layers in there we're going to do
three layers of 11 each so that's how
many nodes are in each layer as it comes
in and that was based on the fact we
have 11 features coming in then i went
ahead and just did three layers probably
get by with a lot less on this but i
didn't want to sit and play with it all
afternoon again this is one of those
things you play with a lot because the
more hidden layers you have the more
resources you're using you can also run
into problems with overfitting with too
many layers and you also have to run
higher iterations the max iteration we
have is set to 500 the default's 200
because i used three layers of 11 each
which is by the way kind of a default i
use i realized that usually you have
about three layers going down and the
number of features going across you'll
see that's pretty common for the first
classifier when you're working in neural
networks but it also means you have to
do higher iterations so we up the
iterations to 500 so that means it's
going through the data 500 times to
program those different layers and
carefully adjust them and we do have a
full tutorials you can go look up on
neural networks and understand the
neural network settings a lot more and
of course we have you're looking over
here where we had our previous model
where we fit it same thing here mlpc fit
x train y train and then we're going to
create our prediction so let's do our
predict and mlpc and it's going to equal
the mlpc and we'll just take the same
thing here predict x test let's just put
that down here dot predict test and if i
run that we've now programmed it we now
have our prediction here same as before
and we'll go ahead and do the copy print
again always be careful with the copy
paste now because you always run the
chance of missing one of these variables
so if you're doing a lot of coding you
might want to skip that copy and paste
and just type it in and let's go ahead
and run this and see what that looks
like and we came up with an 88
we're going to compare that with the 86
from our tree our svm classifier and our
90 from the random forest classifier and
keep in mind random forest classifiers
they do good on mid-sized data the svm
on smaller amounts of data although to
be honest i don't think that's
necessarily the split between the two
and these things will actually come
together if you random a number of times
and we can see down here the noun of
good wines mislabeled with a
setup on there it's on par with our
random forest so we had 2225 shouldn't
be a surprise it's identical it just
didn't do as good with the bad wines
labeling what's a bad one and what's not
see yeah because they had 266 and 7 we
had down here 260 and 13. so mislabeled
a couple of bad wines as good wines so
we've explored three of these basic
classifiers these are probably the three
most widely used right now i might even
throw in the random tree
if we open up their website we go under
supervised learning there's a linear
model we didn't do that almost most of
the data usually just start with a
linear model because it's going to
process the quickest and use the least
amount of resources but you can see they
have linear quadratic they have kernel
ridge there's our support vector from
stochastic gradient nearest neighbors
nearest neighbors is another common one
that's used a lot very similar to the
svm gaussian process cross decomposition
naive bayes this is more of an
intellectual one that i don't see used a
lot but it's like the basis of a lot of
other things decision tree there's
another one that's used a lot ensemble
methods not as much multi-class and
multi-label algorithms feature selection
neural networks that's the other one we
use down here and of course the forest
so you can see there's a in sk learn
there are so many different
options and they've just developed them
over the years we covered three of the
most commonly used ones in here and went
over a little bit over why they're
different neural network just because
it's fun to work in deep learning and
not in shallow learning as i told you
that doesn't mean that the svm is
actually shallow it does a lot of it
covers a lot of things and same thing
with the decision for the random forest
classifier and we notice that there's a
number of other different classifier
options in there these are just the
three most common ones and i'd probably
throw the nearest neighbor in there and
the decision tree which is usually part
of the decision for us depending on what
the back end you're using and since as
human beings if i was in the
shareholders office i wouldn't want to
leave them with a confusion matrix they
need that information for making
decisions but we want to give them just
one particular score and so i would go
ahead and we have our sklearn metrics
we're going to import the accuracy score
and i'm just going to do this on the
random forest since that was our best
model and we have our cm accuracy score
and i forgot to print it if you remember
in jupyter notebook we can just do the
last variable we leave out there will
print and so our cm accurate score we
get is 90
and that matches up here we should
already see that up here in precision so
you can either quote that but a lot of
times people like to see it highlighted
at the very end this is our precision on
this model and then the final stage is
we would like to use this for future so
let's go ahead and take our wine if you
remember correctly we'll do one head of
10. we'll run that remember our original
data set we've gone through so many
steps now we're going to go back to the
original data and we can see here we
have our top 10 our top 10 on the list
only two of them make it as having high
enough quality wine for us to be
interested in them and then let's go
ahead and create some data here we'll
call it x new equals and this is
important this data has to be we just
kind of randomly selected some data
looks an awful lot like some of the
other numbers on here which is what it
should look like and so we have our x
new equals 7.3.58
and so on and then it is so important
this is where people forget this step x
new
equals sc remember sc that was our
standard scalar variable we created
if we go right back up here before we
did anything else we created an sc we
fit it and we transformed it and then we
need to do what transform the data we're
going to feed in
so we're going to go back down here and
we're going to transform our x new and
then we're going to go ahead and use the
where are we at here we go our random
forest and if you remember all it is is
our rfc predict model right there let's
go ahead and just grab that down here
and so our y
new equals here's our rfc predict and we
do our x new in and then it's kind of
nice to know what it actually puts out
so according to this it should print out
what our prediction is for this wine and
oh it's a bad wine okay so we didn't
pick out a good wine for our ex new and
that should be expected most of wine if
you remember correctly only a small
percentage of the wine met our quality
requirements so we can look at this and
say oh we'll have to try another wine
out which is fine by me because i like
to try out new wines and i certainly
have a collection of old wine bottles
and very few of them match but you can
see here we've gone through the whole
process just a quick re rehash we had
our imports we touched a lot on the sk
learn our random forest our svm and our
mlp classifier so we had our support
vector classifier we had our random
forests and we have our neural network
three of the top used classifiers in the
sk learn system and we also have our
confusion metric matrix and our
classification report which we used our
standard scalar for scaling it and our
label encoder and of course we needed to
go ahead and split our data up in our
implot line train and we explored the
data in here for null values we set up
our quality into bins
we took a look at the data and what we
actually have and put a nice little plot
to show our quality what we're looking
at and then we went through our three
different models and it's always
interesting because you spend so much
time getting to these models and then
you kind of go through the models and
play with them until you get the best
training on there without becoming
biased that's always a challenge is to
not over train your data to the point
where you're training it to fit the test
value and finally we went ahead and
actually used it and applied it to
a new wine which unfortunately didn't
make the cut it's going to be the one
that we drink a glass out of and save
the rest from cooking
of course that's according to the random
forest on there because we use the best
model that it came up with
so let's start with what is numpy numpy
is the core library for scientific and
numerical computing in python it
provides high performance
multi-dimensional array object and tools
for working with arrays and i'll go a
step further and say there are so many
other modules in python built on numpy
so the fundamentals of numpy are so
important to latch onto for the python
so you can understand the other modules
and what they're doing number's main
object is a multi-dimensional array it's
a table of elements usually numbers all
of the same type indexed by a tuple of
position integers in numpy dimensions
are called axes
take a one-dimensional array or we have
remember dimensions are also called axes
you can say this is the first axis
0 1 2 3 4 5 and you can see down here it
has a shape of 6 why because there's six
different elements in it in the one
dimension array and they usually denote
that as six comma with an empty node on
there and then we have a two dimensional
array where you can see zero one two
three four five six seven and in here we
have two axes or two dimensions and the
shape is two four so if you were looking
at this as a matrix or in other
mathematical functions you can see
there's all kinds of importance on shape
we're not going to cover shape today but
we will cover that in part two did you
know that numpy's array class is called
nd array for numpy data array now we're
going to take a detour here because
we're working in python and two of my
favorite tools in python is the jupiter
notebook and then i like to use that
sitting on top of anaconda and if you
flip over to jupiter.org that's
t e j-u-p-y-t-e-r dot org you can go in
here you can install it off of here if
you don't want to use the anaconda
notebook but this is the jupiter setup
the documentation on the jupiter jupiter
opens up in your web browser that's what
makes it so nice is it's portable the
files are saved on your computer they do
run in ipython or iron python and you
can create all kinds of different
environments in there which i'll show
you in just a minute i myself like to
use anaconda that's www.anaconda.com
if you install anaconda it will install
the jupyter notebook with the anaconda
separate and you can install jupyter
notebook and it'll run completely
separate from anaconda's jupiter
notebook and you can see here i've now
opened up my anaconda navigator what i
like about the navigator and this is a
fresh install on a new computer which is
always nice i can launch my jupyter
notebook from in here i can bring other
tools so the anaconda does a lot more
and under environments i only have the
one environment and i can open up the
terminal specific to this environment
this one happens to have python37 in it
the most current version as of this
tutorial and the open terminal if you're
going to do your pip installs and stuff
like that for different modules you can
also create different environments in
here so maybe you need a python36
python35 you can see we're having a nice
framework like anaconda really helps so
you don't have to check track that on
your own in the jupyter notebook in your
different jupiter notebook setups we'll
go ahead and launch this jupyter
notebook and then i've set my browser
window for a deep fault of chrome so
it's going to open up in chrome and you
can see here this opens up a folder on
my computer we have a couple different
options on here remember i set the
environment up as python 3.7 you would
install any additional modules that
aren't already installed in your python
on this and it keeps them separate so
you do have to for each environment
install the separate modules so they
match the environment on there and in
here we have a couple things we can look
up what's running
you have your different clusters again
this is i just installed this on a new
machine so i just have the one a couple
things in here that were run on here
recently and what we go on here is we
then have on the upper right new and
from the pull down menu you'll see
python3
and this will open up a new window
and now we're in jupiter python so this
is a python window and we'll just do a
print
and this of course is the so hello world
and we'll run that and it prints out
hello world in the command line there's
a couple special things you have to know
we're not going to do today which is on
graphics if you've never seen this
one of the things you can do you can
also do a equals hello world and if you
just put the a in there now if you do a
bunch of these where you have a equals
hello world b equals goodbye world and
you put a b a and return b it'll only
run the last one but you can see here if
you put the variable down here it will
show you what's in that variable
and that has to do with the jupiter
notebook inline coding so that's not
basic python that's just jupiter
notebook shorthand which you'll see in a
little bit
so back to our numpy numpy array versus
python list python list being the basic
list in your python why should we use
numpy array when we have python list
well first it's fast the numpy array has
been optimized over years and years by
multiple programmers and it's usually
very quick compared to the basic python
list setup it's convenient so it has a
lot of functionality in there there's
not in the basic python list and it also
uses less memory so it's optimized both
for speed and memory use
and let's go ahead and jump into our
jupiter notebook since we're coding best
way to learn coding is to code just like
the best way to learn how to write is
right and the best way to learn how to
cook is cook so let's do some coding
here today
and just like any modules we have to
import numpy we almost always import it
as np that is such a standard so you'll
see that very commonly we can just run
that and now we have access to our numpy
module inside our python
and then the most common thing of course
is to go and create a number array
and in here we can send it a regular
list
and so we'll go ahead and send this a
regular array let's do one two three to
make it simple and then i'm just going
to type in a and we'll run this
as you can see down here the output is
an array of one two three
and we could also do
print
just a reminder that this is an inline
command so that wouldn't work if you're
using a different editor you can see
that it's an array one two three but
we'll go and leave it as a
kind of a nice feature so you can see
what you're doing really quick in the
jupyter notebook
and just like all your other standard
arrays i can go
a of 0
which is going to be a value of 1.
of course we do a of 1. you go all the
way through this
a of 1 has a value of 2 in it
so whether you're using the numpy array
or the basic python list that's going to
be the same that should all look pretty
familiar and be pretty straightforward
remember the first value is always 0
and when we set on there so let's take a
look why we're using numpy because we
went over the slide a little bit but
let's just take a look and see what that
actually looks like and what we want to
look at is the fact that it's fast
convenient and uses less memory so let's
take a glance at that in code and see
what that actually looks like when we're
writing it in python and what the
differences are
and to do this i'm going to go ahead and
import a couple other modules we're
going to import the time module so we
can time it and we're going to import
the system module so that we can take a
look at how much memory it uses and
we'll go and just run those so those are
imported
so we'll do b equals oh range of one
yeah one thousand is fine
and so that's going to create a list of
one thousand zero to nine hundred ninety
nine remember it starts at zero and it
stops right at the one thousand without
actually going to the 1000
and let's go ahead and print
and we want system dot get size of
and we'll pick any integer because we
have you know 0 to 1 000 we'll just
throw one in there five it doesn't
matter because it's gonna whatever
integer we put in there is going to
generate the same value because we're
looking the size of how how much memory
it stores an integer in
and then we want to have the length of
the b that's how many integers are in
there
and if we go ahead and execute this and
run this in a line we'll see oops i did
that wrong comma
if we multiply them together
we'll see it generates 28 000. so that's
the size we're looking at is 28 000 i
believe that's bytes that sounds about
right
so let's go ahead and create this in
numpy
and we'll go with c equals np
and this is a range
so that's the numpy command to do the
same thing that we were just doing in a
list and we'll also use the same value
on there the 1000
and then once we've created the
c
value of c for np dot a range let's go
ahead and print
and we can do that by doing c dot size
times
c dot
item size
when it's very similar we did before we
did get the size of so the c size is the
size of the array and each item size
just reversed
so it's the size of an integer five item
size is going to be the integers and c
size now let's just take a look and see
what that generates
and wow okay we got 4 000 versus 28 000.
that's a significant difference in
memory how much memory we're using with
the array and then let's go ahead and
take a look at speed let's do um oh
let's do size
we tried this with lower values and it
would happen so fast that the npra kept
coming up with zero
because it just rounded it off so size
and let's create an l1
bulls
range of size
and we'll do an l2
i'll just set up to the same thing it's
also range
of size on there there we go
and then we can do on a1
equals
np dot
a range size
and then let's do an a
2 equals np dot
a range
we'll keep it the same size
and what we're going to do is we're
going to take these two different arrays
and we're going to perform some basic
functions on them
but let's go ahead and just load these
up now we'll go ahead and run this so
those are all set in memory
except for the typo here
quickly fix that
there we go so these are now all loaded
in here and let's do a start
equals
time dot time
so it's just going to look at my clock
time and see what time it is
and it will do result equals and let's
do
oh let's say we got
an array and we're going to say
let's do some addition here x
plus y
for x comma y
in
and we'll zip it up here
two different arrays so here's our two
different arrays we're going to multiply
each of the individual things on here l1
l2
there we go so that should add up each
value so l1 plus l2 each value in each
array
and then we want to go ahead and print
and let's say
python list took
and then we'll do
time
dot time
we'll just subtract the start out of
there so time whoops i messed up on some
of the quotation marks on there
okay there we go
time minus the start and we'll convert
that to second so we'll go to this in
milliseconds or times one thousand
and let's hit the run on there it's kind
of fun because you also get a view while
we're doing this
of some ways to manipulate the script
and as you can see also my bed typing
there we go okay so we'll go ahead and
run this
and we can see here that the python list
took 34
actually i have to go back and look at
the conversion on there but you can see
it takes roughly 0.34 of a second and we
go ahead and print the result in here
too
let's do that
we'll run that just so you can see what
the what kind of data we're looking at
and we have the zero two four six eight
so it's just adding them together it
looks pretty straightforward on there
and if we scroll down to the bottom of
the answer again we see python list took
46 a little different time on there
depending on what
core because i have this is on an eight
core computer so it just depends on what
core it's running on what else is
pulling on the computer at the time
and let's go back up here and do our
start time
paste that into here
and this time we're gonna do a result
equals and this is really cool notice
how elegant this is so straightforward
this is a lot of reason people started
using numpy is because i can add the two
arrays together by simply going a1 plus
a2
it makes a lot of sense both looking at
it and it's just very convenient
remember that slide we're looking at
fast convenient and less memory
so look how convenient that is
really easy to read real easy to see
and i don't know if we don't need to
print the result again so let's just go
ahead and print
the time on here
and we'll borrow this from the top part
because i really am a lazy typer
and this isn't the python list this is
the numpy list or number array
and let's go ahead and see how that
comes out
and we get 2.99
so let's take a look at these two
numbers 46 versus 2.99 so we'll just
round this up to 3. that's a huge
difference that's that's like
more than 10 times faster that's like 15
times roughly at a quick glance i'd have
to go do the math to look at it and it's
going to vary a little bit depending on
what's running in the background the
computer obviously so we've looked at
this and if we go back here
we found out it's much faster
yes there's different going to be
different speeds depending on what
you're doing with the array very
convenient easy to read and it uses less
memory so that's the core of the numpy
that's why a lot of people base so many
other modules on numpy and why it's so
widely used
so we did glance at a couple operations
when we were looking at speed and size
let's dive into
a little bit more into the basic
operations
and these are always nice to see i mean
certainly you want to go get a cheat
sheet if you're using it for the first
time you know look things up google is
your friend
we did this we're the most basic numpy
dot array or np.array
and we'll go ahead and create an array
let's do
pairs
one comma two
and then let's do a three comma four and
if we're gonna do that let's do five
comma six
there we go and if we go ahead and take
this and run this and go ahead and do
our a down here so it's in line and i'll
print that out
you can see it makes a nice array for us
so we have a and if you look at that we
have three different objects each with
two values in them and hopefully you're
starting to think well how many
dimensions or indexes is that and you'll
see three by two so let's go ahead and
take a look and let's go how about a dot
in dimensions speaking of which
we'll run that and we have two
dimensions for each object
and then we can do the item size so a
dot
we saw this earlier we looked up how
many items it was up here where we
wanted to multiply item size times the
actual size of the object so the memory
is being used versus the item size
and we should see
four there
memory is compressed down that's always
a good thing
and then the shape the shape is so
important when you're working with data
science
and you're moving it from one format to
another
so we have our shape we just talked
about that we have three by two
three rows by two objects in each one
generally i don't look too much at the
size but the dimensions i'm always
looking up this is nice you can automate
it so you might be converting something
you might need to know how many
dimensions are going into the
next machine learning package so that
you can automatically just have it send
that information over
so we looked at a shape
let's go and create a slightly different
array np dot array let's go ahead and
just do as our original
setup here
and one of the features we can do which
is really important is we can do d type
equals in this case let's do
np
float
64. and so what we've done is converting
all of these into a float and we type in
a
and now instead of having one two three
four five six you see they're all float
values one dot zero there's no actual
zero in there just so it's a one dot or
the one period two three period four
period five period six period
and this again data science i don't know
how many times i've had to convert
something from an integer to a float so
that's going to work correctly in the
model i'm using
so very common features to be aware of
and to be able to get around and use
and we'll also do let's just curiosity
item size
we'll go and run that
and we see that it doubled in size so
it's not a huge increase well doubling
is always a big increase in computers
but it's not a huge increase compared to
what it would be if you're running this
in the python list format
and then we did the shape earlier
without having it set to the float 64.
let's go ahead and do a shape with it
set to 64. and it should be the same
three comma two so it all matches
so we've gone through and remember if
you really if this is all brand new to
you
according to the cambridge study at the
cambridge university if you're learning
a brand new word in a foreign language
the average person has to repeat it
163 times before it's memorized
so a lot of this you build off of it so
hopefully you don't have to repeat it
163 times but we did manage to repeat it
at least twice here if not a little bit
more
and let's go ahead and take this we're
going to look at one more setup on here
and let me just take this last statement
here on the converting our properties of
our data
and instead of float 64
let's do complex let's just see what
that looks like and let's go and print
that out
and run it
and so we now have a complex data set up
and you'll see it's denoted by the one
dot plus zero dot j
and if we flip over here and do a basic
search for numpy data types
better to go to the original web page
but pull up a bunch of these you can see
there's a whole list of different numpy
data types
shorthand complex we have complex
complex 64 complex 128
complex number represented by 264-bit
floats real and imaginary components
one option on there float16 float32
float shorthand for float64 most
commonly used and of course all the
different ones that you can possibly put
into your numpy array so we covered a
basic addition up there we're comparing
how fast it runs but some very basic
components how to set up a numpy array
how many dimensions it has item size
data type item again we went to item
size and there's also
the
shape probably one of the more used i
used a shape all the time very commonly
used
and then down here you can see where we
actually created a numpy complex data
type
so let's look at some other features in
numpy one of them is you could do numpy
dot
zeros
and we're gonna do three comma four
there we go and we'll go ahead and run
this and you can see if i do
dot zeros i create a numpy array of
zeros
this is really important i was
rebuilding my own neural network and i
needed to create an array where i
initialized the weights and i want them
all to be the same weight in this case i
wanted them to start off as zero for the
particular project i was working on and
there's other options like you can do
numpy ones
and we'll do the same thing three comma
four we'll run that
and you can see i've created an array of
numpy ones in this case it comes out as
a float array
and this is an interesting to note
because we have let's go back to our
python and do lrange five
and we'll print the l so there's our
list and if i run that
it doesn't create the range until after
the fact until you actually execute it
that's an upgrade in python python27
actually created the array zero one two
three four this one actually creates the
script and then once it's used it then
actually generates the array
and if we do that in numpy a range
remember that from before
and if we do numpy a range five
and let's do uh l equals
or we can just leave it as numpy that's
fine there we go and just run that
you can see there we actually get an
array zero one two three four for the
value the numpy arrange a range five
generates the actual array and for part
one we're going to do just one more
section on basic
setup
and we're going to concatenation
do a concatenation out example
there we go we're gonna do strings let's
take a look at uh strings what's going
on with there and let's do
oh let's see print
let's do an np
character something new here
and we're going to add and then here's
our brackets for what we're going to add
oh and let's say
let's do
hello
comma hi
and
in the brackets on there let's create
another one
and this one's going to be
a b c
and we'll do
x y z so we're just creating some
randomly making some up on here and then
we'll go ahead and just print this if we
run that and come down here
and of course make sure all your
brackets are open and closed correctly
and then
you can see in here when we concatenate
the example in numpy
it takes the two different arrays that
we set up in there and it combines the
hello with the abc and the high with xyz
and if we can also do something like
print
oh let's do
np character dot
multiply
so there's a lot of different functions
in here again you can look these up it's
probably good to look them all up and
see what they are but it's good to also
just see them in action let's do hello
space comma
[Music]
and we'll run this one
and run that without the error you'll
see it does hello hello hello so we
multiplied it by three and we can also
let's just take this whole thing here
instead of retyping it
and we can do character
center so instead of multiply let's do
center
and over here
keep our hello going
take the space out of there
and let's do center at 20
and fill
character
equals and we'll fill it with dashes
so if we run this
you can see it prints out the hello with
dashes on each side and we keep going
with that we can also in addition to
doing the fill function we can play with
capitalize we can title we can do
lowercase we can do uppercase we can
split split line strip join these are
all the most common ones and let's go
ahead and just look at those and see
what those look like each one of them
here we're going to do the hello world
all-time favorite of mine i would like
to say hello universe and you can see
here we did a capital h with the world
but so we want to capitalize so
capitalize is the first one in the array
so we get hello world on there and we
can also take this
and instead of capitalizing
another feature in here is title and
let's just change this to how are we
doing
how
are you doing instead of
do you
let's run that
and you can see here because we created
as a title it capitalizes the first
letter in each word
and in this one we're going to do
character lower
two different examples here we have an
array we have hello world all
capitalized and we have just hello and
you can see that one is an array and one
is just a string if we run that
you get a array with hello world
lowercase and hello lowercase
and if we're going to do it that way we
can also do it the opposite way there's
also upper
and let's paste those in there and you
can see here we have
character.upper opposite there
python.data and that will do python is
easy
hopefully you're starting to get the
picture that most of the python and the
scripting is very simple
it's when you put the bigger picture
together and starts building these
puzzles and somebody asks you hey i need
the first letter capitalized unless it's
the title and then we have you start
realizing that this can get really
complicated so numpy just makes it
simple and we like that
and so in this case we did python data
it's all uppercase python is easy like
shouting in your messenger python is
easy and then if you're ever processing
text and tokenizing it a lot of times
the first thing you do is we just split
the text and we're just going to run
this
np.character.split are you coming to the
party if we do that returns an array of
each of the individual words are you
coming to the party splitting it by the
spaces
and then if we can split it by spaces we
also need to know how to split it by
lines
and just like we have the basic split
command we also have split lines
hello and you'll see here the scoop in
for our new line
and when we run that if you're following
the split part with the words you should
see hello how are you doing the two
different lines are now split apart
and let's just review three more before
we wrap this up commonly used string
variable manipulations we have strip and
in this case we have
nina admin anita and we're going to
strip a off of there let's see what that
looks like
and then you end up with nin diminish it
basically takes up all leading and
trailing letters in this case we're
looking for a
more common would be a space in there
but it might also be punctuation or
anything like that that you need to
remove from your letters and words and
if we're going to strip and clean data
we also need to be able to reformat it
or join it together so you see here we
have a character join we'll go ahead and
run this
and it has on the first one it splits
he's the letters up by the colon and the
second one by the dash and you can see
how this is really useful if you're
processing in this case a date we have
day month year year month date very
common things to have to always switch
around and manipulate depending on what
they're going into and what you're
working with
and finally let's look at one last
character string we're going to do
replace
if you're doing misinformation this is
good pulling news articles replacing
is and what in this case we're just
doing here's a good dancer and we're
going to replace is with was
and you can see here he was a good
dancer hopefully that's not because he
had a bad fall he just was from like you
know 1920s and has gotten old
so there we go we covered a lot of the
basics in numpy as far as creating an
array very important stuff here when
you're feeding it in how do we know the
shape of it the size of it what happens
when we convert it from a regular
integer into a float value as far as how
much space it takes we saw that that
doubled it item size you have your n
dimensions and probably the most used is
shape and we'll cover more on shape in
part two so make sure you join us on
part two because there's a lot of
important things on shaping in there and
setting them up we also saw that you can
create a
zeros based array you can create one
with ones if we do a range you can see
how it is a lot easier to use to create
its own range or a range as it is in
numpy
you saw how easy it was to add two
arrays we saw that earlier just plus
sign then we got into
doing strings and working with strings
and how to concatenate so if you have
two different arrays of strings you can
bring them together we also saw how you
can fill so you can add a nice headline
dash dash dash well we saw about
capitalize the first letter we saw about
turning it into a title so all the first
letters are capitalized doing lowercase
on all the letters upper for all the
letters just lower and upper nice
abbreviation we also covered how to
split the character set how to strip it
so if you want to strip all the a's out
from leading ai a's and ending a's or
spaces you can do that very easily also
how to join the data sets so here's a
character join option for your strings
and finally we did the character replace
now let's go ahead and dive in there
since we're going right into part two
which is getting some coding going under
our belt
and here in our jupiter notebook we can
go under new and create a new folder
python 3.
i think i forgot to do this last time we
could just do the
control plus plus which in any browser
enlarges the page makes it a lot easier
to see always a nice feature another
beautiful benefit of using jupiter
notebook
and let me go ahead and show you a neat
thing we can do in jupiter this is nice
if you're working with people and you're
doing this as a demo on a large screen
i'm going to do the hashtag or pound
symbol array manipulation kind of a
title that we're working on and then i'm
going to call this cell cell type
markdown as opposed to code and you'll
see it highlights it here and then if i
run it it just turns it into array
manipulation and then we're specifically
going to be working on array
manipulation changing shape to start
with
and we'll go ahead and mark this cell
also a markdown so there's a nice little
look there and then it comes up and you
can see it just like i said it just
highlights it and makes it very in bold
print just making it easier to read not
a python thing but a jupiter thing
that's good to know about especially if
you're working with the shareholders
since they're investing money in you
of course the first thing you want to do
is import we're going to import numpy
as in p that should be standard by now
by now you you start a python program
you're doing some data science numpy is
just something you bring in there and
let's go ahead and create our array and
we're going to do that as the np
dot a range remember that's a 0 what
we're going to do 0 to 9
and then we'll print
a little title on the original array
we'll just print that array a remember
from the first lesson so we have our
array which is 0 1 2 3 4 5 6 7 8.
and let's add a print space in between
let's create a second array b but we
want this to reshape array a
and what does that mean
and the command is simply reshape and
then we have nine items in here and this
is so important right now so be very
aware if i did some weird numbers in
here it's not going to work
and we want multiples of 9 we know that
3 times 3 is 9.
so we're going to reshape our a array by
three by three and then we're going to
print well let's give it a title
oops i have too many brackets in there
modified array and then let's go ahead
and print
our b and let's see what that looks like
and as we come down here you can see
we've taken this and it's gone from
0 1 2 3 4 5 6 7 8 to an array
of arrays and we have 0 1 2 3 4 5 6 7
and so we split this into three by three
and you can guess that if i tried to
reshape this let's just do a five by
three which is fifteen
that's going to give me an error so it's
not going to work you're not gonna be
able to reshape something unless the
shape all the the data in there matches
correctly
so we can take this 9 this flat 9 and
they call it flex it's just a single
array and we can reshape it into a 3x3
array and first you might think matrixes
which this is used for that definitely i
use it a lot in graphing because they'll
come in that i have an array that's x y
comma x y 1 y 1 comma x 2 y 2 and so the
shape of it might be 2 by the length of
the number of points
and i need to separate that into a x
flat array and a y flat array and you
can see this can be very easy to reshape
the array doing that and we can of
course go back we can do b
do a print
and we'll do b dot
latin remember i said it's called a
flatten array
and if we run that
you'll see it just goes back to the
original one it takes this 0 1 2 3 4 5 6
7 8 and flattens it back to a single
array
and then one other feature to be aware
of is if we flatten it one of the
commands we can put in there is order
let me just go ahead and do that order
equals
f
strangely enough f stands for fortran
the whole fortran days i remember
actually studying fortran programming
language
in this case you'll see that it uses the
first like zero three six
as the order so instead of flattening it
like we had before zero one two three
four five six seven eight it now does
zero three six one four seven two five
eight
and if you go to the numpy array page
you can see here that they have the
flatten and you just open up the numpy
and d array flattened setup to look it
up and they have three different options
they have c
f and a
and it's whether to flatten in c which
was based on how the c code works for
flattening originally worked which is
row major fortran which is column major
or preserve the column fortran ordering
from a so whatever it was in the default
is the
c version so the default that you saw
you could put orders equal c and you'd
have the same effect as we saw there
before you could even do order equals a
that would also have the same effect
because that's the default so really the
only other thing you really need to
change on here is to change it to c if
you need it
and you can see right here or f i mean
not c the only thing you really want to
change it to is to your f for the
fortron order which then does it by
column versus by row and let's look at
here we go
reshape
so let's create a range of
and let's reshape it
i will do 4 comma 3 for this one and
remember this is numpy i forgot the np
there
in p dot arrange
and we can type in just a for print or
you can do full print a and of course
the jupiter notebook even have a little
extra print at the beginning we run this
we'll see we create a nice array of 0 1
2 it's reshaped it so we have
4 rows and 3 columns or you could call
that three columns and four rows zero
one two three four five six seven eight
nine ten eleven
but this one is so important we'll do np
transpose
let's go ahead and run that
and it helps if i get all the s's in
there don't leave an s out and you'll
see here we've taken our array if you
remember correctly we had 0 1 2 3 4 5 6
7 8 9 10 11
and we've swapped it so we've gone from
a three by four or a four by three to a
three by four
and this really helps if you're looking
at like a huge number of rows and the
data all comes in like let's say this is
your features in row one your features
in row two
and this is x y z well when you go to
plot it you send it all of x in one
array although y and not one array in
all z in another array
and so it's really important that we can
transpose this rather quickly
this is kind of a fun thing i can
highlight it
and do brackets around it and if you
remember correctly
because we're in jupiter it doesn't
matter whether we do the print or not
it'll automatically print it for us and
you see if i hit the run button it comes
up with the same exact thing
and let's play with the reshape and you
know let's zoom this up a little bit
here
make that even bigger so you can really
see what's going on and let's play with
the reshape just a little bit more we'll
do b equals
in p dot a range let's do 8
and reshape
we'll do 2 comma 4.
let's go ahead and print b
and then run that
and you'll see we have now the two rows
this is a bit more like so we have four
maybe two rows of four things or this
might be all of our x components and our
y components so we can switch it back
and forth real easy
important to know here whether we do 2
comma 4 or in the case of 4 comma 3
this has 12 elements and so however you
split it up it's got to equal 12.
so 4 times 3 equals 12 that's pretty
straightforward same thing down here
2 times 4 equals 8.
if i change this let's say i do 2 comma
3 let's just run that in and you'll find
we get an error because
you can't split 8 up into two rows by
three
you have to pick something that it can
split up and arrange it in so let's go
ahead and run that and just for fun
let's go
reshape our b again if i can type
reshape our b again and what else goes
into eight well we could do two by two
by two
so we can take this out to three
different dimensions
and then of course if we um because this
is going to come out you as a variable
we can just go ahead and run it
and it'll print it we could also do a
print statement on there just like we
did before and you'll see we have two
different groups of two variables of two
different dimensions
so two by two by 2.
and let's go ahead and assign this to a
variable c equals b reshape
and let's do something a little
different
let's roll the axes roll
axes
and we'll take our c
and do 2 comma one
and if we go ahead and run this it's
going to print that out whoops
hit a wrong button there let's do that
one again and you roll the axis and you
can see that we now have a set of zero
one two three four five six seven we now
have the zero two one three four six
five seven
so what's going on here we're taking and
we're rolling the numbers around and
let's just simplify this we'll just do
it with c comma one and run that and so
if we roll a single axis you get 0 1
and then it rolled the 4 5
up and then we have 2 3 6 7 and if we do
2
let me see what happens there this is
one of those things you really have to
play with and start filling what it's
doing
we've now taken 0 2 4 6 1 3 5 7 so you
can see we've now rolled by two digits
instead of rolling the one set up we now
rolled two digits up there and so if we
go back and we do the one so we've
rolled it up zero one four five
and then we're gonna take the two in
there and we've rolled the zero one two
three four five and six seven
so we start rolling these things around
on here there's a lot of different
things you can do on this
but it's another way to manipulate the
numbers on your numpy
and finally let's go ahead and swap
axes
we'll do c
and let's just go ahead and run that
it's going to give me an error on there
that's because it requires multiple
arguments left out the arguments so now
we can swap and we get the zero two one
three four six five seven so you can see
everything's been swapped around so next
thing we want to go over is we want to
go over numpy
arithmetic operations
how can we take these and use these let
me just go ahead and put this cell as a
markdown there we go we'll run that so
it has a nice thing all right nice title
on there that's always helpful
and let's start by creating two arrays
we'll do uh a as an ep np
range a range nine and let's reshape
this
three by three so by now you should be
saying this reshape stuff this should
all look pretty familiar we have a zero
one two three four five six seven eight
on there and let's create a second one b
and this time instead of doing a range
let's do np array we'll just create a
straight up array
and we'll do an array of three objects
so it's going to be
three by one and if we go ahead and
print a b out let me run that this is
actually pretty common to have something
like this where you have a
three by whatever it is in a three by
three array when you're doing your math
you kind of have that kind of setup on
there and what we can do is we can go um
np dot add
a b
don't forget we can always put a print
statement on there
so if we add it you'll see that it just
comes in there and it goes okay we're
adding 10 to everything and we could
actually do something more i'll make it
more interesting 11 10 11 12. so let's
change b it's now 10 11 12 and let's run
that
and you can see that we have
10 and then you had 1 plus 11 is 12
2 plus 12 is 14
13 so 10 plus 3 is 13 11 plus 4 is 15
and 12 plus 5 is 17 and so on
we'll put this back since that's how the
original setup was let's do 10 by 10 by
10
and run that and run that and get the
original answer and if you're going to
add them together we need to go ahead
and subtract
a b
and we run that
we get minus 10 minus 9 minus 8 just
like you would expect
so we have our subtraction 0 minus 10 is
minus 10 and so on and if you're going
to add and subtract you can guess what
the next one is we're going to multiply
and we'll multiply
a b
and this should be pretty
straightforward you should expect this
if we multiply 10 times 0 we got 0 10
times 10 is 10 and so on
and finally if you're going to multiply
what's the last one we got is divide
what happens we do divide a by b
and we run this
and we're going to get 0 and this is
0 divided by 10 is 0 1 divided by 10 is
0.1 2 divided by 10 is 0.2 and so on and
so on
so the math is pretty straightforward it
just makes it very easy to do the whole
set up and again if we went this and
let's say i'll just change this up up
here instead of 10
we do a hundred
and
make this a thousand there we go
if we run that and then we do the add
you can see we got ten
plus a hundred plus a thousand
same thing with the subtract
same thing with the multiply
then you can also see the same thing
here with the divide so a lot of control
there with your array and your math
again let's set this back to 10. oops
it's right up here wrong section
there we go 10. we'll just go ahead and
run these
and get back to where we were
and this brings us to our next section
which is slicing and let's put in our
just make this a cell
cell type markdown and when we run that
of course it gives us a nice looking
slicing there and slicing means we're
just going to take sections of the array
so let's create an array in p a range
let's just do
20.
and if you remember if we do a
we have a 0 to 19.
and then we can do a and remember you
can always print these this can always
be put in a print but because i'm in
jupiter if you're doing a demo in
jupiter that is it's just so great that
you have all these controls on here so
we can slice four
on and this should look familiar because
this is the same as a python and a lot
of other different scripting languages
if we do four go zero one two three it's
the first four in the thing and skip sum
and starts with this one the first four
skip then from there on
you can also do the opposite
and go till the fourth one if we run
that we get 0 1 2 3 quite the opposite
on there we can do a single item so we
can pick object number 5 on the list run
that and 5 happens to be 5 because
that's the order they're in and then
this one's interesting because i can do
s equals slice
and let's create a slice here
and let's do two comma nine comma
yeah let's leave it two on there so
we'll create an s slice on here and then
if we take our array and we do array of
s we're taking our slice in there and
let's go ahead and run that
and let's take a look and see what it
generated here first off we started with
two so we have two at the beginning
we're going to end at nine which happens
to be eight so it stops before the nine
remember when we're doing arrays in
python and then we step two so two four
six eight we could do this as three let
me run that and you can see how the
changes to five eight
and we could do this as let's leave this
at 3 and if we change this to 10
oops let's make it 12. there we go
when we run that we have 2 5 8 11. so
that's pretty straightforward it's a
very nice feature to have on here we can
slice it and take different parts of the
series right out of the middle so now
that we've accessed the different pieces
of our array
let's get into iterating iteration and
this is interesting because
my sister who runs a college data
science
division the first question she asks
is
how do you go through data and she's
asking can you do you know how to
iterate through data do you know how to
do a basic for loop do you know how to
go through each piece of the data and in
numpy they have some cool controls for
that this is a mark down there we go and
run it
it's called the nd iter i'm not sure
what the nd stands for but ndeiter for
iterator so before we do that though
let's create an array or something we
can actually iterate through
we'll call it a equals np
a range
let's do something a little funny here
or funky
and we'll do 0 45
5. i'm not sure
why the guys in the back picked this
particular one was kind of a fun one and
if i run that we do this you can see
we get 0 5 10 15 20 25 30 35 40. that's
what this array looks like
and that's just from our slice you could
this is just a slice that's all that is
is we created a slice of 0 45 0 to 45
step 5. and so we can do with this we
can also do a equals the shape let's go
ahead and take and reshape this
and since there's nine variables in
there we'll do a reshape it three by
three so if we run that
whoops missed something there that is
the a
that really helps
so if we do the a reshape
and we'll go ahead and print that out we
get 0 5 10 15 20 25 30 35 40.
and then we simply do four x in
our numpy and nd enter
of a
colon and we'll just go ahead and print
x
and let's see what happens here when we
run through this and we print each one
of those
it goes all the way through the whole
array so it's the same thing we just saw
before we got 0 5 10
15 20 25 30 35 40. so it prints out each
object in the array so you can go
through and view each one of these
and certainly if you remember you could
also flatten the array and just do for
a and that also and get the same result
there's a lot of ways to do this but
this is the proper way with the nd
iterator because it'll
minimize the amount of resources needed
to go through each of the different
objects in the numpy array
and hopefully you asked this question i
just did that
and the question is how can i change
this instead of doing each object
so first of all let's go ahead and take
my cell type
mark that down run it
and so we're going to work on iteration
order c style and f style remember c
because it came from the c programming
and f because it came from the old
fortran programming so let's give us a
reminder i will do a print a and we'll
do 4 x in
np iterate
a
but we also want to do this in a
specific order and you know what i'm a
really lazy typer so let's go back up
here
this is the in d iterator i knew i was
missing the nd part of a
let's do order
equals
c
we'll print x on there and let's do that
again
and this time order
order equals
f
there we go order equals f let's go
ahead and run this
and see what happens here and the first
thing you're going to notice our
original array 0 5 10 15 20 25 30 35 40.
when we do order c that's the default
0 5 10 15 20 and so on and then when you
come down here
you'll see f order f
0 15 30. so it takes the first digit of
each
on the sub arrays or the second
dimension and then it goes into the
second one 5 20 35 10 25 40. so slightly
different order for iterating through it
if you need to do that so we've covered
reshaping we covered math we've covered
iteration
we've covered a number of things the
next section we want to go ahead and go
over
it's going to be joining arrays so we
need to bring them together let me go
ahead and take the cell and make it a
markdown
cell type markdown there we go and run
that so let's work on joining arrays so
we can bring them together and what
different options we have
and let's do we'll do an np array one
two comma three four
we'll go ahead and print
let's do
oops first
these rays aren't that big so let's just
go ahead and keep it all on one line a
so if we run this first array one two
three four oops i forgot that it
automatically wraps it when you do it
this way so we'll go ahead and keep it
separate
and print
a there we go
and let's go ahead and do a b
and we'll do 5 seven eight and notice
i'm keeping the same shape on these two
arrays
depending on what you're doing those
shapes have to match
and let's go ahead and print
second array
do a print
b
i'm going to run that oops
missed something up there let me fix
that real quick
when i was reformatting it to go on
separate lines i messed that up there we
go run all right so we have first array
one two three four second array five six
seven eight
and we'll put a carriage return on there
and the key word we use is concatenate
and if you're familiar with linux it
usually means you're adding it to the
end on there and we're going to do what
they call a long axis zero
so we have concatenate a b along axis
zero let's go ahead and run that and see
what that looks like
so we have one two three four five six
seven eight so now we have an array that
is four by two has a nice shape of four
by two one here
and if we're going to do it along the
axis zero
you should guess what the next one is
we're going to do it along the one axes
and let's see how those differ from each
other let's just go ahead and run that
and again all we're doing is adding in
the axis equals one so we have our
concatenate we have a b and then axes
one remember a couple things one these
are the same shape so we have a two by
two
same dimensions going in there you're
going to get an error if you're
concatenating and they're not if you
have something that instead of one two
is one two three four five six with a
five six seven eight they'll give you an
error on there in fact let's take a look
and see what happens when we do that let
me just take this
one two three
three four five let's run that and if we
come down here oh we got there it says
all the input ray dimensions except for
the concatenation axes m must match
exactly
so it'll let you know if you mess up
that's always a good thing let's go
ahead and take this back here and let's
go ahead and run that
and so we have our zero axes which is
one two three four five six seven eight
we bring them together
and you'll see a very different set up
here when we do it along the axis one we
end up with instead of
four by two we end up with a two by four
one two five six three four seven eight
and this is changing which axes we're
gonna go ahead and concatenate on what i
find is when you're talking about the
concatenate or the joining arrays you
really got to play with these for a
while to make sure you understand what
you mean by the axes
it looks very intuitive when you're
looking at it actually 0
one two three four five six seven eight
axes one is then splitting in a
different way one two five six three
four seven eight
when you're actually using real data you
start to really get a feel for what this
means and what this does
so if we're going to do that let's go
ahead and look at splitting the array
and do that on the markdown and run it
there we go so we have a nice little
title there
and we'll go ahead and create an array
of nine
let's do np split
we'll do a and we're gonna split it by
three
let's just see what that looks like so
if we split it we get an array zero one
two three four five we get three
separate arrays on here now remember
we're looking at let me just print a up
here
so we're looking at zero one two three
four five six seven eight and then we
can split it into three separate arrays
and let's take this we're gonna do this
right down here just move the a split
down here instead of the three
let's do four comma five and put that in
brackets
and so we do it this way we have zero
one two three
four five six seven eight and that's
kind of interesting i wasn't sure what
to expect on that but we get when you
split it a by four comma five you get a
totally different setup on here as far
as the way it split the array
and to understand how this works i'm
going to change the five to a seven
and this will visually make this a
little bit more clear
so we had four and five it went zero one
two three four five six seven eight and
you see the markers four and five when
we do four and seven i get zero one two
three four five six
seven eight
and so what you're looking at here is
the first markers this is going to go to
four so there's our first split at the
four the marker of four and then the
second split is going to be at position
seven and this is the same thing here
four position five that's why we're
splitting it in those two sections we
could also do it seventh let's just see
what that looks like run
and you can see i now have zero one two
three four five six seven eight
so we can split in all kinds of
different ways and create a different
set of um multiple arrays on here and
split it all kinds of different ways and
before we get into the graphs and other
miscellaneous stuff
let's go ahead and look at
resizing the array i'm going to take
this cell and set the cells a mark down
and run it
give us a nice title there and we'll do
an array uh an input array of one two
three and four five six here i'm just
gonna just print
let's go print
a dot
shape and we'll go ahead and run that
whoops hit a wrong button there
hit the comma instead of the dot so we
have a shape of 2 comma 3 here
and this is important to note because we
start resizing it it's going to mess
with different aspects of the shape
and so we'll go ahead and do a print
scoop in for a blank line there we go
let's do b equals
np dot resize
we're going to resize a
and let's resize it with
3x2
and then we'll just go ahead and
print
b
and print
b period shape not a comma
i'll run that
oops forgot the quotation marks around
the end we'll go ahead and run that and
let's just see what that looks like so
we have
one two three four five six our original
array with a shape of two three
and then we want to go ahead and resize
it by three two and we end up with one
two three four five six and we end up
with the shape of three two that
shouldn't be too much of a surprise you
know we got six elements in there we can
resize it by two three was the original
one and then we're actually just
reshaping is how that kind of comes out
as when you resize it like that but what
happens if we do something a little
different
and let's go ahead and just take this
whole thing and copy it down here so we
can see what that looks like
and instead of doing 3 2 remember last
time i did the um to reshape it
i messed with the numbers and it gave me
an error when you resize it you don't
have to match the numbers they don't
have to be the same dimensions so we can
instead of going from a 2 3 to a 3 2 we
can resize it to a 3 3. so let's take a
look and see how it handles that and we
come down here to 3 3 we end up with 1 2
3 4 5 6 and it repeats 1 2 3.
so it actually takes the data and just
adds a whole other block in there based
on the original data and repeating it
all right now at this point you know
we've been looking at tons of numbers
and moving stuff around
we want to go ahead and do is get a
little visual here because that
certainly you can picture all the
different
numbers on there but let's look at
histogram let's put this into a
histogram let me go ahead and run that
and to do that we're going to use the
matte plot library so from map plot
library we're going to import pi plot as
plt
that's usually the notation you see for
pi plot
so if you ever see plt in a code it's
probably pi plot in the matplot library
and then the guys in the back did a nice
job and gals too guys and gals back
there
our team over at simply learn put
together a nice array for me 20 87 4 40
53 with a bunch of numbers that way we
had something to play with
and what we want to do is we wanted to
plot
the histogram and remember a histogram
says how many times
different numbers come in and then we're
going to put them in bins and we have
been 0 to 20 to 40 to 60 to 80 to 100.
you might in here with the matplot
library they call them bins you might
heard the term buckets or they put them
in buckets that's a really common term
i'm going to give it a title so the way
it works is you do your plt.hist for
histogram your plt title and your plt
show
and we're doing just a single array in
here in the numpy array of a
and let's go ahead and run this piece of
code
taking a moment to come out there says
figure size so it's generating the graph
and you can see we have and let's just
take a look at this and go down a size
there we go okay so now we can see we're
taking a look at here
so between 0 and 20
we have three values so we have a 20
here we have a 4
and a 11 and a 15. zero one two three
it's actually four values but they start
at zeros remember we always count from
zero up
and from twenty to forty we got twenty
this is one forty two
three four
five six
and so you can see in the histogram it
shows that the most common numbers
coming up
is going to be between the 40 and 60
range least common between the 80 and
100. this looks like a age demographics
is what this looks like to me and you
can see where they would have put it in
the buckets of different age groups
which would be a nice way of looking at
this
histograms are so important so powerful
when you're doing demos and explaining
your data so being able to quickly put a
histogram up that shows
what's common and how it's trending is
really important
and using that with a numpy
is really easy
and you know what let's take the same
data and i want to show you why we do
bins or why we have buckets of data i'm
used to calling it buckets why we have
bins let's do it instead of by 20 let's
do it by tens and see what happens
and what happens when you do it by tens
is you miss out on the you can see a
nice curve here on the first one
and on the second one it looks like a
ladder going up and a plummet a ladder
going up and a plummet and a ladder
going down
so the first would be more indicative of
an age group and the second one would be
what you would get if you divide it
incorrectly you wouldn't see the natural
trend of
i don't know what this would be maybe
how much food they eat hopefully not
because i'm in 50 so i'm right in the
middle there that which means i get a
ton of food compared to everybody else
but it's some kind of democrat maybe
it's mental maybe it's knowledge because
we we hit a certain point and we start
losing our marbles start leaking out or
something so you start off knowing
something and then as you get older you
grow more but you see here we lose that
you lose that continuity in the thing if
you split the histogram into too many
bins or too many buckets
and if you actually plotted this by the
individual numbers it would just be a
bunch of dots
on the graph it wouldn't mean a whole
lot
and we've looked at graphs there are
terms that are a ton of useful functions
in numpy
i'm sure there's even new ones that are
going to be in here but let's just cover
some important ones you really need to
know about if you're using the numpy
framework
one of them is line space function this
is generating data so we have a line
space we have 1 3 10 and when we do that
we end up with ten numbers so if you
count them there's ten numbers they're
between one and three and they're evenly
spaced we get one one point two two two
but these are all there's a total of ten
here and it's right between the one and
three range
that could be there's a lot of uses for
that but they're probably more obscure
than a lot of the other common numpy
arrays set up
a real common one is to do summation so
we'll do summation
where you do in this case we create a
numpy array of one of um
two different arrays one two three or
two different dimensions one two three
three four five and we're going to sum
them up under axes zero which is your
columns
and if you remember correctly columns is
the one plus three two plus four three
plus five so we have three columns
and if we change this we'll just flip
this to one
we get two numbers so we get one two
three all added together which equals
six and three plus four plus five which
equals twelve
we'll set this back to zero
there we go since this is we're looking
to actually zero
and these probably could have been some
of these compared with our math section
square root and standard deviation two
very important
tools we use throughout the machine
learning process
in data science
and simply we take the np array we have
again the one two three four five six
three four five i don't know why i need
to keep recreating it probably could
just kept it but we could take the
square root of a so it goes through and
it takes the square root of all the
different terms in a
and we can also take the standard
deviation how much they deviate in value
on there
and there's a ravel function we can run
that
and in p array is x we're going to do x
equals say we change it from a to x
x equals
ravel and this sets it up as columns so
we have one two three four five this is
all columns on here very similar to the
flattened function
so they kind of look almost identical
but we also have the option of doing a
ravel by column
and then another one is log so you can
do mathematical log on your array in
this case we have 1 2 3
and we'll find the
log base 10 for each of those three
numbers
there's a couple of them they don't you
can't just do any number here after log
but there is also log base 2.
log base 10 is pretty commonly used on
here
run that there we go
before we go let's have a little fun
let's do a little practice session here
on some more challenging questions so
you start to think how this stuff fits
together right now we just looked at all
the basics and all the basic tools you
have
so let's do some numpy practice examples
and let's start by figuring out how do
you plot say a sine wave in numpy how
what would that look like and so in this
project we wouldn't have to do this
because i've already run these but we'd
want to go ahead and import our numpy as
np and import our matplot library
pipeline as plt so we get our tools
going here and then we'll break it into
two sections because we need our x y
coordinates in here
so first off let's create our x
coordinates
and our x coordinates we're going to set
to an a range
and we want this error a range since
we're doing
sine and cosine it's going to be between
0 and 0.1
and then we use our np and we actually
can look up numpy stores pi so you have
the option just pulling pi in there
directly from numpy there's a few other
variables that it stores in there that
you can pull from there but we have
numpy pi and we generate a nice range
here and let's go ahead and run this
and just out of curiosity let's see what
x looks like
i always like to do that so we have
point one point two point three point
four so we're going uh zero to
in this case nine point four three times
numpy pi
pi is like three point something
something something so that makes sense
it should be about nine and we're doing
intervals of 0.1 so we create a nice
range of data and then we need to create
our y variable and so y
is going to simply equal np our
numpy.sine
of x
and then once we have our x and y
and if we print let's go and just print
y
see how that will do this let's do this
so it looks print
x print y
so we basically have two arrays of data
so we have like our x-axis and our
y-axis going on there
and this is simply a plt dot plot
because we're going to plot the points
and we'll do x comma y
and then we want to actually see the
graph so we'll do plot dot show
and we'll go ahead and run that
and you see we get a nice sine wave and
here's our number zero through nine
and here's our sine value which
oscillates between minus one and one
like we expected to
then for the next challenge
let's create a six by six two
dimensional array and let one and zero
be placed alternatively across the
diagonals
oh that's a little confusing so let's
think about that we're going to create a
six by six
two dimensional so the shape is six by
six two dimensional array
and let one and zero be placed
alternatively across the diagonals
now if you remember from lesson one we
can fill a whole numpy array with zeros
or ones or whatever so we're going to do
np create a numpy zeros and we're gonna
do a six by six
and we'll go ahead and make sure it
knows it's an integer even though it's
usually the default and just real quick
let's take a look and see what that
looks like so if i run this you can see
i get
six by six grid so six by six zero zero
zero zero zero
now if i understand this correctly when
they say ones and zero placed
alternatively across the diagonals
they want the center diagonal maybe
that's going to stay zero all the way
down
and then the next diagonal will be ones
all the way across diagonally and then
the next one zeros the next one ones the
next one zeros and so on hopefully you
can see my mouse lit up there and
highlighting it
so let's take a little piece of code
here
and we'll do z
1 colon colon 2 comma colon colon 2
equals 1. and wow that's a mouthful
right there so let's go ahead and run
this and see what that's doing and so
what we're doing is we're saying hey
let's look at in this case row one
there's one and then we're going to go
every other row two so we're gonna skip
a row so skip here skip here skip here
so we're going down
this way and we're going every other row
going this way it's hard to highlight
columns
so you can see right here where the that
we're not touching each row like this
row right here is not being touched okay
so we're going to start with row one and
then we're going to skip a row and
another one and so we're going every two
rows and then in every two rows we're
looking at every two
starting with the beginning that's what
this thing blank means
so we're going to start with the
beginning and we're going to look at all
of them but we're going to skip every
two
so starting with row one
we look at all the rows but we do we do
it by two steps so we go one skip one
you know one skip one one skip one one
if you left this out it'd do every one
this would just be ones in fact let's
see what this looks like if i go like
this
and run it
you can see that i guess get ones
so
this notation allows us to go down
each row row by row and we're going to
do every other row
set up on there and so if we're going to
start with row one we
also control z
try that there we go we'll start with
row zero again we're going to go each
row step two so we'll start with row
zero
and we'll go every other row
and this time we'll start with one
column one and again we go every other
one going down
step that's what that step two is and
skipping every other one we're gonna set
that equal to one so let's see what that
looks like
and you can see here we get our answer
we get zero one zero zero but it has the
ones going in diagonals
on every other diagonal and 0 on every
other one
a little bit of a brain teaser that one
trying to get that one to work out so
you can see how you can arrange your
rows and here's your step and your
different access on there
and then the next one is
find the total number and locations of
missing values in the array the first
challenge is to create some missing
numbers
so let's create our ray z we're going to
do uh numpy.random.rand
10 comma 10. and before we do the second
part let me just take the second part
out
and let's just see what that looks like
so let's run that
and there we go so we have a 10 by 10
random array it randomly is picking out
numbers
and
next we want to go ahead and take our
random integer size equals five
and then we're going to do a random
random 10 size equals five so in the z
we're going to select
a number of random spaces here and set
them equal to
null value
and let's go ahead and run that so you
can see what that looks like and if we
look at the array
we've created one two
three
four
there should be a fifth one in here my
eyes may be filling me so we've created
a series of oh because zero zero to five
zero one two three four so we got five
there are different null values on here
and
this is kind of a neat notation to
notice that we can generate
random integers size equals five so this
generates
five by five miniature grid inside of
this to tell it where to put the nands
at so that's kind of a cool little thing
you can do
and then we want to look up and see how
many null values are in there
and this is simply just
np is none of z
simple
so if it is none then we want to sum it
up so we're going to sum up all of the
different null values on there now let's
do one one more feature in here which is
really cool
let's go ahead and print the indexes so
np arguer np is
nan of z so we're going to create our
own another np array and let's run this
and we'll see here that comes up with
the four indexes
so we did count four of them up there
it tells you where they are one nine two
zero four six five four
and then let's go ahead and run this
again run run there we go this time i
got five that's what get for random
numbers another fun one
that i always like to do it's very
similar because we have np is not z dot
sums we're summing the number of
nands and we can get the indexes and you
can reshape the indexes but you can also
just do we'll do an inds where np is n
of z
and let's just print let's print that
print
inds let's see what that looks like
and it's very similar we have we have
zero one three zero six three eight six
nine three
but i've split it into two different
arrays
so we have our x and our y kind of
coordinates going there and what i can
now do is i can now do z
inds
equals and at this point you can also
instead of getting the sum you can get
the means or the all the numbers and
that kind of thing or the average as it
is so that'd be one thing you could do
and you could pick up the average that's
very common in data science to get the
average and just use that for a value
we'll go and just set it to zero and
then let's go ahead and print our z
and run that
and you can see we come down here
we have wherever there was a null value
it is now
zero
and you can set this whatever you want
this is another way to replace data or
help clean data
depending on what it is you're doing
so wow we covered a lot of stuff so
quick rehash going over everything we
went into there we looked at array
manipulation changing the shape
how to switch that around we even had
the flattened down there which remember
we have another command lower that's
similar we could change the order by f
remember f stands for fortran very
strange connotation but there's c and f
c is the standard
and f switches it to a different order
to be honest i usually have to look it
up because i almost never use f but when
you need it you're like oh my gosh it
was the other order just do a quick
google so we talked about reshape making
sure that the dimensions are the same
you don't want to have like something
that has 12 objects in it and reshape it
to
see 11 and 5 because it doesn't work it
doesn't divide into 12. we can transpose
so we can switch them so we can go from
a 4 by 3 to a 3 by 4. oops i did that
the other way around
three by four to four by three
we covered reshaping the array we did
the roll the axes you can do some weird
things with swapping and rolling axes
and transposing the numbers
we dug a little bit into the arithmetic
so we talked about
adding we talked about subtracting
multiplying dividing
and you know at this point it's so
important we just look up the
numpy mathematics and you can see here
they have just about everything your
trigonometry
uh your hyperbolic functions roundings
sums products differences
there are so many all your different
miscellaneous mathematical connotations
so you know google it go to the main
numpy page and look at the different
setups you can do on there
so we covered that
and we did slicing how to break it apart
we did iterating over the array
we covered
joining arrays and how to concatenate
remember concatenate just means add on
to it so in this case how are you adding
b onto a is how you read that from linux
you should catch the concatenate
because that's used regularly there
splitting the array we talked about how
to split the array in different ways so
you can split it in
array of arrays all kinds of different
ways to split the array up how to resize
it and remember resize does not have to
have the same shape but if you resize it
it will take the data and begin at the
beginning and add new rows on if the
size is bigger if it's smaller it
truncates it it just cuts the end off
we looked at how to do a histogram and
how to plot that
uh we mentioned
buckets or bins as they call them in uh
pie plot and then we covered a lot of
other useful functions in numpy talked
about the line space setup for doing
numbers in a series
how to sum the axes up again that's part
of the mathematical formulas there that
we looked at there's a sum there's also
means and median
all of those you can compute in num b
and you can also do the square root and
standard deviation
the ravel function very similar to the
flat
to be honest i almost always just use
the flat but you know the ravel has its
own kind of functionality that it does
and then we went into some numpy
practice examples we challenged you to
create a sine wave in numpy and how to
do that we're kind of looking for that a
range remember how we do the a range and
you can
have your beginning value your n value
which they did is three times pi and not
be pi
and we're going to do intervals of 0.1
and then y just equals the numpy sign of
x there's our math from the math page
we're just looking at remember that it's
right at the top
and uh finally we went down here we had
this kind of a little brain teaser how
to do diagonal zeros and ones
playing with the different connotations
of z of the uh numpy array
and then we did a random size and we
played a little bit with how to with the
null values playing with null values
if you're doing any data science you
know null values are like a headache
what do you do with them big sets of
data you get rid of them small debt sets
of data you have to factor something in
there like figure out the average or the
median there and then replace it with
that
so what is pandas
pandas is a python package for data
analysis which provides flexible and
powerful data structures for data
storing and manipulation it is useful
for data wrangling and data manipulation
now pandas is nothing but a software
library which was written for the python
programming language for data
manipulation and analysis in particular
it offers data structures and operations
for manipulating numerical tables and
time series
it is built on top of another package
called numpy which provides support for
multi-dimensional arrays
as one of the most popular data
wrangling packages python works well
with many other data science modules
inside the python ecosystem and is
typically included in every python
distribution from those that come with
the operating system to commercial
vendor distributions like active states
active python
now let's answer the question what are
panda's data frames
data frames are similar to sql tables or
the spreadsheets that you work with in
excel or calc
in many cases data frames are faster
easier to use and more powerful than
tables or spreadsheets because they're
an integral part of python and numpy
ecosystem
now let's take a look at the different
types of pandas data frames
the first type of pandas data frames is
the normal data frame itself a data
frame contains multiple columns each
having a single data type each column
has multiple rows
the next type of pandas data frames is a
series a series is nothing but a
one-dimensional array which can hold any
data type it has access labels or
indexes
a series is a data frame which consists
of a single column the column can have
multiple data types stored within it
now let's look at some fundamental data
frame operations
the first fundamental data frame
operation that we're going to be looking
at is how to create a pandas data frame
we start off by importing pandas with it
in the short form pd so we import pandas
as pd
anywhere that you see the abbreviation
pd it means that pandas is being called
we also import numpy as np
now let's start off by creating a small
numpy array which contains two columns
and four rows
so as you can see here we have a small
numpy array which has two columns and
four rows
uh containing the values one and two
three and four five and six and seven
and eight now let's convert this
but why does this not look like a
numpy's array
that's because while printing it we call
the pandas data frame function on our
numpy array the data here is the data
given from
1s2 1 and 1 is to 1 which is from the
first column to the last column and from
the first row to the last row
the index that we've given it is the
first column
and the columns that we've given it is
the data 0 is to 1 and nothing else
which is nothing but the top row of our
numpy array
finally when you print it out you get a
data frame which looks like this
now let's see how we can convert a csv
file from a data frame
so to read in a csv file as a data frame
we just use the pd.read csv function and
we pass in the location of where our
data
of where our csv file is stored
let's run this and see what our actual
data frame looks like when we read it
from a csv file
so this is what our data frame looks
like the csv file that we've used here
is the grades.csv file
as you can see it has nine
columns and somewhere around
16 rows
it contains the name of different
students along with their ssn number and
how much they've scored in various tests
along with the final grade
now let's look at different data
structures that we can also convert into
a data frame
first let's start off with the two
dimensional array over here we have a
numpy array which contains the values 1
2 3 4 5 6 and seven eight nine
to convert this into a data frame all we
have to do is use the pd.data frame
function and pass in the two-dimensional
array
next let's also convert a dictionary and
use it as an input to a period or data
frame function
so over here as you can see our dict1 is
our dictionary which has the key value
one and the corresponding
values as one and three
the other key value is two and the
corresponding value is one and two and
the final key value is three the
corresponding values are two and four
here the key values are gonna be our
columns and their corresponding values
are going to be the column values so
when we run this you should see a data
frame which has three columns and two
rows as each column has only two values
stored within it
we can also create a data frame
we can also use a data frame as input to
a data frame function
uh
basically all you have to do is pass in
your data and your columns so this over
here is gonna be a single column with
the data stored within it being four
five six and seven
over here as you can see we're not using
a two dimensional array we're not using
a dictionary we're not even using a csv
file which is directly passing in the
name of our columns and our data to the
data frame function
and finally we're gonna create a series
uh so as you know a series is nothing
but a data frame which has a single
column uh and the column can have
different values stored within it over
here we're going to pass in a dictionary
as the input to our series as the input
to our series function in pandas
so let's run this cell and see what our
final output is going to be like
so over here as you can see we've passed
in our two dimensional array which
contains different row values and these
row values
are reflected here
uh because we didn't pass in any column
labels it's direct it's just gonna use
indices as the column values
uh it's gonna start from zero and then
go on go upwards until wherever the
columns end and the same goes for the
row values
now this is at the dictionary that we're
passing in and this is what it looks
like in the form of a data frame so as
you can see uh the key values that we
have will be the
column labels and the corresponding
values will be the row values
because we didn't pass any labels for
our rows it's gonna automatically just
number the rows starting from the zeroth
indice
this is the data frame that we this is
when we passed in pt.dataframe function
we had given a as the column label
and we passed in the corresponding
column values as four five six and seven
so this is what it looks like and
finally this is our series
so as you can see in these series we
passed in different keys and we've given
the values
that is what is reflected here
the key value belgium and the
corresponding value brussels will appear
in the form of a series now over here
the column is only this so the you know
corresponding values these are nothing
but the row labels so belgium here is
the row label and brussels is the column
value
uh because this is not a data frame as
you can see the label that we get for it
is a d type object
now the data frames that we've all been
that we've looked at so far have a
finite value which you can easily see
it's very easy to see and count how many
columns this data frame has and how many
rows are corresponding to the column
values
but what happens when you have a huge
data frame how are you supposed to know
how many rows and columns it has at that
time
for that you can just use the data frame
attribute which is called shape so over
here as you can see uh when we find the
shape of our df1 data frame which is
this data frame
we get 16 and nine the shape is nothing
but the row values and the column values
we can also find the index of the data
frame
so as of now we haven't set an index for
a data frame so it doesn't have a
definite index it just says that it
starts at the zero value and stops at
the 16th value which is nothing but the
number of rows our data frame has and
the step size is one which means that
after
one
of the row changes
now let's see how we can change column
names in a data frame
so as you can see in the data frame here
given here the column names are kind of
weird the first column name is last name
without any double quotes and the
remaining column names all have double
quotes around them when you're referring
to a column or when you need to call
them or print them out this may cause a
little bit of problems so let's see how
we can change the column names
so to call change the column names all
you have to do is call the df1 dot
columns attribute of our data frame
and initialize it to the new column
names
so as you can see the name of the
columns have been changed now they no
more have any more double quotes
if you want
any change that you make here will
immediately be reflected in the name of
the columns of our data frames
let's keep this as first name itself for
now
next let's see how we can select rows
and columns from a data frame
this can be done using the lock and
ilock function
the lock function is usually used to
select the columns and the ilock
function is usually used to select
rows from a data frame
but let's get more into details about
how lock and i log differ from each
other the lock function will work on
labels which means if you're given lock
ssn as given here it will look for the
value of ssn in in your data frame
it will specifically look for the index
or the column or the row which is
labeled ssn
ilock will work on positions that means
as you if you've given an ilok 4 it will
look for values of your data frame that
are at the position 4.
now let's run these two and see how they
differ
so as you can see here using the log
function we've accessed only the column
ssn
as you can see the first at the first
position it is used to specify your rows
and we haven't selected any rows we've
accessed the column ssn by directly
calling its label
uh as you know the log function takes
only labels
so when you're using the log function
you have to pass in a label to your lock
function
now
uh the iloc function will select
all the values and in the row at the
fourth position or at the fourth index
the row which has an index of four here
is bumpkin fred so when you run this you
should get the value of the row at the
fourth index
and all of it should contain bumpkin
fred's data
yeah so basically what is happening is
you're getting the
row which has the index of four
now let's see how we can add rows and
columns to our data frame
uh before we can add rows and columns to
a data frame let's see the differences
between lock and i lock in a lot more
detail
uh so here we have our pandas data frame
we've created a new pandas data frame
and let's print it and see how it looks
so a data frame has three columns 48 49
and 50 and the row indexes are one two
and three
the first thing that we're going to do
is use the log function to access our
row how do we know it's a row because we
haven't mentioned any columns if we were
accessing columns we'd have to put a
colon here but uh
because it's we've just passed to uh
it's going to look for rows so as you
can see
we have access the row which
has a label of 2 over here which is 4 5
and 6. and when we use i log it's gonna
find the the values at the second
position or the second index now our
rows and columns are usually zero
indexed which means the first row and
the first column will have an index of
zero next one will have an index of one
and the final one will have an index of
two
so
this is actually the 0th row this is the
first row and this is the second row
this is why when we pass i lock 2 we get
the values 7 8 and 9.
now let's see how we can add new values
using lock and i lock to our data frame
so again let's start off by creating a
numpy array
which has
columns
a b and c and the row labels as one two
and three
and the row labels as one two and three
so this is our two dimensional data
frames which we've created with the help
of a numpy array the values in the
corresponding rows and columns are one
two three four five six and seven eight
nine
now let's
use
now let's use log to add a new value to
our data frame this should be four this
shouldn't be three this will make an
index label 4 and add the new values so
as you can see already our data frame
row index ends at 3
but using lock we're gonna make a new
row label 4 and add the entire row to
our data frame uh because you know that
lock works on labels and there is no
label 4 when we pas when we use this
function
lock is going to run through all of our
rows and see that there is no row label
4. so it's going to create one by itself
with the values 60 50 and 40.
now as you know using i log we can
change the values at different positions
so uh this is gonna
uh using lock and i lock we can change
the values
and lock will change it and lock will
change it at the label 2.
so
uh over here this just this will
directly change the value of the label 2
to 11 12 and 13.
similarly when we use i lock for the
same function when we pass in 2 to ilock
it's going to change the values at the
label 3 or at the
second index because as i told you it
starts with 0th index so this is 0 1 and
2.
so the values at label 3 are going to
get changed this is how you can change
the row values or add new rows to your
data frame now let's see how it how you
can add a column to a data frame uh to
add a column all you have to do is
specify a new column in your data frame
uh so over here what we're doing is we
are adding a new column d and the values
of uh the column d are gonna be the
index values we're also creating another
column after the column d called the
column e and over here we're manually
passing in the values as 0 1 2 and
0 1 2 and 3.
so when we print
this
as you can see first of all we have the
uh
this is sorry
so let's run this and as you can see the
first uh column that we've added is the
column d which has the index labels
uh starting from one and ending at four
next as you can see after create after
adding the value after adding the column
d you also added the column e and we've
given it the values 0 1 2 and 3.
now let's see how we can delete rows and
columns in a data frame
so let's just take a look at our df1
again this is the grades data frame
where
you have different students you have
their ssn number and you have 10 marks
in different text tests along with their
final grade to drop a column or a row
all you have to do is use the drop
function of our pandas data frames
so over here we use the name of the data
frame dot drop the column or row that we
want to drop and the axis that it is on
over here the axis is 1
which means that uh which
uh means columns when the access is zero
it means that you're specifying the rows
so uh but because you want to drop a
grade
column all we're going to do is specify
the axis as 1.
so when we finally run it as you can see
our df1 has dropped the grade column
now what happens if you don't know the
name of your column
but you know the position at which you
want to drop a column at you so all you
have to do is drop a column by
specifying the position
to specify the position you can use the
column function
and
this is where you enter the column name
so when you only have single parent
thesis that's when you can directly
access the column when you have double
parenthesis that means that you're using
it to access the position the column
that we're gonna drop is at position 1
and we know it's a column because access
value is one so let's run this in c
now as i told you before rows and
columns are zero index so this is zero
and this is one the column that you see
here dropped here is the first name
column which is at position one
so now we've seen how you can drop
columns let's say you can drop rows
uh you can directly drop a row by using
its index over here we're dropping the
row which is at index one which was the
alfred row
so this row got dropped and you as you
can see because we're dropping the row
at index one in
the corresponding label of index one
also got dropped and uh it's missing
so this is how you drop a row from our
data frame now let's say you can change
the index of a data frame
so first let's look at the index of a
df1
as you can see it doesn't really have an
index
it just says that it has a range of
indexes which is starting from 0 and
ending at 16.
on you can set the index of a data frame
by using the set index function
and over here let's set the index as ssn
index is nothing but
the starting value of our data frame so
it just specifies from where our data
frame is starting from
this is going to be at the new zeroth
position
so when you run this
the value of your ssn
is taken as your
index or is taken as your index labels
so now if you want to access any element
in this data frame
depending on its position or if you want
to access any you know like row
depending on the index you're gonna have
to use these index values
you can't use 0 1 2 3 anymore you're
going to have to use the ssn value of
the corresponding row
so far we've worked with the complete
data frame now what happens if you don't
want to use a complete data frame but
only want to use a small portion of it
you can get the small portion by using
slicing
slicing is nothing but using an operator
to select a set of rows or columns from
a data frame
you can perform slicing by using the
lock and ilock function
so in this case suppose you want to get
the test results of
this student for the last name gertie so
over here we're using the boolean
operator to go through a data frame and
get the last name gertie
uh once we arrive at the name gertie
we're gonna extract the values of on of
our first name because we're checking by
the last name so we already have that
we're gonna get her first name and all
the test scores and her final grade
so when i run this uh it's you're gonna
get a small portion of your data frame
so when i run this you're going to get a
small portion of your data frame
depending on the values that you've
given
now suppose you don't know
the name that you want suppose over here
we had like a condition to go on we knew
that we wanted the test results for
gertie what if we just want a certain
portion of the data frame say we want
the first three columns or we only want
the column values from first name to
test three for like the first four to
five rows
uh so for this you can use i log the i
log function as you know the difference
between log and i lock is that the iloc
function will
get the values depending on their index
while the log function will get it
depending on the label value
so when you don't know the label value
you can use the ilock function
here using the iloc function we're gonna
get all the column values for the second
row index
so we've again done the same thing the
only difference is we've used the iloc
function
now let's see this again suppose you
want to get all values up until the
third index and you want all the column
values for it
so this is what you get so as you can
see over here we've specified the third
index value so why we're only getting
the values of index up until two
this is because when slicing in pandas
the start bound which over here the
start bound is zero is included in the
output but the stop bound which is which
over here is three
the stop bound is always one step beyond
the row that you want to select if you
want to get the value of the first
three rows or if you want to get the
value of the fir of the index up until
two
you're gonna have to specify the third
index because this index is out of
bounds
it is not going to be included in the
output
so that is one thing that you have to
keep in mind
uh at the same time this applies for
columns 2. so as you know columns are
indexed from the zero label so as you
can see here this is going to be 0 1 2 3
5 6 7 8. so if you want to get
the eighth column if you want to get all
values up until the eighth column you're
gonna have to go out of bounds and put
nine here
that's when all of your column values
are going to be given but if you put
eight over here
starting from 0 the last column is going
to be excluded
at the same time you don't have to use
the lock and ilock function using i lock
and i lock you can specify the row
values as well as the column values but
if you want you can directly just
slice from the data frame by using
parenthesis values
now over here
you can only get a row slice you cannot
slice the columns
but as you can see
using
df1 2 is to 5 we've got in all values
starting from the second index all the
way up until
the fourth index which is
and as i've said the fifth index will be
out of bound
so these are a couple of ways that you
can slice your data frame again this
works only on the rows you cannot use
this on the columns if you want to slice
for rows and columns you're going to
have to use the lock or the i lock
function
so we talk about what is cross cross is
a high level deep learning api written
in python for easy implement
implementation
of neural networks uses deep learning
frameworks such as tensorflow pytorch
etc is back in to make computation
faster
and this is really nice because as a
programmer there is so much stuff out
there and it's evolving so fast
it can get confusing and having some
kind of high level order in there we can
actually view it and easily program
these different neural networks
is really powerful it's really powerful
to have something out really quick
and also be able to start testing your
models and seeing where you're going
so crossworks by using complex deep
learning frameworks such as tensorflow
pytorch
ml played etc as a back end for fast
computation
while providing a user friendly and easy
to learn front end
and you can see here we have the cross
api specifications and under that you'd
have like tf cross for tensorflow thano
cross and so on and then you have your
tensorflow workflow that this is all
sitting on top of
and this is like i said it organizes
everything the heavy lifting is still
done by tensorflow or whatever you know
underlying package you put in there and
this is really nice because you don't
have to
dig as deeply into the heavy end stuff
while still having a very robust package
you can get up and running rather
quickly
and it doesn't distract from the
processing time because all the heavy
lifting is done by packages like
tensorflow this is the organization on
top of it
so the working principle of cross
the working principle of keras is cross
uses computational graphs to express and
evaluate mathematical expressions
you can see here we put them in blue
they have the expression
expressing complex problems as a
combination of simple mathematical
operators where we have like the
percentage or in this case in python
that's usually your left your remainder
or multiplication
you might have the operator of x to the
power of 0.3 and it uses useful for
calculating derivatives by using uh back
propagation so for doing with neural
networks we send the error back up to
figure out how to change it
uh this makes it really easy to do that
without
really having not banging your head and
having to hand write everything it's
easier to implement distributed
computation and for solving complex
problems uh specify input and outputs
and make sure all nodes are connected
and so this is really nice as you come
in through is that as your layers are
going in there you can get some very
complicated
different setups nowadays which we'll
look at in just a second
and this just makes it really easy to
start spinning this stuff up and trying
out the different models so we look
across models across model we have a
sequential model
sequential model is a linear stack of
layers where the previous layer leads
into the next layer
and this if you've done anything else
even like the sk learn with their neural
networks and propagation and any of
these setups this should look familiar
you should have your input layer it goes
into your layer 1 layer 2 and then to
the output layer
and it's useful for simple classifier
decoder models
and you can see down here we have the
model equals across sequential this is
the actual code you can see how easy it
is
we have a layer that's dense your layer
one
has an activation they're using the relu
in this particular example and then you
have your name layer 1 layer dense relu
name layer 2 and so forth
and they just feed right into each other
so it's really easy just to stack them
as you can see here and it automatically
takes care of everything else for you
and then there's a functional model and
this is really where things are at this
is new make sure you update your cross
or you'll find yourself running this
doing the functional model you'll run
into an error code because this is a
fairly new release
and he uses multi input and multi output
model the complex model which forks into
two or more branches
and you can see here we have our image
inputs equals your cross input shape
equals 32 by 32 by 3.
you have your dense layers dense 64
activation relu
this should look similar to what you
already saw before
but if you look at the graph on the
right it's going to be a lot easier to
see what's going on
you have two different inputs
and one way you could think of this is
maybe one of those is a small image and
one of those is a full-sized image
and
that feedback goes into you might feed
both of them into onenote because it's
looking for one thing
and then only into one node for the
other one and so you can start to get
kind of an idea that there's a lot of
use
for this kind of split and this kind of
setup
we have multiple information coming in
but the information is very different
even though it overlaps and you don't
want to send it through the same neural
network
and they're finding that this trains
faster and is also has a better result
depending on how you split the data and
how you fork the models coming down
and so in here we do have the two
complex models coming in
uh we have our image inputs which is a
32 by 32 by three or three channels or
four if you're having an alpha channel
uh
you have your dense your layers dense is
64 activation using the relu very common
x equals dense inputs x layers dense x64
activation equals relu x outputs equals
layers dense 10
x model equals cross model inputs equals
inputs outputs equals outputs name
equals minced model
uh so we add a little name on there and
again this is this kind of split here
this is setting us up to
have the input go into different areas
so if you're already looking at cars you
probably already have this answer what
are neural networks uh but it's always
good to get on the same page and for
those people who don't fully understand
neural networks to dive into them a
little bit or do a quick overview
neural networks are deep learning
algorithms modeled after the human brain
they use multiple neurons which are
mathematical operations to break down
and solve complex mathical problems
and so just like the neuron one neuron
fires in and it fires out to all these
other neurons or nodes as we call them
and eventually they all come down to
your output layer
and you can see here we have the really
standard graph input layer a hidden
layer and an output layer
one of the biggest parts of any data
processing is your data preprocessing
so we always have to touch base on that
with a neural network like many of these
models they're kind of uh when you first
start using them they're like a black
box you put your data in
you train it and you test it and see how
good it was
and you have to pre-process that data
because bad data in is
bad outputs so in data pre-processing we
will create our own data examples set
with keras the data consists of a
clinical trial conducted on 2100
patients ranging from ages 13 to 100
with a the patients under 65 and the
other half over 65 years of age
we want to find the possibility of a
patient experiencing side effects due to
their age
and you can think of this in today's
world with covid
what's going to happen on there and
we're going to go ahead and do an
example of that in our
live hands-on like i said most of this
you really need to have hands-on to
understand so let's go ahead and bring
up our anaconda and open that up and
open up a jupiter notebook for doing the
python code in now if you're not
familiar with those you can use pretty
much any of your
setups i just like those for doing demos
and showing people especially
shareholders it really helps because
it's a nice visual so let me go and flip
over to our anaconda and the anaconda
has a lot of cool two tools they just
added data lore and ibm watson studio
cloud into the anaconda framework
but we'll be in the jupiter lab or
jupiter notebook i'm going to do jupiter
notebook for this because i use the lab
for like large projects with multiple
pieces because it has multiple tabs
where the notebook will work fine for
what we're doing
and this opens up in our browser window
because that's how jupiter dope so
jupiter
notebook is set to run
and we'll go under new
create a new python 3 and it creates an
untitled python we'll go ahead and give
this a title and we'll just call this a
cross
tutorial
and let's change that to capital there
we go i'm going to just rename that
and the first thing we want to go ahead
and do
is uh
get some pre-processing tools involved
and so we need to go ahead and import
some stuff for that like our numpy do
some random number generation
i mentioned sklearn or your site kit if
you're installing sklearn the sk learn
stuff it's a scikit you want to look up
that should be a tool of anybody who is
doing data science if you're not if
you're not familiar with the sklearn
toolkit
it's huge but there's so many things in
there that we always go back to
and we want to go ahead and create some
train labels and train samples for
training our data
and then
just a note of what we're we're actually
doing in here let me go ahead and change
this this is kind of a fun thing you can
do we can change the code to markdown
and then markdown code is nice for doing
examples once you've already built this
uh our example data we're going to do
experimental
there we go
experimental drug was tested on 2 100
individuals between 13 to 100 years of
age
half the participants are under 65 and
95 percent of participants are under 65
experience no side effects well 95
participants over 65 percent experience
side effects so that's kind of where
we're starting at
and this is just a real quick example
because we're going to do another one
with a little bit more
complicated information
and so we want to go ahead and generate
our setup
so we want to do for i and range and we
want to go ahead and create if you look
here we have random integers
trading the labels append so we're just
creating some random data
let me go ahead and just run that
and so once we've created our random
data and if you if i mean you can
certainly ask for a copy of the code
from simplylearn they'll send you a copy
of this or you can
zoom in on the video and see how we went
ahead and did our train samples of pin
[Music]
and we're just using this i do this kind
of stuff all the time i was running a
thing on that had to do with errors
following a bell-shaped curve on
a standard distribution error
and so what do i do i generate the data
on a standard distribution error to see
what it looks like and how my code
processes it since that was the baseline
i was looking for in this we're just
doing
generating random data for our setup on
here
and we could actually go in
print some of the data up let's just do
this print
we'll do train
samples and we'll just do the first
five pieces of data in there to see what
that looks like
and you can see the first five pieces of
data in our train samples is 49 85 41 68
19.
just random numbers generated in there
that's all that is and we generated
significantly more than that
um let's see 50 up here 1000 yeah so
there's 1 000 here 1 000 numbers we
generated and we could also if we wanted
to find that out we could do a quick
print the length of it
and
so or you could do a shape kind of thing
and if you're using numpy
although the link for this is just fine
and there we go it's actually 2100 like
we said in the demo setup in there
and then we want to go ahead and take
our labels oh that was our train labels
we also did samples didn't we
so we could also print
do the same thing
labels
and let's change this to
labels
and labels
and run that just to double check and
sure enough we have 2100 and they're
labeled one zero one zero one zero
i guess that's if they have symptoms or
not one symptoms zero none and so we
want to go ahead and take our train
labels and we'll convert it into a numpy
array
and the same thing with our samples
and let's go ahead and run that
and we also shuffle uh this is just a
neat feature you can do in numpy right
here
put my drawing thing on which i didn't
have on earlier
i can take the data and i can shuffle it
so we have our so it's it just
randomizes it that's all that's doing
we've already randomized it so it's kind
of an overkill it's not really necessary
but if you're doing a larger package
where the data is coming in and a lot of
times it's organized somehow and you
want to randomize it just to make sure
that that you know the input doesn't
follow a certain pattern
that might create a bias in your model
and we go ahead and create a scalar
the scalar range
minimum max scalar feature range zero to
one
uh then we go ahead and scale the uh
scaled train samples so we're going to
go ahead and fit and transform the data
so it's nice and scaled
and that is the age
so you can see up here we have 49 85 41.
we're just moving that so it's going to
be between zero and one and so this is
true with any of your neural networks
you really want to convert the data
to zero and one otherwise you create a
bias
so if you have like a hundred creates a
bias versus
the math behind it gets really
complicated
if you actually start multiplying stuff
because a lot of multiplication addition
going on in there
that higher end value will eventually
multiply down and it will have a huge
bias as to how the model
fits it and then it will not fit as well
and then one of the fun things we can do
in jupiter notebook is that if you have
a variable you're not doing anything
with it it's the last one on the line
it will automatically print
and we're just going to look at the
first five samples on here it's just
going to print the first five samples
and you can see here we go 0.9195.791
so everything's between 0 and 1.
and that just shows us that we scaled it
properly and it looks good
it really helps a lot to do these kind
of print ups halfway through
you never know what's going to go on
there
i don't know how many times i've gotten
down and found out that the data is sent
to me that i thought was scaled was not
and then i have to go back and track it
down and figure it out on there
uh so let's go ahead and create our
artificial neural network
and for doing that this is where we
start diving into tensorflow and cross
tensorflow
if you don't know the history of
tensorflow
it helps to jump into we'll just use
wikipedia
be careful don't quote wikipedia on
these things because you get in trouble
but it's a good place to start back in
2011 google brain built disbelief as a
proprietary machine learning setup
tensorflow became the open source for it
so tensorflow was a google product
and then it became
uh open sourced and now it's just become
probably one of the defactos when it
comes for neural networks as far as
where we're at
so when you see the tensorflow setup
it's got like a huge following there are
some other setups like
the scikit under the sk learn has their
own little neural network
but the tensorflow is the most robust
one out there right now
and cross sitting on top of it makes it
a very powerful tool so we can leverage
both the cross
easiness in which we can build a
sequential setup
on top of tensorflow
and so in here we're going to go ahead
and do our input of tensorflow
uh and then we have the rest of this is
all cross here from number two down
uh
we're going to import from tensorflow
the cross
connection and then you have your
tensorflow cross models import
sequential it's a specific kind of model
we'll look at that in just a second if
you remember
from the files that means it goes from
one layer to the next layer to the next
layer there's no funky splits or
anything like that
and then we have from tensorflow across
layers we're going to import our
activation and our dense layer
and we have our optimizer atom
this is a big thing to be aware of how
you optimize uh your data when you first
do it atoms as good as any atom is
usually there's a number of optimizer
out there there's about uh there's a
couple of main ones but atom is usually
assigned to bigger data
it works fine usually the lower data
does it just fine but atom is probably
the mostly used but there are some more
out there and depending on what you're
doing with your layers your different
layers might have different activations
on them and then finally down here
you'll see
our setup where we want to go ahead and
use the metrics
and we're going to use the tensorflow
cross metrics
for categorical cross entropy
so we can see how everything performs
when we're done that's all that is a lot
of times you'll see us
go back and forth between tensorflow and
then scikit has a lot of really good
metrics also for measuring these things
again it's the end of the day at the end
of the story how good does your model do
and we'll go ahead and load all that and
then comes the fun part um i actually
like to spend hours messing with these
things
and
four lines of code you're like oh you're
gonna spend hours on four lines of code
um no we don't spend hours on four lines
of code that's not what we're talking
about when i say spend hours on four
lines of code uh what we have here i'm
gonna explain that in just a second we
have a model and it's a sequential model
if you remember correctly we mentioned
the sequential up here where it goes
from one layer to the next
and our first layer is going to be our
input
it's going to be what they call dents
which is uh
usually it's just dense and then you
have your input and your activation
how many units are coming in we have 16
what's the shape
what's the activation and this is where
it gets interesting
um because we have in here
uh relu
on two of these and softmax activation
on one of these
there are so many different options for
what these mean
and how they function how does the relu
how does the soft max function
and they do a lot of different things
we're not going to go into the
activations in here that is what really
you spend hours doing is looking at
these different activations
and just
some of it is just
almost like you're playing with it
like an artist you start getting a fill
for like a
inverse tangent activation or the 10 h
activation
takes up a huge processing amount
so you don't see it a lot
yet
it comes up with a better solution
especially when you're doing uh when
you're analyzing word documents and
you're tokenizing the words
and so you'll see this shift from one to
the other because you're both trying to
build a better model
and if you're working on a huge data set
you'll crash the system it'll just take
too long to process
and then you see things like soft max
softmax
generates an interesting
setup
where
a lot of these when you talk about
rayleigh it lets me do this uh rayleigh
there we go relu has um a setup where if
it's less than zero it's zero and then
it goes up
and then you might have what they call
lazy
setup where it has a slight negative to
it so that the errors can translate
better same thing with softmax it has a
slight laziness to it so that errors
translate better all these little
details
make a huge difference on your model
so one of the really cool things about
data science that i like is you build
your
what they call you build to fail and
it's an interesting
design setup oops i forgot the
end of my
code here
the concept of build a file is you want
the model as a whole to work so you can
test your model out
so that you can do
you can get to the end and you can do
your let's see where was it
over shot down here
you can test your test out the quality
of your setup on there
and
see where did i do my tensorflow oh here
we go it was right above me there we go
we start doing your cross entropy and
stuff like that is you need a full
functional set of code so that when you
run it
you can then test your model out and say
hey it's either this model works better
than this model and this is why
and then you can start swapping in these
models and so when i say spend a huge
amount of time on pre-processing data
it's probably 80 of your programming
time
well between those two it's like 80 20.
you'll spend a lot of time on the models
once you get the model down once you get
the whole code and the flow down
set
depending on your data your models get
more and more robust as you start
experimenting with different inputs
different data streams and all kinds of
things and we can do a simple model
summary here
here's our sequential here's a layer
output a parameter
this is one of the nice things about
cross is you just you can see right here
here's our sequential one model boom
boom boom boom everything's set and
clear and easy to read
so once we have our model built uh the
next thing we're going to want to do is
we're going to go ahead and train that
model
and so the next step is of course model
training
and when we come in here
this a lot of times it's just paired
with the model because it's so
straightforward
it's nice to
print out the model setup so you can
have a tracking
but here's our model
the keyword in cross is compile
optimizer atom learning rate another
term right there that we're just
skipping right over
that really becomes the meat of
[Music]
the setup is your learning rate so
whoops i forgot that i had an arrow but
i'll just underline it
a lot of times the learning rate set to
0.00
set to 0.01
depending on what you're doing
this learning rate
can overfit and underfit
so you'd want to look up
i know we have a number of tutorials out
on overfitting and under fitting that
are really worth reading once you get to
that point and understanding and we have
our loss
sparse categorical cross entropy
so this is going to tell keras how far
to go until it stops
and then we're looking for metrics of
accuracy so we'll go ahead and run that
and now that we've compiled our model
we want to go ahead and
run it fit it so here's our model fit
we have our scaled train samples
our train labels our validation split
in this case we're going to use 10
percent of the data for validation
batch size another number you kind of
play with not a huge difference as far
as how it works
but it does affect how long it takes to
run and it can also affect the bias a
little bit most of the time though a
batch size is between 10 to 100
depending on just how much data you're
processing in there we want to go ahead
and shuffle it we're going to go through
30 epochs
and
put the verb rose of two let me just go
ahead and run this
and you can see right here here's our
epic here's our training
here's our loss
now if you remember correctly up here we
set the loss let's see where was it
compiled our data
there we go loss uh so it's looking at
the sparse categorical cross entropy
this tells us that as it goes how how
how much
how how much does the
error go down
is the best way to look at that and you
can see here the lower the number the
better it just keeps going down
and vice versa accuracy we want let's
see where's my accuracy
value accuracy at the end
and you can see 0.619 0.69 0.74 it's
going up we want the accuracy would be
ideal if it made it all the way to one
but we also the loss is more important
because
it's a balance
you can have 100 accuracy and your model
doesn't work because it's over fitted
again you won't look up
overfitting and under fitting models
and we went ahead and went through
30 epics it's always fun to kind of
watch your code going
to be honest i
usually uh
the first time i run it i'm like oh
that's cool i get to see what it does
and after the second time of running it
i'm like i like to just not see that and
you can repress those of course in your
code
repress the warnings in the printing
and so the next step is going to be
building a test set and predicting it
now
so here we go we want to go ahead and
build our test set and we have just like
we did our training set
a lot of times you just split your your
initial set up
but we'll go ahead and do a separate set
on here
and this is just what we did above
there's no difference as far as
the randomness that we're using to build
this set on here
the only difference is that
we already
did our scalar up here well it doesn't
matter because the data is going to be
across the same thing but this should
just be just transformed down here
instead of fit transform
because you don't want to refit your
data
on your
testing data
there we go now we're just transforming
it because you never want to transform
the test data
easy to mistake to make especially on an
example like this where we're not doing
you know we're randomizing the data
anyway so it doesn't matter too much
because we're not expecting something
weird
and then we went ahead and do our
predictions the whole reason we built
the model as we take our model we
predict
and we're going to do here's our x scale
data batch size 10 verbose
and now we have our predictions in here
and we could go ahead and do a
oh we'll print
predictions
and then i guess i could just put down
predictions in five so we can look at
the first five of the predictions
and what we have here is we have our
age
and uh the prediction on this age versus
what is what we think it's going to be
but what we think is going to they're
going to have uh symptoms or not
and the first thing we notice is that's
hard to read because we really want to
yes no answer
so we'll go ahead and just
round off the predictions
using the arg max
the numpy arg max for prediction so it
just goes to 001
and
if you remember this is a jupiter
notebook so i don't have to put the
print i can just put in
rounded predictions and we'll just do
the first five
and you can see here zero one zero zero
zero so that's what the predictions are
that we have coming out of this
is no symptoms symptoms no symptoms
symptoms no symptoms
and just as we were talking about at the
beginning we want to go ahead and
take a look at this there we go
confusion matrix is for accuracy check
most important part
when you get down to the end of the
story how accurate is your model before
you go and play with the model and see
if you can get a better accuracy out of
it and for this we'll go ahead and use
the site kit
the sk learn metrics site kit being
where that comes from
import confusion matrix
some iteration tools and of course a
nice matplot library
that makes a big difference so it's
always nice to
we have a nice graph to look at a
picture is worth a thousand words
um and then we'll go ahead and call it
cm for confusion matrix why true equals
test labels why predict rounded
predictions
and we'll go ahead and load in our cm
and i'm not going to spend too much time
on the plotting
going over the different plotting code
you can spend
like whole we have whole tutorials on
how to do your different plotting on
there uh what we do have here is we're
gonna do a plot confusion matrix there's
our cm our classes normalize false title
confusion matrix
c map is going to be in blues
and you can see here we have uh to the
nearest c map titles all the different
pieces whether you put tick marks or not
the marks the classes the color bar
so a lot of different information on
here as far as how we're doing the
printing of the of the confusion matrix
you can also just dump the confusion
matrix into a seaborn and real quick get
an output
it's worth knowing how to do all this
when you're doing a presentation to the
shareholders
you don't want to do this on the fly you
want to take the time to make it look
really nice like our guys in the back
did and let's go ahead and do this
forgot to put together our cm plot
labels
we'll go ahead and run that
and then we'll go ahead and call the
little the
definition
for our mapping
and you can see here plot confusion
matrix that's our the little script we
just wrote and we're going to dump our
data into it
so our confusion matrix
our classes
title confusion matrix and let's just go
ahead and run that
and you can see here we have our basic
setup
no side effects 195
had side effects 200
no side effects that had side effects so
we predicted the 10 of them
who actually had side effects and that's
pretty good i mean i i don't know about
you but you know that's five percent
error on this and this is because
there's 200 here that's where i get five
percent is uh divide these both by by
two and you get five out of a hundred uh
you can do the same kind of math up here
not as quick on the fly because it's 15
and 195 not an easily rounded number but
you can see here where they have 15
people
who predicted to have no
with the no side effects but had side
effects
kind of set up on there and these
confusion matrix are so important at the
end of the day
this is really where where you show uh
whatever you're working on comes up and
you can actually show them hey this is
how good we are or not how messed up it
is
so this was a uh i spent a lot of time
on some of the parts but you can see
here is really simple
we did the random generation of data but
when we actually built the model coming
up here uh here's our model summary
and we just have the layers on here that
we built with our model on this and then
we went ahead and trained it and ran the
prediction
now we can get a lot more complicated
let me flip back on over here because
we're going to do another uh demo
so that was our basic introduction to it
we talked about the uh oops there we go
okay so implementing a neural network
with cross after creating our samples
and labels we need to create our cross
neural network model we will be working
with a sequential model which has three
layers and this is what we did we had
our input layer our hidden layers and
our output layers and you can see the
input layer
coming in
was the age factor we had our hidden
layer and then we had the output are you
going to have symptoms or not
so we're going to go ahead and go with
something a little bit more complicated
training our model is a two-step process
we first compile our model and then we
train it in our training data set
so we have compiling compiling converts
the code into a form of understandable
by machine
we use the atom in the last example a
gradient descent algorithm to optimize a
model and then we trained our model
which means it let it uh learn on
training data
and i actually had a little backwards
there but this is what we just did is we
if you remember from our code we just
had let me go back here
here's our model that we created
summarized
we come down here and we compile it
so it tells it hey we're ready to build
this model and use it
and then we train it this is the part
where we go ahead and fit our model and
put that information in here and it goes
through the training on there
and of course we scaled the data which
was really important to do and then you
saw we did the creating a confusion
matrix with keras
as we are performing classifications on
our data we need a confusion matrix to
check the results a confusion matrix
breaks down the various
misclassifications as well as correct
classifications to get the accuracy
and so you can see here this is what we
did with the true positive false
positive true negative false negative
and that is what we went over let me
just scroll down here
on the end we printed it out and you can
see we have a nice printout of our
confusion matrix
with the true positive false positive
false negative true negative and so the
blue ones uh we want those to be the
biggest numbers because those are the
better side and then
we have our false predictions on here
as far as this one so had no side
effects but we predicted
let's see no side effects predicting
side effects and vice versa now uh
saving and loading models with karas
we're going to dive into a more
complicated demo
and you can say oh that was a lot of
complication before well if you broke it
down we randomized some data we created
the
kara set up we compiled it we trained it
we predicted and we ran our matrix
so we're going to dive into something a
lot a little bit more fun is we're going
to do a face mask detection with karass
so we're going to build a cross model to
check if a person is wearing a mask or
not in real time and this might be
important if you're at the front of a
store this is something today which is
might be very useful as far as some of
our you know making sure people are safe
and so we're going to look at mask and
no mask and let's start with a little
bit on the data
and so in my data i have with a mask you
can see they just have a number of
images showing the people in masks and
again
if you want some of this information
contact simply learn and they can send
you some of the information as far as
people with and without masks so you can
try it on your own and this is just such
a wonderful example of this setup on
here
so before i dive into
the mass detection talk about being in
the current with a covid and seeing if
people are wearing masks
this particular example i had to go
ahead and update to a python 3.8 version
uh it might run in a three seven i'm not
sure i haven't i kind of skipped three
seven and installed three eight
uh so i'll be running in a three python
three eight um and then you also wanna
make sure your tensorflow is up to date
because the um they call functional
layers with that's where they split if
you remember correctly from back
let's take a look at this
remember from here the functional model
and a functional layer allows us to feed
in the different layers into different
you know different nodes into different
layers and split them a very powerful
tool
very popular right now in the edge of
where things are with neural networks
and creating a better model so i've
upgraded to python 3.8 and let's go
ahead and open that up and
go through
our next example which includes
multiple layers
programming it to recognize whether
someone wears a mask or not and then uh
saving that model so we can use it in
real time so we're actually almost a
full
end-to-end development of a product here
of course this is a very simplified
version and it'd be a lot more to it
you'd also have to do like recognizing
whether it's someone's face or not all
kinds of other things go into this
so let's go ahead and jump into that
code
and we'll open up a new python three
oops python three
it's working on it there we go
and then we want to go ahead and train
our mask we'll just call this train mask
and we want to go ahead and train mask
and save it
so it's it's
safe mask train mask detection
not to be confused with masking data a
little bit different we're actually
talking about a physical mask on your
face
and then from the cross standpoint we
got a lot of imports to do here
and i'm not going to dig too deep on the
imports
we're just going to go ahead and notice
a few of them
so we have in here
alt d there we go
i have something to draw with a little
bit here
we have our
image processing
and the image processing right here let
me underline that
deals with how do we bring images in
because most images
are like a
square grid and then each value in there
has three values for the three different
colors
cross and tensorflow do a really good
job of working with that so you don't
have to do all the heavy listening and
figuring out what's going to go on uh
and we have the mobile net average
pooling 2d
this again is
how do we deal with the images and
pulling them uh dropout's a cool thing
worth looking up
if you haven't when you as you get more
and more into cross and tensorflow
uh it will auto drop out certain notes
that way you'll get a better
the notes just kind of die
they find that they actually create more
of a bias and help
and they also add processing time so
they remove them
and then we have our flatten that's
where you take that huge array with the
three different colors and you find a
way to flatten it so it's just a one
dimensional array instead of a two by
two by three
uh dense input we did that in the other
one so that should look a little
familiar
oops there we go our input our model
again these are things we had on the
last one
here's our optimizer with our atom
we have some pre-processing on the input
that goes along with bringing the data
in
more pre-processing with image to array
loading the image
this stuff is so nice it looks like a
lot of works you have to import all
these different modules in here but the
truth is is it does everything for you
you're not doing a lot of pre-processing
you're letting the software do the
pre-processing
and we're going to be working with the
setting something to categorical
again that's just a conversion from a
number to a category zero one doesn't
really mean anything it's like true
false
label binarizer the same thing uh we're
changing our labels around and then
there's our train test split
classification report
our
im utilities let me just go ahead and
scroll down here a notch for these
this is something a little different
going on down here this is not part of
the
tensorflow or the sklearn this is the
site kit setup and tensorflow above
uh the path this is part of
opencv
and we'll actually have another tutorial
going out with the opencv so if you want
to know more about opencv you'll get a
glance on it in
this software especially the net the
second piece when we reload up the data
and hook it up to a video camera we're
going to do that on this round
but this is part of the opencv thing
you'll see cv2 is usually how that's
referenced
but the im utilities has to do with how
do you rotate pictures around and stuff
like that
uh and resize them and then the matte
plot library for plotting because it's
nice to have a graph tells us how good
we're doing and then of course our numpy
numbers array and just a straight os
access wow so that was a lot of imports
like i said i'm not going to spend i
spent a little time going through them
but we didn't want to go too much into
them
and then i'm going to create
some variables that we need to go ahead
initialize
we have the learning rate number of
epics to train for and the batch size
and if you remember correctly we talked
about the learning rate
to the negative 4.0001
a lot of times it's .001 or 0.001
usually it's in that
variation depending on what you're doing
and how many epochs and they kind of
play with the epics
the epics is how many times we're going
to go through all the data
now i have it as two
the actual setup is for 20 and 20 works
great the reason i have it for two is it
takes a long time to process
one of the downsides of jupiter
is that jupiter isolates it to a single
kernel so even though i'm on an eight
core processor uh with 16 dedicated
threads only one thread is running on
this no matter what so it doesn't matter
so it takes a lot longer to run even
though
tensorflow really scales up nicely and
the batch size is how many pictures do
we load at once in process
again those are numbers you have to
learn to play with depending on your
data and what's coming in and the last
thing we want to go ahead and do is
there's a directory with a data set
we're going to run
and this just has images of mass and not
masks
and if we go in here you'll see data set
and then you have pictures with mess
they're just images of people with mass
on their face
uh and then we have the opposite let me
go back up here
without masks so it's pretty
straightforward they look kind of askew
because they tried to format them into
very similar setup on there so they're
they're
mostly squares you'll see some that are
slightly different on here
and that's kind of important thing to do
on a lot of these data sets
get them as close as you can to each
other and we'll we actually will run in
the in this processing of images up here
and the cross layers and importing and
dealing with images it does such a
wonderful job of converting these that a
lot of it we don't have to do a whole
lot with
so you have a couple things going on
there
and so we're now going to be this is now
loading the
images and let me see
and we'll go ahead and create data and
labels here's our
here's the features going in which is
going to be our pictures and our labels
going out
and then for categories in our list
directory directory and if you remember
i just flashed that at you it had
face mask or or no face mask those are
the two options and we're just going to
load into that we're going to append the
image itself and the labels so we'll
just create a huge array
and you can see right now this could be
an issue if you had more data at some
point
thankfully i have a 32 gig hard drive or
ram
even that
doesn't you could do with a lot less of
that probably under 16 or even 8 gigs
would easily load all this stuff
and there's a conversion going on in
here i told you about how we are going
to convert
the size of the image so it resizes all
the images that way our data is all
identical the way it comes in
and you can see here with our labels we
have without mask without mask without
mask
the other one would be with mask those
are the two that we have going in there
and then we need to change it to the one
not hot encoding
and this is going to take our um
up here we had was it labels and data
we want the labels to be categorical so
we're going to take labels and change it
to categorical and our labels then equal
a categorical list
we'll run that and again if we do uh
labels and we just do the last or the
first 10 let's do the last 10 just
because
minus 10 to the end there we go
just so we can see where the other side
looks like we now have one that means
they have a mask one zero one zero so on
uh one being they have a mask and zero
no mask
and if we did this in reverse
i just realized that this might not make
sense if you've never done this before
let me run this
zero one
so zero
is uh do they have a mask on zero do
they not have a mask on one so this is
the same as what we saw up here without
mask one equals
um the second value is without mass so
with mass without mask
and that's just a with any of your data
processing
we can't really zero if you have a zero
one output
uh
it causes issues as far as training and
setting it up so we always want to use a
one hot encoder if the values are not
actual
uh linear value or regression values or
not actual numbers
if they represent a thing
and so now we need to go ahead and do
our train x test x train y test y
train split test data
and we'll go ahead and make sure it's
going to be random and we'll take 20
percent of it for testing and the rest
for
setting it up as far as training their
model
this is something that's become so cool
when they're training these set
they realize we can augment the data
what does augment mean
well if i rotate the data around and i
zoom in i zoom out i rotate it
share it a little bit flip it
horizontally
fill mode as i do all these different
things to the data it
is able to it's kind of like increasing
the number of samples i have
so if i have all these perfect samples
what happens when we only have part of
the face or the face is tilted sideways
or
all those little shifts cause a problem
if you're doing just a standard set of
data so we're going to create an augment
in our image data generator
which is going to rotate zoom and do all
kinds of cool things and this is worth
looking up this image data generator and
all the different features it has
a lot of times i'll the first time
through my models i'll leave that out
because i want to make sure there's a
thing we call build to fail which is
just cool to know
you build the whole process and then you
start adding these different things in
so that you can better train your model
and so we go and run this and then we're
going to load
and then we need to go ahead and you
probably would have gotten an error if
you hadn't put this piece in right here
i haven't run it myself because the guys
in the back did this
we take our base model and one of the
things we want to do is we want to do a
mobile net v2
and this we this is a big thing right
here include the top equals false
a lot of data comes in with a label on
the top row
so we want to make sure that that is not
the case
and then the construction of the head of
the model that will be placed on the top
of the base model
we want to go ahead and set that up
and you'll see a warning here i'm kind
of ignoring the warning because it has
to do with the
size of the pictures and the weights for
input shape
so they'll switch things to defaults to
say hey we're going to auto shape some
of this stuff for you you should be
aware that with this kind of imaging
we're already augmenting it by moving it
around and flipping it and doing all
kinds of things to it
so that's not a bad thing in this but
another data it might be if you're
working in a different domain
and so we're going to go back here and
we're going to have we have our base
model we're going to do our head model
equals our base model output
and what we got here is we have an
average pooling 2d pool size 77 head
model
head model flatten
so we're flattening the data
so this is all
processing and flattening the images and
the pooling has to do
with some of the ways it can process
some of the data we'll look at that a
little bit when we get down to the lower
levels processing it
and then we have our dents we've already
talked a little bit about a dense just
what you think about and then the head
model has a drop out
of 0.5
what we can do with a dropout
the dropout says that we're going to
drop out a certain amount of nodes while
training
so when you actually use the model
it will use all the nodes but this drops
certain ones out and it helps
stop biases from performing so it's
really a cool feature in here they
discovered this a while back
we have another dense mode this time
we're using soft max activation
lots of different activation options
here soft max is a real popular one for
a lot of things and so was the relu
and you know there's we could do a whole
talk on activation formulas
and why what their different uses are
and how they work
when you first start out you'll you'll
use mostly the relu and the softmax for
a lot of them
just because they're some of the basic
setups it's a good place to start
uh and then we have our model equals
model inputs equals base model dot input
outputs equals head model so again we're
still building our model here we'll go
ahead and run that
and then we're going to loop over all
the layers in the base model and freeze
them so they will not be updated during
the first training process
so for layer and base model layers
layers dot trainable equals false
a lot of times when you go through your
data
you want to kind of jump in part way
through
i
i'm not sure why in the back they did
this for this particular example
but i do this a lot when i'm working
with series and
specifically in stock data i wanted to
iterate through the first set of 30 data
before it does anything
i would have to look deeper to see why
they froze it on this particular one
and then we're going to compile our
model
so compiling the model atom
init layer decay
initial learning rate over epics
and we go ahead and compile our loss is
going to be the binary cross entropy
which we'll have that print out
optimizer for opt metrics is accuracy
same thing we had before not a huge jump
as far as
the previous code
and then we go ahead and we've gone
through all this and now we need to go
ahead and fit our model uh so train the
head of the network print info training
head run
now i skipped a little time because it
you'll see the run time here is at 80
seconds per epic takes a couple minutes
for it to get through on a single kernel
one of the things i want you to notice
on here while we're well it's finishing
the processing
is that we have up here our augment
going on so
anytime the trainex and trading y go in
there's some randomness going on there
is jiggling it around what's going into
our setup
uh of course we're batch sizing it so
it's going through whatever we set for
the batch values how many we process at
a time
and then we have the steps per epic
the train x the batch size validation
data here's our
test x and test y where we're sending
that in
and this again it's validation one of
the important things to know about
validation is
our
when both our training data and our test
data have about the same accuracy that's
when you want to stop that means that
our model isn't biased
if you have a higher accuracy on your
testing
you know you've trained it and your
accuracy is higher on your actual test
data then something in there is probably
uh has a bias and it's overfitted
so that's what this is really about
right here with the validation data and
validation steps
so it looks like it's let me go ahead
and see if it's done processing looks
like we've gone ahead and gone through
two epics again you could run this
through about 20
with this amount of data and it would
give you a nice refined model at the end
we're going to stop at two because i
really don't want to sit around all
afternoon and i'm running this on a
single thread
so now that we've done this we're going
to need to evaluate
our model and see how good it is and to
do that we need to go ahead and make our
predictions
these are predictions on our test x
to see what it thinks are going to be
so now it's going to be evaluating the
network and then we'll go ahead and go
down here
and we will need to
turn the index in because remember it's
it's either 0 or 1 it's a
0 1 0 1 so you have two outputs
not wearing wearing a mask not wearing a
mask
and so we need to go ahead and take that
argument at the end and change those
predictions to a
0 or 1 coming out
and then
to finish that off we want to go ahead
and let me just put this right in here
and do it all in one shot we want to
show a nicely formatted classification
report so we can see what that looks
like on here
and there we have it we have our
precision uh it's 97 with a mask
there's our f1 score support without a
mass 97 percent
um so that's pretty high
set up on there you know you three
people are going to sneak into the store
who
are without a mask and
thinks they have a mask and there's
going to be three people with a mask
that's going to flag the person at the
front to go oh hey look at this person
you might not have a mask
that's if i guess it's just set up in
front of a store
so there you have it and of course one
of the other cool things about this is
if someone's walking in to the store and
you take multiple pictures of them
um
you know this is just an it would be a
way of flagging and then you can take
that average of those pictures and make
sure they match or don't match if you're
on the back end and this is an important
step because we're gonna this is just
cool i love doing this stuff
so we're gonna go ahead and take our
model and we're gonna save it
so model save massdetector.model we're
going to give it a name
we're going to save the format
in this case we're going to use the h5
format
and so this model we just programmed has
just been saved
so now i can load it up into say another
program what's cool about this is let's
say i want to have somebody work on the
other part of the program well i just
saved the model they upload the model
now they can
use it for whatever and then if i get
more information
and we start working with that at some
point
i might want to update this model and
make a better model and this is true of
so many things where i take this model
and maybe i'm running a prediction on
making money for a company and as my
model gets better
i want to keep updating it and then it's
really easy just to push that out to the
actual end user
and here we have a nice graph you can
see the training loss and accuracy as we
go through the epics
we only did the you know only shows just
the one epic coming in here but you can
see right here as the
value loss train accuracy and value
accuracy
starts switching and they start
converging and you'll hear converging
this is a convergence they're talking
about when they say you're you're
i know when i work in the
scikit with sk learn neural networks
this is what they're talking about a
convergence is our
loss and our accuracy come together and
also up here and this is why i'd run it
more than just two epochs as you can see
they still haven't converged all the way
so that would be a cue for me to keep
going
but what we want to do is we want to go
ahead and create a new
python 3
program
and we just did our train mask so now
we're going to go ahead and import that
and use it and show you in a live action
get a view of both myself in the
afternoon along with my background of an
office which is in the middle still of
reconstruction for another month
and we'll call this a mask
detector
and then we're going to grab a bunch of
a few items coming in
we have our
mobile net v2 import pre-processing
input so we're still going to need that
we still have our tensorflow image to
array we have a load model that's where
most of stuff's going on
this is our cv2 or opencv again i'm not
going to dig too deep into that we're
going to flash a little opencv code at
you
and we actually have a tutorial on that
coming out
our numpy array our im utilities which
is part of
the opencv or cv2 setup
and then we have of course time and just
our operating system so those are the
things we are going to go ahead and set
up on here and then we're going to
create
this takes just a moment
our module here which is going to do all
the heavy lifting
uh so we're going to detect and predict
the mask we have frame face net mass net
these are going to be generated by our
open cv we have our frame coming in and
then we want to go ahead and
create a mask around the face it's going
to try to detect the face and then set
that up so we know what we're going to
be processing through our model
and then there's a frame shape here this
is just our height versus width that's
all hw stands for
they've called it blob which is a cv2
dnn blob form image frame
so this is reformatting this frame
that's going to be coming in literally
from my camera and we'll show you that
in a minute that little piece of code
that shoots that in here
and we're going to pass the blob through
the network and obtain the face
detections
so face net dot set import blob
detections face net forward
print detections shape
so this is this is what's going on here
this is that model we just created we're
going to send that in there and i'll
show you in a second where that is but
it's going to be under face net
and then we go ahead and initialize our
list of faces their corresponding
locations and the list of predictions
from our face mask network
we're going to loop over the detections
and this is a little bit more work than
you think um as far as looking for
different faces whatever the view of a
crowd of faces
um so we're looping through the
detections and the shapes going through
here
and probability associated with the
detection here's our confidence of
detections
we're going to filter out weak detection
by ensuring the confidence is greater
than the minimum confidence
so we've said it remember 0 to 1 so 0.5
would be our minimal confidence probably
is pretty good
[Music]
and then we're going to put in compute
bounding boxes for the object if i'm
zipping through this it's because we're
going to do an open cv and i really want
to stick to just the cross part
and so i'm just kind of jumping through
all this code you can get a copy of this
code from simply learn and take it apart
or look for the opencv coming out
and we'll create a box the box sets it
around the image
ensure the bounding boxes fall within
the dimensions of the frame
so we create a box around what's going
to what we hope is going to be the face
extract the face roi convert it from bgr
to rgb channel
again this is an open cv issue not
really an issue but it has to do with
the order i don't know how many times
i've forgotten to check the order colors
when working with opencv
because there's all kinds of fun things
when red becomes blue
and blue becomes red
and we're going to go ahead and resize
it process it frame it
face frame
setup again the face the cvt color we're
going to convert it
we're going to resize it
image to array pre-process the input
pin the face locate face start x dot y
and x boy that was just a huge amount
and i skipped over a ton of it but the
bottom line is we're building a box
around the face
and that box because the open cv does a
decent job of finding the face and that
box is going to go in there and see hey
does this person have a mask on it
and so that's what that's what all this
is doing on here and then finally we get
down to this where it says predictions
equals mass net dot predict faces batch
size 32
so these different images where we're
guessing where the face is are then
going to go through and
generate an array of faces if you will
and we're going to look through and say
does this face have a mask on it and
that's what's going right here is our
prediction that's the big thing that
we're working for
and then we return the locations and the
predictions the location just tells
where on the picture it is
and then the
prediction tells us what it is is it a
mask or is it not a mask
all right so we've loaded that all up
so we're going to load our serialized
face detector model from disk
and we have our the path that it was
saved in obviously going to put in a
different path depending on where you
have it or however you want to do it and
how you saved it on the last one where
we trained it
uh then we have our weights path
and so finally our face net here it is
equals cb2.dnn.readnet
prototext path weights path and we're
going to load that up on here so let me
go ahead and run that
and then we also need to i'll just put
it right down here i always hate
separating these things in there
and then we're going to load the actual
mass detector model from disk this is
the the model that we saved so let's go
ahead and run that on there also so this
is pulling on all the different pieces
we need for our model
and then the next part is we're going to
create open up our video
and this is just kind of fun because
it's all part of the opencv
the video set up
and we just put this all in as one
there we go
so we're going to go ahead and open up
our video we're going to start it and
we're going to run it until we're done
and this is where we get some real like
kind of live action stuff which is fun
this is what i like working about with
images and videos
is that when you start working with
images and videos it's all like right
there in front of you it's visual and
you can see what's going on
uh so we're gonna start our video
streaming this is grabbing our video
stream source zero start
uh that means it's grabbing my main
camera i have hooked up um
and then you know starting video you're
going to print it out
here's our video source equals 0 start
loop over the frames from the video
stream
oops a little redundancy there
let me close
just leave it that's how they headed in
the code so uh so while true we're going
to grab the frame from the threaded
video stream and resize it to have the
maximum width of 400 pixels so here's
our frame we're going to read it
from our visual
stream
we're going to resize it
and then we have we're returning
remember we return from the our
procedure the location and the
prediction so detect and predict mask
we're sending it the frame we're sending
it the face net and the mast net so
we're sending all the different pieces
that say this is what's going through on
here
and then it returns our location and
predictions and then for our box
and predictions in the location and
predictions
and the boxes is again this is an open
cv set that says hey this is a box
coming in from the location
because you have the two different
points on there
and then we're going to unpack the box
in predictions and we're going to go
ahead and do mask without a mask equals
prediction
we're going to create our label
no mask and create color if the label
equals mask
l0225 and you know it's going to make a
lot more sense when i hit the run button
here
but we have the probability of the label
we're going to display the label and
bounding box rectangle in the output
frame
and then we're going to go ahead and
show the output from the frame cv to im
show frame frame and then the key equals
cp2 weight key one we're just going to
wait till the next one comes through
from our feed
and we're going to do this until
we hit the stop button pretty much
so are you're ready for this to see if
it works we've distributed our our model
we've loaded it up into our distributed
uh code here we've got a hooked into our
camera and we're going to go ahead and
run it
and there it goes it's going to be
running and we can see the data coming
down here and we're waiting for the
pop-up
and there i am in my office with my
funky headset on
and you can see in the background my
unfinished wall and it says up here no
mask oh no i don't have a mask on
i wonder if i cover my mouth
what would happen
you can see my no mask
goes down a little bit i wish i brought
a mask into my office uh it's up at the
house but you can see here that this
says you know there's a 95 98
chance that i don't have a mask on and
it's true i don't have a mask on right
now and this could be distributed this
is actually an excellent little piece of
script that you can start you know you
install somewhere on a video feed on a
on a security camera or something and
then you have this really neat uh setup
saying hey do you have a mask on when
you enter a store or a public
transportation or whatever it is where
they're required to wear a mask
let me go and stop that
now if you want a copy of this code
definitely give us a hauler we will be
going into opencv in another one so i
skipped a lot of the opencv
code in here as far as going into detail
really focusing on the cross
saving the model uploading the model and
then processing a streaming video
through it so you can see that the model
works we actually have this working
model that hooks into
the video camera
which is just pretty cool a lot of fun
so i told you we're going to dive in and
really roll up our sleeve and do a lot
of coding today we did the basic uh
demo up above for just pulling in across
and then we went into a cross model
where we pulled in data to see whether
someone was wearing a mask or not so
very useful in today's world as far as a
fully running application
thank you all for watching this video on
top 5 python libraries by simply learn i
hope it was useful and informative if
you have any queries please feel free to
put them in the comment section of the
video thank you for watching
hi there if you like this video
subscribe to the simply learn youtube
channel and click here to watch similar
videos turn it up and get certified
click here