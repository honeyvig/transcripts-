big data is a skill set that probably
all the data scientists have at this
moment
that take us to spark one of the most
popular tools for working with big data
spark has recently made a data frame api
available for data scientists to use
replacing its former harvard reliance on
rdd manipulations
so let's have a quick look at the agenda
of today's video
i will start this session with the
introduction to pi spark
next we will understand why sparks data
frame
then we will have a look at its feature
after that we will learn the use of data
frames
and then i will show you guys how to
create data frame from various sources
and will finish off this video with two
amazing use case which will help you to
understand data frames better
next we have introduction to pi spark
introduction to pi spark
python pixbox provides an interface for
apache spark
it enables you to create spark
application using the python apis and
give you access to the pi spark shell
enabling interactive data analysis in a
distributed setting
most of the pi spark functionality
including spark sql data frame streaming
mlib and spark core are supported by pi
spark
to build more scalable analysis and
pipelines pi spark is a useful language
to learn if you are already familiar
with python tools like pandas
next is what is spy spot data frame a
distributed grouping of rows with name
columns is known as a data frame
in layman's terms it is equivalent to an
excel sheet with columns header or a
table in a relational database
additionally it has several traits in
common with rdt
we can create a data frame or rdd only
once we cannot modify it after applying
transformations our data frame or rdd
can be transformed
in other words a task is not carried out
until an action is taken
both the rdd and the data frame have a
distributed design
i'm sure you might be wondering why
exactly we need data frames now let's
discuss that
data frames features
distributed data frames are distributed
data collections that are arranged into
rows and columns in pi spark
our data frame has a name and the type
for each column
data frames are comparable to
conventional database tables in that
they are organized and brief
the next feature is lazy evolution
although scala may be executed lazily
and spark is written in scala
spark's default execution mode is lazy
this means that up until the action is
involved no operation over an rdd
data frame or data set are ever computed
next is
immutable immutable storage includes
data frames data sets and rdd
the world immutability means inability
to change
when used in reference to an object it
signifies that its state cannot be
changed once it is formed compared to
python these data frames are immutable
and provide less flexibility when
manipulating rows and columns
now we are going to look at the demo how
to create data frames
let's go ahead and create some data
frames so now i used google cloud
instead of pi spark shell
as i personally find it easier to work
with
it comes down to your choice so guys
let's go ahead and start the demo
so now i will show you how to create a
data frame in pricebar firstly we need
to have spark configured on our device
to use the file function
for this you need to open your command
prompt and then type in this command to
install facebook vip install
so now here we are ready to get started
we can create our data frame in price
park shell but i prefer to use google
collab for myself you can choose to do
it you know pi spark shell or jupiter
notebook also
so
here let us open a new notebook
now firstly we will have to set up a
price pack
so we will type in the same command pip
install by spark
now as you can see that by spark is set
up we can move on to creating an example
of data frame for departments and
students
so now as you can see my spark is
setting up
importing price
item
importing my spa sql
now as you can see that pi spark is set
up we can move on to create an example
data frame for departments and student
so now after the import is successful i
am going to create a student database
using the row function
consists of columns named as first name
last name email page and row number
so now we will create data for some
students
now in students we have columns named as
first name last name
email
each and androgen
so now we will create data for some
students
we will write student one equal to
suitable
then inside this brackets we will by
john
like the first name
then the model which is the last name
then we will write the email
age and roll number
again we will do the same for the rest
of them like we will just copy this one
and then paste after that we will change
the specific first name and last name
email id etc
like most of them two three four and
five also we will change the specific
name
like student 2 is cassie
and there is no surname so we have to
write none
[Music]
then we will come to lois
because lois has a last name so it will
be lopez
it's
so student for his chap
and his surname is doris
because he don't have a email id so it
will be none
and the 8 is 23
so for student 5 who is
michael
we will do the same procedure
as the surname is reaction
and the main mail is i repeat and the
email is michael at the rate of
unique or same
as you can see that all these students
data are in the same format as given
first name last name
agent role number
so let us create some department data as
well
so now we will go on text
and
let's create
the department
again we will use the row function
so department one equals row then inside
the brickets
id is equal to zero 0 1 we can take
and name is equal to subject
so name is equal to maths
the subject
or whatever you want to take
now coming to department two it will be
the same as department one
just have to enter the row
then inside the bracket
the same id is equal to whatever the id
you want to take
because we are going in the serial
number so it should be zero zero two
zero zero one
and uh name is equal to
we should take english
now coming to department 3
it will also go same
starting with the row
id
that is 003
and again the name of the subject
uh we can take the sets
so
it will also go save for the department
for
we should just copy and paste
zero zero four
name of the subject show chemistry
or whatever you want to
think
so now suppose you want to look at the
values of student 2
we will use the print command to print
student 2
so let's do
coming to code
and now print
in bracket
student 2
so you can see here the values of row
student 2
the first name is cassie last name is
not specified so it has been printed as
null value
before moving further we should check if
there is any error or correction we
should make
let's see okay
so
there are some double quotes
and email is also missing
as you can see only the single quotes
will work
and because we mentioned the email for
other students so we have to mention the
email in the elements also
so
yeah
okay so it's all fine let's have a look
let's just cross check and run that
yeah it's working
so
moving forward
so you can see here the values of row
student 2 the first name is cassie last
name is not specified so it has been
printed as null value
then we have the email cassie at the
rate of uni
dot sem
under ages 22
and roll number which is 14526
so now see how easy it is to use python
and now we can also create some
instances for our departments and
students data okay now so we have to
click on the text post
then
we have to write only create the
department
with
student instances
tab
so
now we have to click on the code
so as for department student one is
equal to
department student one
is equal to row
and inside its department is equal to
department one
and student is equal to student one and
student 2
now for the others we just have to copy
this
and paste like we have done earlier
and you just have to change the
serial number of student
department
here we are creating some instances
which shows us the student each
department consists of
let me show you how it is done
so you just need to execute the command
so
so we just have to go on the code
and type
print
department
with student 3 we will take student 3.
now we should cross check first
let's see if there is some mistake or
something
okay
so we have to correct this
now we should run and check
if it is running or not
okay so here's the output
we get the roll number which is one
three five three six
now finally we can create our data frame
let us create an array of sequence of
instances for our data frame
so
let's start
creating data frame
now we will go again on the code
and the
department
with students
is equal to
department with student one
and then
department with student two and then
department with student three would get
along
department needs to round three
here department 1 consists of student 1
and department 2 consists of students 3
and 4 and department three consists of
student four and five
then we need to create spark context
using from pi spark
dot context so we are gonna write
uh we should go on code first
after selecting the code we need to
create spark context using
from pi stock
okay
five spark
dot
context
import spark context
and then
from
from pi spark dot
sql dot session
import spark session
sc is equal to spam
so basically after this we can create
data frame
using department with students
so let's
go on the text
so now let's run this program let's see
if it is working
okay
so now moving forward uh we'll go to the
code then
df is equal to spark dot create
data frame
df that is the department for student
now we will go to the text
and
let's
go for display data frame
so now we can display our data frame
using display function so let's
go to the code again
and write display function
that is df
so now
as you see we have the structure of
department which consists of two strings
id and name
and the student array which consists of
three strings first name last name and
email and two integer values which are
age and row number
now as you know how to create a data
frame let us move on the code pandemic
use case
so here we have the data of all the
countries affected by cov19 pandemic
and it is the csv format so i am going
to show you how it is done
firstly we will import spark session
from pi spark sql using the command so
let's do that
so before going further we should run
this first
and let's see
yeah
so
now we will go to the text and write
data frame from csv
because i am going to show how it is
done
so
firstly we will import spark session
from pi spark sql using the command
that is from firespark
dot sql
spark session
let's run this also
yeah
so
now let's initiate the spark session to
create a data frame by executing the
following command
that is
spark let's go to the code again and
write
spark is equal to
spark session
dot builder
dot app
dot app name
and then will break it
create data frame
from csv file
because here we are learning how to
create this data frame from csv file
so create data frame from csv file
get or create
so now as the spark session is initiated
we can use the spark dot read command to
read the csv file and convert it into
our data frame
which we have named as poet 19
underscore df
so let's again go to the code
okay so before that we should run this
code first
so it's not
okay
i believe there must be some syntax
error
let's find out okay
so
this should be capital
noise will work like this yeah it's
working
so now as the spark session is initiated
we can use the spark dot read command to
read the csv file of course
and convert it into our data frame which
we have named as govet underscore tf
so let's
go it
go to the code we are already in the
code
so go with underscore df
is equal to
spark dot read
csv
then in the bracket
path is equal to
double quotes
and then content country wise latest csv
underscore is underscore latest dot
csv
okay
so here as you see i have provided a
path
to the csv file you can do this by
uploading it on collab
so let me show you how it is done
so as it is uploaded now we can right
click on the file and copy the path into
our spark.read command
otherwise if you are doing it in the spy
spark shell you can directly copy the
path of the file from the local
directory
we have used comma as a separator and as
you see i have set header equal to true
this is because otherwise the data frame
would take the first row as the initial
values of the data set
so now after successful execution of the
command our data
which we are going to
so yeah it's running so now i will tell
you how to upload this file let's go
into the files
on the document
then
i have already uploaded this file that's
country wisely
so you just have to choose the file
and it will be visible on your screen
as you can see here
so
with the next step
now after successful execution of the
command our data is created and now
let's try out some commands which we can
use on our data plane
so let's go to the
code firstly let us use the dot show
command to display our data frame that
is go with underscore df show
so now let's check first if it's correct
or not
yeah it's working and we have the data
set here
so now as you can see here we have the
country or region number of confirmed
cases active cases recovered cases
number of death and so on
now to look at the total number of
records in our data frame we can use
covet underscore df count so for that we
have to go to the code again
okay
so let's type covert underscore
df dot com
sorry
dot com okay
so with this
we should also check it first
i hope it's working
yeah
we have the output
so here we have total 187 the cons or
rows in our data frame
now if you want to see the columns
although we have seen it using the
df.show function
but if you want to see just the name of
column separately you can use
go with underscore df dot columns
so
df
dot
count
first we have to
run it
okay okay so there is a correction
actually there should
be
a column
instead of
[Music]
so
i run the code
and
that's the output
so as you can see here we have all the
columns listed out
now here we can simply count the number
of columns but for more complex data
frame
we can use
lan cover tf columns for that we have to
go to code again
and
then in brick it
covet
underscore
tf
df
dot
columns
and packet closed
let's run this
so now you can see there are 15 columns
in our data frame
now let's print the schema of our data
frame
let's go to code again
we have to write
go with sorry
go with underscore
df
dot
print
camera
and
closed so let's run this also
okay
so as you can see the types of column we
have been mentioned here and you can set
no label as they're either true or false
because in some data frames maybe some
of the values are supposed to be null
so moving on further you can also
describe a particular column
using
covet
underscore
df dot to
describe
and bracket
we should use double quote yeah
so
world health organization reason
no sorry we should use
a
single yeah
single point
because it will not work with the other
ones
okay so dot
show
and then yeah
so
as you can see the column
w ato region have been described here
count is equal to 187
that means it has total 187 nicots
so here mean and standard deviation are
null
as we have string values
so the minimum value is africa and the
maximum is western pacific
alphabetically
we can also describe one more column as
per
let's go to code first
with
covet
underscore
df dot to
subscribe
okay
dot
show
let's run this
here also count of total record is 187
and main value is approx to 34 000
and the standard deviation is near about
twenty one thousand thirty three hundred
twenty six
and the minimum and maximum values are
given as zero
and ninety eight thousand seven hundred
and fifty two
so now we can also select a particular
column or more than one column to show
their details
by using
so
don't select
[Music]
here single point
country
slash
vision
[Music]
space slash space
handed kisses
dot show let's see if it's working
okay so it's working
here you can see the data of two columns
separately
we also have a filter function to filter
out the data of a particular column at
given position
and also we can do that by using
covet
underscore
df
filter
intricate
covet
underscore
df dot
dates
thousand
dot
show
okay
here we can see all the records in our
data frame where the death count is
greater than 1000
we can also count the number of records
shown in by the above command using
count function instead of using show
function
so for that
let's see what is the command function
that is go right
df
dot to filter
go with
we have used the same command actually
that we have used earlier but instead of
show you just have to write count
so
because you are using count function
here
so
df
dot
x
is
1 000
and
instead of show just you have to write
count
that's it
that's run
that is 44 so the total number of record
in the data frame with more than 1000
deaths is 44.
huh we can also filter out more than one
condition like this and for that let's
see
go to code the command should be
covered
underscore tier
dot filter
double brackets
greater than one
000 of course because we have used this
information earlier also
and
call it
underscore
sorry
go with underscore
actor
that is greater than
ten thousand
just closed okay
dot
show don't forget to close okay
let's run this for now
let's cross check what's the error
coming you also have to take care of the
small mistakes
go with underscore df dot filter
double click it yeah so here's the
mistake
underscore
go with dot debt
greater than 1000
and
here's also one mistake
yeah
so go with unless go dot active
ten thousand dot show let's run it again
so it work now here we all the records
where the count is greater than 1000 and
also the number of active cases is
greater than 10 000
we can also use order by function to
arrange our data frame in ascending or
descending order so for that
we have to use the command
that is
covet
underscore tf
dot order
dot order
by
in bracket
active
dot show
let's run it
so the error is
here
okay
this is called camel casing
b should be capital and y should be
small
okay so let's run it again
yeah
so as you can see data in our data frame
is arranged in ascending order
now we will try on running some sql
queries on our data frame first
we need to
create a temporary table using
the command
i'm using the
[Music]
command coverage underscore tf
dot
register
temp
table
in bracket
covet
sorry c should be capital
go with
underscore
table now we can use the command
that is
spark
dot to
sql
in bracket
single
quit
select
from covetable
dot show
so
to show all the content of the table we
can also select a particular column
using
this last
command
that is
spark dot
sql
in bracket
select
sorry
recovered
from
it
okay
dot
show
so you can see the data from a
particular column of the table is
fetched out
basically these were the top 10
functions
so with this we have come to the end of
the session on pi spark data frames
i hope you found the session both
interesting and exciting if you have any
questions about any of the topics
covered in this session or if you need
resources used in this session
please let us know in the comment
section below and our team of experts
will be pleased to respond as soon as
possible
thank you until next time this is umrah
from the simply land team signing off
continue to learn and stay safe
hi there if you like this video
subscribe to the simply learn youtube
channel and click here to watch similar
videos to nerd up and get certified
click here