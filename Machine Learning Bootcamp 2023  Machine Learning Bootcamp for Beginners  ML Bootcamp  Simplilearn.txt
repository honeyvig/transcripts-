foreign
welcome to the machine learning boot
camp today we have something truly
remarkable in store for you get ready to
embark on an exhilarating Journey to the
realm of machine learning I am thrilled
to present to you our Ultimate Machine
learning bootcamp machine learning The
Cutting Edge field has revolutionized
the way we solve complex problems has
now become accessible to all with the
explosion of Big Data and advancements
in computing power machine learning has
emerged as the driving force behind the
next wave of innovation across
Industries but what exactly is machine
learning well it's the science of
teaching computers to learn and make
decisions without being explicitly
programmed it's the path to unlock
hidden insights predict future outcomes
and automate processes like never before
and in this boot camp we will equip you
with an essential skills and knowledge
to become a proficient machine learning
practitioner
but wait you might be wondering who is
this bootcamp for the answer is simple
anyone with a passion for technology and
a desire to shape the future whether you
are a student a working professional or
a curious individual eager to expand
your horizons this boot camp is designed
to cater to all skill levels from
beginners to season experts and now
according to recent studies machine
learning related job postings have
increased by staggering 344 percent in
the past five years companies across the
globe are actively seeking professionals
who can harness the power of data and
build intelligent systems with this
bootcamp you will gain a Competitive
Edge in the job market and open doors to
exciting career opportunities
Welcome to our machine learning bootcamp
where we will explore a wide range of
topics to equip you with the skills you
need to thrive in this exciting field
throughout this program we will cover
the following areas and starting with
introduction to machine learning here we
will understand the foundations and
principles of machine learning then
we'll learn what is machine learning
here we will explore how computers learn
from data and make predictions or
decisions without explicit programming
then we have types of machine learning
here we'll discover supervised learning
and supervised learning and
reinforcement learning then we have top
10 applications of machine learning here
we will showcase the immense impact of
machine learning across Industries then
we have mathematics for machine learning
we will cover essential mathematical
Concepts behind machine learning
algorithms then we'll learn what is deep
learning that is delving into the
fascinating world of deep learning and
neural networks then we have linear
versus logistic regression that is
comparing and contrasting collection and
classification algorithms then we have
what is a confusion Matrix here we will
understand how to evaluate the
performance of classification models
then we'll see the demo on heart attack
prediction here we will build a machine
learning model to predict heart attack
likelihood then we have what is name
base exploring the new base algorithm
for classification task then we have
k-means clustering here we'll understand
how to group data points based on
similarity then we have regression
algorithm we will learn about algorithms
for predicting continuous values then we
have top 10 machine learning projects
for 2023. here we'll showcase Innovative
projects with significant impact then we
have the project that is Project fake
news detection here we will apply
machine learning techniques to combat to
spread of fake news then we will learn
what is opencv and see a project on
object detection here we will introduce
opencv and guide you through object
detection process then we have machine
learning interview questions that will
prepare you for job interviews with
essential questions
so get ready for an immersive learning
experience as you delve into these
fascinating topics in a machine learning
boot camp if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply lens professional
certificate program in Ai and machine
learning from Purdue University in
collaboration with IBM should be a right
choice for more details use the link
mentioned in the description box with
that in mind but before we begin let's
check out what one of our Learners has
to say about our courses hey
Ontario Canada I have been in I.T sector
for the past 20 years I recently took
the professional certificate program in
artificial intelligence and machine
learning the course has changed the way
I look at things and helped me back some
amazing freelance projects I started my
career in 1999 and over the years I have
worked with many companies my last tenor
was with IBM Canada my aim was to
restart my carrier and learn something
that would help accelerate my career
I thought artificial intelligence can
make me Future Ready
the course in artificial intelligence
and machine learning is provided by
simply learn in association with Purdue
University which is why I chose the
course I did not have high expectations
from online course but my experience was
simply awesome the quality of
interaction within the course was simply
amazing the course faculty was also very
experienced and knowledgeable
after the course my knowledge has grown
manifold I have
a coding skills I am able to get some
new freelance projects
start up with my friend where I feel
that the learning from the course to be
very helpful I am really delighted and
happy in my free time I try to create
meaningful content on YouTube and I talk
about new technologies and what kind of
courses professionals should be taking
along with many other topic I definitely
recommend this course to everyone after
all when it comes to new skills to
advance your career there should be no
compromise you should always learn from
the band
are you one of those people who think
machine learning is the answer to time
travel hit start Magic the term machine
learning can be spotted everywhere these
days from self-driving cars to voice
recognization software and a robotic
activities the study of machine learning
focus on the software that can be
learned from the examples it is a subset
of artificial intelligence and it refers
to the intelligence exhibited by the
machines that is a computer system well
we at the humans work to make such
commuter system as intelligent as us
Google's rapid and precise search result
emails that identifies spam and a credit
card that identify false transactions
are all possible because of machine
learning machine learning makes
predictions or decision using a complex
statistics and a mathematical models
that makes our life easier
no wonder it has applications in various
industries from marketing and finance to
healthcare Agriculture and entertainment
the growth in this technology has led to
increase in demand for the professionals
who can understand and use the what
amount of data every day companies like
Facebook Google Tesla and many more are
constantly hiring machine learning
professionals with salary as going
hazard
so that's machine learning wrap update
tools humans learn from their past
experiences
and machines follow instructions given
by humans
but what if humans can train the
machines to learn from their past data
and to what humans can do act much
faster well that's called machine
learning but it's a lot more than just
learning it's also about understanding
and reasoning so today we will learn
about the basics of machine learning
so that's Paul he loves listening to new
songs
he either likes them or dislikes them
Paul decides this on the basis of the
song's Tempo genre intensity and the
gender of voice for Simplicity let's
just use Tempo and intensity for now so
here Tempo is on the x-axis ranging from
relaxed to fast whereas intensity is on
the y-axis ranging from light to Soaring
we see that Paul likes the song with
fast tempo and soaring intensity while
he dislikes the song with relaxed Tempo
and light intensity so now we know
Paul's choices let's say Paul listens to
a new song Let's name it as song a song
a has fast tempo and a soaring intensity
so it lies somewhere here looking at the
data can you guess whether Paul will
like the song or not correct so Paul
likes this song by looking at Paul's
past choices we were able to classify
the unknown song very easily right let's
say now Paul is citizens to a new song
Let's label it as song b so song b lies
somewhere here with medium Tempo and
medium intensity neither relaxed nor
fast neither light nor soaring now can
you guess where the ball likes it or not
not able to guess where this ball will
like it or dislike it other choices
unclear correct we could easily classify
song A but when the choice became
complicated as in the case of song b yes
and that's where machine learning comes
in let's see how in the same example for
song p if you draw a circle around the
song b we see that there are four words
for like whereas one vote for dislike if
we go for the majority votes we can say
that Paul will definitely like the song
that's all this was a basic machine
learning algorithm also it's called K
nearest neighbors so this is just a
small example in one of the many machine
learning algorithms quite easy right
believe me it is but what happens when
the choices become complicated as in the
case of song b that's when machine
learning comes in it learns the data
builds the prediction model and when the
new data point comes in it can easily
project for it more the data bet the
model higher will be the accuracy there
are many ways in which the machine
learns it could be either supervised
learning unsupervised learning or
reinforcement learning let's first
quickly understand supervised learning
suppose your friend gives you 1 million
coins of three different currencies say
one rupee one Euro and one dirham each
coin has different weights for example a
coin of one rupee weighs 3 grams one
Euro weighs 7 grams and one dirham
weighs 4 grams your model will predict
the currency of the coin here your
weight becomes the feature of coins
while currency becomes the label when
you feed this data to the machine
learning model it learns which feature
is associated with which sleep for
example it will learn that if a point is
of 3 grams it will be a one rupee coin
let's give a new coin to the machine on
the basis of the weight of the new coin
your model will predict the currency
hence supervised learning uses labeled
data to train the model here the machine
knew the features of the object and also
the labels associated with those
features on this note let's move to
unsupervised learning and see the
difference suppose you have Cricut data
set of various players with their
respective scores and wickets taken when
you feed this data set to the machine
the machine identifies the pattern of
player performance so it plots this data
with the respective wickets on the
x-axis while it runs on the y-axis while
looking at the data you will clearly see
that there are two clusters the one
cluster are the players who scored High
runs and took less wickets while the
other cluster is of the players who
scored less chance but took many wickets
so here we interpret these two clusters
as batsman and Bowlers the important
point to note here is that there were no
labels of batsman and Bowlers hence the
learning with unlabeled data is
unsupervised learning so we saw
supervised learning where the data was
labeled and the unsupervised learning
where the data was unabled and then
there is reinforcement learning which is
reward-based learning or we can say that
it works on the principle of feedback
here let's say you provide the system
with an image of a dog and ask it to
identify it the system identifies it as
a cat so you give a negative feedback to
the machine saying that it's a dog's
image the machine will learn from the
feedback and finally if it comes across
any other image of a dog it will be able
to classify it correctly that is
reinforcement learning to generalize
machine learning model let's see a
flowchart input is given to a machine
learning model which then gives the
output according to the algorithm
applied if it's right we take the output
as a final result else we provide
feedback to the training model and ask
it to predict until it learns I hope
you've understood supervised and
unsupervised learning so let's have a
quick quiz you have to determine whether
the given scenarios uses supervised or
unsupervised learning simple right
scenario 1 Facebook recognizes your
friend in a picture from an album of
tagged photographs
scenario 2 Netflix recommends new movies
based on someone's Past movie choices
scenario 3 analyzing Bank data for
suspicious transactions and flagging the
fraud transactions think wisely and
comment below your answers moving on
don't you sometimes wonder how is
machine learning possible in today's era
well that's because today we have
humongous data available everybody is
online either making a transaction or
just surfing the internet and that's
generating a huge amount of data every
minute and that data my friend is the
key to analysis also the memory handling
capabilities of computers have lastly
increased which helps them to process
such huge amount of data at hand without
any delay and yes computers now have
great computational Powers so there are
a lot of applications of machine
learning out there to name a few machine
learning is used in healthcare where
Diagnostics are predicted for doctor's
review the sentiment analysis that the
tech Giants are doing on social media is
another interesting application of
machine learning fraud detection in the
finance sector and also to predict
customer shown in the e-commerce sector
while booking a cab you must have
encountered search pricing often where
it says the fare of your trip has been
updated continue booking yes please I'm
getting late for office well that's an
interesting machine learning model which
is used by Global Taxi giant Uber and
others where they have differential
pricing in real time based on demand the
number of cars available bad weather
Rush R Etc so they use the search
pricing model to ensure that those who
need a cab can get one also it uses
predictive modeling to predict where the
demand will be high with a goal that
drivers can take care of the demand and
search pricing can be minimized great
hey Siri can you remind me to book a cab
at 6 pm today okay I'll remind you
thanks no problem here we have our um
today let me tell you what is machine
learning machine learning works on the
development of computer programs that
can access data and use it to
automatically learn and improve from
experience watch a robot builder
construct house in two days this was
back in July 29th
2016. so that's pretty impressive this
amount of time to continue to grow in
its development and it's smart enough to
leave spaces in the brickwork for wiring
and plumbing and can even cut and shape
bricks to size
Amazon Echo relies on machine learning
and with more data it becomes more
accurate play your favorite music order
pizza from Domino's voice control your
home request rides from Uber have you
ever wondered the difference between AI
machine learning and deep learning
artificial intelligence a technique
which enables machines to mimic human
behavior the SE really important because
this is how we are able to gauge how
well our computations or what we're
working on works is the fact that we're
mimicking human behavior we're using
this to replace human work and make it
more efficient and make it more
streamlined and more accurate and so the
center of artificial intelligence is the
big picture of all this put together IBM
deep blue chess electronic game
characters those are just a couple
examples of artificial intelligence
machine learning a technique which uses
statistical methods enabling machines to
learn from their past data so this means
if you have your input from last time
and you have your answer you use that to
help prove the next guess it makes for
the correct answer IBM Watson Google
search algorithm email spam filters
these are all part of machine learning
and then deep learning which is a subset
of machine learning composing algorithms
that allow a model to trade itself and
perform tasks alphago natural speech
recognition these are a couple examples
deep learning is associated with tools
like neural networks where it's kind of
a black box as it learns it changes all
these things that are as a human we
would have a very hard time tracking and
it's able to come up with an answer from
that now let's see how machine learning
works first we start with training the
data once we've trained the data the
train we go into the machine learning
algorithm which then puts the data into
a processing which then goes down to
machine another machine learning
algorithm and then we take new data
because you have to test whatever you
did to make sure it works correctly and
we put that into the same algorithm once
we do that we check our prediction we
check our results and from the
prediction if we've set aside some
training date and we find out it didn't
do a good job predicting it and it gets
a thumbs down as you see then we go back
to the beginning and we retrain the
algorithm and a lot of times it's not
just about getting the wrong answer it's
about continually trying to get a better
answer so you'll see the first time you
might be like oh this is not the answer
I want depending on what domain you're
working in whether it medical economical
business stocks whatever you try out
your model and if it's not giving you a
good answer you retrain it if you think
you can get a better answer you retrain
it and you keep doing that until you get
the best answer you can if you are an
aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Purdue University in collaboration with
IBM should be a right choice for more
details use the link in the description
box below with that in mind
human versus artificial intelligence
humans are amazing let's just face it
we're amazing creatures we're all over
the planet we're exploring every Nick
Chinook we've gone to the Moon uh we've
gone into outer space we're just amazing
creatures we're able to use the
available information to make decisions
to communicate with other people
identify patterns and data remember what
people have said adapt to new situations
so let's take a look at this so you can
get a picture you're a human being so
you know what it's like to be human
let's take a look at artificial
intelligence versus the human artificial
intelligence develops computer systems
that can accomplish texts that require
human intelligence
so we're looking at this one of the
things that computers can do is they can
provide more accurate results this is
very important recently I did a project
on cancer whereas identifying markers
and as a human being you look at that
and you might be looking at all the
different images and the data that comes
off of them and say I like this person
so I want to give them a very good
outlook and the next person you might
not like so you want to give them a bad
Outlook well with artificial
intelligence you're going to get a
consistent prediction of what's going to
come out interacts with humans using
their natural language we've seen that
as probably the biggest development
feature right now that's in the
commercial Market that everybody gets to
use as we saw with the example of Alexa
they learn from their mistakes and adapt
to new environments so we see this
slowly coming in more and more and they
learn from the data and automate
repetitive learning repetitive learning
has a lot to do with the neural networks
you have to program thousands upon
thousands of pictures in there and it's
all automated so as today's computers
evolve it's very quick and easy and
affordable to do this
what is machine learning and deep
learning all about
imagine this say you had some time to
waste not that any of us really have a
lot of time anymore to just waste in
today's world
and you're sitting by the road and you
have a whole lot and a whole lot of time
passes by there's a few hours
and suddenly you wonder
how many cars buses trucks and so on
passed by in the six hours now chances
are you're not going to sit by the road
for six hours and count buses cars and
trucks unless you're working for the
city and you're trying to do City
Planning and you want to know hey do we
need to add a new truck route maybe we
need a bicycle length we have a lot of
bicyclists here that kind of thing so
maybe City Planning would be great for
this
machine learning well the way machine
Learning Works is we have labeled data
with features okay so you have a truck
or a car a motorcycle a bus or a bicycle
and each one of those are labeled it
comes in and based on those labels and
comparing those features it gives you an
answer it's a bicycle it's a truck it's
a motorcycle this is a little bit more
in depth on this
and the model here it actually the
features we're looking at would be like
the tires someone sits there and figures
out what a tire looks like it takes a
lot of work if you try to try to figure
a difference between a car tire a
bicycle tire a motorcycle tire uh so in
the machine learning field this could
take a long time if you're going to do
each individual aspect of a car and try
to get a result on there and that's what
they did do that was a very this is
still used on smaller amounts of data we
figure out what those features are and
then you label them
deep learning so with deep learning one
of our Solutions is to take a very large
unlabeled data set
and we put that into a training model
using artificial neural networks and
then that goes into the neural network
itself when we create a neural network
and you'll see the arrows are actually
kind of backward but uh which actually
is a nice point because when we train
the neural network
we put the bicycle in and then it comes
back and says if it's a truck it comes
back and says well you need to change
that to bicycle and then it changes all
those weights going backward they call
it back propagation and let it know it's
a bicycle and that's how it learns
once you've trained the neural network
you then put the new data in and they
call this testing the model so you need
to have some data you've kept off to the
side where you know the answer to and
you take that and you provide the
required output and you say okay is this
is this neural network working correctly
did it identify a bike as a bike a truck
is a truck a motorcycle as a motorcycle
let's just take a little closer look at
that
determining what objects are present in
the data so how does deep learning do
this and here we have the image of the
bike it's 28 by 28 pixels that's a lot
of information there could you imagine
trying to guess that this is a bicycle
image by looking at each one of those
pixels and trying to figure out what's
around it and we actually do that as
human beings it's pretty amazing we know
what a bicycle is and even though it
comes in as all this information and
what this looks like is the image comes
in
it converts it into a bunch of different
nodes in this case there's a lot more
than what they show here and it goes
through these different layers and
outcomes and says okay this is a bicycle
a lot of times they call this the magic
Black Box why because as we watch it go
across here all these weights and all
the math behind this and it's not it's a
little complicated on the math side you
really don't need to know that when
you're programming or doing working with
the Deep learning but it's like magic
you don't know you really can't figure
out what's going to come out by looking
what's in each one of those dots and
each one of those lines are firing and
what's going in between them so we like
to call it the magic box so that's where
deep learning comes in
and in the end it comes up and you have
this whole neural network it comes up
and it says okay we fire all these
different pixels and we connect all
these different dots and gives them
different weights and it says okay this
is a bicycle and that's how we determine
what the object is present in the data
with deep learning
machine learning we're going to take a
step into machine learning here and
you'll see how these fit together in a
minute the system is able to make
predictions or take decisions based on
past data that's very important for
machine learning is that we're looking
at stuff and based on what's been there
before we're creating a decision on
there we're creating something out of
there we're coloring a beach ball we're
telling you what the weather is in
Chicago
what's nice about machine learning is a
very powerful processing capability it's
quick and accurate outcomes so you get
results right away once you program the
system the results are very fast
and the decisions and predictions are
better they're more accurate they're
consistent you can analyze very large
amounts of data some of these data
things that they're analyzing now are
petabytes and terabytes of data it would
take hundreds of people hundreds of
years to go through some of this data
and do the same thing that the machine
learning can do in a very short period
of time and it's inexpensive compared to
hiring hundreds of people so because a
very affordable way to move into the
future is to apply the machine learning
to whatever businesses you're working on
and deep Learning Systems think and
learn like humans using artificial
neural networks again it's like a magic
box performance improves with more data
so the more data the Deep learning gets
the more it gives you better results
it's scalability so you can scale it up
you can scale it down you can increase
what you're looking at currently you
know we're limited by the amount of
computer processing power as to how big
that can get but that envelope
continually gets pushed every day on
what it can do
problem solved in an end-to-end method
so instead of having to break it apart
and you have the first piece coming in
and you identify tires and the second
piece is identifying labeling handlebars
and then you bring that together that if
it has handlebars and tires it's a
bicycle and if it has something that
looks like a large Square it's probably
a truck the neural networks does this
all in one network you don't really know
what's going on in all those weights and
all those little bubbles but it does it
pretty much in one package that's why
the neural network systems are so big
nowadays and coming into their own
best features are selected by the system
and it this is important they kind of
put it's on a bullet on the side here
it's a subset of machine learning this
is important we talk about deep learning
it is a form of machine learning there's
lots of other forms of machine learning
data analysis but this is the newest and
biggest thing that they apply to a lot
of different packages and they use all
the other machine learning tools
available to work with it and it's very
fast to test you put in your information
you then have your group of tests and
then you held some aside you see how
does it do it's very quick to test it
and see what's going on with your deep
learning and your neural network
are they really all that different
AI versus machine learning versus deep
learning concepts of AI
so we have concepts of II you'll see
natural language processing machine
learning and approach to create
artificial intelligence so it's one of
the subsets of artificial intelligence
knowledge representation automated
reasoning computer vision robotics
machine learning versus AI versus deep
learning or Ai and machine learning and
deep learning
so we look at this we have ai with
machine learning and deep learning and
so we're going to put them all together
we find out that AI is a big picture we
have a collection of books it goes
through some deep learning the Digital
Data is analyzed text mining comes
through the particular book you're
looking for maybe it's a genre books is
identified and in this case we have a
robot that goes and gives a book to the
patron I have yet to be at a library
that has a robot bring me a book but
that will be cool when it happens so
we'll look at some of the pieces here
this information goes into uh there's as
far as this example the translation of
the handwritten printed data to digital
form
that's pretty hard to do that's pretty
hard to go in there and translate
hundreds and hundreds of books and
understand what they're trying to say if
you've never read them so in this case
we used a deep learning because you can
already use examples where they've
already classified a lot of books and
then they can compare those texts and
say oh okay this is a book on automotive
repair this is a book on robotic
building the Digital Data is in analyzed
then we have more text mining using
machine learning so maybe we'd use a
different program to do a basic classify
what you're looking for and say oh
you're looking for auto repair and
computers so you're looking for
automated cars once it's identified then
of course it brings you the book
so here's a nice summation of what we
were just talking about AI with machine
learning and deep learning deep learning
is a subset of machine learning which is
a subset of artificial intelligence
so you can look at artificial
intelligence as the big picture how does
this compare to The Human Experience in
either doing the same thing as a human
we do or it does it better than us and
machine learning which has a lot of
tools is something that learns from data
past experiences it's programmed it
comes in there and it says hey we
already had these five things happen the
sixth one should be about the same and
then there's a lot of tools in machine
learning but deep learning then is a
very specific tool in machine learning
it's the artificial neural network which
handles large amounts of data and is
able to take huge pools of experiences
pictures and ideas and bring them
together
real life examples
artificial intelligence news generation
very common nowadays as it goes through
there and finds the news articles or
generates the news based upon the news
feeds or the back end coming in it says
okay let's give you the actual news
based on this there's all the different
things Amazon Echo they have a number of
different Prime music on there of course
there's also the Google command and
there's also Cortana there's tons of
smart home devices now where we can ask
it to turn the TV on or play music for
us that's all artificial intelligence
from front to back you're having a human
experience with these computers and
these objects that are connected to the
processing machine learning spam
detection very common machine learning
doesn't really have the human
interaction part
so this is the part where it goes and
says okay that's a Spam that's not a
Spam and it puts it in your spam folder
search engine result refining another
example of machine learning whereas it
looks at your different results and it
Go and it is able to categorize them as
far as this had the most hits this is
the least viewed this has five stars you
know however they want to weight it all
exam good examples of machine learning
and then the Deep learning deep learning
another example is as you have like a
exit sign in this case is translating it
into French sortie I hope I said that
right
neural network has been programmed with
all these different words and images and
so it's able to look at the exit in the
middle and it goes okay we want to know
what that is in French and it's able to
push that out in France French and learn
how to do that
and then we have chat Bots I remember
when Microsoft first had their little
paper clip
um boy that was like a long time ago
they came up and you would type in there
and chat with it these are growing you
know it's nice to just be able to ask a
question and it comes up and gives you
the answer and instead of it being were
you just doing a search on certain words
it's now able to start linking those
words together and form a sentence in
that chat box
types of AI and machine learning
types of artificial intelligence this in
the next few slides are really important
so one of the types of artificial
intelligence is reactive machines
systems that only react they don't form
memories they don't have past
experiences they have something that
happens to them and they react to it my
washing machine is one of those if I put
a ton of clothes in it and they had all
clumped on one side it automatically
adds a weight to re-sitter it so that my
washing machine is actually a reactive
machine working with whatever the load
is and keeps it nice and so when it
spins it doesn't go thumping against the
side limited memory another form of
artificial intelligence systems look
into the past information is added over
a period of time and information is
short-lived when we're talking about
this and you look at like a neural
network that's been programmed to
identify cars it doesn't remember all
those pictures it has no memory as far
as hundreds of pictures you process
through it all it has is this is the
pattern I use to identify cars as the
final output for that neural network we
looked at
so when they talk about limited memory
this is what they're talking about
they're talking about I've created this
based on all these things but I'm not
going to remember anyone specifically
theory of Mind systems being able to
understand human emotions and how they
affect decision making to adjust their
behaviors according to their human
understanding
this is important because this is our
page mark this is how we know whether it
is an artificial intelligence or not is
it interacting with humans in a way that
we can understand
without that interaction is just an
object so we talk about theory of mind
we really understand how it interfaces
that whole if you're in web development
user experience would be the term I
would put in there so the theory of mine
would be user experience how's the whole
UI connected together and one of the
final things is as we get into
artificial intelligence is systems being
aware of themselves understanding their
internal States and predicting other
people's feelings and act appropriately
so as artificial intelligence continues
to progress we see ones they're trying
to understand well what makes people
happy how would they increase our
happiness how would they keep themselves
from breaking down if something's broken
inside they have that self-awareness to
be able to fix it and just based on all
that information predicting which action
would work the best what would help
people if I know that you're having a
cup of coffee first thing in the morning
is what makes you happy as a robot I
might make you a cup of coffee every
morning at the same time to help your
life and help you grow that'd be the
self-awareness is being able to know all
those different things
types of machine learning and like I
said on the last slide this is very
important this is very important if you
decide to go in and get certified in
machine learning or know more about it
these are the three primary types of
machine learning the first one is
supervised learning systems are able to
predict future outcome based on past
data requires both an input and an
output to be given to the model for it
to be trained
so in this case we're looking at
anything where you have 100 images of a
bicycle
and those hundred images you know are
bicycle so it's their preset someone
already looked at all hundred images and
said these are pictures of bicycles and
so the computer learns from those and
then it's given another picture
and maybe the next picture is a bicycle
and it says oh that resembles all these
other bicycles so it's a bicycle and the
next one's a car and it says that's not
a bicycle that would be supervised
learning because we had to train it we
had to supervise it unsupervised
learning systems are able to identify
hidden patterns from the input data
provided by making the data more
readable and organized the patterns
similarities or anomalies become more
evident you'll heard the term cluster
how do you cluster things together some
of these things go together some of
these don't this is unsupervised where
it can look at an image and start
pulling the different pieces of the
image out because they aren't the same
the human all the parts of the human are
not the same as a fuzzy tree behind them
so slightly out of focus which is not
the same as the beach ball it's
unsupervised because we never told it
what a beach ball was we never told it
what the human was and we never told it
that those were trees all we told it was
hey separate this picture by things that
don't match and things that do match and
come to together
and finally there's reinforcement
learning systems are given no training
it learns on the basis of the reward
punishment it received for performing
its Last Action it helps increase the
efficiency of a tool function or a
program reinforced learning a
reinforcement learning is kind of you
give it a yes or no yes you gave me the
right response no you didn't and then it
looks at that and says oh okay so based
on this data coming in what I gave you
was a wrong response so next time I'll
give you a different one
comparing machine learning and deep
learning so remember that deep learning
is a subcategory of machine learning so
it's one of the many tools and so they
were grouping a ton of machine learning
tools all together linear regression K
means clustering there's all kinds of
cool tools out there you can use in
machine learning enables machines to
take decisions to make decisions on
their own based on past data enables
machines to make decisions with the help
of artificial neural networks so it's
doing the same thing but we're using an
artificial neural network as opposed to
one of the more traditional machine
learning tools needs only a small amount
of training data this is very important
when you're talking about machine
learning they're usually not talking
about huge amounts of data we're talking
about maybe your spreadsheet from your
business and your totals for the end of
the year when you're talking about
neural networks you usually need a large
amount of data to train the data so
there's a lot of training involved if
you have under 500 points of data that's
probably not going to go into machine
learning or maybe you have like the case
of one of the things 500 points of data
and 30 different fields it starts
getting really confusing there and
artificial intelligence or machine
learning and the Deep learning aspect
really shines when you get to that
larger data that's really complex
works well on a low end systems so a lot
of the machine learning tools out there
you can run on your laptop with no
problem and do the calculations there
where with the machine learning usually
needs a higher end system to work it
takes a lot more processing power to
build those neural networks and to train
them it goes through a lot of data we're
talking about the general machine
learning tools most features need to be
identified in advanced and manually
coded so there's a lot of human work on
here the machine learns the features
from the data it is provided so again
it's like a magic box you don't have to
know what a tire is it figures it out
for you
the problem is divided into parts and
solved individually and then combined so
machine learning you usually have all
these different tools and use different
tools for different parts
and the problem is solved in an
end-to-end manner so you only have one
neural network or two neural networks
that is bringing the data in and putting
it out it's not going through a lot of
different processes to get there and
remember you can put machine learning
and deep learning together so you don't
always have just the Deep learning
solving the problem you might have a
solving one piece of the puzzle
with regular machine learning emotional
machine learning tools out there they
take longer to test and understand how
they work
and with the Deep learning is pretty
quick once you build that neural network
you test it and you know
so we're dealing with very crisp rules
limited resources you have to really
explain how the decision was made when
you use most machine learning tools but
when you use the Deep learning tool
inside the machine learning tools the
system takes care of it based on its own
logic and reasoning and again it's like
a magic Black Box you really don't know
how it came up with the answer you just
know it came up with the right answer a
glimpse into the future so a quick
glimpse into the future artificial
intelligence be using it to detecting
crimes before they happen humanoid AI
helpers which we already have a lot of
there'll be more and more maybe it'll
actually be Androids that'd be cool to
have an Android that comes and get stuff
out of my fridge for me machine learning
increasing efficiency in healthcare
that's really big in all the forms of
machine learning better marketing
techniques any of these things if we get
into the Sciences it's just off the
scale machine learning and artificial
intelligence go everywhere and then the
subcategory Deep learning increased
personalization in so what's really nice
about the Deep learning is it's going to
start now catering to you that'll be one
of the things we see more and more of
and we'll have more of a hyper
intelligent personal assistant I'm
excited about that if you are an
aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Purdue University in collaboration with
IBM should be a right choice for more
details use the link in the description
box below with that in mind
now let's look into the types of machine
learning
machine learning is primarily of three
types first one is supervised machine
learning as the name suggests you have
to supervise your machine learning while
you train it to work on its own it
requires labeled training data next up
is unsupervised learning wherein there
will be training data but it won't be
labeled
finally there is reinforcement learning
wherein the system learns on its own
let's talk about all these types in
detail let's try to understand how
supervised Learning Works look at the
pictures very very carefully the monitor
depicts the model or the system that we
are going to train
this is how the training is done we
provide a data set that contains
pictures of a kind of a fruit say an
apple
then we provide another data set which
lets the model know that these pictures
where that of a fruit called Apple
this ends the training phase now what we
will do is we provide a new set of data
which only contains pictures of Apple
now here comes the fun part the system
can actually tell you what fruit it is
and it will remember this and apply this
knowledge in future as well
that's how supervised Learning Works you
are training the model to do a certain
kind of an operation on its own
this kind of a model is generally used
into filtering spam maze from your email
accounts as well
yes surprise aren't you
so let's move on to unsupervised
learning now let's say we have a data
set which is cluttered in this case we
have a collection of pictures of
different fruits we feed this data to
the model and the model analyzes the
data to figure out patterns in it in the
end it categorizes the photos into three
types as you can see in the image based
on their similarities
so you provide the data to the system
and let the system do the rest of the
work simple isn't it this kind of a
model is used by Flipkart to figure out
the products that are well suited for
you honestly speaking this is my
favorite type of machine learning out of
all the three and this type has been
widely shown in most of the Sci-Fi
movies lately let's find out how it
works imagine a newborn baby
you put a burning candle in front of the
baby the baby does not know that if it
touches the flame its fingers might get
burnt
so it does that anyway and gets hurt the
next time you put that candle in front
of the baby it will remember what
happened the last time and would not
repeat what it did that's exactly how
reinforcement learning works
we provide the machine with a data set
wherein we ask it to identify a
particular kind of a fruit in this case
an apple
so what it does as a response it tells
us that it's a mango but as we all know
it's a completely wrong answer so as a
feedback we tell the system that it's
wrong it's not a mango it's an apple
what it does it learns from the feedback
and keeps that in mind
when the next time when we ask a same
question it gives us the right answer it
is able to tell us that it's actually an
apple that is a reinforced response so
that's how reinforcement learning works
it learns from his mistakes and
experiences this model is used in games
like Prince of Persia or Assassin's
Creed or FIFA wherein the level of
difficulty increases as you get better
with the games
just to make it more clear for you let's
look at a comparison between supervised
and unsupervised learning firstly the
data involved in case of supervised
learning is labeled as we mentioned in
the examples previously
we provide the system with a photo of an
apple and let the system know that this
is actually an apple
that is called label data so the system
learns from the label data and makes
future predictions
now unsupervised learning does not
require any kind of label data because
its work is to look for patterns in the
input data and organize it
the next point is that you get a
feedback in case of supervised learning
that is once you get the output the
system tends to remember that and uses
it for the next operation
that does not happen for unsupervised
learning and the last point is that
supervised learning is mostly used to
predict data whereas unsupervised
learning is used to find out hidden
patterns or structures in data
I think this would have made a lot of
things clear for you regarding
supervised and unsupervised learning hey
there learner simply learn brings you a
comprehensive professional certificate
program in Ai and machine learning from
Purdue University that is in
collaboration with IBM that will cover a
wide range of topics that will Empower
you with the knowledge and skills needed
to excel in the field of AI to learn
more about this course you can find the
course link mentioned in the description
box their NS types data denotes the
individual pieces of factual information
collected from various sources it is
stored process and later used for
analysis
and so we see here just a huge grouping
of information a lot of tech stuff money
dollar signs numbers uh and then you
have your performing analytics to drive
insights and hopefully you have a nice
share your shareholders gather it at the
meeting and you're able to explain it in
something they can understand
so we talk about data types of data we
have in our types of data we have a
qualitative categorical
and you think nominal ordinal then you
have your quantitative or numerical
which is discrete or continuous
and let's look a little closer at those
data type vocabulary always people's
favorite is the vocabulary words okay
not mine
but let's dive into this what we mean by
nominal nominal they are used to label
various label our variables without
providing any measurable value
uh country gender race hair color Etc
it's something that you either mark true
or false this is a label it's on or off
either they have a red hat on or they do
not uh so a lot of times when you're
thinking nominal data labels uh think of
it as a true false kind of setup and we
look at ordinal this is categorical data
with a set order or a scale to it and
you can think of salary range as a great
one movie ratings Etc you see here the
celery rains if you have ten thousand to
twenty thousand number of employees
earning that rate is 150 20 000 to 30
000 100 and so forth some of the terms
you'll hear is bucket uh this is where
you have ten different buckets and you
want to separate it into something that
makes sense into those 10 buckets and so
when we start talking about ordinal a
lot of times when you get down to the
breastbones again we're talking true
false so if you're a member of the 10 to
20K Reigns so forth those would each be
either part of that group or you're not
but now we're talking about buckets and
we want to count how many people are in
that bucket
quantitative numerical data falls into
two classes discrete or continuous and
so data with a final set of values which
can be categorized class strength
questions answered correctly and runs
hit and Cricut a lot of times when you
see this you can think integer and a
very restricted integer I.E you can only
have 100 questions on a test so you can
it's very discreet I only have a hundred
different values that it can attain so
think usually you're talking about
integers but within a very small range
they don't have an open end or anything
like that
uh so discrete is very solid simple to
count set number continuous on the other
hand continuous data can take any
numerical value within a range so water
pressure weight of a person Etc usually
we start thinking about float values
where they can get phenomenally small in
their in what they're worth and there's
a whole series of values that falls
right between discrete and continuous
you can think of the stock market you
have dollar amounts it's still discrete
but it starts to get complicated enough
when you have like you know jump in the
stock market from
525.33 to
580.67 cents there's a lot of Point
values in there it'd still be called
discrete but you start looking at it as
almost continuous because it does have
such a variance in it now we talk about
no we did we went over nominal and
ordinal almost true false charts and we
looked at quantitative and numerical
data data which we'll start to get into
numbers discrete you can usually a lot
of times discrete will be put into it
could be put into true false but usually
it's not so we want to address this
stuff and the first thing we want to
look at is the very basic which is your
algebra so we're going to take a look at
linear algebra you can remember back
when your euclidean geometry we have a
line well let's go through this we have
a linear algebra as the domain of
mathematics concerning linear equations
and their representations in Vector
spaces and through matrixes I told you
we're going to talk about Matrix is
so a linear equation is simply
2x plus 4y minus 3z equals 10. very
linear 10x plus 12.4 y equals z and now
you can actually solve these two
equations by combining them and this
we're talking about a linear equation
and the vectors we have a plus b equals
c now we're starting to look at a
direction and these values usually think
of an x y z plot so each one is a
direction and the actual distance of
like a triangle a b is C and then your
Matrix can describe all kinds of things
I find matrixes confuse a lot of people
not because they're particularly
difficult but because of the magnitude
and the different things they're used
for
and a matrix is a chart or a you know
think of a spreadsheet but you have your
rows and your columns and you'll see
here we have a times b equals c very
important to know your counts uh so
depending on how the math is being done
what you're using it for making sure you
have the same rows and number of columns
or a single number there's all kinds of
things that play in that that can make
matrixes confusing but really it has a
lot more to do with what domain you're
working in are you adding in multiple
polynomials where you have like uh a x
squared plus b y plus you know you start
to see that can be very confusing versus
a very straightforward Matrix and let's
just go a little deeper into these
because these are such primary this is
what we're here to talk about is these
different math mathematical computations
that come up
so we're looking at linear equations
let's dig deeper into that one an
equation having a maximum order of one
is called a linear equation
so it's linear because when you look at
this we have ax plus b equals c which is
a one variable
we have two variable ax plus b y equals
c a X plus b y plus z c z equals D and
so forth but all of these are to the
power of one you don't see x squared you
don't see X cubed so we're talking about
linear equations that's what we're
talking about in their addition if you
have already dived into say neural
networks you should recognize this ax
plus b y plus c z
setup plus The Intercept which is
basically your your neural network each
node adding up all the different inputs
and we can drill down into that most
common formula is your y equals MX plus
c
so you have your y equals the M which is
your slope your X Value Plus C which is
your y-intercept they kind of labeled it
wrong here
threw me for a loop but the the C would
be your y-intercept so when you set x
equal to 0 y equals c and that's that's
your y-intercept right there uh and
that's they they just had a reversed
value of y when x equals zero equals the
y-intercept which is C and your slope
gradient line which is your M so these
are y equals two X plus three and
there's lots of easy ways to compute
this this way this is why we always
start with the most basic one when we're
solving one of these problems and then
of course the uh one of the most
important takeaways is the slope
gradient of the line so the slope is
very important that M value in this case
we went ahead and solved this
if you have y equals two X plus three
you can see how it has a nice line graph
here on the right
so matrixes a matrix refers to a
rectangular representation of an array
of numbers arranged in columns and rows
so we're talking M rows by n columns
here a11 is denotes the element of the
first row in the First Column similarly
a 12 and it's really pronounced a11 in
this particular setup so it's a row one
column one a 12 is a of Row one column
two first row and second column and so
on
and there's a lot of ways to denote this
I've seen these as like a capital letter
a smaller case a for the top row or I
mean you can see where they can go all
kinds of different directions as far as
the value
you just take a moment to realize there
needs to be some designation as far as
what row it's in and what column it's in
and we have our basic operations we have
addition so when you think about
addition you have uh two matrixes of two
by two and you just add each individual
number in that Matrix and then when you
get to the bottom you have in this case
the solution is 12 10 plus 2 is 12 5
plus 3 is 8 and so on and the same thing
with subtraction
now again your counting Matrix is you
want to check your dimensions of the
Matrix the shape you'll see shape come
up a lot in programming so we're talking
about Dimensions we're talking about the
shape if the two shapes are equal this
is what happens when you add them
together or subtract them
and we have multiplication when you look
at the multiplication you end up at the
very a slightly different setup going
now if we look at our last one where
we're like why this always gets to me
when we get to matrixes they don't
really say why you multiply matrixes
um you know my first thought is one
times two four times three but if you
look at this we get one times two plus
four times three one times three plus
four times five
uh six times two plus three times three
six times three plus three times five if
you're looking at these matrixes uh
think of this more as an equation and so
we have if you remember when we back up
here for our multiple line equations
let's just go back up a couple slides
where we were looking at uh two
variables so this is a two variable
equation ax plus b y equals c
and this is a way to make it very quick
to solve these variables and that's why
you have the Matrix and that's why you
do
the multiplication the way they do and
this is the dot product of one times two
plus four times three
one times three plus four times five
six times two plus three times three
six times three plus three times five
and it gives us a nice little uh 14 23
21 and 33 over here which then can be
used and reduced down to a sample
formula as far as solving the variables
as you have enough inputs uh and then in
Matrix operations when you're dealing
with a lot of matrixes now keep in mind
multiplying matrixes is different than
finding the product of two matrixes okay
so we're talking about multiplication
we're talking about solving for
equations when you're finding the
product you are just finding one times
two keep that in mind because that does
come up I've had that come up a number
of times where I'm altering data and I
get confused as to what I'm doing with
it uh transpose flipping the Matrix over
its diagonal comes up all the time where
you have you still have 12 but instead
of it being a 12 8 it's now 12 14 8 21.
you're just flipping the columns in the
rows and then of course you can do an
inverse changing the signs of the values
of across this main diagonal and you can
see here we have the inverse a to the
minus 1 and ends up with uh instead of
12 8 14 12 is now minus 22 minus 12.
vectors Vector just means we have
a value and a Direction
and we have down four numbers here on
our vector
in mathematics a one-dimensional matrix
is called a vector so if you have your X
plot and you have a single value that
value is along the x axis and it's a
single Dimension if you have two
Dimensions you can think about putting
them on a graph you might have X and you
might have y and each value denotes a
direction and then of course the actual
distance is going to be the hypothesis
so that triangle and you can do that
with three dimensionals X Y and Z and
you can do it all the way to nth
Dimensions so when they talk about the K
means for categorizing and how close
data is together they will compute that
based on the Pythagorean theorem so you
would take the square of each value add
them all together and find the square
root and that gives you a distance as
far as where that point is where that
Vector exists or an actual point value
and then you can compare that point
value to another one and it makes a very
easy comparison versus comparing 50 or
60 different numbers and that brings us
up to I Gene vectors and I Gene values
uh hygiene vectors the vectors that
don't change their span while
transformation
and hygiene values the scalar values
that are associated to the vectors
conceptually you can think of the vector
as your picture you have a picture it's
two Dimensions X and Y
and so when you do those two dimensions
and those two values or whatever that
value is
um that is that point but the values
change when you skew it and so if we
take and we have a vector a
and that's a set value uh B is your is
your you have a and b which is your I
Gene Vector 2 is the I Gene value so
we're altering all the values by two
that means we're maybe we're stretching
it out One Direction making it tall if
you're doing picture editing
um that's one of the places this comes
in but you can see when you're
transforming uh your different
information how you transform it is then
your hygiene value and you can see here
a vector after line Transit transition
we have 3A a a is the I Gene Vector
three is the iogene value so a doesn't
change that's whatever we started with
that's your original picture and three
is skewing in One Direction and maybe uh
B is being skewed another Direction and
so you have a nice tilted picture
because you've altered it by those by
the hygiene values
so let's go ahead and pull up a demo on
linear algebra and to do this I'm going
to go through my trusted Anaconda into
my Jupiter notebook
and we'll create a new uh notebook
called linear algebra
since we are working in Python we're
going to use our numpy I always import
that as NP or numpy array probably the
most popular
module for doing matrixes and things in
given that this is part of a series I'm
not going to go too much into numpy we
are going to go ahead and create two
different variables a for a numpy array
10 15 and b 29
. we'll go ahead and run this and you
can see there's our two arrays 10 15 29
and I went and added a space there in
between
so it's easier to read
and since it's the last line we don't
have to put the print statement on it
unless you want we can simply but we can
simply do a plus b so when I run this we
have 10 15 29 and we get 30 24 which is
what you expect 10 plus 20 15 plus 9 you
could almost look at this addition as
being
um
just adding up the columns on here
coming down and if we wanted to do it a
different way we could also do a DOT t
plus b dot T remember that t flips them
and so if we do that we now get them we
now have 30 24 going the other way we
could also do something kind of fun
there's a lot of different ways to do
this
uh as far as a plus b I can also do a
plus b dot t
and you're going to see that that will
come out the same the 3024 whether I
transpose A and B are transpose them
both at the end
and likewise we can very easily subtract
two vectors I can go a minus B
and we run that and we get minus 10 6.
now remember this is the last line in
this particular section and so I don't
have to put the print around it
and just like we did before
we can transpose either the individual
or we can transpose the main setup and
then we get a minus 10 6 going the other
way
now we didn't mention this in our notes
but you can also do a scalar
multiplication
and just put down the scalar so you can
remember that
and what we're talking about here is I
have this array here U and if I go a
times U
H we'll take the value 2 we'll multiply
it by every value in here so 2 times 30
is 60 2 times 15
and just like we did before
this happens a lot because when you're
doing matrixes you do need to flip them
you get 60 30 coming this way
so in numpy uh we have what they call
Dot product
and uh with this this is in a
two-dimensional vectors it is the
equivalent of two matrix multiplication
remember we were talking about matrix
multiplication uh where it is the
well let's walk through it
we'll go ahead and start by defining two
numpy arrays we'll have uh 10 20 25 6 or
our U and our V and then we're going to
go ahead and do if we take
the values and if you remember correctly
an array like this would be 10 times 25
plus 20 times 6. we'll go ahead and
print that
there we go
and then we'll go ahead and do the NP
dot dot of U comma
V
and we'll find when we do this we go and
run this we're going to get
370
370. so this is a strain multiplication
where they use it to solve
linear algebra when you have multiple
numbers going across and so this could
be very complicated we could have a
whole string of different variables
going in here but for this we get a nice
value for our Dot multiplication
and we did addition earlier which is
just your basic addition and of course
the Matrix you can get very complicated
on these or in this case we'll go ahead
and do let's create two complex matrixes
this one is a matrix of
um you know 12 10 4 6 4 31. we'll just
print out a so you can see what that
looks like here's print a
we print a out you can see that we have
a
two by three layer Matrix for a and we
can also put together always kind of fun
when you're playing with print values we
could do something like this we could go
in here
there we go we could print a we have it
end with equals a run and this kind of
gives it a nice look here's your Matrix
that's all this is comma N means it just
tags it on the end that's all all that
is doing on there and then we can simply
add in what is a plus b and you should
already guess because this is the same
as what we did before there's no
difference when we do a simple vector
addition we have 12 plus 2 is 14 10 plus
8 is 18 and so on
and just like we did The Matrix addition
we can also do a minus B into our Matrix
subtraction
and we look at this we have what 12
minus 2 is 10 10 minus eight uh where
are we
oh there we go eight minus ah
confusing what I'm looking at I should
have reprinted out the original numbers
but we can see here 12 minus 2 is of
course 10 10 minus 8 is 2 4 minus 46 is
minus 42 and so forth so same as its
attraction as before we just call it
Matrix subtraction it's identical
now if you remember up here we had a
scalar Edition where we're adding just
one number
to a matrix you can also do scalar
multiplication and so simply if you have
a single value a and you have B which is
your array we can also do a times B
when we run that you can see here we
have 2 times 4 is 8 5 times 4 is 20 and
so forth you're just multiplying the 4
across each one of these values
and this is an interesting one that
comes up a little bit of a brain teaser
is Matrix and Vector multiplication
and so we're looking at this
we are let's do a regular arrays it
doesn't necessarily have to be a numpy
array we have a
which has our array of arrays and B
which is a single array and so we can
from here
do the DOT
a b
and this is going to return two values
and the verse value is that it's you
could say it's like uh we're doing the
this array B array
first with a and then with a second one
and so it splits it up so you have a
matrix of vector multiplication and you
can mix and match when you get into
really complicated uh back end stuff
this becomes more common because you're
now you've got layers upon layers of
data and so you you'll end up with a
matrix and a set of Bolt Vector matrices
do you want to multiply
now keep in mind that if you're doing
data science a lot of times you're not
looking at this this is what's going on
behind the scenes so if you're in the
site kit looking at sklearn where you're
doing linear regression models this is
some of the math that's hidden behind
the scenes that's going on
other times you might find yourself
having to do part of this and manipulate
the data around so it fits right and
then you go back in and you run it
through the psy kit and if we can do
um
up here we did a matrix and Vector
multiplication we can also do Matrix to
matrix multiplication and if we run this
where we have the two matrixes you can
see we have a very complicated array
that of course comes out on there for
our DOT and just to reiterate it we have
our transposer Matrix which is your dot
t
and so if we create a matrix a and we do
transpose it you can see how it flips it
from 5 10 15 20 25 30 to 5 15 25 10 20
30. rows and columns
and certainly with the math this comes
up a lot it also comes up a lot with X Y
plotting when you put into Pi plot you
have one format where they're looking at
Pairs and numbers and then they want L
of x's and all y's so you know the
transpose is an important tool both for
your math and for plotting and all kinds
of things
another tool that we didn't discuss uh
is your identity Matrix
and this one is more definition
uh the identity Matrix we have here one
where we just did two so it comes down
as one zero zero one one zero zero zero
one zero it creates a diagonal of one
and what that is is when you're doing
your identities you could be comparing
all your different features to the
different features and how they
correlate and of course when you have a
feature one compared to feature one to
itself it is always one where usually
it's between zero one depending on how
well correlates so when we're talking
about identity Matrix that's what we're
talking about right here is that you
create this preset Matrix and then you
might adjust these numbers depending on
what you're working with and what the
domain is
and then another thing we can do to kind
of wrap this up we'll hit you with the
most complicated uh piece of this puzzle
here is an inverse a matrix
and let's just go ahead and put the um
it's a lengthy description
let's go and put the description this is
straight out of the
the website for numpy so given a square
Matrix a here's our Square Matrix a
which is two one zero zero one zero one
two one keep in mind three by three it's
Square it's got to be equal it's going
to return the Matrix a inverse
satisfying dot a
um a inverse so here's our matrix
multiplication
um and then of course it equals the dot
uh yeah a inverse of a with an identity
shape of a DOT shape zero this is just
reshaping the identity
that's a little complicated there uh so
we're going to have our here's our array
uh we'll go ahead and run this and you
can see what we end up with is we end up
with uh an array 0.5 minus 0.5 and so
forth with our two one one going down
two one zero zero one zero one two one
um getting into a little deep on the
math understanding when you need this is
probably really is is what's really
important when you're doing data science
versus uh handwriting this out and
looking up the math and handwriting all
the pieces out you do need to know about
the linear algorithm inverse of a so if
it comes up you can easily pull it up or
at least remember where to look it up we
took a look at the algebra side of it
let's go ahead and take a look at the
calculus side of what's going on here
with the machine learning so calculus oh
my goodness and differential equations
you got to throw that in there because
that's all part of the bag of tricks
especially when you're doing large
neural networks but it also comes up in
many other areas the good news is most
of it's already done for you in the back
end so when it comes up you really do
need to understand from the data science
not data analytics data analytics means
you're digging deep into actually
solving these math equations in a neural
network is just a giant differential
equation
uh so we talk about calculus we're going
to go ahead and understand it by talking
about cars versus time and speed
uh so it helps to calculate the
spontaneous rate of change
so suppose we plot a graph with the
speed of a car with respect to time so
as you can see here going down the
highway probably merged into the highway
from an on-ramp so I had to accelerate
so my speed went way up
stuck in traffic merged into the traffic
traffic opens up and I accelerate again
up to the speed limit and maybe Peter's
off up there so you can look at this as
as
the speed versus time I'm getting faster
and faster because I'm continually
accelerating and if I hit the brakes it
go the other way
so the rate of change of speed with
respect to time is nothing but
acceleration how fast are we
accelerating
the acceleration is the area between the
start point of X and the endpoint of
Delta X
so we can calculate a simple if you had
X and Delta X we could put a line there
and that slope of the line is our
acceleration
now that's pretty easy when you're doing
linear algebra but I don't want to know
it just for that line in those two
points I want to know it across the
whole of what I'm working with that's
where we get into calculus
so we talk about the distance between X
and Delta X it has to be the smallest
possible near to zero in order to
approximate the acceleration
so the idea is that instead of I mean if
you ever did took a basic calculus class
they would draw bars down here and you
would divide this area up let's go back
up a screen you divide this area up of
this time period up into maybe 10
sections and you'd use that and you
could calculate the acceleration between
each one of those 10 sections kind of
thing and then we just keep making that
space smaller and smaller until Delta X
is almost
infinitesimally small
and so we get a function of a equals a
limit as H goes to 0 of a function of a
plus h minus a function of a over H and
that is your Computing the slope of the
line
we're just Computing that slope under
smaller and smaller and smaller samples
uh and that's what calculus is calculus
is the integral you can see down here we
have our nice integral sign looks like a
giant s and that's what that means is
that we've taken this down to as small
as we can for that sampling
so we're talking about calculus we're
finding the area under the slope is the
main process in the integration similar
small intervals are made of the smallest
possible length of X Plus Delta X where
Delta X approaches almost an
infinitesimally small space
and then it helps to find the overall
acceleration by summing up all the links
together so we're summing up all the
accelerations from the beginning to the
end and so here's our integral we sum of
a of x times D of x equals a plus c
that is our basic calculus here
so when we talk about multivariate
calculus
multivariate calculus deals with
functions that have multiple variables
and you can see here we start getting
into some very complicated equations
change in W over change of time equals
change of w over change of Z the
differential of Z to DX the differential
of x to DT it gets pretty complicated
and it really translates into the
multivariate integration using double
integrals and so you have the the sum of
the sum of f of x of Y of D of a equals
the sum from C to D and A to B of f of x
o y d x d y equals the sum of a to B sum
of C to D of f x or y d y d x
understanding the very specifics of
everything going on in here and actually
doing the math is use the calculus one
calculus two and differential equations
uh so you're talking about three full
length courses to dig into and solve
these math equations what we want to
take from here is we're talking about
calculus we're talking about summing of
all these different slopes and so we're
still solving a linear uh expression
we're still solving y equals m x plus b
but we're doing this for infantismally
small x's and then we want to sum them
up that's what this integral sign means
the sum of a of x d of x equals a plus c
and when you see these very complicated
multivariate differentiation using the
chain rule when we come in here and we
have the change of w to the change of T
equals a change of w DZ
and so forth that's what's going on here
that's what these means we're basically
looking for the area under the curve
which really comes to how is the change
changing the speed's going up how is
that changing and then you end up with a
multiple layer so if I have three layers
of neural networks how is the third
layer changing based on the second layer
changing which is based on the first
layer changing and you get the picture
here that now we have a very complicated
multivariate integration with integrals
the good news is we can solve this
mathematically and that's what we do
when you do neural networks and reverse
propagation so the nice thing is that
you don't have to solve this on paper
unless you're a data analysis and you're
working on the back end of integrating
these formulas and building the script
to actually build them so we talk about
applications of calculus it provides us
the tools to build an accurate
predictive model so it's really behind
the scenes we want to guess at what the
change or the change of the changes
that's a little goofy I I know I just
threw that out there's kind of a meta
term but if you can guess how things are
going to change then you can guess what
the new numbers are
multivariate calculus explains the
change in our Target variable in
relation to the rate of change in the
input variables
so there's our multiple variables going
in there if one variable is changing how
does it affect the other variable
and then in gradient descent calculus is
used to find the local and Global Maxima
and this is really big we're actually
going to have a whole section here on
gradient descent because it is really I
mean I talked about neural networks and
how you can see how the different layers
go in there but gradient descent is one
of the most key things for trying to
guess the best answer to something
so let's take a look at the code behind
gradient descent and before we open up
the code let's just do real quick
gradient descent
let's say we have a curve like this and
most common
is that this is going to represent your
error oops
error there we go error uh hard to read
there and I want to make the error as
low as possible
and so when I'm looking at it is I want
to find this line here
which is the minimum value so we're
looking for the minimum
and it does that by sampling there
and then it based on this it guesses it
might be someplace here and it goes hey
this is still going down it goes here
and then goes back over here and then
goes a little bit closer and it's just
playing a high low until it gets to that
spot that bottom spot
and so we want to minimize the error in
on the flip note you could also want to
be maximizing something you want to get
the best output of it that's simply
minus the value so if you're looking for
where the peak is this is the same as a
negative for where the valley is and
looking for that Valley that's all that
is and this is a way of finding it so
the cool thing is all the heavy
lifting's done I actually ended up
putting together one of these a while
back as uh when I didn't know about
sidekick and I was just starting boy
it's a long while back and uh is playing
high low how do you play high low not
get stuck in The Valleys figure out
these curves and things like that well
you do that and the back end is all the
calculus and differential equations to
calculate this out the good news is you
don't have to do those
uh so instead we're going to put
together the code and let's go ahead
and see what we can do with that
so uh guys in the back put together a
nice little piece of code here which is
kind of fun uh some things we're gonna
note and this is this is really
important stuff because when you start
doing your data science and digging into
your machine learning models uh you're
gonna find these things are stumbling
blocks the first one is current X where
do we start at keep in mind
your model that you're working with is
very generic so whatever you use to
minimize it the first question is where
do we start and we started at this
because the algorithm starts at x equals
three so we arbitrarily picked five
learning rate is how many bars to skip
going one way or the other I'm in fact
I'm going to separate that a little bit
because these two are really important
if we're dealing with something like
this where we're talking about well
here's our here's the function we're
going to use our gradient of our
function 2 times X plus five keep it
simple so that's a function we're going
to work with so if I'm dealing with
increments of a thousand point one is
going to be a very long time and if I'm
dealing with increments of 0.001
0.1 is going to skip over my answer so I
won't get a very good answer and then we
look at Precision this tells us when to
stop the algorithm so again very
specific to what you're working on if
you're working with money
and you don't convert it into a float
value you might be dealing with 0.01
which is a penny that might be your
Precision you're working with
and then of course the previous step
size Max iterations we want something to
cut out at a certain point usually
that's built into a lot of minimization
functions and then here's our actual
formula we're going to be working with
and then we come in we go while previous
step size is greater than precision and
itters is less than Max and Max itters
say that 10 times fast
um
we're just saying if it's uh if we're if
we're still greater than our Precision
level we still got to keep digging
deeper and then we also don't want to go
past a thou or whatever this is a
million or 10 000 running that's
actually pretty high I almost never do
Max iterations more than like 100 or 200
rare occasions you make up to four or
five hundred if it's uh depending on the
problem you're working with uh so we
have our previous equals our current
that way we can track time wise uh the
current now equals the current minus the
rate times the formula of our previous X
so now we've generated our new version
uh previous step size equals the
absolute current previous
so we're looking for the change in x
errors equals iterations plus one that's
how we know to stop if we get too far
and then we're just going to print the
local minimum occurs at axon here and if
we go ahead and run this
uh you can see right here it gets down
to this point and it says hey
um
local minimum is minus
3.322 for this particular series we
created and this is created off of our
formula here Lambda x 2 times X Plus 5.
now
when I'm running this stuff you'll see
this come up a lot
in with the sklearn kit and one of the
nice reasons of breaking this down the
way we did is I could go over those top
pieces those top pieces are everything
when you start looking at these
minimization toolkits in built-in code
and so from
um we'll just do it's actually Docs
got
scipy.org and we're looking at
the scikit
there we go optimize minimize
you can only minimize one value you have
the function that's going in this
function can be very complicated so we
used a very simple function up here it
could be there's all kinds of things
that could be on there and there's a
number of methods to solve this as far
as how they shrink down and your X
naught there's your there's your start
value so your function your start value
there's all kinds of things that come in
here that you can look at which we're
not going to
optimization automatically create
constraints bounds some of this it does
automatically but you really the big
thing I want to point out here is you
need to have a starting point you want
to start with something that you already
know is mostly the answer if you don't
then it's going to have a heck of a time
trying to calculate it out
or you can write your own little script
that does this and does a high low
guessing and tries to find the max value
that brings us to statistics what this
is kind of all about is figuring things
out a lot of vocabulary and statistics
ah so statistics well I guess it's all
relative it's definitely not an adult
class so a bunch of stuff going on
statistics statistics concerns with the
collection organization analysis
interpretation and presentation of data
that is a mouthful
um so we have from end to end where
where does it come from is it valid what
does it mean how do we organize it how
do we analyze it and then you got to
take those analysis and interpret it
into something that uh people can use
kind of reduce it to understandable and
nowadays you have to be able to present
it if you can't present it then no one
else is going to understand what the
heck you did
so we look at the terminologies uh there
is a lot of terminologies depending on
what domain you're working in so clearly
if you're working in a domain that deals
with
viruses and T cells and and how does you
know where does that come from and
you're studying the different people
then you can have a population if you
are working with
um
mechanical gear you know a little bit
different if you're looking for the
wobbling statistics to know when to
replace a rotor on a machine or
something like that that can be a big
deal you know we have these huge fans
that turn
in our sewage processing systems and so
those fans they start to wobble and hum
and do different things that the sensors
pick up at one point do you replace them
instead of waiting for it to break in
which case it costs a lot of money
instead of replacing a bushing you're
replacing the whole fan unit
uh an interesting project that came up
for our city a while back so population
all objects are measurements whose
properties are being observed uh so
that's your population all the objects
it's easy to see it with people because
we have our population in large but in
the case of these sewer fans we're
talking about how the fan units that's
the population of fans that we're
working with
you have a parameter a matrix that is
used to represent a population or
characteristic
you have your sample a subset of the
population studied you don't want to do
them all because then you don't have a
if you come up with a conclusion for
everyone you don't have a way of testing
it so you take a sample sometimes you
don't have a choice you can only take a
sample of what's going on you can't
study the whole population and a
variable a metric of interest for each
person or object in a population
types of sampling
we have a probabilistic approach
selecting samples from a larger
population using a method based on the
theory of probability
and we'll go into a little bit more
deeper on these we have random
systematic stratified and then you have
non-probabilistic approach selecting
samples based on the subjective Judgment
of the researcher rather than random
selection it has to do with convenience
trying to reach a quota or snowball
uh and they're very biased that's one of
the reasons you'll see this big stamp on
it says biased uh so you gotta be very
careful on that
so probabilistic sampling uh when we
talk about a random sampling we select
random size samples from each group or
category so we it's as random as you can
get uh we talk about systematic sampling
we're selecting random size samples from
each group or category with a fixed
periodic interval
uh so we kind of split it up this would
be like a Time setup or our different
categories and you might ask your
question what is a category or a group
uh if you look at I'm going to go back a
window let's say we're studying
economics of different of an area
we know pretty much that based on their
culture where they came from they might
need to be separated and so they might
say separated I don't mean separated
from their their place where they live I
mean as far as the analysis we want to
look at the different groups and make
sure they're all represented
so if we had like an 80 uh of a group
that is uh say Hispanic and or Indian
and also in that same area we have 20 20
who are let's call our expatriates they
left America and they're nice and uh
your Caucasian group we might want to
sample a group that is representative of
both uh so we're talking about
stratified sampling and we're talking
about groups those are the groups we're
talking about and that brings us to
stratified sampling selecting
approximately equal size samples from
each group or category
uh this way we can actually separate the
categories and give us an insight into
the different cultures and how that
might affect them in that area
so you can see these are very very
different kind of depends on what you're
working with as far as your data and
what you're studying and so we can see
here just a little bit more we have
selecting 25 employees from a company of
250 employees randomly don't care
anything about them what groups they're
in which office they're in nothing
uh and we might be selecting one
employee from every 50 unique employees
in a company of 250 employees and then
we have selecting one employee from
every branch in the company office so we
have all the different branches there's
our group or our categories by the
branch and the category could depend on
what you're studying so it has a lot of
variation on there you see this kind of
grouping and categorizing is also used
to generate a lot of misinformation uh
so if you only study one group and you
say this is what it is then everybody
assumes that's what it is for everybody
and so you've got to be very careful of
that and it's very unethical thing to
kind of do
so types of Statistics we talk about
statistics we're going to talk about
descriptive and inferential statistics
there are so many different terms and
statistics to break it up so we so we're
talking about a particular
setup
so we're talking about descriptive and
inferential uh statistics the base of
the word describe is pretty solid you're
describing the data what does it look
like with inferential statistics we're
going to take that from the small
population to a large population so if
you're working with a drug company you
might look at the data and say these
people were helped by this drug they did
80 better as far as their health or 80
percent better survival rate than the
people
who did not have the drug so we can
infer that that drug will work in the
greater populace and will help people so
that's where you get your inferential so
we are predicting how it's going to
affect the greater population
so descriptive statistics it is used to
describe the basic features of data and
form the basis of quantitative analysis
of data
so we have a measure of central
Tendencies we have your mean median and
mode
and then we have a measure of spread
like your range your interquartile range
your variance in your standard deviation
and we're going to look at all these a
little deeper here in a second but one
of them you can think of is
how the data difference
differences you know what's the max Min
range all that stuff is your spread and
anything that's just a single number is
usually your central Tendencies measure
of central tendencies
so we talk about the mean it is the
average of the set of values considered
what is the average outcome of
whatever's going on
and then your median separates the
higher half and the lower half of data
so where's the center point of all your
different data points so your mean might
have some a couple really big numbers
that skew it so that the average is much
higher than if you took those outliers
out where the median would by separating
the high from the low might give you a
much lower number you might look at and
say oh that's that's odd Y is the
average so much higher than the median
well it's because you have some outliers
or why is it so much lower
and then the mode is the most frequent
appearing value this is really
interesting if you're studying economics
and how people are doing you might find
that the most common
income like in the U.S was at one point
twenty four thousand a year
where the average was closer to 80 000
and it's like wow what a difference well
there's some people have a lot of money
and so that skews that way up so the
average person is not making that kind
of money and then you look at the median
income and you're like well the median
income is a little bit closer to the
average so it does create a very
interesting way of looking at the data
again these are all uh Central
Tendencies single numbers you can look
at for the whole spread of the data
and we look at the measure of central
Tendencies the mean is the average marks
of a student's in a classroom so here we
have the mean some of the marks of the
students total number of students and as
we talked about the median if we have 0
through 10 and we take half the numbers
and put them on one side of the line
half the numbers on the other side of
the line we end up with five in the
middle and then the mode what Mark was
scored by most of the students in a test
in a simple case where most people
scored like an 82 percent and got
certain problems wrong easy to figure
out uh not so easy when you have
different areas where like you have like
the um oh let's go back to economy a
little bit more difficult to calculate
if you have a large group of scores that
makes 30 000 and a slightly bigger group
that makes twenty six thousand so what
do you put down for the mode uh
certainly there's a number of ways to
calculate that and there's actually a
different variations depending on what
you're doing so now we're looking at a
measure of spread uh range what's the
difference between the highest and the
lowest value first thing you want to
look at you know it's uh we had
everybody in the test scored between 60
and 100 percent so we got 100 or maybe
60 to 90 percent it was so hard that a
lot of people could not get a hundred
percent
um you have your interquartile range
quartiles divide a rank ordered data set
into four equal parts
very common thing to do is part of all
the basic packages whether you're
working in uh data frames with pandas
whether you're working in Scala whether
you're working in R you'll see this come
up where they have range your Min your
Max and then it'll have your
interquartile range how does it look
like in each quarter of data variants
measures how far each number in the set
is from the mean and therefore from
every other number in the set
uh so you have like how much turbulence
is going on in this data
and then the standard deviation it is a
measure the variance or the dispersion
of a set of values from the mean
and you'll usually see if I'm doing a
graph I might have the value graphed and
then based on the the error I might grab
graph the standard deviation in the
error on the graph as a background so
you can see how far off it is so
standard deviation is used a lot
so measurement of spread marks of a
student out of 100 we have here from 50
to 63 or 50 to 90 so the range maximum
marks minimum marks we have 90 to 45 and
the spread of that is 45 90 minus 45 and
then we have the interquartile range
using the same marks over there you can
see here where the median is and then
there's a first quarter the second
quarter and the third quarter based on
splitting it apart by those values
and to understand the variance and
standard deviation we first need to find
out the mean so here's our you know
calculating the average there we end up
approximately 66 for the average and
then we look at that in the variance
once we know the means we can do equals
the marks minus the mean squared Y is a
squared because one you want to make
sure it's you don't have like if you if
you're putting all this stuff together
you end up with an error as far as one's
negative one's positive one's a little
higher one's a little lower so you
always see the squared value and over
the total observations and so the
standard deviation equals the square
root of the variance which is
approximately 16. and if you were
looking at a predictable model you would
be looking at the deviation based on the
error how much error does it have that's
again really important to know if you're
if your prediction is predicting
something what's a chance of it being
way off or just a little bit off
brings you a comprehensive professional
certificate program in Ai and machine
learning from Purdue University that is
in collaboration with IBM that will
cover a wide range of topics that will
Empower you with the knowledge and
skills needed to excel in the field of
AI to learn more about this course you
can find the course link mentioned in
the description box
now that we've looked at the tools as
far as some of the basics for doing your
statistics and what we're talking about
let's go ahead and pull up a little demo
and show you what that looks like in
Python code so you can get some little
Hands-On here for that let's go back
into our jupyter notebook in Python now
almost all of this you can do in numpy
last time we worked in numpy this time
we're going to go ahead and use pandas
and if you remember from pandas on here
this is basically a data frame rows
columns let's just go ahead and do a
print df.head
and run that
and you can see we have the name Jane
Michael William Rosie Hanna sat in their
salaries on here and of course instead
of having to do all those hand
calculations and add everything together
and divide by the total we can do
something very simple on this like use
the command mean in pandas and so if I
go ahead and do this print DF
pick our column salary because we want
to find the means of that color e
we want to find the means of that column
and we go and print this out and you can
see that the average income on here is
71
000.
uh and let's just go ahead and do this
we'll go ahead and put in means
and if we're going to do that we also
might want to find the median
and the median is very similar
except it actually is just median uh
we're used to means in average it's kind
of interesting that those are they use
the two different words
there can be in some computation slight
differences but for the most part the
means is the average uh and then the
median
oops let's put a
median here do you have salary that way
it displays a little better we can see
the median is 54
000. so the halfway mark is
significantly below the average why
because we have somebody in here who
makes 189 000. darn you Rosie for
throwing off our numbers but that's
something you'd want to notice this is
this is the difference between these is
huge and so is what is the meaning
behind that when you're studying a
populist and looking at the different
data coming in and of course we also
want to find out hey what's the most
common
income that people make
in this little tiny sample and so we'll
go ahead and do the mode
and you can see here with the mode uh
it's at 50 000.
so this is this is very telling that
most people are making fifty thousand
the middle point is at fifty four
thousand so half the people are making
more than that what that tells me is
that if the most common income is way is
below the median then there's a few
there's a you know there's a lot of high
salaries going up but there's some
really low salaries in there and so this
trend which is very common in statistic
and when you're in analyzing the economy
in different people's income is pretty
common and the bigger difference between
these is also very important when we're
studying statistics and when you hear
someone just say hey the average income
was you might start asking questions at
that point why aren't you talking about
the median income why aren't you talking
about the mode the most common income
what are you hiding and if you're doing
these analysis you should be looking at
these saying hey why are this
discrepancies why are these so different
and of course with any analysis it's
important to find out the minimum
and the maximum so we'll go ahead it's
just simply
dot min
so pull up your minimum and then dot Max
pulls up the maximum pretty
straightforward on as far as
translating it and knowing what you're
you know put the your lowest value and
what your highest value is here which
you'll use to generate like a spread
later on and real quick on no mode note
that it puts mode zero like I said
there's a couple different ways you can
compute the mode
um although you know standard one's
pretty good we can of course do the
range which is your max minus your Min
so now we have a range of 149 000
between the upper end and the lower end
and you might want to be looking up the
individual values on all of these but it
turns out there is a describe
feature in pandas
and so in pandas we can actually do DF
salary describe and if we do this you
can see we have that there's seven
setups here's our mean our standard
deviation which we didn't compute yet
which would just be a DOT STD and you
gotta be a little careful because when
it computes it it looks for axes and
things like that we have our minimum
value and here's our quartiles
our maximum value and then of course the
name salary uh so these are these are
the basic statistics you can pull them
up and like just describe this is a
dictionary so I could actually do
something like um
in here I could actually go uh count and
run and now it just prints the count
so because this is a dictionary you can
pull any one of these values out of here
it's kind of a quick and dirty way to
pull all the different information and
then split it up depending on what you
need now if I just walked in and gave
you this information in a meeting
at some point you would just kind of
fall asleep that's what I would do
anyway
um so we want to go ahead and see about
graphing it here we'll go ahead and put
it into a histogram and plot that graph
on it
of the salaries and let's just go ahead
and put that in here so
we do our matplot inline remember that's
a Jupiter's notebook thing a lot of the
new version of the matplot library does
it automatically but just in case I
always put it in there import matplot
library pipelot is PLT that's my
plotting
and then we have our data frame I guess
I really don't need to respell the data
frame maybe we could just remind
ourselves what's in it so we'll go ahead
and just print
DF that way we still have it and then we
have our salary DF salary salary.plot
history title salary distribution color
gray
plot axvline salary the mean value so
we're going to take the mean value
um color violet line style Dash this is
just all making it pretty uh what color
dashed line line width of two that kind
of thing and the median and let's go
ahead and run this just so you can see
what we're talking about
and so up here we are taking on our plot
so here's the data here's our our data
frame printed out so you can see it with
the salaries we'll look at the salary
distribution and just look at this the
way there the salary is distributed
um you have our in this case we did
Let's see we had red for the median
we have violet
for our average or mean and you can just
see how it really here's our outlier
here's our person who makes a lot of
money here's the average and here's the
median and so as you look at this you
can see wow based on the average it
really doesn't tell you much about what
people are really taking home all it
does is tell you how much money is in
this you know what the average salary is
so some of the things you want to take
away in addition to this is that it's
very easy to plot
um
an axv line these are these up and down
lines for your markers
um and as you just just play the data I
mean you can add all kinds of things to
this and get really complicated keeping
it simple is pretty straightforward I
look at this and I can see we have a
major outlier out here we can definitely
do a histogram and stuff like that but
you know pictures worth a thousand words
what you really want to make sure you
take away is that we can do a basic
describe which pulls all this
information out and we can print any of
the individual information from the
describe because this is a dictionary
and so if we want to go ahead and look
up
the mean value we can also do describe
mean so if you're doing a lot of
Statistics being able to
doesn't have the print on there so it's
only going to print the last one which
happens to be the mean you can very
easily reference any one of these and
then you can also if you're doing
something a little bit more complicated
and you don't need just the basics you
can come through and pull any one of the
individual
um
references from the from the pandas on
here so now we've had a chance to
describe our data let's get into
inferential statistics inferential
statistics allows you to make
predictions or inferences from data and
you can see here we have a nice little
picture movie ratings and
if we took this group of people and said
hey how many people like the movie
dislike it can't say and then you ask
just a random person who comes out of
the movie who hasn't been in this study
you can infer that 55 chance of saying
liked 35 chance of saying disliked or a
10 or 11 chance of can't say so that's
real basics of what we're talking about
is you're going to infer that the next
person is going to follow these
statistics
uh so let's look at Point estimation it
is a process of finding an approximate
value for a population's parameter like
mean or average from random samples of
the population let's take an example of
testing vaccines for covid-19 vaccines
and flu bugs all that it's a pretty big
thing of how do you test these out and
make sure they're going to work on the
populace
a group of people are chosen from the
population medical trials are performed
results are generalized for the whole
population so here's a protected there's
our small group up here where we've
selected them we run medical trials on
them and then the results work for the
population a nice diagram with the
arrows going back and forth in the very
scary covet virus in the middle of one
and let's take a look at the
applications of inferential statistics
very Central is what they call
hypotheses testing and the confidence
interval which go with that and then as
we get into
probability we get into our binomial
theorem our normal distribution in
central limit theorem
hypothesis testing hypothesis testing is
used to measure the plausibility of a
hypothesis assumption by using sample
data now when we talk about theorems
Theory
hypothesis keep in mind that if you are
in a philosophy class theory is the same
as hypothesis where theorem is a
scientific uh statement that is
something that has been proven although
it is always up for debate because in
science we always want to make sure
things are up to debate so a hypothesis
is the same as a philosophical class
calling a theory where theory in science
is not the same theory in science says
this has been well proven gravity is a
theory so if you want to debate the
theory of gravity try jumping up and
down if you want to have a theory about
why the economy is collapsing in your
area that is a philosophical debate very
important I've heard people mix those up
and it is a pet peeve of mine when we
talk about hypotheses testing the steps
involved in hypotheses testing is first
we formulate a hypothesis we figure out
the right test to test our hypothesis we
execute the test and we make a decision
it and so when you're talking about a
hypothesis you're usually trying to
disprove it if you can't disprove it and
it works for all the facts then you
might call that ethereum at some point
so in a use case let's consider an
example we have four students we're
given a task to clean a room every day
sounds like working with my kids they
decided to distribute the job of
cleaning the room among themselves they
did so by making four chits which has
their names on it and the name that gets
picked up has to do the cleaning for
that day Rob took the opportunity to
make chits and wrote everyone's name on
it so here's our four people Nick Rob
imlia imlia and summer
now Rick Emilia and summer are asking us
to decide whether Rob has done some
Mischief in preparing the chits I.E
whether Rob has written his name on one
of the chit for that we will find out
the probability of Rob getting the
cleaning job on first day second day
third day and so on till 12 days the
probability of Rob getting the job
decreases every day I.E his turn never
comes up then definitely he has done
some Mischief while making the chits so
the probability of Rob not doing work on
day one is three out of four there's a
0.75 chance that he didn't do work uh
two days three fourths times
three-fourths equals 0.56
three days you have three fours three
fours three-fourths which equals 0.42
uh when you get to day 12 it's 0.032
Which is less than 0.05 remember this
0.05 that comes up a lot when we're
talking about certain values when we're
looking at statistics Rob is cheating as
he wasn't chosen for 12 consecutive days
that's a very high probability when on
day 12 he still hasn't gotten the job
cleaning the room
so we come up to our important important
terminologies we have null hypothesis
a general statement that states that
there is no relationship between two
measured phenomenon or no association
among the groups
alternative hypothesis contrary to the
null hypothesis it states whenever
something is happening a new theory is
preferred instead of an old one and so
the two hypotheses go hand in hand so
your null this is always interesting in
in when talking about data science and
the math behind it it's about proving
that the things have no correlation null
hypothesis says these two have zero
relation to each other where the
alternative hypothesis says hey we found
a relation this is what it is
we have p-value the p-value is a
probability of finding the observed or
more extreme results when the null
hypotheses of a study question is true
and the T value it is simply the
calculated difference represented in
units of standard error the greater the
magnitude of T the greater the evidence
against the null hypothesis and you can
look at the T values being specific to
the test you're doing
where the p-value is derived from your T
value and you're looking for what is
called a five percent or the 0.05
showing that it has a high correlation
so digging in deeper let's assume that a
new drug is developed with the goal of
lowering the blood pressure more than
the existing drug and this is a good one
because the null value here isn't that
you don't have any drug then null value
here is it is better than existing drug
the new drug doesn't lower the blood
pressure more than the existing drug
now if we get that that says our null
hypothesis is correct there is no
correlation and the new drug is not
doing its job the alternative hypothesis
the new drug does significantly lower
the blood pressure more than an existing
drug uh yay we got a new drug out there
and that's our alternative hypothesis or
the H1 or h a
and we look at the p-value results from
the evidence like medical trials showing
positive results which will reject the
null hypothesis
and again they're looking for a 0.05 or
5 percent and the T value comparing all
the positive test results and finding
means of different samples in order to
test hypothesis so this is specific to
the test how what percentage of increase
did they have
and this leads us to the confidence
intervals a confidence interval is a
range of values we are sure our true
values of observations lie in
let's say you asked a dog owner around
you and asked them how many cans of food
do you buy for your per year for your
dog
through calculations you've got to know
that the on an average around 95 percent
of the people bought around 200 to 300
cans of food hence we can say that we
have a confidence interval of 2 300
where 95 percent of our values lie in
that data spread and this the graph
really helps a lot so you can start
seeing what you're looking at here where
you have the 95 percent you have your
peak in this case it's a normal
distribution so you have a nice bell
curve equal on both sides it's not
asymmetrical and 95 percent of all the
values lie within a very small range and
then you have your outliers the 2.5
percent going each way
so we touched upon hypothesis and we're
going to move into probability so you
have your hypothesis once you've
generated your hypothesis we want to
know the probability of something
occurring probability is a measure of
the likelihood of an event to occur any
event can be predicted with total
certainty and can only be predicted as a
likelihood of its occurrence so any
event cannot be predicted with total
search and T can only be predicted as a
likelihood of its occurrence score
prediction how good you're going to do
in whatever sport you're in weather
prediction stock prediction if you've
studied physics and Chaos Theory even
the location of the chair you're sitting
on has a probability that it might move
three feet over
granted that probability is one in like
uh I think we calculated as under one in
trillions upon trillions so it's
the better the probability the more
likely it's going to happen there are
some things that have such a low
probability that we don't see them so we
talk about random variable a random
variable is a variable whose possible
values are numerical outcomes of a
random phenomena so we have the coin
tossed how many heads will occur in the
series of 20 coin flips probably you
know the on average they're 10 but you
really can't know because it's very
random how many times a red ball is
picked from a bag of balls if there's
equal number of red balls and blue balls
and green balls in there how many times
the sum of digits on two dice result are
five each
so you know there's how often you're
going to roll two fives on your pair of
Dives
so in a use case let's consider the
example of rolling two dice we have a
random variable outcome equals y you can
take values two three four five six
seven eight nine ten eleven twelve
so we have a random variable and a
combination of dice and instead of
looking at how many times both dice for
roll five let's go ahead and look at
total sum of five and you have in as far
as your random variables you can have a
one four equals five four one two three
three two
so four of those roles can be four if
you look at all the different options
you have four of those random roles can
be a five
and if we look at the total number
which happens to be 36 different options
uh you can see that we have four out of
36 chance every time you roll the dice
that you're going to roll a total of
five you're gonna have an outcome of
five
and uh we'll look a little deeper as to
what that means but you could think of
that at what point if someone never
rolls a five or they always roll a five
can you say hey that person's probably
cheating we'll look a little closer at
the math behind that but let's just
consider this is one of the cases is
rolling two dice and gambling
there's also a binomial distribution it
is the probability of getting success or
failure as an outcome in an experiment
or trial that is repeated multiple times
and the key is is by meaning two
binomial so passing or filling an exam
winning or losing a game and getting
either head or tails so if you ever see
binomial distribution it's based on a
true false kind of setup you win or lose
let's consider a use case and let's
consider the game of football between
two clubs Barcelona and Dortmund the
teams will have to play a total of four
matches and we have to find out the
chances of Barcelona winning the series
so we look at the total games and we're
looking at five different games or
matches let's say that the winning
chance for Barcelona is 75 or 0.75 that
means at each game they have a 75 chance
that they're going to win that game and
losing chances are 25 or 0.25 clearly
0.75 plus 0.25 equals one so that
accounts for 100 of the game probability
for getting K wins in in matches is
calculated
and we we're talking like so if you have
five games uh and you want to know if I
play
um how many wins in those five games
should I get what's a percentage on
those and the probability for getting K
wins and N matches is calculated by p x
equals k equals nck P to the k q to the
N minus K here p is a probability of
success and Q is the probability of
failure and so we can do total games of
n equals five where k equals zero one
two three four five P which is the
chance of winning is 0.75 Q the chance
of losing equals 1 minus P which equals
1 minus 0.075 which equals 0.25 the
probability that Barcelona will lose all
of the matches can then just plug in the
numbers and we end up with a point zero
zero zero nine seven six five six two
five so very small chance they're going
to lose all their matches
and we can plug in the value for two
matches probability of the Barcelona
will win at least two matches is 0.0878
and of course we can go on to the
probability of the Barcelona will win
three matches the 0.26 and of course
four matches and so on and it's always
nice to take this information
um and let's find the accumulated
discrete probabilities for each of the
outcomes where Barcelona has won three
or more matches x equals three x equals
four x equals five
and we end up with the p equals 0.264
plus 0.395 plus 237 which equals 0.89
in reality
the probability of Barcelona winning the
series is much higher than 0.75 and it's
always nice to uh
put out a nice graph so you can actually
see the number of wins to the
probability and how that pans out with
our binomial case
continuing in our important terminology
location the location of the center of
the graph depends on the mean value and
this is some very important things so
much of the data we look at and when you
start looking at probabilities almost
always has a normalized look like the
graph in the middle
uh but you do have left skewed where the
data is skewed off to the left and you
have more stuff happening out to the
left and you have right skewed data and
so when this comes up and these
probabilities come up where they're
skewed it's really important to take a
closer look at that mostly you end up
with a normalized set of data but you
got to also be aware that sometimes it's
a skewed data
and then the height height of the slope
inversely depends upon the standard
deviation
so you can see down here the standard
deviation is really large it kind of
squishes it out and if the standard
deviation is small then most of your
data is going to hit right there in the
middle you can have a nice Peak and so
being aware of this that you might have
a probability that fits certain data but
it has a lot of outliers so you're if
you have a really high standard
deviation
if you're doing stock market analysis
this means your predictions are probably
not going to make you much money where
if you have a very small deviation you
might be right on Target and set to
become a millionaire which leads us to
the z-score z-score tells you how far
from the mean a data point is it is
measured in terms of standard deviations
from the mean around 68 percent of the
results are found between one standard
deviation around 95 percent of the
results are found between two standard
deviations and you read the symbols of
course they love to throw some Greek
letters in there we have mu minus two
Sigma mu is just a quick way it's that
kind of funky you it just means the mean
and then the sigma is the standard
deviation and that's the o with that
little arrow off to the right or the
little waggly Tail Going up the o with
it with a line on it so mu minus 2 Sigma
is your 95 of the results are found
between two standard deviations
Central limit theorem this goes back to
the skew if you remember we were looking
at the skew values on this previous
slide have left skewed normalized and
right skewed when we're talking about it
being skewed or not skewed the
distribution of the sample means will be
approximately normally distributed
evenly distributed not skewed if you
take large random samples from the
population with the mean mu and the
standard deviation Sigma with
replacement
and you can see here of course we have
our mu minus two Sigma and the spread
down here the mean the median and the
mode and so you're talking about very
large populations
these numbers should come together and
you shouldn't have a skewed value if you
do that's a flag that something's wrong
that's why this is so important to be
aware of what's going on with your data
where your samples are coming from and
the math behind it and if you're going
to do all this we got to jump into
conditional probability
the conditional probability of an event
a is a probability that the event will
occur given the knowledge that an event
to be has already occurred and you'll
see this as Bayes theorem b-a-y-e-s Bays
and this is red
I mean you have these funky looking
little p brackets a b this is the
probability of a being true while B is
already true
and you have the probability of B being
true when a is already true so P B of A
probability of a being true divided by
the probability of B being true
and we talk about Bayes theorem which
occurred back in the 1800s when he
discovered this this is such an
important formula and it's really it's
not if you actually do the math you
could just kind of do
um
um x y equals J K and then you divide
them out and you're going to see the
same math but it works with
probabilities which makes it really nice
and so if you have a set you might have
uh eight or nine different studies going
on in different areas different people
have done the studies they brought them
together
if we look at today's covet virus the
virus spread certainly the studies done
in China versus the studies the way
they're done in the U.S that data is
different in each of those studies but
if you can find a place where it
overlaps where they're studying the same
thing together you can then compute the
changes that you need to make in one
study to make them equal
and this is also true if you have a
study of one group and you want to find
out more about it so this formula is
very powerful and it really has to do
with the data collection part of the
math and data science and understanding
where your data is coming from and how
you're going to combine different
studies in different groups
and we're going to go into a use case
let's find out the chance of a person
getting lung disease due to smoking and
this is kind of interesting the way they
word this let's say that according to
medical report provided by the hospital
states that around 10 percent of all
patients they treated suffered long lung
disease so we have kind of a generic
medical report they further found out by
a survey that 15 percent of the patients
that visit them smoke
so we have 10 percent that are lung
disease and 15 percent of the patients
smoke and finally five percent of the
people continued smoke even when they
had lung disease not the brightest
choice but you know it is an addiction
so it can be really difficult to kick
and so we can look at the probability of
a uh prior probability of 10 people
having lung disease
and then probability B probability that
a patient smokes is 15 percent
uh and the probability of B if B then a
the probability of a patient smokes even
though they have lung disease is five
percent
and probability of a is B probability
that the patient will have lung disease
if they smoke and then we put the
formulas together you get a nice
solution here you get the probability of
a of B probability that the patient will
have lung disease if they smoke
and you can just plug the numbers right
in and we get a 3.33 percent chance
hence there is a 3.33 chance that a
person who smokes will get a lung
disease
so we're gonna pull up a little python
code and always my favorite roll up the
sleeves
keep in mind we're going to be doing
this
um kind of like the back end way
so that you can see what's going on and
then later on we're going to create
um we'll get into another demo which
shows you some of the tools are already
pre-built for this
let's start by creating a set so we're
going to create a set with curly braces
this means that our set has only unique
values so you have a list you have your
tuples which can never change and then
you have in this case the the set so
four seven you can't create a four seven
comma four it'll delete the four out so
it's only unique values and if you use
dictionaries
quick reminder this should look familiar
because it is a dictionary where you
have a value and that value is assigned
to or that key is assigned to a value
uh so you could have a key value set up
as a dictionary so it's like a
dictionary without the value it's just
the keys and they all have to be unique
and if we run this we have a set of four
seven
we can also take a list of regular setup
and I'm going to go ahead and just throw
in another number in here four
and run it and you can see here if I
take my list one two three four four and
I convert it to a set and here it is my
set from list equals set my list
the result is one two three four so it
just deletes that last four right out of
there
and with the sets you can also go in
there and print here is my set my set
three is in the set and then if you do
three in my set
that's going to be a logic function and
one in my set 6 is not in the set and so
forth if we run this
we get three is in the set true one is
in the set false because three five
seven is another one six is in the set 6
is not in the set so not in my set
you can also use this with a list we
could have just used 357 and it would
have the same response on there is three
and usually you do if three is in but
three in my set is still works on just a
regular list
and we'll go ahead and do a little
iteration we're going to do kind of the
dice one remember
um uh one two three four five six and so
we're going to bring in an iteration
tool and import product as product
and I'll show you what that means in
just a second so we have our two dice we
have dice a
and it's going to be a set of values and
you can only have one value for each one
that's why they put it in a set and if
you remember from range it is up to
seven so this is going to be one two
three four five six it will not include
the seven and the same thing for our
dice B
and then we're going to do is we're
going to create a list
which is the product
of A and B so what's a plus b
and if we go ahead and run this it'll
print that out and you'll see in this
case when they say product because it's
an iteration tool
we're talking about creating a tuple of
the two so we've now created a tuple of
all possible outcomes of the dice where
dice a is one two three one to six and
dice B is one to six and you can see one
to one one to two one to three and so
forth you remember we had a slide on
this earlier where we talked about
um the different all the different
outcomes of a dice
we can play around with this a little
bit uh we can do in dice equals two dice
faces one two three four five six
uh another way of doing what we did
before and then we can create an event
space where we have a set which is the
product of the dice faces repeat equals
end Dice and we'll go and just run this
and you can see here it just again puts
it through all the different possible
variables we can have
and then if we want to take the same set
on here and print them all out like we
had before we can just go through four
outcome and event space outcome and
equals
so the event space is creating
uh sequence and as you can see here when
we print it out it Stacks them versus
going through and putting them in a nice
line
and we'll go ahead and do something
let's go print
since we have the End Printing with a
comma that just means it's just gonna
it's not going to hit the return going
down to the next line uh and we'll go
ahead and do the links
of our event space uh I'll be an
important variable we're going to want
to know in a minute
and of course I get carried away with my
typing of length I will print it twice
and it'll give me an error so we have 36
different possible variations here
and we might want to calculate something
like
um what about the multiple of three what
if we want to have
the probability of the multiple of three
in our setup
and so we can put together the code for
the outcome and event space of X Y
equals outcome if X Plus y
remainder three so we're going to divide
by 3 and look at the remainder and it
equals zero
then it's a fable outcome we're going to
pop that outcome on the end there
and we'll turn it into a set so the
favor outcome equals a set not necessary
because we know it's not going to be
repeating itself but just in case we'll
go ahead and do that
and if we want to print out the outcome
we can go ahead and see what that looks
like and you can see here these are all
multiples of three one plus two is three
five plus four is nine which divided by
three is three and so forth
and just like we looked up the length of
the one before let's go ahead and print
the length
of our F outcome
so we can see what that looks like
there we go
and of course I did forget to add the
print in the middle because We're
looping through and putting an end on
the on the setup on there so we're going
to put the print in there and if I run
this you can see uh
we end up with 12. so we have 36 total
options
we have 12 that are multiple that add up
to a multiple of three
and we can easily convert the
probability of this by simply taking the
length of our favorable outcome over the
length of the event space
and if we print it out let me put that
in there probability
last lines we just type it in we end up
with the 0.33333 chance
and it's roughly a third
and we might want to make this look nice
so let's go ahead and put in another
line there the probability of getting
the sum which is a multiple of three is
0.33333
we can compute the same thing for five
dice
and if we do this for five dice and go
and run it you can see we just have a
huge amount of choices so just goes on
and on down here and we can look at the
length of the event space
and we have over
776 choices that's a lot of choices
and if we want to ask the question like
we did above uh what is the sum where
the sum is a multiple of five but not a
multiple of three
we can go through all of these different
options and then you can see here D1 D2
D3 D4 D5 equals the outcome and if you
add these all together and they're
division by five does not have a
remainder of zero but the remainder is
also of a division by three is not equal
to zero so the multiple of five is equal
to zero but the multiple three is not we
can just append that on here and then we
can look at that uh favorable outcome
we'll go ahead and set that and we'll
just take a look at this what's our
length
of our favorable outcome
it's always good to see what we're
working with and so we have 904 out of
770
6 and then of course we can just do a
simple division to get the probability
on here what's the probability that
we're going to roll a multiple of five
when you add them together
but not a multiple of three
and so we're just going to divide those
two numbers and you can see here we get
0.116255 or 11.62 percent
and so you can really have a nice visual
that this is not really complicated in
math right here on probabilities it's
just how many options do you have and
how many of those are you possibly going
to be able to come up with with the
solution you're looking for and this
leads us to a confusion Matrix
a confusion Matrix is a table which is
used to describe the performance of a
classification model on a set of test
data for which the True Values are known
and so you'll see on the left we have
the predicted and the actual and we have
a negative false negative positive true
positive
and then we have false positive and true
negative and you can think of this as
your predicted model what does that mean
that means if you divided your data and
you used two-third of us to create the
model
you might then test it against an actual
case for the last third and see how well
it comes out how many times was it true
positive versus uh
false positive we get a false positive
response and you can imagine in medical
situations this is a pretty big deal you
don't want to give a false positive so
you might adjust your model accordingly
so you don't have a false positive say
with the covet virus test it'd be better
to have a false negative when they go
back and get retested then to have
thirty percent false positives where
then the test is pretty much invalid
so in a use case like cancer prediction
let's consider an example where a cancer
prediction model is put to the test for
its accuracy and precision actual result
of a person's medical report is compared
with the prediction made by the machine
learning model
and so you can see here here's our
actual predicted whether they have
cancer or not you know cancer a big one
you don't want to have a false positive
I mean a false negative in other words
you don't want to have it tell you that
you don't have cancer when you do so
that would be something you'd really be
looking for in this particular domain
you don't want a false negative
and this is again you know you've
created a model you have hundreds of
people or thousands of pieces of data
that come in there's a real famous case
study where they have the imagery and
all the measurements they take and
there's about 36 different measurements
they take and then if you run the a
basic model you want to know just how
accurate is how many negative results do
you have that are either telling people
they have cancer that don't or telling
people that don't have cancer that they
do and then we can take these numbers
and we can feed them into our accuracy
our precision and our recall
so accuracy precision and recall
accuracy metric to measure how
accurately the results are predicted
and this is your total true where you
got the right results you add them
together the true positive the true
negative over all the results so what
percentage of them were accurate versus
what were wrong
we're talking about Precision is a
metric to measure how many of the
correctly predicted cases are actually
turned out to be positive
uh so we have a Precision on true
positive again if you're talking about
like uh coveted testing with the viruses
uh you really want this to be a high
number you want this true that to be the
center point where you might have the
opposite if you're dealing with cancer
where you want no false negatives
uh so this is your metric on here
Precision is your test positive true
positive plus false positive
and then your recall how many of the
actual positive cases we were able to
predict quickly with our model uh so
test positive is a test positive plus
the false negative on there and we'll
want to go ahead and do a demo on the
naive Bayes classifier before I get too
far into uh naive Bay's classifier
because we're going to pull it from the
SK learn or the scikit let's go ahead
kind of an interesting page here for
classifiers when you go into the SK
learn kit there's a lot of ways to do
classification and I'll just zoom up in
here so you can see some of the titles
uh there's everything from the nearest
neighbor linear
but we're going to be focusing on the
naive Bayes over here and this is just a
sample data set that they put together
and you can see how some of these have a
very different output the naive Bayes
remember is set up as probably the most
simplified calculator or setup sections
out there and so what we've been talking
about with the true false and stuff like
that where there's a
an A belief that there is a independent
assumption between the features where
the features are very assumed to have
some kind of connection then we can go
ahead and use that for the prediction
and so that's what we're using is a
naive Bayes classifier versus many of
the other classifiers that are out there
for this we're going to use uh The
Social Network ads it's a little data
set on here and let me go and just open
that up the file
here we go it has user ID gender age
estimated salary uh purchased and so we
have you can see the user ID mail 19
estimated salary 19
000 and purchase zero so it's either
going to make a purchase or not so look
at that last one 0 1 we should be
thinking of binomials we should be
thinking of a simple naive Bayes
classifier kind of setup
so if we close this out we're going to
go ahead and import our numpy as NP
would be nice to have a good visual of
our data so we'll put in our matplot
library
here's our pandas our data frame
and then we're going to go ahead and
import the data set and the data set is
going to be we're going to read it from
The Social Network ads.csv then we're
going to print the head just so you can
see it again even though I showed you it
in the file and x equals the data set I
location
two three values and Y is going to be
the 4 column four let me just run this
so it's a little easier to go over that
you can see right here we're going to be
looking at 0 1 2 is age and estimated
salary so two three
and that's what I location just means um
that we're looking at the number versus
a regular location a regular location
you'd actually say age and estimated
salary
and then column four is did they make a
purchase they purchased something
uh so those are the three columns we're
going to be looking at when we do this
and we've gone ahead and imported these
and imported the data so now our data
set is all set with this information in
it
and we'll need to go ahead and split the
date up so we need our from the SK learn
model selection we can import train test
split this does a nice job we can set
the random state so it randomly picks
the data and we're just going to take 25
percent of it's going to go into the
test our X test and our y test and the
75 will go to x train and Y train that
way once we create our model we can then
have data to see just how accurate or
how well it has performed with our
prediction
the next step in pre-processing our data
is to go ahead and do feature scaling
now a lot of this to start to look
familiar if you've done a number of the
other modules in setup you should start
noticing that we bring in our data we
take a look at what we're working with
we go ahead and split it up into
training and testing
uh in this case we're going to go ahead
and scale it scale it means we're
putting it between a value of minus one
and one
or someplace in the Middle Ground there
this way if you have any huge set you
don't have this huge um setup if we go
back up to here where salary salary is
20 000 versus age 35. well there's a
good chance with a lot of the back end
math that 20 000 will skew the results
and the estimated salary will have a
higher impact than the age instead of
balancing them out and letting the
calculations weigh them properly
and finally we get to actually create
our naive Bayes model
and then we're going to go ahead and
import the gaussian naive Bays
and the gaussian is is the most basic
one that's what we're looking at now it
turns out though if you go to the SK
learn kit they have a number of
different ones you can pull in there
there's a
um Bernoulli I don't know I've never
used that one categorical
um complement and here's our gaussian so
there's a number of different options
you can look at gaussian when you come
to the naive Bayes is the most commonly
used so we're talking about the naive
Bayes that's usually what people are
talking about when they when they're
pulling this in and one of the nice
things about the gaussian if you go to
their website to sklearn the naibes
gaussian there's a lot of cool features
one of them is you can do partial fit on
here that means if you have a huge
amount of data you don't have to process
it all at once you once you can batch it
into the gaussian NB model and there's
many other different things you can do
with it as far as fitting the data and
how you manipulate it
we're just doing the basics so we're
going to go ahead and create our
classifier we're going to equal the
gaussian NB
and then we're going to do a fit we're
going to fit our training data and our
training solution so X train y train
and we'll go ahead and run this and it's
going to tell us that it ran the code
right there
and now we have our trained classifier
model
so the next step is we need to go ahead
and run a prediction we're going to do
our y predict equals the
classifier.predict X test so here we fit
the data and now we're going to go ahead
and predict
and now we get to our confusion Matrix
so from the SK learn Matrix metrics you
can import your confusion Matrix
just as saves you from doing all the
simple math that does it all for you and
then we'll go ahead and create our
confusion metrics with the Y test and
the Y predict so we have our actual and
we have our predicted value
and you can see from here this is the
chart we looked at here's predicted so
true positive false positive false
negative true negative
and if we go ahead and run this there we
have it 65 3725
and in this particular prediction we had
65 were predicted the truth as far as a
purchase they're going to make a
purchase and we guess three wrong
and then we had 25 we predicted would
not purchase and seven of them did so
there's our our confusion Matrix
at this point if you were with your
shareholders or a board meeting you
would start to hear some snoozing if
they were looking at the numbers and you
say hey here's my confusion Matrix
so let's go ahead and visualize the
results
we're going to pull from the matplot
library colors import listed color map
and this is actually my machine's going
to throw an error because this is being
um
because of the way the setup is I have a
newer version on here than when they put
together the demo and we need our xset
and our y set which is our X train and Y
train
and then we'll create our X1 X2
and we'll put that into a grid and we
send our xset minimum stop and our xat
Max stop and if you come all the way
over here we're going to step 0.01 this
is going to give us a nice line is what
that's doing and then we're going to
plot the Contour plot the X limit plot
the Y limit and put the scatter plot in
there let's go ahead and run this uh to
be honest when I'm doing these graphs
there's so many different ways to do
that there's so many different ways to
put this code together
to show you what we're doing it's a lot
easier to pull up the graph and then go
back up and explain it
so the first thing we want to note here
when we're looking at the data
is this is the training set
and so we have those who didn't make a
purchase we've drawn a nice area for
that that's defined by the naive Bayes
setup and then we have those who did
make a purchase the green and you can
see that some of the green Drops Fall
Into the red area and some of the red
dots fall into the green so even our
training set isn't going to be a hundred
percent we couldn't do that
and so we're looking at our different
data coming down
we can kind of arrange our X1 X2 so we
have a nice plot going on and we're
going to create the Contour
that's that nice line that's drawn down
the middle on here with the red green
that's where that's what this is doing
right here with the reshape and notice
that we had to uh
do the dot T if you remember from numpy
if you did the numpy module you end up
with pairs you know X X1 X2 X 1 X2 next
row and so forth you have to flip it so
it's all one row you have all your X
ones and all your x2s so that's what
we're kind of looking for right here on
this setup
uh and then the scatter plot is of
course your scattered data across there
we're just going through all the points
that puts these nice little dots onto
our setup on here and we have our
estimated salary and our age and then of
course the dots are did they make a
purchase or not
and just a quick note this is kind of
funny you can see up here where it says
xset yset equals uh X train y train
which seems kind of a little weird to do
this is because this is probably
originally a definition so it's this own
module that could be called over and
over again
and which is really a good way to do it
because the next thing we're going to
want to do is do the exact same thing
but we're going to visualize the test
set results that way we can see what
happened with our test group our 25
percent
and you can see down here we have the
test set and it if you look at the two
graphs next to each other this one
obviously has 75 percent of the data so
it's going to show a lot more
this is only 25 percent of the data you
can see that there's a number that are
kind of on the edge as to whether they
could guess by age and income they're
going to make a purchase or not but that
said it still is pretty clear it's
pretty good as far as how much the
estimate is and how good it does
now graphs are really effective for
showing people what's going on but you
also need to have the numbers and so
we're going to do from sklearn we're
going to import metrics
and then we're going to print our
metrics classification report from the Y
test and the Y predict
and you can see here we have Precision
Precision of zeros is 90 there's our
recall
0.96 we have an F1 score and a support
and we have our Precision the recall on
getting it right and then we can do our
accuracy the macro average and the
weighted average so you can see it it
pulls in pretty good as far as how
accurate it is you could say it's going
to be about 90 percent is going to guess
correctly
um that it that they're not going to
purchase and we had an 89 chance that
they are going to purchase and then the
other numbers as you get down
have a little bit different meaning but
it's pretty straightforward on here
here's our accuracy and here's our micro
average and the weighted average and
everything else you might need and if
you forgot the exact definition of
accuracy it is the true positive true
negative over all of the different
setups Precision is your true positive
overall positives true and false and
recall is a true positive over truth
positive plus false negative
and we can just real quick flip back
there
so you can see those numbers on here
here's our Precision here's our recall
and here's our accuracy on this hey
there Lana simply land brings you a
comprehensive professional certificate
program in Ai and machine learning from
Purdue University that is in
collaboration with IBM that will cover a
wide range of topics that will Empower
you with the knowledge and skills needed
to excel in the field of AI to learn
more about this course you can find the
course link mentioned in the description
box
machine learning has improved our lives
in a number of wonderful ways today
let's talk about some of these I'm Rahul
from Simply learn and these are the top
10 applications of machine learning
first let's talk about virtual personal
assistance Google Assistant Alexa
Cortana and Siri now we've all used one
of these at least at some point in our
lives now these help improve our lives
in a great number of ways for example
you could tell them to call someone you
could tell them to play some music you
could tell them to even schedule an
appointment so how do these things
actually work first they record whatever
you're saying send it over to a server
which is usually in a cloud decoded with
the help of machine learning in neural
networks and then provide you with an
output so if you ever noticed that these
systems don't work very well without the
internet that's because the server
couldn't be contacted next let's talk
about traffic predictions now say I
wanted to travel from Buckingham Palace
to Lord's cricket ground the first thing
I'd probably do is to get on Google Maps
so search it
and let's put it here
so here we have the path you should take
to get to large cricket ground now here
the map is a combination of red yellow
and blue now the blue regions signify a
clear road that is you won't encounter
traffic there yellow indicate that they
are slightly congested and red means
they are heavily congested so let's look
at the map a different version of the
same map and here as I told you before
red means heavily congested yellow means
slow moving and blue means clear
so how exactly is Google able to tell
you that the traffic is clear slow
moving or heavily congested so this is
the help of machine learning and with
the help of two important measures first
is the average time that's taken on
specific days at specific times on that
route the second one is the real-time
location data of vehicles from Google
Maps and with the help of sensors some
of the other popular map services are
Bing Maps maps.me and here we go next up
we have social media personalization so
say I want to buy a drone and I'm on
Amazon and I want to buy a DJI mavic Pro
the thing is it's close to one lap so I
don't want to buy it right now but the
next time I'm on Facebook I'll see an
advertisement for the product next time
I'm on YouTube I'll see an advertisement
even on Instagram I'll see an
advertisement so here with the help of
machine learning Google has understood
that I'm interested in this particular
product hence it's targeting me with
these advertisements this is also with
the help of machine learning let's talk
about email spam filtering now this is a
spam that's in my inbox now how does
Gmail know what spam and what's not spam
so Gmail has an entire collection of
emails which have already been labeled
as spam or not spam so after analyzing
this data Gmail is able to find some
characteristics like the word lottery or
winner from then on any new email that
comes to your inbox goes through a few
spam filters to decide whether it's spam
or not now some of the popular spam
filters that Gmail uses is content
filters header filters General Blacklist
filters and so on next we have online
fraud detection now there are several
ways that online fraud can take place
for example there's identity theft where
they steal your identity fake accounts
where these accounts only last for how
long the transaction takes place and
stop existing after that and man in the
middle attacks where they steal your
money while the transaction is taking
place the feed forward neural network
helps determine whether a transaction is
genuine or fraudulent so what happens
with feed forward neural networks are
that the outputs are converted into hash
values and these values become the
inputs for the next round so for every
real transaction that takes place
there's a specific pattern a fraudulent
transaction would stand out because of
the significant changes is that it would
cause with the hash values Stock Market
trading machine learning is used
extensively when it comes to Stock
Market trading now you have stock market
indices like nikai they use long
short-term memory neural networks now
these are used to classify process and
predict data when there are time lags
the one known size and duration now this
is used to predict stock market trends
assist to Medical Technology now medical
technology has been innovated with the
help of machine learning diagnosing
diseases has been easier from which we
can create 3D models that can predict
where exactly there are lesions in the
brain it works just as well for brain
tumors and Ice chemic stock lesions they
can also be used in fetal Imaging and
cardiac analysis now some of the medical
fields that machine learning will help
assist in is disease identification
personalized treatment drug Discovery
clinical research and radiology and
finally we have automatic translation
now say you're in a foreign country and
you see Billboards and signs that you
don't understand that's where automatic
translation comes of help now how does
automatic translation actually work the
technology behind it is the same as the
sequence of sequence learning which is
the same thing that's used with chatbots
here the image recognition happens using
convolutional neural networks and the
text is identified using optical
character recognition furthermore the
sequence to sequence algorithm is also
used to translate the text from one
language to the other so without much
further Ado let's get started so deep
learning is considered to be a part of
machine learning so this diagram very
nicely depicts what deep learning is at
a very high level you have the
all-encompassing artificial intelligence
which is more a concept rather than a
technology or a technical concept right
so it is it's more of a concept at a
very high level artificial intelligence
under the herd is actually machine
learning and deep learning and machine
learning is a broader concept you can
say or a broader technology and deep
learning is a subset of machine learning
the primary difference between machine
learning and deep learning is that deep
learning uses neural networks and it is
suitable for handling large amounts of
unstructured data and last but not least
one of the major differences between
machine learning and deep learning is
that in machine learning the feature
extraction or the feature engineering is
done by the data scientists manually but
in deep learning since we use neural
networks the feature engineering happens
automatically so that's a little bit of
a quick difference between machine
learning and deep learning and this
diagram very nicely depicts the relation
between artificial intelligence machine
learning and deep learning now why do we
need deep learning machine learning was
there for quite some time and it can do
a lot of stuff that probably what deep
learning can do but it's not very good
at handling large amounts of
unstructured data like images Voice or
even text for that matter so traditional
machine learning is not that very good
at doing this traditional machine
learning can handle large amounts of
structured data but when it comes to
unstructured data it's a big challenge
so that is one of the key
differentiators for deep learning so
that is number one and increasingly for
artificial intelligence we need image
recognition and we need to process
analyze images and voice that's recent
deep learning is required compared to
let's say traditional machine learning
it can also perform complex algorithms
more complex than let's say what machine
learning can do and it can achieve best
performance with the large amounts of
data so the more you have the data let's
say reference data or label data the
better the system will do because the
training process will be that much
better and last but not least with deep
learning you can really avoid the manual
process of feature extraction those are
some of the reasons why we need deep
learning some of the applications of
deep learning deep learning has made
major inroads and it is a major area in
which deep learning is applied is
Healthcare and within Healthcare
particularly oncology which is basically
cancer related stuff one of the issues
with cancer is that a lot of cancers
today are curable they can be cured they
are detected early on and the challenge
with that is when a Diagnostics is
performed let's say an image has been
taken of a patient to detect whether
there is cancer or not you need a
specialist to look at the image and
determine whether it is the patient is
fine or there is any onset of cancer and
the number of Specialists are limited so
if we use deep learning if we use
automation here or if we use artificial
intelligence here then the system can
with a certain amount of the good amount
of accuracy determine whether a
particular patient is having cancer or
not so the prediction or the detection
process of a disease like cancer can be
expedited the detection process can be
expedited can be faster without really
waiting for a specialist we can
obviously then once the application once
the artificial intelligence detects or
predicts that there is an onset of a
cancer this can be cross-checked by a
doctor but at least the initial
screening process can be automated and
that is where the current focus is with
respect to deep learning in healthcare
what else robotics is another area deep
learning is majorly used in robotics and
you must have seen nowadays robots are
everywhere humanoids the industrial
robots which are used for manufacturing
process you must have heard about Sophia
who got a citizenship with Saudi Arabia
and so on there are multiple such robots
which are knowledge oriented but there
are also industrial robots are used in
Industries in the manufacturing process
and increasingly in security and also in
defense for example image processing
video is fed to them and they need to be
able to detect objects obstacles and so
on and so forth so that's where deep
learning is used they need to be able to
hear and make sense of the sounds that
they are hearing that needs deep
learning as well so robotics is a major
area where deep learning is applied then
we have self-driving cars or autonomous
cars you must have heard of Google's
autonomous car which has been tested for
millions of miles and pretty much
incident free there were of course a
couple of incidents here and there but
it is considered to be fairly safe and
there are today a lot of Automotive
companies in fact pretty much every
automotive company worth its name is
investing in self-driving cars or
autonomous cars and it is predicted that
in the next probably 10 to 15 years
these were will be in production and
they will be used extensively in real
life right now they are all in r d and
in test phases but pretty soon these
will be on the road so this is another
area where deep learning is used and how
is it used where is it used within
autonomous driving the car actually is
fed with video of surroundings and it is
supposed to process that information
process that video and determine if
there are any obstacles it has to
determine if there are any cars in the
site will detect whether it is driving
in the lane also it has to determine
whether the signal is green or red so
that accordingly it can move forward or
wait so for all these video analysis
deep learning is used in addition to
that the training overall training to
drive the car happens in a deep learning
environment so again a lot of scope here
to use deep learning couple of other
applications are mission translation so
today we have a lot of information and
very often this information is in one
particular language and more
specifically in English and people need
information in various parts of the
world it is pretty difficult for human
beings to translate each and every piece
of information or every document into
all possible languages there are
probably at least hundreds of languages
or if not more to translate each and
every document into every language is
pretty difficult therefore we can use
deep learning to do pretty much like a
real-time translation mechanism so we
don't have to translate everything and
keep it ready but we train applications
or artificial intelligence systems that
will do the translation on the Fly for
example you go to somewhere like China
and you want to know what is written on
a signboard that is impossible for
somebody to translate that and put it on
the web or something like that so you
have an application which is train to
translate stuff on the fly so you
probably this can be running on your
mobile phone on your smartphone you scan
this the application will instantly
translate that from Chinese to English
that is one then there could be web
applications where there may be a
research document which is all in maybe
Chinese or Japanese and you want to
translate that to study that document or
in that case you need to translate so
therefore deep learning is used in such
situations as well and that is again on
demand so it is not like you have to
translate all these documents from other
languages into English in one shot and
keep it somewhere that is again and
pretty much an impossible task but on a
neat basis so you have systems that are
trained to translate on the fly so
Mission translation is another major
area where deep learning is used then
there are a few other upcoming areas
where synthesizing is done by neural
nets for example music composition and
generation of music so you can train a
neural net to produce music even to
compose music so this is a fun thing
this is still upcoming it needs a lot of
effort to train such neural land it has
been proved that it is possible so this
is a relatively new area and on the same
lines colorization of images so these
two images on the left hand side is a
grayscale image or a black and white
image this was colored by a neural net
or a deep learning application as you
can see it's done a very good job of
applying the colors and obviously this
was trained to do this colorization but
yes this is one more application of deep
learning what are the different types of
machine learning algorithms machine
learning algorithms are broadly
classified into three types the
supervised learning unsupervised
learning and reinforcement learning
supervised learning in turn consists of
techniques like regression and
classification and unsupervised learning
we use techniques like Association and
clustering and reinforcement learning is
the recently developed technique and it
is very popular in gaming some of you
must have heard about alphago so this
was developed using reinforcement
learning primary difference between
supervised learning and unsupervised
learning supervised learning is used
when we have historical data and we have
labeled data which means that we know
how the data is classified so we know
the classes if we are doing
classification or we know the values
when we are doing regression so if we
have historical data with these values
which are known as labels then we use
supervised learning in case of
unsupervised learning we do not have
past labeled data historical labeled
data so we use techniques like
Association and clustering to maybe form
clusters or new classes maybe and then
we move from there in case of
reinforcement learning the system learns
pretty much from scratch there is an
agent and there is an environment the
agent is given a certain Target and it
is rewarded when it is moving towards
that Target and it is penalized if it is
moving in a direction which is not
achieving that Target so it's more like
a carrot and stick model so what is the
difference between these three types of
algorithms supervised algorithms or
supervised learning algorithms are used
when you have a specific Target value
that you would like to predict the
target could be categorical having two
or more possible outcomes or classes if
you will that is what is classification
or the target could be a value which can
be measured and that's where we use
regression like for example weather
forecasting you want to find the
temperature whereas in classification
you want to find out whether this is a
fraud or not a fraud or if it is email
spam whether it is Spam or not spam so
that is a classification example so if
you know or doesn't is known as labeled
information if you have the labeled
information then you use supervised
learning in case of unsupervised
learning we have input data but we don't
have the labels or what the output is
supposed to be so that is when we use
unsupervised learning techniques like
clustering and Association and we try to
analyze the data in case of
reinforcement learning it allows the
agent to automatically determine the
ideal Behavior within a specific context
and it has to do this to maximize the
performance like for example playing a
game so the agent is told that you need
to score the maximum score possible
without losing lives so that is a Target
that is given to the agent and it is
allowed to learn from scratch play the
game itself multiple times and slowly it
will learn the behavior which will
increase the score and keep the lives to
the maximum that's an example of
reinforcement learning we look at a
different machine learning algorithms we
can divide them into three areas
supervised unsupervised reinforcement
we're only going to look at supervise
today unsupervised means we don't have
the answers we're just grouping things
reinforcement is where we give positive
and negative feedback to our algorithm
to program it and it doesn't have the
information until after the fact but
today we're just looking at supervised
because that's where linear regression
fits in in supervised data we have our
data already there and our answers for a
group and then we use that to program
our model and come up with an answer the
two most common uses for that is through
the regression and classification now
we're doing linear regression so we're
just going to focus on the regression
side and in the regression we have
SIMPLE linear regression we have
multiple linear regression and we have
polynomial linear regression now on
these three simple linear regression is
the examples we've looked at so far
where we have a lot of data and we draw
a straight line through it multiple
linear regression means we have multiple
variables remember where we had the
rainfall and the crops we might add
additional variables in there like how
much food do we give our crops when do
we Harvest them those would be
additional information add into our
model and that's why it'd be multiple
linear regression and finally we have
polynomial linear regression that is
instead of drawing a line we can draw a
curved line line through it now that you
see where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
be the price of a product in the future
we can guess whether it's going to go up
or down or should I buy today housing
sales to estimate the number of houses a
builder would sell and what price in the
coming months score predictions Cricket
fever to predict the number of runs a
player would score in the coming matches
based on the previous performance I'm
sure you can figure out other
applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory understanding linear
regression linear regression is the
statistical model used to predict the
relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get since linear regression
is based on drawing a line through data
we're going to jump back and take a look
at some euclidean geometry the simplest
form of a simple linear regression
equation with one dependent and one
independent variable is represented by y
equals m times X plus C if you look at
our model here we plotted two points on
here X1 and y1 X2 and Y2 y being the
dependent variable remember that from
before and next being the independent
variable so y depends on whatever X is m
in this case is the slope of the line
where m equals the difference in the Y2
minus y1 and X2 minus X1 and finally we
have C which is the coefficient of the
line or where it happens to cross the
zero axes let's go back and look at an
example we used earlier of linear
regression we're going to go back to
plotting the amount of crop yield based
on the amount of rainfall and here we
have our rainfall remember we cannot
change rainfall and we have our crop
yield which is dependent on the rainfall
so we have our independent and our
dependent variables we're going to take
this and draw a line through it as best
we can through the middle of the data
and then we look at that we put the red
point on the y-axis is the amount of
crop yield you can expect for the amount
of rainfall represented by the Green Dot
so if we have an idea what the rainfall
is for this year and what's going on
then we can guess how good our crops are
going to be and we've created a nice
line right through the middle to give us
a nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive into the math and the formulas that
go behind this and what's going on
behind the scenes
I want you to note that when we get into
the case study and we actually apply
some python script that this math you're
going to see here is already done
automatically for you you don't have to
have it memorized it is however good to
have an idea what's going on so if
people reference the different terms
you'll know what they're talking about
let's consider a sample data set with
five rows and find out how to draw the
regression line we're only going to do
five rows because if we did like the
rainfall with hundreds of points of data
that would be very hard to see what's
going on with the mathematics so we'll
go ahead and create our own two sets of
data and we have our independent
variable X and our dependent variable Y
and when X was one we got y equals 2
when X was 2 y was 4 and so on and so on
if we go ahead and plot this data on a
graph we can see how it forms a nice
line through the middle you can see
where it's kind of grouped going upwards
to the right the next thing we want to
know is whether the means is of each of
the data coming in the X and the Y the
means doesn't mean anything other than
the average so we add up all the numbers
and divide by the total so one plus two
plus three plus four plus five over five
equals three and the same for y we get
four if we go ahead and plot the means
on the graph we'll see we get three
comma four which draws a nice line down
the middle a good estimate here we're
going to dig deeper into the math behind
the regression line now remember before
I said you don't have to have all these
formulas memorized or fully understand
them even though we're going to go into
a little more detail of how it works and
if you're not a math whiz and you don't
know if you've never seen the sigma
character before which looks a little
bit like an e that's opened up that just
means summation that's all that is so
when you see the sigma character it just
means we're adding everything in that
row and for computers this is great
because as a programmer you can easily
iterate through each of the X Y points
and create all the information you need
so in the top half you can see where
we've broken that down into pieces and
as it goes through the first two points
it computes the squared value of x the
squared value of y and x times Y and
then it takes all of X and adds them up
all of Y adds them up all of x squared
adds them up and so on and so on and you
can see we have the sum of equal to 15
the sum is equal to 20 all the way up to
x times Y where the sum equals 66 6.
this all comes from our formula for
calculating a straight line where y
equals the slope times X plus the
coefficient C so we go down below and
we're going to compute more like the
averages of these and we'll explain
exactly what that is in just a minute
and where that information comes from is
called the square means error but we'll
go into that in detail in a few minutes
all you need to do is look at the
formula and see how we've gone about
Computing it line by line instead of
trying to you have a huge set of numbers
pushed into it and down here you'll see
where the slope m equals and then the
top part if you read through the
brackets you have the number of data
points times the sum of x times Y which
we computed one line at a time there and
that's just the 66 and take all that and
you subtract it from the sum of x times
the sum of Y and those have both been
computed so you have 15 times 20 and on
the bottom we have the number of lines
times the the sum of x squared easily
computed as 86 for the sum minus I'll
take all that and subtract the sum of x
squared and we end up as we come across
with our formula you can plug in all
those numbers which is very easy to do
on the computer you don't have to do the
math on a piece of paper or calculator
and you'll get a slope of 0.6 and you'll
get your C coefficient if you continue
to follow through that formula you'll
see it comes out as equal to 2.2
continuing deeper into what's going
behind the scenes let's find out the
predicted values of Y for corresponding
values of X using the linear equation
where m equals 0.6 and C equals 2.2
we're going to take these values and
we're going to go ahead and plot them
we're going to predict them so y equals
0.6 times where x equals 1 plus 2.2
equals 2.8 and so on and so on and here
the Blue Points represent the actual y
values and the brown points represent
the predicted y values based on the
model we created the distance between
the action show and predicted values is
known as residuals or errors the best
fit line should have the least sum of
squares of these errors also known as
e-square if we put these into a nice
chart we can see X and you can see why
what the actual values were and you can
see why I predicted you can easily see
where we take y minus y predicted and we
get an answer what is the difference
between those two and if we square that
y minus y prediction squared we can then
sum those squared values that's where we
get the 0.64 plus the 0.36 plus 1 all
the way down until we have a summation
equals 2.4 so the sum of squared errors
for this regression line is 2.4 we check
this error for each line and conclude
the best fit line having the least e
Square value in a nice graphical
representation we can see here where we
keep moving this line through the data
points to make sure the best fit line
has the least squared distance between
the data points and the regression line
now we we only looked at the most
commonly used formula for minimizing the
distance there are lots of ways to
minimize the distance between the line
and the data points like sum of squared
errors sum of absolute errors root mean
square error Etc which you want to take
away from this is whatever formula is
being used you can easily using a
computer programming and iterating
through the data calculate the different
parts of it that way these complicated
formulas you see with the different
summations and absolute values are
easily computed one piece at a time up
until this point we've only been looking
at two values X and Y well in the real
world it's very rare that you only have
two values when you're figuring out a
solution so let's move on to the next
topic multiple linear regression let's
take a brief look at what happens when
you have multiple inputs so in multiple
linear regression we have well we'll
start with the simple linear regression
where we had y equals M plus X plus C
and we're trying to find the value of y
now with multiple linear regression we
have multiple variables coming in so
instead of having just X we have X1 X2
X3 and instead of having just one slope
each variable has its own slope attached
to it as you can see here we have M1 M2
M3 and we still just have the single
coefficient so when you're dealing with
multiple linear regression you basically
take your single linear regression and
you spread it out so you have y equals
M1 times X1 plus M2 times X2 so on all
the way to m to the nth x to the nth and
then you add your coefficient on there
implementation of linear regression now
we get into my favorite part let's
understand how multiple linear
regression works by implementing it in
Python if you remember before we were
looking at a company and just based on
its R and D trying to figure out its
profit we're going to start looking at
the expenditure of the company we're
going to go back to that we're going to
predict as profit but instead of
predicting it just on the r d we're
going to look at other factors like
Administration costs marketing costs and
so on and from there we're going to see
if we can figure out what the profit of
that company is going to be to start our
coding we're going to begin by importing
some basic libraries and we're going to
be looking through the data before we do
any kind of linear regression we're
going to take a look at the data to see
what we're playing with then we'll go
ahead and format the data to the format
we need to be able to run it in the
linear regression model and then from
there we'll go ahead and solve it and
just see how valid our solution is so
let's start with importing the basic
libraries now I'm going to be doing this
in Anaconda Jupiter notebook a very
popular IDE I enjoy such a visual to
look at and so easy to use just any ID
for python will work just fine for this
so break out your favorite python IDE so
here we are in our Jupiter notebook let
me go ahead and paste our first piece of
code in there and let's walk through
what libraries were importing first
we're going to import numpy as in p and
then I want you to skip one line and
look at import pandas as PD these are
very common tools that you need with
most of your linear regression the numpy
which stands for number python is
usually denoted as NP and you have to
almost have that for your SK learn
toolbox so you always import that right
off the beginning pandas although you
don't have to have it for your sklearn
libraries it does such a wonderful job
of importing data setting it up into a
data frame so we can manipulate it
rather easily and it has a lot of tools
also in addition to that so we usually
like to use the pandas when we can and
I'll show you what that looks like the
other three lines are for us to get a
visual of this data and take a look at
it so we're going to import
matplotlibrary.pi plot as PLT and then
Seaborn as SNS Seabourn works with the
matplot library so you have to always
import matplot library then Seabourn
sits on top of it and we'll take a look
at what that looks like you could use
any of your own plotting libraries you
want there's all kinds of ways to look
at the data these are just very common
ones and the Seabourn is so easy to use
it just looks beautiful it's a nice
representation that you can actually
take and show somebody and the final
line is the Amber signed matplot library
in line that is only because I'm doing
an inline IDE my interface in the
Anaconda Jupiter notebook requires I put
that in there or you're not going to see
the graph when it comes up let's go
ahead and run this it's not going to be
that interesting so we're just setting
up variables in fact it's not going to
do anything that we can see but it is
importing these different libraries and
setup the next step is load the data set
and extract independent and dependent
variables now here in this slide you'll
see companies equals pd.read CSV and it
has a long line there with the file at
the end one thousand companies.csv
you're going to have to change this to
fit whatever setup you have and the file
itself you can request just go down to
the commentary below this video and put
a note in there and simply learn we'll
try to get in contact with you and
Supply you with that file so you can try
this coding yourself so we're going to
add this code in here and we're going to
see that I have companies equals
pd.reader underscore CSV and I've
changed this path to match my computer C
colon slash simply learn slash 1000
underscore companies.csv and then below
there we're going to set the x equals to
companies under the I location and
because this is companies is a PD data
set I can use this nice notation that
says take every row that's what the
colon the first colon is comma except
for the last column that's what the
second part is where we have a colon
minus one and we want the values set
into there so X is no longer a data set
a panda's data set but we can easily
extract the data from our pandas data
set with this notation and then y we're
going to set equal to the last row well
the question is going to be what are we
actually looking at so let's go ahead
and take a look at that and we're going
to look at the companies dot head which
lists the first five rows of data and
I'll open up the file in just a second
so you can see where that's coming from
but let's look at the data in here as
far as the way the pandas sees it when I
hit run you'll see it breaks it out into
a nice setup this is what pandas one of
the things pandas is really good about
is it looks just like an Excel
spreadsheet you have your rows and
remember when we're programming we
always start with zero we don't start
with one so it shows the first five rows
zero one two three four and then it
shows your different columns R and D
spend Administration marketing spend
State profit it even notes that the top
are column names it was never told that
but pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
opened it up as a text editor and you
can see at the top we have r d spend
comma Administration comma marketing
spin comma State comma profit carries
return I don't know about you but I'd go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is we can see the
data to me it doesn't mean a whole lot
maybe you're an expert in business and
Investments and you understand what 165
349.20 compared to the administration
cost of 136
897.80 so on so on helps to create the
profit of 192 261 and 83 cents that
makes no sense to me whatsoever no pun
intended so let's flip back here and
take a look at our next set of code
where we're going to graph it so we can
get a better understanding of our data
and what it means so at this point we're
going to use a single line of code to
get a lot of information so we can see
where we're going with this let's go
ahead and paste that into our notebook
and see what we got going and so we have
the visualization and again we're using
SNS which is hand does as you can see we
imported the matplot library.pi plot is
PLT which then the Seaborn uses and we
imported the Seaborn as SNS and then
that final line of code helps us show
this in our inline coding without this
it wouldn't display and you can display
it to a file in other means and that's
the matplot library in line with the
Amber sign at the beginning so here we
come down to the single line of code
Seaborn is great because it actually
recognizes the panda data frame so I can
just take the companies dot core for
coordinates and I can put that right
into the Seaborn and when we run this we
get this beautiful plot and let's just
take a look at what this plot means if
you look at this plot on mine the colors
are probably a little bit more purplish
and blue than the original one we have
the columns and the rows we have R and D
spending we have a demonstration we have
marketing spending and profit and if you
cross index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look look at the scale on the
right way up in the dark why because
those are the same data they have an
exact correspondence so R and D spending
is going to be the same as r d spending
and the same thing with Administration
costs so right down the middle you get
this dark row or dark diagonal row that
shows that this is the highest
corresponding data that's exactly the
same and as it becomes lighter there's
less connections between the data so we
can see with profit obviously profit is
the same as profit and next it has a
very high correlation with r d spending
which we looked at earlier and it has a
slightly less connection to marketing
spending and even less to how much money
we put into the administration so now
that we have a nice look at the data
let's go ahead and dig in and create
some actual useful linear regression
models so that we can predict values and
have a better profit now that we've
taken a look at the visualization of
this data we're going to move on to the
next step instead of just having a
pretty picture we need to generate some
hard data some hard values so let's see
what that looks like we're going to set
up our linear regression model in two
steps the first one is we need to
prepare some of our data so it fits
correctly and let's go ahead and paste
this code into our jupyter notebook and
what we're bringing in is we're going to
bring in the sklearn pre-processing
where we're going to import the label
encoder and the one hot encoder to use
the label encoder we're going to create
a variable called label encoder and set
it equal to capital L label capital E
encoder this creates a class that we can
reuse for transferring the labels back
and forth now about now you should ask
what labels are we talking about let's
go take a look at the data we processed
before and see what I'm talking about
here if you remember when we did the
companies dot head and we printed the
top five rows of data we have our
columns going across we have column 0
which is R and D spending column one
which is Administration column two which
is marketing spending and column three
is State and you'll see under State we
have New York California Florida now to
do a linear regression model it doesn't
know how to process New York it knows
how to process a number so the first
thing we're going to do is we're going
to change that New York California and
Florida and we're going to change those
to numbers that's what this line of code
does here x equals and then it has the
colon comma 3 in Brackets the first part
the colon comma means that we're going
to look at all the different rows so
we're going to keep them all together
but the only row we're going to edit is
the third row and in there we're going
to take the label coder and we're going
to fit and transform the X also the
third row so we're going to take that
third row we're going to set it equal to
a transformation and that transformation
basically tells it that instead of
having a New York it has a zero or a one
or a two and then finally we need to do
a one hot encoder which equals one hot
in order categorical features equals
three and then we take the X and we go
ahead and do that equal to one hot
encoder fit transform X to array this
final transformation preps our data
Force so it's completely set the way we
need it is just a row of numbers even
though it's not in here let's go ahead
and print X and just take a look at what
this data is doing you'll see you have
an array of arrays and then each array
is a row of numbers and if I go ahead
and just do row 0 you'll see I have a
nice organized row of numbers that the
computer now understands we'll go ahead
and take this out there because it
doesn't mean a whole lot to us it's just
a row of numbers
next on setting up our data we have
avoiding dummy variable trap this is
very important why because the computers
automatically transformed our header
into the setup and it's automatically
transformed all these different
variables so when we did the encoder the
encoder created two columns and what we
need to do is just have the one because
it has both the variable and the name
that's what this piece of code does here
let's go ahead and paste this in here
and we have x equals x colon comma one
colon all this is doing is removing that
one extra column we put in there when we
did our one hot encoder and our label
encoding let's go ahead and run that and
now we get to create our linear
regression model and let's see what that
looks like here and we're going to do
that in two steps the first step is
going to be in splitting the data now
whenever we create a predictive model of
data we always want to split it up so we
have a training set and we have a
testing set that's very important
otherwise we'd be very unethical without
testing it to see how good our fit is
and then we'll go ahead and create our
multiple linear regression model and
train it and set it up let's go ahead
and paste this next piece of code in
here and I'll go ahead and shrink it
down a size or two so it all fits on one
line so from the sklearn module
selection we're going to import train
test split and you'll see that we've
created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually reference these
when we're doing different models
usually see that a capital x and you see
the train and the test and the lowercase
Y what this is is X is our data going in
that's our RND spin our Administration
our marketing and then Y which we're
training is the answer that's the profit
because we want to know the profit of an
unknown entity so that's what we're
going to shoot for in this tutorial the
next part train test a split we take X
and we take y we've already created
those X has the columns with the data in
it and Y has a column with profit in it
and then we're going to set the test
size equals 0.2 that basically means 20
percent so twenty percent of the rows
are going to be tested we're going to
put them off to the side so since we're
using a thousand lines of data that
means that 200 of those lines we're
going to hold off to the side to test
for later and then the random State
equals zero we're going to randomize
which ones it picks to hold off to the
side we'll go ahead and run this it's
not overly exciting so setting up our
variables but the next step is the next
step we actually create our linear
regression model now that we got to the
linear regression model we get that next
piece of the puzzle let's go ahead and
put that code in there and walk through
it so here we go we're going to paste it
in there and let's go ahead and since
this is a shorter line of code let's
zoom up there so we can get a good look
and we have from the sklearn dot linear
underscore model we're going to import
linear regression now I don't know if
you recall from earlier when we were
doing all the math let's go ahead and
flip back there and take a look at that
do you remember this or we had this long
formula on the bottom and we were doing
all this summarization and then we also
looked at setting it up with the
different lines and then we also looked
all the way down to multiple linear
regression where we're adding all those
formulas together all of that is wrapped
up in this one section so what's going
on here is I'm going to create a
variable called regressor and the
regressor equals the linear regression
that's a linear regression model that
has all that math built in so we don't
have to have it all memorized or have to
compute it individually and then we do
the regressor.fet in this case we do X
train and Y train because we're using
the training data X being the data n and
y being profit what we're looking at and
this does all that math for us so within
one click and one line we've created the
whole linear regression model and we fit
the data to the linear regression model
and you can see that when I run the
regressor it gives an output linear
regression it says copy x equals True
Fit intercept equals true in jobs equal
one normalize equals false it's just
giving you some general information on
what's going on with that regressor
model now that we've created our linear
regression model let's go ahead and use
it and if you remember we kept a bunch
of data aside so we're going to do a why
predict variable and we're going to put
in the X test and let's see what that
looks like scroll up a little bit paste
that in here predicting the test set
results so here we have y predict equals
regressor dot predict X test going in
and this gives us y predict now because
I'm in Jupiter in line I can just put
the variable up there and when I hit the
Run button it'll print that array out I
could have just as easily done print y
predict so if you're in a different IDE
that's not an inline setup like the
Jupiter notebook you can do it this way
print y predict and you'll see that for
the 200 different test variables we kept
off to the side is going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a quick flash at what's going
on behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has a
coefficients for us we can simply just
print regressor dot coefficient
underscore when I run this you'll see
our coefficients here and if we can do
the regressor coefficient we can also do
the regressor intercept and let's run
that and take a look at that this all
came from the multiple regression model
and we'll flip over so you can remember
where this is going into and where it's
coming from you can see the formula down
here where y equals M1 times X1 plus M2
times X2 and so on and so on plus C the
coefficient so these variables fit right
into this formula y equals slope one
times column one variable plus slope two
times column two variable all the way to
the m into the n and x to the N plus C
the coefficient or in this case you have
minus 8.89 to the power of two etc etc
times the First Column and the second
column and the third column and then our
intercept is the minus one zero three
zero zero nine point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the r squared value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from
sklearn.metrix we're going to import R2
score that's the r squared value we're
looking at the error so in the R2 score
we take our y test versus our y predict
y test is the actual values we're
testing that was the one that was given
to us so we know are true the Y predict
of those 200 values is what we think it
was true and when we go ahead and run
this we see we get a
0.9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93 percent correct but
you do want that in the upper 90s oh and
higher shows that this is a very valid
prediction based on the R2 score and if
r squared value of 0.91 or 92 as we got
on our model remember it does have a
random generation involved this proves
the model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression so now that we
have a successful linear regression
model if you an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply lens professional
certification program in Ai and machine
learning from Purdue University in
collaboration with IBM should be a right
choice for more details use the link in
the description box below with that in
mind all right what is logistic
regression as I mentioned earlier a
plastic regression is an algorithm for
performing binary classification so
let's take an example and see how this
works let's say your car has not been
serviced for quite a few years and now
you want to find out if it is going to
break down in the near future so this is
like a classification problem find out
whether your car will break down or not
so how are we going to perform this
classification so here's how it looks if
we plot the information along the X and
Y axis X is the number of years since
the last service was performed and why
is the probability of your car breaking
down and let's say this information was
this data that was collected from
several car users it's not just your car
but several car users so that is our
labeled data so the data has been
collected and for for the number of
years and when the car broke down and
what was the probability and that has
been plotted along x and y axis so this
provides an idea or from this graph we
can find out whether your car will break
down or not we'll see how so first of
all the probability can go from 0 to 1
as you all aware probability can be
between 0 and 1. and as we can imagine
it is intuitive as well as the number of
years are on the Lower Side maybe one
year two years or three years till after
the service the chances of your car
breaking down are very limited right so
for example chances of your car breaking
down on the probability of your car
breaking down within two years of your
last service are 0.1 probability
similarly three years is maybe 0.3 and
so on but as the number of years
increases let's say if it was six or
seven years there is almost a certainty
that your car is going to break down
that is what this graph shows so this is
an example of a application of the
classification algorithm and we will see
in little details how exactly logistic
regression is applied here one more
thing needs to be added here is that the
dependent variables outcome is discrete
so if we are talking about whether the
car is going to break down or not so
that is a discrete value the why that we
are talking about the dependent variable
that we are talking about what we are
looking at is whether the car is going
to break down or not yes or no that is
what we are talking about so here the
outcome is discrete and not a continuous
way so this is how the logistic
regression curve looks let me explain a
little bit what exactly and how exactly
we are going to
determine the class or the outcome
rather so for a logistic regression
curve a threshold has to be set saying
that because this is a probability
calculation remember this is a
probability calculation and the
probability itself will not be zero or
one but based on the probability we need
to decide what the outcome should be so
there has to be a threshold like for
example 0.5 can be the threshold let's
say in this case so any value of the
probability below 0.5 is considered to
be 0 and any value above 0.5 is
considered to be 1. so an output of
let's say 0.8 will mean that the car
will break down so that is considered as
an output of 1 and let's say an output
of 0.29 is considered as 0 which means
that the car will not break down so
that's the way logistic regression works
now let's do a quick comparison between
logistic regression and linear
regression because they both have the
term regression in them so it can cause
confusion so let's try to remove the
confusion so what is linear regression
linear regression is a process is once
again an algorithm for supervised
learning however here you are going to
find a continuous value you are going to
determine a continuous value it could be
the price of a real estate property it
could be your height how much height
you're going to get or it could be a
stock price these are all continuous
values these are not discrete compared
to a yes or no kind of a response that
we are looking for in logistic
regression so this is one example of a
linear regression let's say at the HR
team of a company tries to find out what
should be the salary hike of an employee
so they collect all the details of their
existing employees their ratings and
their salary hikes what has been given
and that is the labeled information that
is available and the system learns from
us it is string and it learns from this
labeled information so that when a new
employee's information is fat based on
the rating it will determine what should
be dying so this is a linear regression
problem and a linear regression example
now salary is a continuous value you can
get five thousand five thousand five
hundred five thousand six hundred it is
not discrete like a cat or a dog or an
apple or a banana these are discrete or
a yes or a no these are discrete values
right so this way you are trying to find
continuous values is where we use linear
regression so let's say just to extend
on this scenario we now want to find out
whether this employee is going to get a
promotion or not so we want to find out
that is the discrete problem right a yes
or no kind of a problem in this case we
actually cannot use linear regression
even though we may have labeled data so
this is the label data So based on the
employee rating these are the ratings
and then some people got the promotion
and this is the ratings for which people
did not get promotion that is a no and
this is the rating for which people got
promotion we just plotted the data about
whether a person has got an employer's
got promotion or not yes no right so
there is nothing in between and what is
the employees rating okay and ratings
can be continuous that is not an issue
but the output is discrete in this case
whether employee got promotion yes no
okay so if we try to plot that and we
try to find a straight line this is how
it would look and as you can see it
doesn't look very right because looks
like there will be lot of errors this
root mean square error if you remember
for linear regression would be very very
high and also the the values cannot go
beyond zero or Beyond one so the graph
should probably look somewhat like this
clipped at 0 and but still this straight
line doesn't look right therefore
instead of using a linear equation we
need to come up with something different
and therefore the logistic regression
model looks somewhat like this so we
calculate the probability and if we plot
that probability not in the form of a
straight line but we need to use some
other equation we will see very soon
what the depression is then it is a
gradual process right so you see here
people with some of these ratings are
not getting any promotions and then
slowly uh at certain rating they get
promotion so that is a gradual process
and this is how the math behind logistic
regression looks so we are trying to
find the odds for a particular event
happening and this is the formula for
finding the odd so the probability of an
event happening divided by the
probability of the event not happening
so P if it is the probability of the
event happening probability of the
person getting a promotion and divided
by the probability of the person not
getting a promotion that is 1 minus p
this is how you measure the odds now the
values of the odds range from 0 to
Infinity so when this probability is 0
then the odds will the value of the odds
is equal to zero and when the
probability becomes 1 then the value of
the odds is 1 by 0 that will be Infinity
but the probability itself remains
between 0 and 1. now this is how an
equation of a straight line love so Y is
equal to beta0 plus beta 1 x where beta0
is the y-intercept and beta 1 is the
slope of the line if we take the odds
equation and take a log of both sides
then this would look somewhat like this
and the term logistic is actually
derived from the fact that we are doing
this we take a log of p x by 1 minus p x
this is an extension of the calculation
of odds that we have seen right and that
is equal to beta0 plus beta 1X which is
the equation of the straight line and
now from here if you want to find out
the value of PX we will see we can take
the exponential on both sides and then
if we solve that equation we will get
the equation of PX like this p x is
equal to 1 by 1 plus e to the power of
minus beta0 plus beta 1 x and recall
this is nothing but the equation of the
line which is equal to y y is equal to
beta0 plus beta 1 X so that this is the
equation also known as the sigmoid
function and this is the equation of the
logistic regression and all right and if
this is plotted this is how the sigmoid
curve is obtained hey there learner
simplinon brings you a comprehensive
professional certificate program in Ai
and machine learning from Purdue
University that is in collaboration with
IBM that will cover a wide range of
topics that will Empower you with the
knowledge and skills needed to excel in
the field of AI to learn more about this
course you can find the course link
mentioned in the description box so
let's compare linear and logistic
regression how they are different from
each other let's go back so linear
regression is solved or used to solve
regression problems and logistic
regression is used to solve
classification problems so both are
called regression but linear regression
is used for solving regression problems
where we predict continuous values
whereas logistic regression is used for
solving classification problems where we
have had to predict discrete values the
response variables in case of linear
regression are continuous in nature
whereas here they are categorical
discrete in nature and linear regression
helps to estimate the dependent variable
when there is a change in the
independent variable whereas here in
case of logistic regression it helps to
calculate the probability or the
possibility of a particular event
happening and linear regression as the
name suggests is a straight line that's
why it's called linear regression
whereas logistic regression is a sigmoid
function and the curve is the shape of
the curve is s it's an s-shaped curve
this is another example of application
of logistic regression in weather
prediction whether it's going to rain or
not rain now keep in mind both are used
in weather prediction if we want to find
the discrete values like whether it's
going to rain or not rain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so just keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not it
is going to be sunny or not it is going
to snow or not these are all logistic
regression examples a few more examples
classification of objects this is a
again another example of logistic
regression now here of course one
distinction is that these are
multi-class classification so logistic
regression is not used in its original
form but it is used in a slightly
different form so we say whether it is a
dog or not a dog I hope you understand
so instead of saying is it a dog or a
cat or a elephant we convert this into
saying so because we need to keep it to
Binary classification so we say is it a
dog or not a dog is it a cat or not a
cat so that's the way logistic
regression can be used for classifying
objects otherwise there are other
techniques which can be used for
performing multi-class classification
Healthcare plastic regression is used to
find the survival rate of a patient so
they take multiple parameters like drama
score and age and so on and so forth and
they try to predict the rate of survival
all right now finally let's take an
example and see how we can apply
logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into jupyter notebook and show the
code but before that let me take you
through a couple of slides to explain
what we are trying to do so let's say
you have an eight by eight image and
there the image has a number one two
three four and you need to train your
model to predict what this number is so
how do we do this so the first thing is
obviously in any machine learning
process you train your model so in this
case we are using logistic regression so
and then we provide a training set to
train the model and then we test how
accurate our model is with the test data
which means that like any machine
learning process we split our initial
data into two parts training set and
test set with the training set we train
our model and then with the test set we
test the model didn't we get good
accuracy and then we use it for for
inference right so that is typical
methodology of
training testing and then deploying of
machine learning models so let's take a
look at the code and see what we are
doing so I'll not go line by line but
just take you through some of the blocks
so first thing we do is import all the
libraries and then we basically take a
look at the images and see what is the
total number of images we can display
using matplotlib some of the images or a
sample of these images and then we split
the data into training and test as I
mentioned earlier and we can do some
exploratory analysis and then we build
our model we train our model with a
training set and then we test it with
our test set and find out how accurate
our model is using the confusion Matrix
the heat map and use heat map for
visualizing this and I will show you in
the code what exactly is the confusion
Matrix and how it can be used for
finding the accuracy in our example we
get an accuracy of about 0.94 which is
pretty good or 94 which is pretty good
all right so what is the confusion
Matrix this is an example of a confusion
Matrix and this is used for identifying
the accuracy of a classification model
or like a logistic regression model so
the most important part in a confusion
Matrix is that first of all this as you
can see this is a matrix and the size of
the Matrix depends on how many outputs
we are expecting
so the most important part here is that
the model will be most accurate when we
have the maximum numbers in its diagram
like in this case that's why it has
almost 93 94 percent because the
diagonals should have the maximum
numbers and the others other than
diagnose the cells other than the
diagonal should have very few numbers so
here that's what is happening so there
is a two here there are there's a one
here but most of them are along the
diagonal this what does this mean this
means that the number that has been fed
is zero and the number that has been
detected is also zero so the predicted
value and actual value are the same so
along the diagonals that is true which
means that let's let's take this
diagonal right if the maximum number is
here that means that like here in this
case it is 34 which means that 34 of the
images that have been fed are rather
actually there are two
misclassifications in there so 36 images
have been fed which have number four and
out of which 34 have been predicted
correctly as number four and one has
been predicted as number eight and
another one has been predicted as number
nine so these are two missed
classifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100 accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like a
hundred percent accurate model okay so
that's uh just of how to use this Matrix
how to use this confusion Matrix I know
the name uh is a little funny sounding
confusion Matrix but actually it is not
very confusing it's very straightforward
so you are just plotting what has been
predicted and what is the labeled
information on what is the actual data
that's also known as the ground proof
sometimes okay these are some fancy
terms that are used so predicted label
and the actual name that's all right
okay yeah so we are showing a little bit
more information here so 38 have been
predicted and here you will see that all
of them have been predicted correctly
there have been 38 zeros and predicted
value and the actual value is exactly
the same whereas in this case right it
has there are I think 37 plus 5 yeah 42
have been fed the images 42 images are
of Digit three and uh the accuracy is
only 37 of them have been accurately
predicted three of them have been
predicted as number seven and two of
them have been predicted as number eight
and so on and so forth okay all right so
with that let's go into Jupiter notebook
and see how the code looks so this is
the code in in jupyter Notebook for
logistic regression in this particular
demo what we are going to do is train
our model to recognize digits which are
the images which have digits from let's
say 0 to 5 or 0 to 9 and
um and then we will see how well it is
trained and whether it is able to
predict these numbers correct Lee or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and then the
last line in this block is to load the
digits so let's go ahead and run this
code then here we will visualize the
shape of these digits so we can see here
okay if we take a look this is how the
shape is 1797 by 64. these are like
eight by eight images so that's that's
what is reflected in this shape now from
here onwards we are basically once again
importing some of the libraries that are
required like numpy and matplot and we
will take a look at some of the sample
images that we have loaded so this one
for example creates a figure and then we
go ahead and take a few sample images to
see how they look so let me run this
code and so that it becomes easy to
understand so these are about five
images sample images that we are looking
at zero one two three four so this is
how the images this is how the data is
okay and based on this we will actually
train our logistic regression model and
then we will test it and see how well it
is able to recognize so the way it works
is the pixel information so as you can
see here this is an eight by eight pixel
kind of a image and the each pixel
whether it is activated or not activated
that is the information available for
each pixel now based on the pattern of
this activation and non-activation of
the various pixels this will be
identified as a zero for example right
similarly as you can see so overall each
of these numbers actually has a
different pattern of the pixel
activation and that's pretty much that
our model needs to learn from for which
a number what is the pattern of the
activation of the pixels right so that
is what we are going to train our model
okay so the first thing we need to do is
to split our data into training and test
data set right so whenever we perform
any training we split the data into
training and tests so that the training
data set is used to train the system so
we pass this probably multiple times and
then we test it with the test data set
and the split is usually in the form of
there and there are various ways in
which you can split this data it is up
to the individual preferences in our
case here we are splitting in the form
of 23 and 77 so when we say test size as
20.23 that means 23 percent of that
entire data is used for testing and the
remaining 77 percent is used for
training so there is a readily available
function which is called train test
split so we don't have to write any
special code for the splitting it will
automatically split the data based on
the proportion that we give here which
is test size so we just give the test
size automatically training size will be
determined and we pass the data that we
want to split and the the results will
be stored in X underscore train and Y
underscore train for the training data
set and what is X underscore train these
are these are the features right which
is like the independent variable and why
underscore train is the label right so
in this case what happens is we have the
input value which is or the features
value which is in X underscore train and
since this is a labeled data for each of
them each of the observations we already
have the label information saying
whether this digit is a zero or a one or
a two so that that's this is what will
be used for comparison to find out
whether the the system is able to
recognize it correctly or there is an
error for each observation it will
compare with this right so this is the
label so the same way X underscore train
y underscore train is for the training
data set X underscore test y underscore
test is for the test data set okay so
let me go ahead and execute this code as
well and then we can go and check
quickly what is the how many entries are
there and in each of this so X
underscore train the shape is
1383 by 64 and Y underscore train has
1383 because there is uh nothing like
the second part is not required here and
then X underscore test shape VC is 414
so actually there are 414 observations
in test and 1383 observations in train
so that's basically what these four
lines of code are are saying okay then
we import the logistic regression
library and which is a part of
scikit-learn so we we don't have to
implement the logistic regression
process itself we just call these uh the
function and let me go ahead and execute
that so that we have the logistic
regression Library imported now we
create an instance of logistic
regression right so logistic regr is a
is an instance of logistic regression
and then we use that for training our
model so let me first execute this code
so these two lines so the first line
basically creates an instance of
logistic regression model and then the
second line is where we are passing our
data the training data set right this is
our the the predictors and this is our
Target we are passing this data set to
train our model all right so once we do
this in this case the data is not large
but by and large and the training is
what takes usually a lot of time so we
spend in machine learning activities in
machine learning projects we spend a lot
of time for the training part of it okay
so here the data set is relatively small
so it was pretty quick so all right so
now our model has been trained using the
training data set and we want to see how
accurate this is so what we'll do is we
will test it out in probably faces so
let me first try out how well this is
working for one image okay I will just
try it out with one image my the first
entry in my test data set and see
whether it is uh correctly predicting or
not so and in order to test it so for
training purpose we use the fit method
there is a method called fit which is
for training the model and once the
training is done if you want to test for
a particular value new input you use the
predict method okay so let's run the
predict method and we pass this
particular image and we see that the
shape is or the prediction is four so
let's try a few more let me see for the
next 10 seems to be fine so let me just
go ahead and test the entire data set
okay that's basically what we will do so
now we want to find out how accurately
this has is performed so we use the
score method to find what is the
percentages of accuracy and we see here
that it has performed up to 94 percent
accurate okay so that's on this part now
what we can also do is we can
um also see this accuracy using what is
known as a confusion Matrix so let us go
ahead and try that as well so that we
can also visualize how well this model
has done so let me execute this piece of
code which will basically import some of
the libraries that are required and we
we basically create a confusion Matrix
an instance of confusion matrix by
running confusion Matrix and passing
these values so we have so this
confusion underscore Matrix method takes
two parameters one is the Y underscore
test and the other is the prediction so
what is the Y underscore test these are
the labeled values which we already know
for the test data set and predictions
are what the system has predicted for
the test data set okay so this is known
to us and this is what the system has
the model has generated so we kind of
create the confusion Matrix and we will
print it and this is how the confusion
Matrix looks as the name suggests it is
a matrix and the key point out here is
that the accuracy of the model is
determined by how many numbers are there
in the diagonal the more the numbers in
the diagonal the better the accuracy is
okay and first of all the total sum of
all the numbers in this whole Matrix is
equal to the number of observations in
the test data set that is the first
thing right so if you add up all these
numbers that will be equal to the number
of observations in the test data set and
then out of that the maximum number of
of them should be in the diagonal that
means the accuracy is pretty good if the
the numbers in the diagonal are less and
in all other places there are a lot of
numbers which means the accuracy is very
low the diagonal indicates a correct
prediction that so this means that the
actual value is same as the predicted
value here again actual value is same as
the predictive value and so on right so
the moment you see a number here that
means the actual value is something and
the predicted value is something else
right similarly here the actual value is
something and the predicted value is
something else so that is basically how
we read the confusion Matrix now how do
we find the accuracy you can actually
add up the total values in the diagonal
so it's like 38 plus 44 Plus 43 and so
on and divide that by the total number
of test observations that will give you
the percentage accuracy using a
confusion Matrix now let us visualize
this confusion Matrix tricks in a
slightly more sophisticated way using a
heat map so we will create a heat map
with some We'll add some colors as well
it's uh it's like a more visually
visually more appealing so that's the
whole idea so if we let me run this
piece of code and this is how the heat
map looks and as you can see here the
diagonals again are all the values are
here most of the values so which means
reasonably this seems to be reasonably
accurate and yeah basically the accuracy
score is 94 percent this is calculated
as I mentioned by adding all these
numbers divided by the total test value
so the total number of observations in
test data set okay so this is the
confusion Matrix for logistic regression
all right so now that we have seen the
confusion Matrix let's take a quick
sample and see how well the system has
classified and we will take a few
examples of the data so if we see here
we picked up randomly a few of them so
this is uh number four which is the
actual value and also the predicted
value both are four this is an image of
zero so the predicted value is also zero
actual value is of course 0 then this is
the image of 9. so this has also been
predicted correctly nine and actual
value is 9 and this is the image of one
and again this has been predicted
correctly as like the actual value okay
so this was a quick demo of logistic
regression how to use logistic
regression to identify images if you are
an aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search Nomo simple
lens professional certification program
in Ai and machine learning from Purdue
University in collaboration with IBM
should be a right choice for more
details use the link in the description
box below with that in mind there's all
kinds of regression models that come out
of this so we put them aside about side
we have our linear regression which is a
predictive number
used to predict a dependent output
variable based on Independent input
variable
accuracy is a measured using least
squares estimation
so that's where you take it you could
also use absolute value the least
squares is more popular there's reasons
for that mathematically and also for
computer runtime
uh but it does give you an an accuracy
based on the the least Square estimation
the best fit line is a straight line and
clearly that's not always used at all
the regression models there's a lot of
variations on that the output is a
predicted integer value
again this is we're talking when we talk
about linear regression and we're
talking about regression it means the
numbers coming out linear usually means
we're looking for that line
versus a different model and it's used
in business domain forecasting stocks
it's used as a basis of almost of most
predictions with numbers so if you're
looking at a lot of numbers you're
probably looking at a linear regression
model
for instance if you do just the high
lows of the stock exchange and you're
going to take a lot more of that if you
want to make money off the stock
you'll find that the linear regression
model fits uh probably better than
almost any of the other models or even
you know high-end neural networks and
all these other different machine
learning and AI models because they're
numbers they're just a straight set of
numbers you have a high value low value
volume that kind of thing so when you're
looking at something that's straight
numbers and are connected in that way
usually you're talking about a linear
regression model and that's where you
want to start a logistic regression
model used to classify dependent output
variable based on Independent input
variable so just like the linear
regression model and like all of our
machine learning tools you have your
features coming in and so in this case
you might have a label you know an image
or something like that is probably the
very popular thing right now labeling
broccoli and vegetables or whatever
accuracy is measured using maximum
likelihood estimation the best fit is
given by a curve and we saw that um
we're talking about linear regression
you definitely are talking about a
straight line although there is other
regression models that don't use
straight lines and usually when you're
looking at a logistic regression the
math as you saw was still kind of a
euclidean line but it's now got that
sigmoid activation which turns it into a
heavily weighted curve and the output is
a binary value between 0 and 1. and it's
used for classification image processing
as I mentioned is is what people usually
think of
although they use it for classification
of
um like a window of things so you could
take a window of stock history and you
could clap generate classifications
based on that and separate the data that
way if it's going to be that this
particular pattern occurs is going to be
upward trending or downward trending
in fact a number of stock
Traders use that not to tell them how
much to bid or what to bid but they use
it as to whether it's worth looking at
the stock or not whether the Stock's
going to go down or go up and it's just
a zero one do I care do I even want to
look at it so let's do a demo so you can
get a picture of what this looks like in
Python code let's predict the price at
which insurance should be sold to a
particular customer based on their
medical history
we will also classify on a mushroom data
set to find the poisonous and
non-poisonous mushrooms
and when you look at these two datas the
first one we're looking at the price so
the price is a number so let's predict
the price which the insurance should be
sold to and the second one is we're
looking at either as poisonous or it's
not poisonous so first off before we
begin the demo I'm in the Anaconda
Navigator
in this one I've loaded the python 3.6
and using the Jupiter notebook and you
can use jupyter notebook by itself you
can use the Jupiter lab which allows
multiple tabs it's basically the
notebook with tabs on it
but the Jupiter notebook is just fine
and it'll go into Google Chrome which is
what I'm using for my Internet Explorer
and from here we open up new and you'll
see python3 and again this is loaded
with python 3.6 and we're doing the
linear versus logic regression or logit
you'll see l-o-g-i-t
is one of the one of the names that kind
of pops up when you do a search on here
but it is a logic we're looking at the
logistic regression models
and we'll start with the linear
regression because it's easy to
understand you draw a line through stuff
and so in programming we got a lot of
stuff to unfold here in our in our
startup as we pre-load all of our
different parts
let's go ahead and break this up we have
at the beginning import pandas
so this is our data frame uh it's just a
way of storing the data think of a you
talk about a data frame think of a
spreadsheet you know rows and columns
it's a nice way of viewing the data
and then we have we're going to be
bringing in our pre-processing labeling
code coder I'll show you what that is
when we get down to it it's easier to
see in the data but there's some data in
here like sex it's male or female so
it's not like an actual number it's
either your one or the other that kind
of stuff ends up being encoded that's
what this label encoder is right here
we have our test split model
if you're going to build a model you do
not want to use all the data you want to
use some of the data and then test it to
see how good it is and if it can't have
seen the data you're testing on until
you're ready to test it on there and see
how good it is
and then we have our logistic regression
model our categorical one and then we
have our linear regression model these
are the two these right here let me just
um
um clear all that there we go these two
right here are what this is all about
logistic versus linear is it categorical
are we looking for a true false or are
we looking for a specific number
and then finally usually at the very end
we have to take and just ask how
accurate is our model did it work if
you're trying to predict something in
this case we're going to be doing
um
Insurance costs how close to the
insurance cost does it measure that we
expect it to be you know if you're an
insurance company you don't want to
promise to pay everybody's medical bill
and not be able to
and in the case of the mushrooms you
probably want to know just how much at
risk you are for following this model uh
as far as whether you're going to get a
poisonous mushroom and die or not
so we'll look at both of those and we'll
get talk a little bit more about the
shortcomings and the value of these
different processes so let's go ahead
and run this this is loaded the data set
on here and then because we're in
Jupiter notebook I don't have to put the
print on there we just do data set and
by and it prints out all the different
data on here and you can see here for
our insurance because we're starting
with we're loading that with our pandas
and it prints it in a nice format where
you can see the age sex body mass index
number of children smoker so this might
be something that the insurance company
gets from the doctor it says hey we're
gonna this is what we need to know
to give you a quote for what we're going
to charge you for your insurance
and you can see that it has 1 338 rows
and seven columns you can count the
columns one two three four five six
seven so there's seven columns on here
and the column we're really interested
in is charges I want to know what the
charges are going to be what can I
expect
not a very good Arrow drawn
um
what to expect them to charge on there
so is this going to be you know is this
person going to cost me uh sixteen
thousand eight hundred eighty four
dollars or is this person only going to
cost me uh
3866. how do we guess that so that we
can guess what the minimal charge is for
their insurance
and then there's one other thing you
really need to notice on this data
I mentioned it before but I'm going to
mention it again because
pre-processing data is so much of the
work in data science
um sex well how do you how do you deal
with female versus male
um are you a smoker yes or no what does
that mean region how do you look at
regions not a number how do you draw a
line between Southwest and Northwest
um you know they're objects it's either
your Southwest or your Northwest just
not like I'm southwest I guess you could
do longitude and latitude but the data
doesn't come in like that it comes in as
true false or whatever you know it's
either your Southwest or your Northwest
so we need to do a little bit of
pre-processing of the data on here to
make this work
there we go okay so let's take a look
and see what we're doing with
pre-processing
and again this is really where you spend
a lot of time with data science is
trying to understand how and why you
need to do that and so we're going to do
you'll see right up here
label and then we're going to do the do
a label encoder one of the modules we
brought in so this is SK learns label
encoder
I like the fact that it's all pretty
much automated but if you're doing a lot
of work with label encoder you should
start to understand how that fits
um and then we have uh label dot fit
right here where we're going to go ahead
and do the data set Dot
sex.dropduplicates and then for data set
sex we're going to do the label
transform the data set sex and so we're
looking right here at male or female
and so it usually just converts it to a
zero one because there's only two
choices on here
same thing with the smoker it's zero or
one so we're going to transfer the
transa change the smoker 0 1 on this
and then finally we did region down here
region does it a little bit different
we'll take a look at that and it it's I
think in this case it's probably going
to do it because we did it on this label
transform
with this particular setup it gives each
region a number like 0 1 2 3. so let's
go and take a look and see what that
looks like I'm going to run this
and you can see that our new data set
has age that's still a number six is
zero or one so zero is female one is
male
number of children we left that alone uh
smoker one or zero says no or yes on
there we actually just do one for no
zero or no yeah one for no I'm not sure
how it organized them but it turns the
smoker into zero or one yes or no
uh and then region it did this as zero
one two three so there's three regions
now a lot of times in in when you're
working with data science and you're
dealing with uh regions or even word
analysis
um instead of doing one column and
labeling it zero one two three a lot of
times you increase your features and so
you would have region Northwest would be
one column yes or no region Southwest
would be one column yes or no true zero
one but for this this this particular
setup this will work just fine on here
now that we spend all that time getting
it set up uh here's the fun part uh
here's the part where we're actually
using our setup on this and you'll see
right here we have our
why linear regression data set drop the
charges because that's what we want to
predict
and so our X I'm sorry our X linear data
set dropped the charges because that's
what we're going to predict we're
predicting charges right here so we
don't want that as our input for our
features
and our y output is charges that's what
we want to guess we want to guess what
the charges are
and then what we talked about earlier is
we don't want to do all the data at once
so we're going to take
0.3 means 30 percent we're going to take
30 percent of our data and it's going to
be as the trade as the testing site so
here's our y test and our X test down
there
um and so that part our model will never
see it until we're ready to test to see
how good it is and then of course right
here you'll see our training set and
this is what we're going to train it
we're going to trade it on 70 of the
data
and then finally the big ones this is
where all the magic happens this is
where we're going to create our magic
setup and that is right here our linear
model we're going to set it equal to the
linear regression model
and then we're going to fit the data on
here
and then at this point I always like to
pull up
um if you if you if you're working with
a new models good to see where it comes
from and this comes from the site kit
learn
and this is the SK learn linear model
linear regression that we imported
earlier
and you can see they have different
parameters the basic parameter works
great if you're dealing with just
numbers I mentioned that earlier with
stock high lows this model will do as
good as any other model out there for do
if you're doing just the very basic high
lows and looking for a linear fit a
regression model fit
um and which one of the things
when I'm looking at this as I look for
methods
and you'll see here's our fit that we're
using right now and here's our predict
and we'll actually do a little bit in
the middle here as far as looking at
some of the parameters hidden behind it
the math that we talked about earlier
and so we go in this we go ahead and run
this
you'll see it loads a linear regression
model and just has a nice output that
says hey I loaded the linear regression
model
and then the second part is we did the
fit and so this model is now trained our
linear model is now trained on the
training data
and so one of the things we can look at
is the for idx and call a name and
enumerate X linear train columns
kind of an interesting thing this prints
out the coefficients so when you're
looking at the back end of the data you
remember we had that formula BX X1 plus
bxx2 plus the inner plus the intercept
and so forth these are the actual
coefficients that are in here this is
what it's actually multiplying these
numbers by
and you can see like region gets a minus
value so when it heads it up I guess a
region you can read a lot into these
numbers uh it gets very complicated
there's ways to mess with them if you're
doing a basic linear regression model
usually don't look at them too closely
but you might start looking in these and
saying hey you know what uh smoker look
how smoker impacts the cost
it's just massive so this is a flag that
hey the value of the smoker really
affects this model
and then you can see here where the body
mass index so somebody who is overweight
is probably less healthy and more likely
to have cost money and then of course
age is a factor and then you can see
down here we have sexist benefactor also
and it just it changes as you go in
there negative number it probably has
its own meaning on there
again it gets really complicated when
you dig into the workings and how the
linear model works on that
and so we can also look at the intercept
this is just kind of fun so it starts at
this negative number and then adds all
these numbers to it that's all that
means that's our intercept on there and
that fits the data we have on that
and so you can see right here we can go
back and
oops give me just a second there we go
we can go ahead and predict the unknown
data and we can print that out
and if you're going to create a model to
predict something we'll go ahead and
predict it here's our y prediction value
linear model predict
and then we'll go ahead and create a new
data frame in this case from our X
linear test group
we'll go ahead and put the cost back
into this data frame and then the
predicted cost we're going to make that
equal to our y prediction
and so when we pull this up you can see
here that we have the actual cost and
what we predicted the cost is going to
be
a lot of ways to measure the accuracy on
there but we're going to go ahead and
jump into our mushroom data
and so in this you can see here we've
run our basic model we've built our
coefficients you can see the intercept
the back end you can see how we're
generating a number here
now with mushrooms we want a yes or no
we want to know whether we can eat them
or not
and so here's our mushroom file we're
going to run this take a look at the
data and again you can ask for a copy of
this file send a note over to
simplylearn.com
and you can see here that we have a
class the cap shape cap surface and so
forth so there's a lot of feature in
fact there's 23 different columns in
here going across
and when you look at this I'm not even
sure what these particular like p e e p
e I don't even know what the class is on
this
I'm going to guess by the notes that the
class is poisonous or edible
so if you remember before we had to do a
little pre-coding on our data same thing
with here we have our cap shape which is
b or X or k
we have cap color these really aren't
numbers so it's really hard to do
anything with just a a single number so
we need to go ahead and turn those into
a label encoder which again there's a
lot of different encoders with this
particular label encoder it's just
switching it to 0 1 2 3 and giving it an
integer value
in fact if you look at all the columns
all of our columns are labels and so
we're just going to go ahead and loop
through all the columns in the data and
we're going to transform it into a label
encoder and so when we run this you can
see how this gets shifted from
xbxx K to 0 1 2 3 4 5 or whatever it is
class is 0 1 1 being poisonous zero
looks like it's editable
and so forth on here so we're just
encoding it if you were doing this
project depending on the results you
might encode it differently like I
mentioned earlier you might actually
increase the number of features as
opposed to laboring 0 1 2 3 4 5. in this
particular example it's not going to
make that big of a difference how we
encode it
and then of course we're looking for the
class whether it's poisonous or edible
so we're going to drop the class in our
X Logistics model and we're going to
create our y Logistics model is based on
that class so here's our x y
and just like we did before we're going
to go ahead and split it
using 30 percent for test
70 percent to program the model on here
and that's right here whoops there we go
there's our train and test
and then you'll see here on this next
setup
this is where we create our model all
the magic happens right here
we go ahead and
create a logistics model I have up the
max iterations if you don't
change this for this particular problem
you'll get a warning that says this has
not converged
because that that's what it does is it
goes through the math and it goes hey
can we minimize the error and it keeps
finding a lower and lower error and it
still is changing that number so that
means it hasn't conversed yet it hasn't
find the lowest amount of error it can
and the default is 100. there's a lot of
settings in here so when we go in here
to let me pull that up from the sklearn
so we pull that up from the SK learn
model
you can see here we have our logistic it
has our different settings on here that
you can mess with most of these work
pretty solid on this particular setup so
you don't usually mess a lot usually I
find myself adjusting the iteration and
I'll get that warning and then increase
the iteration on there and just like the
other model you can go just like you did
with the other model we can scroll down
here and look for our methods
and you can see there's a lot of methods
available on here and certainly there's
a lot of different things you can do
with it
but the most basic thing we do is we fit
our model make sure it's set right and
then we actually predict something with
it so those are the two main things
we're going to be looking at on this
model is fitting and predicting there's
a lot of cool things you can do that are
more advanced but for the most part
these are the two which I use when I'm
going into one of these models and
setting them up
so let's go ahead and close out of our
sklearn setup on there and we'll go
ahead and run this
and you can see here it's now loaded
this up there we now have a
logistic model and we've gone ahead and
done a predict here also just like I was
showing you earlier
so here's where we're actually
predicting the data so we we've done our
first two lines of code as we create the
model we fit the model to our training
data and then we go ahead and predict
for our test data now in the previous
model we didn't dive into the test score
I think I just showed you graph and we
can go in there and there's a lot of
tools to do this we're going to look at
the model score on this one and let me
just go ahead and run the model score
and it says that it's pretty accurate
we're getting a roughly 95 accuracy well
that's good one 95 accuracy
85 accuracy might be good for a lot of
things
but when you look at something as far as
whether you're going to pick a mushroom
on the side of the trail and eat it we
might want to look at the confusion
Matrix and for that we're going to put
in our y logistic test the actual values
of edible and unedible and we're going
to put in our prediction value
and if you remember on here let's see I
believe it's poisonous was one zero is
edible
so let's go ahead and run that 0 1 0 is
good so here is a confusion Matrix and
this is if you're not familiar with
these we have true true true false
true false false false so it says out of
the edible mushrooms we correctly
labeled 1201 mushrooms edible that were
edible
and we correctly measured
1113 poisonous mushrooms as poisonous
but here's the kicker
I labeled
56 edible mushrooms as being
poisonous well that's not too big of a
deal we just don't eat them
but I measured 68 mushrooms as being
edible that were poisonous
so probably not the best choice to use
this model to predict whether you're
going to eat a mushroom or not
and you'd want to dig a Little Deeper
before you uh start picking mushrooms
off the side of the trail so a little
warning there when you're looking at any
of these data models looking at the
error and how that error fits in with
what domain you're in domain in this
case being edible mushrooms
be a little careful make sure that
you're looking at them correctly
so we've looked at uh edible or not
edible we've looked at a regression
model as far as the N values what's
going to be the cost and what our
predicted cost is so we can start
figuring out how much to charge these
people for their insurance
and so these really are the fundamentals
of data science when you pull them
together when I say data science talk
about your machine learning code
and hopefully you got a little bit out
of here again you can contact our simply
learned team and get a copy of these
files or get more information on this
if you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Purdue University in collaboration with
IBM should be a right choice for more
details use the link in the description
box below with that in mind
a confusion Matrix represents a table
layout of the different outcomes of
prediction
and results of a classification problem
and helps visualize its outcomes
and so you see here we have our simple
chart predicted and actual
the confusion Matrix helps us identify
the correct predictions of a model for
different individual classes as well as
the errors so you'll see here that the
values predicted by our classifier are
along the rows this is what we're going
to guess it is or our model is guessing
what this is based on its training so
we've already trained the model
to guess whether it's spam or not spam
or whatever it is you're working on
and then the actual values of our data
set are along the columns
so this is the actual value that's
supposed to be
people who can speak English will be
classified as positives so because they
have a remember zero one do you speak
English yes no and you could extend this
that they might have do you speak French
do you speak whatever languages and so
you might have a whole lot of
classifiers that you would look at each
one of these people who cannot speak
English will be classified as negatives
so there'll be a zero so you know zero
ones
the number of times are actual positive
values are equal to predicted positive
values gives us true positive TP the
number of times are actual negative
values are equal to predictive negative
values gives us true negative TN
the number of times our model wrongly
predicts negative values as positives
gives us a false positive
FP
and you'll see when you're working with
these a lot you know memorizing that
it's false positive you can easily
figure out what that is and pretty soon
you're just looking at the FP or the TP
depending on what you're working on and
the numbers times our model wrongly
predicts positive values as negatives
and gives us a false negative FP
now I'm going to do a quick step out
here
let's say you're working in the medical
and we're talking about cancer
do you really want a bunch of false
negatives you want zero under false
negative
so when we look at this confusion Matrix
if you have five percent false positives
and five percent false negatives it'd be
much better to even have twenty percent
false positives because they go in and
test it in zero false negatives
let's say it might be true if you're
working on uh say a car driving is this
a safe place for the car to go well you
really don't want any false positives
you know yes this is safe right over the
cliff
so again when you're working on the
project or whatever it is you're working
on this chart suddenly has huge value we
were talking about spam email how many
important emails say from your banking
overdraft charge coming in that you want
to be a a true false negative you don't
want it to go in the spam folder
likewise you want to get as much of the
spam out of there but you don't want to
miss anything really important
confusion Matrix metrics are performance
measures which help us find the accuracy
of our classifier there are four main
metrics accuracy precision recall and F1
score the F1 score is the one I usually
hear the most and accuracy is usually
what you put on your chart when you're
sending in front of the shareholders how
accurate is it people understand
accuracy
F1 score is a little bit more on the
math side and so you got to be a little
careful when you're quoting F1 scores in
the when you're sitting there with all
the shareholders because a lot of them
will just glaze over so confusion Matrix
metrics are performance measures which
help us find the accuracy of our
classifier there are four main metrics
accuracy the accuracy is used to find
the portion of the correctly classified
values it tells us how often our
classifier is right it is the sum of all
True Values divided by the total values
and this makes sense again it's one of
those things
I don't want to you know it depends on
what you're looking for are you looking
for uh not to miss any spam mails are
you looking to drive down the road and
not run anybody over
Precision is used to calculate the
model's ability to classify positive
values correctly it answers the question
when the model predicts a positive value
how often is it right it is the true
positive divided by the total number of
predicted positive values again in this
one uh depends on what project you're
working on whether this is what you're
going to be focusing on
so recall it is used to calculate the
model's ability to predict positive
values how often does the model actually
predict the correct positive values
it is a true positive divided by the
total number of actual positive values
and then your F1 score it is the
harmonic mean of recall and precision it
is useful when you need to take both
precision and recall into account
consider the following two confusion
Matrix derived from two different
classifier to figure out which one
performs better we can find the
confusion Matrix for both of them
and you can see we're back to uh does it
classify whether they can speak English
or are non-speaker they speak some they
don't know the English language and so
we put these two uh confusion matrixes
out here we can go ahead and do the math
behind that we can look up the accuracy
that's a tpn plus TN over the TF plus TN
plus FP plus FN and so we get an
accuracy of 0.8125
and we have a Precision if you do the
Precision which is your TP truth
positive over TP plus FP
we get 0.891
and if we do the recall we'll end up
with the 0.825 that's your TP over TP
plus FN
and then of course your F1 score which
is two times Precision times recall over
Precision plus recall
and we get the 0.857
and if we do that with another model
let's say we had two different models
and we're trying to see which one we
want to use for whatever reason we might
go ahead and compute the same things we
have our accuracy our precision and our
recall and our F1 score
and as we're looking at this we might
look at the accuracy because that's
really what we're interested in is uh
how many people are we able to classify
as being able to speak English I really
don't want to know if I'm you know I I
really don't want to know if they're
non-speakers
um I'd rather Miss 10 people speaking
English instead of 15. and so you can
see from these charts we probably go
with the first model because it does a
better job guessing who speaks English
and has a higher accuracy because in
this case that is what we're looking for
so uh with that we'll go ahead and pull
up a demo so you can see what this looks
like in the python setup in in the
actual coding for this we'll go into
Anaconda Navigator if you're not
familiar with Anaconda it's a really
good tool to use as far as doing display
in demos
and for quick development as a data
scientist I just love the package
now if you're going to do something
heavier lifting there's some limitations
with anaconda and with the setup but in
general you can do just about anything
in here with your python and for this
we'll go with Jupiter notebook Jupiter
lab is the same as Jupiter notebook
you'll see they now have integration
with pi charm if you work in pi charm uh
certainly there's a lot of other
Integrations that Anaconda has and we've
opened up
um I simply learned files I work on and
create a new file called confusion
Matrix demo and the first thing we want
to note is the data we're working with
here I've opened it up in a word pad or
notepad or whatever
uh you can see it's got a row of
headers comma separated and then all the
data going down below
and then I save this in the same file so
I don't have to remember what path I'm
working on of course if you have your
data separated and you're working with a
lot of data you probably want to put it
into a different folder or file
depending on what you're doing
and the first thing we want to do is go
ahead and import our tools we're going
to use the pandas that's our data frame
if you haven't had a chance to work with
the data frame please review Panda's
data frame and go into simply learn you
can pull up the pandas data frame
tutorial on there
and then we're going to use uh the
scikit framework which is all denoted as
sklearn and I can just pull this in you
can see here's the scikit dash learn.org
with the stable version that you can
import into your python
and from here we're going to use the
train test split
for splitting our data we're going to do
some pre-processing we're going to do
use the logistic regression model that's
our actual machine learning model we're
using and then with this core this
particular setup is about is we're going
to do the accuracy score the confusion
Matrix and the classifier report
so let me go ahead and run that and
bring all that information in
and just like we open the file we need
to go ahead and load our data in here so
we're going to go ahead and do our
pandas read CSV and then just because
we're in jupyter Notebook we can just
put data to read the data in here a lot
of times we'll actually let me just do
this I prefer to do the just the head of
the date or the top part
and you can see we have age sex I'm not
sure what CP stands for test BPS
cholesterol so a lot of different
measurements if you were in this domain
you'd want to know what all these
different measurements mean
I don't want to focus on that too much
because when we're talking about data
science a lot of times you have no idea
what the data means if you've ever
looked up the breast cancer measurement
it's just a bunch of measurements and
numbers uh unless you're a doctor you're
gonna have no idea what those
measurements mean
but if it's your specialty in your
domain you better know them so we're
going to go ahead and create Y and it's
going to we're going to set it equal to
the Target so here's our Target value
here
and there it's either one or zero
so we have a classifier if you're
dealing with one zero true false what do
you have you have a classifier
and then our X is going to be uh
everything except for the Target so
we're going to go ahead and drop the
target axis equals one remember that's
columns versus the index for rows X is
equals zero would would give you an
error but you would drop like row two
and then we'll go ahead and just print
that out so you can see what we're
looking at and uh here we have Y data X
data and you can see from the X data we
have the X head and we can go ahead and
just do print
the Y head data
and run that
so this is all loading the data that
we've done so far if there's a confusion
in there go back and rewind the tape and
review it
and then we need to go ahead and split
our data into our X train X test y train
y test and then keep in mind you always
want to split the data before we do the
scalar and the reason is is that you
want the scalar on the training data to
be set on the training data data or fit
to it but not on the test data think of
this as being out in the field you're
not it could actually alter your results
so it's always important to do make sure
whatever you do to the training data or
whatever fit you're doing is always done
on the training not on the test and then
we want to go ahead and scale the data
now we are working with linear
regression model I'll mention this here
in a minute when we get to the actual
model
so some sometimes you don't need to
scale it when you're working with linear
regression models it's not going to
change your result as much as say a
neural network where it has a huge
impact
uh but we're going to go ahead and take
here's our X train X test y train y test
we create our scalar we go ahead and
scale the scale is going to fit the X
train
and then we're going to go ahead and
take our X train and transform it and
then we also need to take our X test and
transform it based on the scale on here
so that our X is now between that nice
set minus one to one and so this is all
uh our pre data setup
and
hopefully all of that looks fairly
familiar to you if you've done a number
of our other classes and you're up to
the setup on here
and then we want to go ahead and do is
create our model and we're going to use
a logistic regression model
and from the logistic regression model
we're going to go ahead and fit our X
train and Y train and then we'll run our
predicted value on here
and let's just go ahead and run that and
so now we are we actually have like our
X test and our prediction so if you
remember from
our Matrix we're looking for the actual
versus the prediction and how those
compare
and if I take us back up here you're
going to notice that we imported the
accuracy score the confusion Matrix and
the classification report
and there's of course our logistic
regression the model we're using for
this
and I did mention I was going to talk a
little bit about scalar and the
regression model
the scalar on a lot of your regression
models uh your basic Mass standard
regression models and I'd have to look
it up for the logistic regression model
when you're using a standard regression
model you don't need to scale the data
it's already just built in by the way
the model works
in most cases but if you're in a neural
network and you're there's a lot of
other different setups than you really
want to take this and fit that on there
and so we can go in and do the accuracy
and this is if you remember correctly we
were looking at the accuracy
with the English speaking so this is
saying our accuracy as to whether this
person is I believe this is the heart
data set
um
it's going to be accurate about 85
percent of the time as far as whether
it's going to predict the person's going
to have a heart condition
or the one as it comes up with the zero
one on there
which would mean at this point that you
have an 85 percent being correct on
telling someone they're extremely high
risk for a heart attack kind of thing
and so we want to go ahead and create
our confusion Matrix and let me just do
that
of course the software does everything
for us so we'll go ahead and run this
and you can see right here
um here's our 25
uh
prediction uh correct predictions right
here
and if you remember from our slide I'll
just bring this over so it's a nice
visual we have our true positive false
positive
uh so we had 25 which were true that it
said hey this person is going to be high
risk at heart and we had four that were
still high risk that has said were false
so out of these 25 people or out of
these 29 people and that makes sense
because you have 0.85 out of 29 people
it was correct on 25 of them and so uh
here's our accuracy score we were just
looking at that our accuracy is your
true positive and your true negative
over all of them so how true is it and
there was our accuracy coming up here
0.85 and then we have our nice Matrix
generated from that
and you can see right here is a similar
Matrix we had going for that from the
slide and this starts to let's just
start asking questions at this point
um so if you're in a board meeting or
you're working with this you really want
to start looking at this data here and
saying well
is this good enough is uh this number of
people and hopefully you'd have a much
larger data set in my is my confusion
Matrix showing for the true positive and
false positive is that acceptable for
what we're doing uh and of course if
you're going to put together whatever
data you're putting out you might want
to separate the true negative false
positive false negative true positive
and you can simply do that by doing the
confusion Matrix and then of course the
Ravel part lets you set that up so you
can just split that right up into a nice
Tuple and the final thing we want to
show you here in the coding on this part
is the confusion Matrix metrics
and so we can come in here and just use
the Matrix equals classification report
the Y test and the predict
and then we're going to take that
classification report and go ahead and
print that out
and you can see here it does a nice job
uh giving you your accuracy your micro
average your weighted average you have
your Precision your recall your F1 score
and your support all in one window so
you can start looking at this data and
saying oh okay our precision's at 0.83
uh 0.87 for getting a a positive and
0.83 for the negative site for a zero
and we start talking about whether this
is a valid information or not to use and
when we're looking at heart attack
prediction we're only looking at one
aspect what's the chances of this person
having a heart attack or not you might
have something where we went back to the
languages maybe you also want to know
whether they speak English or Hindi or
French and you can see right here that
we can now take our confusion Matrix and
just expand it as big as we need to
depending on how many different
classifiers we're working on if you are
an aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then such no more simply
lens professional certification program
in Ai and machine learning from Purdue
University in collaboration with IBM
should be a right choice for more
details use the link in the description
box below with that in mind what is a
decision tree let's go through a very
simple example before we dig in deep
decision tree is a tree shape diagram
used to determine a course of action
each branch of the tree represents a
possible decision or currents or
reaction let's start with a simple
question how to identify a random
vegetable from a shopping bag so we have
this group of vegetables in here and we
can start off by asking a simple
question is it red and if it's not then
it's going to be the purple fruit to the
left probably an eggplant if it's true
it's going to be one of the red fruits
is a diameter greater than two if false
is going to be a what looks to be a red
chili and if it's true it's going to be
a bell pepper from the capsicum family
so it's a capsicum
problems that decision tree can solve so
let's look at the two different
categories the decision tree can be used
on it can be used on the classification
the true false yes no and it can be used
on regression where we figure out what
the next value is in a series of numbers
or a group of data in classification the
classification tree will determine a set
of logical if-then conditions to
classify problems for example
discriminating between three types of
flowers based on certain features in
regression a regression tree is used
when the target variable is numerical or
continuous in nature we fit the
regression model to the Target variable
using each of the independent variables
each split is made based on the sum of
squared error
before we dig deeper into the mechanics
of the decision tree let's take a look
at the advantages of using a decision
tree and we'll also take a glimpse at
the disadvantages the first thing you'll
notice is that it's simple to understand
interpret and visualize it really shines
here because you can see exactly what's
going on in a decision tree little
effort is required for data preparation
so you don't have to do special scaling
there's a lot of things you don't have
to worry about when using a decision
tree it can handle both numerical and
categorical data as we discovered
earlier and non-linear parameters don't
affect its performance so even if the
data doesn't fit an easy curved graph
you can still use it to create an
effective decision or prediction
if we're going to look at the advantages
of a decision tree we also need to
understand the disadvantages of a
decision tree the first disadvantage is
overfitting overfitting occurs when the
algorithm captures noise in the data
that means you're solving for one
specific instance instead of a general
solution for all the data High variance
the model can get unstable due to small
variation in data low bias tree a highly
complicated decision tree tends to have
a low bias which makes it difficult for
the model to work with new data
decision tree important terms before we
dive in further we need to look at some
basic terms we need to have some
definitions to go with our decision tree
in the different parts we're going to be
using we'll start with entropy entropy
is a measure of Randomness or
unpredictability in the data set for
example we have a group of animals in
this picture there's four different
kinds of animals and this data set is
considered to have a high entropy you
really can't pick out what kind of
animal it is based on looking at just
the four animals as a big clump of
entities so as we start splitting it
into subgroups we come up with our
second definition which is Information
Gain Information Gain it is a measure of
decrease in entropy after the data set
is split so in this case based on the
color yellow we've split one group of
animals on one side as true and those
who aren't yellow as false as we
continue down the yellow side we split
based on the height true or false equals
10 and on the other side High height is
less than 10 true or false and as you
see as we split it the entropy continues
to be less and less and less and so our
Information Gain is simply the entropy
E1 from the top and how it's changed to
E2 in the bottom and we'll look at the
deeper math although you really don't
need to know a huge amount of math when
you actually do the programming in
Python because it'll do it for you but
we'll look on the actual math of how
they compute entropy finally we went on
the different parts of our tree and they
call the leaf node Leaf node carries a
classification or the decision so it's a
final end at the bottom the decision
node has two or more branches this is
where we're breaking the group up into
different parts and finally you have the
root node the topmost decision node is
known as the root node
how does a decision tree work wonder
what kind of animals I'll get in the
jungle today maybe you're the hunter
with a gun or if you're more into
photography you're a photographer with a
camera so let's look at this group of
animals and let's try to classify
different types of animals based on
their features using a decision tree so
the problem statement is to classify the
different types of animals based on
their features using a decision tree the
data set is looking quite messy and the
entropy is high in this case so let's
look at a training set or a training
data set and we're looking at color
we're looking at height and then we have
our different animals we have our
elephants our giraffes our monkeys and
our tigers and they're of different
colors and shapes let's see what that
looks like and how do we split the data
we have to frame the conditions that
split the data in such a way that the
Information Gain is the highest note
gain is the measure of decrease in
entropy after splitting so the formula
for entropy is the sum that's what this
symbol looks like that looks like kind
of like a the funky e of K where I
equals 1 to k k would represent the
number of animal the different animals
in there where value or P value of I
would be the percentage of that animal
times the log base 2 of the same the
percentage of that animal let's try to
calculate the entropy for the current
data set and take a look at what that
looks like and don't be afraid of the
math you don't really have to memorize
this math just be aware that it's there
and this is what's going on in the
background and so we have three drafts
two tigers one monkey two elephants a
total of eight animals gather and if we
plug that into the formula we get an
entropy that equals three over eight so
we have three drafts a total of eight
times the log usually they use base two
on the log so log base 2 of 3 over 8
plus in this case let's say it's the
elephants two over eight two elephants
over a total of eight times log base two
two over eight plus one monkey over
total of eight log base 2 1 over 8 and
plus two 2 over 8 of the Tigers log base
2 over 8. and if we plug that into our
computer or calculator I obviously can't
do logs in my head we get an entropy
equal to
0.571 the program will actually
calculate the entropy of the data set
similarly after every split to calculate
the gain now we're not going to go
through each set one at a time to see
what those numbers are we just want you
to be aware that this is a Formula or
the mathematics behind it gain can be
calculated by finding the difference of
the subsequent entropy values after a
split now we will try to choose a
condition that gives us the highest gain
we will do that by splitting the data
using each condition and checking that
the gain we get out of them the
condition that gives us the highest gain
will be used to make the first split can
you guess what that first split will be
just by looking at this image as a human
is probably pretty easy to split it
let's see if you're right if you guessed
the color yellow you're correct let's
say the condition that gives us the
maximum gain is yellow so we will split
the data based on the color yellow if
it's true that group of animals goes to
the left if it's false it goes to the
right the entropy after the splitting
has to decreased considerably however we
still need some splitting at both the
branches to attain an entropy value
equal to zero so we decide to split both
the nodes using height as the condition
since every Branch now contains single
label type we can say that entropy in
this case has reached the least value
and here you see we have the giraffes of
the Tigers the monkey and the elephants
all separated into their own groups this
tree can now predict all the classes of
animals present in the data set with a
hundred percent accuracy that was easy
use case loan repayment prediction let's
get into my favorite part and open up
some Python and see what the programming
code and the scripting looks like in
here we're going to want to do a
prediction and we start with this
individual here who's requesting to find
out how good his customers are going to
be whether they're going to be pay their
loan or not for this bank and from that
we want to generate a problem statement
to predict if a customer will repay loan
amount or not and then we're going to be
using the decision tree algorithm in
Python let's see what that looks like
and let's dive into the code in our
first few steps of implementation we're
going to start by importing the
necessary packages that we need from
Python and we're going to load up our
data and take a look at what the data
looks like so the first thing I need is
I need something to edit my Python and
run it in so let's flip on over and here
I'm using the Anaconda Jupiter notebook
now you can use any python IDE you like
to run it in but I find the Jupiter
notebooks really nice for doing things
on the Fly and let's go ahead and just
paste that code in the beginning and
before we we start let's talk a little
bit about what we're bringing in and
then we're going to do a couple things
in here we have to make a couple changes
as we go through this first part of the
import the first thing we bring in is
numpy as NP that's very standard when
we're dealing with mathematics
especially with a very complicated
machine learning tools you almost always
see the numpy come in for your num your
number it's called number python it has
your mathematics in there in this case
we actually could take it out but
generally you'll need it for most of
your different things you work with and
then we're going to use pandas as PD
that's also a standard the pandas is a
data frame setup and you can liken this
to taking your basic data and storing it
in a way that looks like an Excel
spreadsheet so as we come back to this
when you see NP or PD those are very
standard uses you'll know that that's
the pandas and I'll show you a little
bit more when we explore the data in
just a minute then we're going to need
to split the data so I'm going to bring
in our train test and split and this is
coming from the sklearn package cross
validation and just a minute we're going
to change that and we'll go over that
too and then there's also the SK dot
tree import decision tree classifier
that's the actual tool we're using
remember I told you don't be afraid of
the mathematics it's going to be done
for you well the decision tree
classifier has all that mathematics in
there for you so you don't have to
figure it back out again and then we
have sklearn.metrix for accuracy score
we need to score our our setup that's
the whole reason we're splitting it
between the training and testing data
and finally we still need the sklearn
import tree and that's just the basic
tree function that's needed for the
decision tree classifier and finally
we're going to load our data down here
and I'm going to run this and we're
going to get two things on here one
we're going to get an error and two
we're going to get a warning let's see
what that looks like so the first thing
we had is we have an error why is this
error here well it's looking at this it
says I need to read a file and when this
was written the person who wrote it this
is their path where they stored the file
so let's go ahead and fix that
and I'm going to put in here my file
path I'm just going to call it full file
name and you'll see it's on my C drive
and this is very lengthy setup on here
where I stored the data2.csv file
don't worry too much about the full path
because on your computer it'll be
different the data.2 CSV file was
generated by simply learn
if you want a copy of that you can
comment down below and request it here
in the YouTube
and then if I'm going to give it a name
full file name
I'm going to go ahead and change it here
to full
file name so let's go ahead and run it
now and see what happens
and we get a warning
when you're coding understanding these
different warnings and these different
errors that come up is probably the
hardest lesson to learn
so let's just go ahead and take a look
at this and use this as a opportunity to
understand what's going on here if you
read the warning it says the cross
validation is depreciated so it's a
warning on it's being removed and it's
going to be moved in favor of the model
selection
so if we go up here we have
sklearn.cross validation and if you
research this and go to the sklearn site
you'll find out that you can actually
just swap it right in there with model
selection
and so when I come in here and I run it
again
that removes a warning what they've done
is they've had two different developers
develop it in two different branches
and then they decided to keep one of
those and eventually get rid of the
other one that's all that is and very
easy and quick to fix
before we go any further I went ahead
and opened up the data from this file
remember the the data file we just
loaded on here the data underscore 2.csv
let's talk a little bit more about that
and see what that looks like both as a
text file because it's a comma separated
variable file and in a spreadsheet this
is what it looks like as a basic text
file you can see at the top they've
created a header and it's got one two
three four five columns and each column
has data in it and let me flip this over
because we're also going to look at this
in an actual spreadsheet so you can see
what that looks like and here I've
opened it up in the open Office calc
which is pretty much the same as Excel
and zoomed in and you can see we've got
our columns and our rows of data a
little easier to read in here we have a
result yes yes no we have initial
payment last payment credit score house
number if we scroll way down
we'll see that this occupies a thousand
and one lines of code or lines of data
with the first one being a column and
then 1000 lines of data
now as a programmer
if you're looking at a small amount of
data I usually start by pulling it up in
different sources I can see what I'm
working with
but in larger data you won't have that
option it would just be too too large so
you need to either bring in a small
amount that you can look at it like
we're doing right now or we can start
looking at it through the python code so
let's go ahead and move on and take the
next couple steps to explore the data
using python let's go ahead and see what
it looks like in Python to print the
length and the shape of the data so
let's start by printing the length of
the database we can use a simple Lin
function from python
and when I run this you'll see that it's
a thousand long and that's what we
expected there's a thousand lines of
data in there if you subtract the column
head this is one of the nice things when
we did the balance data from the panda
read CSV you'll see that the header is
row zero so it automatically removes a
row
and then shows the data separate it does
a good job sorting that data out for us
and then we can use a different function
and let's take a look at that and again
we're going to utilize the tools in
panda
and since the balance underscore data
was loaded as a panda data frame
we can do a shape on it and let's go
ahead and run the shape and see what
that looks like
what's nice about this shape is not only
does it give me the length of the data
we have a thousand lines it also tells
me there's five columns so we were
looking at the data we had five columns
of data and then let's take one more
step to explore the data using Python
and now that we've taken a look at the
length and the shape let's go ahead and
use the pandas module for head another
beautiful thing in the data set that we
can utilize so let's put that on our
sheet here and we have print data set
and balance data.head and this is a hand
as print statement of its own so it has
its own print feature in there and then
we went ahead and gave a label for a
print job here of data set just a simple
print statement
and when we run that
and let's just take a closer look at
that let me zoom in here
there we go
pandas does such a wonderful job of
making this a very clean
readable data set so you can look at the
data you can look at the column headers
you can have it when you put it as the
head it prints the first five lines of
the data
and we always start with zero so we have
five lines we have zero one two three
four instead of one two three four five
that's a standard scripting and
programming set as you want to start
with the zero position and that is what
the data head does it pulls the first
five rows of data puts in a nice format
that you can look at and view very
powerful tool to view the data so
instead of having to flip and open up an
Excel spreadsheet or open Office Cal or
trying to look at a word doc where it's
all scrunched together and hard to read
you can now get a nice open view of what
you're working with we're working with a
shape of a thousand long five wide so we
have five columns and we do the full
data head you can actually see what this
data looks like the initial payment last
payment credit scores house number so
let's take this now that we've explored
the data and let's start digging into
the decision tree so in our next step
we're going to train and build our data
tree and to do that we need to First
separate the data out we're going to
separate into two groups so that we have
something to actually train the data
with and then we have some data on the
side to test it to see how good our
model is remember with any of the
machine learning you always want to have
some kind of test set to weigh it
against so you know how good your model
is when you distribute it let's go ahead
and break this code down and look at it
in pieces
so first we have our X and Y
where did X and Y come from well X is
going to be our data
and Y is going to be the answer or the
target you can look at it source and
Target
in this case we're using X and Y to
denote the data in and the data that
we're actually trying to guess what the
answer is going to be and so to separate
it we can simply put in x equals the
balance of the data dot values the first
brackets
means that we're going to select all the
lines in the database so it's all the
data and the second one says we're only
going to look at columns one through
five remember always start with zero
zero is a yes or no and that's whether
the loan went default or not so we want
to start with one if we go back up here
that's the initial payment and it goes
all the way through the house number
well if we want to look at one through
five we can do the same thing for Y
which is the answers and we're going to
set that just equal to the zero row so
it's just the zero row and then it's all
rows going in there so now we've divided
this into two different data sets
one of them with the
data going in and one with the answers
next we need to split the data
and here you'll see that we have it
split into four different parts
the first one is your X training your X
test your y train your y test
simply put we have X going in where
we're going to train it and we have to
know the answer to train it with
and then we have X test where we're
going to test that data and we have to
know in the end what the Y was supposed
to be
and that's where this train test split
comes in that we loaded earlier in the
modules this does it all for us and you
can see they set the test size equal to
0.3 so that's roughly 30 percent will be
used in the test and then we use a
random state so it's completely random
which rows it takes out of there and
then finally we get to actually build
our decision tree and they've called it
here clf underscore entropy that's the
actual decision tree or decision tree
classifier and in here they've added a
couple variables which we'll explore in
just a minute and then finally we need
to fit the data to that so we take our
clf entropy that we created and we fit
the X train and since we know the
answers for X trade or the Y train we go
ahead and put those in and let's go
ahead and run this and what most of
these sklearn modules do is when you set
up the variable in this case when we set
the clf entropical decision tree
classifier it automatically prints out
what's in that decision tree there's a
lot of variables you can play Within
here and it's quite beyond the scope of
this tutorial able to go through all of
these and how they work but we're
working on entropy that's one of the
options we've added that it's completely
a random state of 100 so 100 percent and
we have a max depth of three now the max
depth if you remember above when we were
doing the different graphs of animals
means it's only going to go down three
layers before it stops and then we have
minimal samples of leaves is five so
it's going to have at least five leaves
at the end so I'll have at least three
splits I'll have no more than three
layers and at least five end leaves with
the final result at the bottom now that
we've created our decision tree
classifier not only created it but
trained it let's go ahead and apply it
and see what that looks like so let's go
ahead and make a prediction and see what
that looks like we're going to paste our
predict code in here
and before we run it let's just take a
quick look at what it's doing here we
have a variable y predict that we're
going to do
and we're going to use our variable clf
entropy that we created
and then you'll see dot predict and it's
very common in the SK learn modules that
they're different tools have the predict
when you're actually running a
prediction
in this case we're going to put our X
test data in here
now if you delivered this for use an
actual commercial use and distributed it
this would be the new loans you're
putting in here to guess
whether the person is going to be pay
them back or not in this case we need to
test out the data and just see how good
our sample is how good of our tree does
at predicting the loan payments and
finally since Anaconda Jupiter notebook
works as a command line for python we
can simply put the Y predict en to print
it I could just as easily have put the
print
and put brackets around y predict en to
print it out we'll go ahead and do that
it doesn't matter which way you do it
and you'll see right here that it runs a
prediction this is roughly 300 in here
remember it's 30 percent of a thousand
so you should have about 300 answers in
here
and this tells you which each one of
those lines of our test went in there
and this is what our why predict came
out
so let's move on to the next step we're
going to take this data and try to
figure out just how good a model we have
so here we go since sklearn does all the
heavy lifting for you and all the math
we have a simple line of code to let us
know what the accuracy is and let's go
ahead and go through that and see what
that means and what that looks like
let's go ahead and paste this in and let
me zoom in a little bit
there we go
so you have a nice full picture
and we'll see here we're just going to
do a print accuracy is
and then we do the accuracy score
and this was something we imported
earlier if you remember at the very
beginning let me just scroll up there
real quick so you can see where that's
coming from
that's coming from here down here from
sklearn.metrix import accuracy score
and you could probably run a script make
your own script to do this very easily
how accurate is it how many out of 300
do we get right and so we put in our y
test that's the one we ran the predict
on and then we put in our y predict en
that's the answers we got and we're just
going to multiply that by a hundred
because this is just going to give us an
answer as a decimal and we want to see
it as a percentage and let's run that
and see what it looks like
and if you see here we got an accuracy
of 93.6667
so when we look at the number of loans
and we look at how good our model fit we
can tell people it has about a 93.6
fitting to it so just a quick recap on
that we now have accuracy set up on here
and so we have created a model that uses
a decision tree algorithm to predict
whether a customer will repay the loan
or not the accuracy of the model is
about 94.6 percent the bank can now use
this model to decide whether it should
approve the loan request from a
particular customer or not and so this
information is really powerful we may
not be able to as individuals understand
all these numbers because they have
thousands of numbers that come in but
you can see that this is a smart
decision for the bank to use a tool like
this to help them to predict how good
their profit is going to be off of the
loan balances and how many are going to
default or not if you are an aspiring
machine learning engineer looking for
online training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply lens professional
certification program in Ai and machine
learning from Purdue University in
collaboration with IBM should be a right
choice for more details use the link in
the description box below with that in
mind let us dig deep into the theory of
exactly how it works and let's look at
what is random forests random forests or
random decision Forest is a method that
operates by constructing multiple
decision trees the decision of the
majority of the trees is chosen by the
random Forest as the final decision and
let's uh we have some nice Graphics here
we have a decision tree and they
actually use a real tree to denote the
decision tree which I love and given a
random some kind of picture of a fruit
this decision tree decides that the
output is it's an apple and we have a
decision tree 2 where we have that
picture of the fruit goes in and this
one decides that it's a limit and the
decision three tree gets another image
and it decides it's an apple and then
this all goes together and what they
call the random forest and this random
Forest then looks at it and says okay I
got two votes for apple one vote for
lemon the majority is Apples so the
final decision is apples to understand
how the random Forest works we first
need to dig a little deeper and take a
look at the random forest and the actual
decision tree and how it builds that
decision tree and looking closer at how
the individual decision trees work we'll
go ahead and continue to use the fruit
example since we're talking about trees
and forests a decision tree is a tree
shaped diagram to use to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction so in here we
have a bowl of fruit and if you look at
that it looks like they switched from
lemons to oranges we have oranges
cherries and apples and the first
decision of the decision tree might be
is a diameter greater than or equal to
three and if it says false it knows that
they're cherries because everything else
is bigger than that so all the cherries
fall into that decision so we have all
that data we're training we can look at
that we know that that's what's going to
come up
is the color orange well goes hmm orange
or red well if it's true then it comes
out as the orange and if it's false that
leaves apples
so in this example it sorts out the
fruit in the bowl or the images of the
fruit a decision tree these are very
important terms to know because these
are very Central to understanding the
decision tree when working with them the
first is entropy everything on the
decision tree and how it makes this
decision is based on entropy entropy is
a measure of Randomness or
unpredictability in the data set then
they also have Information Gain
the leaf node the decision node and the
root node we'll cover these other four
terms as we go down the tree but let's
start with entropy so starting with
entropy we have here a high amount of
Randomness what that means is that
whatever's coming out of this decision
if it was going to guess based on this
data it wouldn't be able to tell you
whether it's a lemon or an apple it
would just say it's a fruit
uh so the first thing we want to do is
we want to split this apart and we take
the initial data set we're going to
create a data set one and a data set two
we just split it in two and if you look
at these new data sets after splitting
them the entropy of each of those sets
is much less so for the first one
whatever comes in there it's going to
sort that data and it's going to say
okay if this data goes this direction
it's probably an apple and if it goes
into the other direction it's probably a
lemon so that brings us up to
Information Gain it is a measure of
decrease in the entropy after the data
set is split what that means in here is
that we've gone from one set which has a
very high entropy to two lower sets of
entropy and we've added in the values of
E1 for the first one and E2 for the
second two which are much lower and so
that information gain is increased
greatly in this example and so you can
find that the information grain simply
equals decision E1 minus E2 as we're
going down our list of definitions we'll
look at the leaf node and the leaf node
carries the classification or the
decision
so we look down here to the leaf node we
finally get to our set one or our set
two when it comes down there and it says
okay this object's gone into set one if
it's gone into set one it's going to be
split by some means and we'll either end
up with apples on the leaf node or a
lemon on the leaf node and on the right
it'll either be an apple or lemons those
Leaf nodes are those final decisions or
classifications that's the definition of
leaf node in here if we're going to have
a final Leaf where we make the decision
we should have a name for the nodes
above it and they call those decision
nodes a decision node decision node has
two or more branches and you can see
here where we have the five apples and
one lemon and in the other case the five
lemons and one apple they have to make a
choice of which tree It Goes Down based
on some kind of measurement or
information given to the tree and that
brings us to our last definition the
root node the top most decision node is
known as the root node and this is where
you have all of your data and you have
your first decision it has to make or
the first split in information
so far we've looked at a very general
image with the fruit being split let's
look and see exactly what that means to
split the data and how do we make those
decisions on there hey there Lana simply
learn brings you a comprehensive
professional certificate program in Ai
and machine learning from Purdue
University that is in collaboration with
IBM that will cover a wide range of
topics that will Empower you with the
knowledge and skills needed to excel in
the field of AI to learn more about this
course you can find the course link
mentioned in the description box
let's go in there and find out how does
a decision tree work
so let's try to understand this and
let's use a simple example and we'll
stay with the fruit we have a bowl of
fruit and so let's create a problem
statement and the problem is we want to
classify the different types of fruits
in the bowl based on different features
the data set in the bowl is looking
quite messy and the entropy is high in
this case so if this bowl was our
decision maker it wouldn't know what
choice to make it has so many choices
which one do you pick Apple grapes or
lemons and so we look in here we're
going to start with a training set
so this is our data that we're training
our data with and we have a number of
options here we have the color and under
the color we have red yellow purple we
have a diameter three three one three
three one and we have a label Apple
lemon Grapes apple lemon grapes and how
do we split the data we have to frame
the conditions to split the data in such
a way that the Information Gain is the
highest it's very key to note that we're
looking for the best gain we don't want
to just start sorting out the smallest
piece in there we want to split it the
biggest way we can and so we measure
this decrease in entropy that's what
they call it entropy there's our entropy
after splitting and now we'll try to
choose a condition that gives us the
highest gain we will do that by
splitting the data using each condition
and checking the gain that we get out of
them the conditions that give us the
highest gain will be used to make the
first split so let's take a look at
these different conditions we have color
we have diameter and if we look
underneath that we have a couple
different values we have diameter equals
three color equals yellow red diameter
equals one and when we look at that
you'll see over here we have one two
three four threes that's a pretty hearty
selection so let's say the condition
gives us a maximum gain of three so we
have the most pieces fall into that
range so our first split from our
decision node is we split the data based
on the diameter is it greater than or
equal to three if it's not that's false
it goes into the great bolt and if it's
true it goes into a bowl fold of lemon
and apples the entropy after splitting
has decreased considerably so now we can
make two decisions if you look at
they're very much less chaos going on
there this node has already attained an
entropy value of zero as you can see
there's only one kind of label left for
this Branch so no further splitting is
required for this node however this node
on the right is still requires a split
to decrease the entropy further so we
split the right node further are based
on color if you look at this if I split
it on color that pretty much cuts it
right down the middle it's the only
thing we have left in our choices of
color and diameter too and if the color
is yellow it's going to go to the rifle
and if it's false it's going to go to
the left Bowl so the entropy in this
case is now zero so now we have three
moles with zero entropy there's only one
type of data in each one of those bowls
so we can predict a lemon with a hundred
percent accuracy and we can predict the
Apple also with 100 accuracy along with
our grapes up there so we've looked at
kind of a basic tree in our forest but
what we really want to know is how does
a random Forest work as a whole so to
begin our random Forest classifier let's
say we already have built three trees
and we're going to start with the first
tree that looks like this just like we
did in the example this tree looks at
the diameter if it's greater than or
equal to three it's true otherwise it's
false so one side goes to the smaller
diameter one side goes to larger
diameter and if the color is orange it's
going to go to the right true we're
using oranges now instead of limits and
if it's red it's going to go to the left
false we build a second tree very
similar but split differently instead of
the first one being split by a diameter
this one when they created it if you
look at that first Bowl it has a lot of
red objects so it says is the color red
because that's going to bring our
entropy down the fastest and so of
course if it's true it goes to the left
if it's false it goes to the right and
then it looks at the shape false or true
and so on and so on and tree three is
the diameter equal to one and it came up
with this because there's a lot of
cherries in this bowl so that would be
the biggest split on there is is the
diameter equal to one that's going to
drop the entropy the quickest and as you
can see it splits it into true if it
goes false and they've added another
category does it grow in the summer and
if it's false it goes off to the left if
it's true it goes off to the right let's
go ahead and bring these three trees you
can see them all in one image so this
would be three completely different
trees categorizing a fruit and let's
take a fruit now let's try this and this
fruit if you look at it we've blackened
it out you can't see the color on it so
it's missing data remember one of the
things we talked about earlier is that a
random Forest works really good if
you're missing data if you're missing
pieces so this fruit has an image but
maybe as a person had a black and white
camera when they took the picture and
we're going to take a look at this and
it's going to have they put the color in
there so ignore the color down there but
the diameter equals three we find out it
grows in the summer equals yes and the
shape is a circle and if you go to the
right you can look at what one of the
decision trees did this is the third one
is the diameter greater than equal to
three is a color orange well it doesn't
really know on this one but if you look
at the value it say true and go to the
right tree two classifies it as cherries
is a color equal red is is a shape a
circle true it is a circle so this would
look at it and say oh that's a cherry
and then we go to the other classifier
and it says is the diameter equal one
well that's false does it grow in the
summer true so it goes down and looks at
as oranges so how does this random
Forest work the first one says it's an
orange the second one said it was a
cherry and the third one says it's an
orange
and you can guess that if you have two
oranges and one says it's a cherry uh
when you add that all together the
majority of the vote says orange so the
answer is it's classified as an orange
even though we didn't know the color and
we're missing data on it I don't know
about you but I'm getting tired of fruit
so let's switch and I did promise you
we'd start looking at a case example and
get into some python coding today we're
going to use the case the iris flower
analysis
oh this is the exciting part as we roll
up our sleeves and actually look at some
python coating before we start the
python coding we need to go ahead and
create a problem statement wonder what
species of Iris do these flowers belong
to let's try to predict the species of
the flowers using machine learning in
Python let's see how it can be done so
here we begin to go ahead and Implement
our python code and you'll find that the
first half of our implementation is all
about organizing and exploring the data
coming in let's go ahead and take this
first step which is loading the
different modules into Python and let's
go ahead and put that in our favorite
editor whatever your favorite editor is
in this case I'm going to be using the
Anaconda Jupiter notebook which is one
of my favorites certainly there's
notepad plus plus and eclipse and dozens
of others or just even using the python
terminal window any of those will work
just fine to go ahead and explore this
python coding so here we go let's go
ahead and flip over to our Jupiter note
book and I've already opened up a new
page for Python 3 code and I'm just
going to paste this right in there and
let's take a look and see what we're
bringing into our python the first thing
we're going to do is from the
sklearn.datasets import load Iris now
this isn't the actual data this is just
the module that allows us to bring in
the data the load Iris and the iris is
so popular it's been around since 1936
when Ronald Fisher published a paper on
it and they're measuring the different
parts of the flower and based on those
measurements predicting what kind of
flower it is and then if we're going to
do a random Forest classifier we need to
go ahead and import our random forest
classifier from the sklearn module so
sklearn dot Ensemble import random force
classifier and then we want to bring in
two more modules and these are probably
the most commonly used modules in Python
and data science with any of the other
modules that we bring in and one is
going to be pandas we're going to import
pandas as PD p D is a common term used
for pandas and pandas is basically
creates a data format for us where when
you create a pandas data frame it looks
like an Excel spreadsheet and you'll see
that in a minute when we start digging
deeper into the code panda is just
wonderful because it plays nice with all
the other modules in there and then we
have numpy which is our numbers Python
and the numbers python allows us to do
different mathematical sets on here
we'll see right off the bat we're going
to take our NP and we're going to go
ahead and Seed the randomness with it
with zero so
np.random.seed is seating that as zero
this code doesn't actually show anything
we're going to go ahead and run it
because I need to make sure I have all
those loaded and then let's take a look
at the next module on here the next six
slides including this one are all about
exploring the data remember I told you
half of this is about looking at the
data and getting it all set so let's go
ahead and take this code right here the
script and let's get that over into our
Jupiter notebook and here we go we've
gone ahead and run the Imports and I'm
going to paste the code down here
and let's take a look and see what's
going on the first thing we're doing is
we're actually loading the iris data and
if you remember up here we loaded the
module that tells it how to get the IRS
data now we're actually assigning that
data to the variable Iris and then we're
going to go ahead and use the DF to
Define data frame
and that's going to equal PD and if you
remember that's pandas as PD so that's
our pandas
and Panda data frame and then we're
looking at Iris data and columns equals
Irish feature names
and we're going to do the DF head and
let's run this so you can understand
what's going on here
the first thing you want to notice is
that our DF has created what looks like
an Excel spreadsheet and in this Excel
spreadsheet we have set the columns so
up on the top you can see the four
different columns and then we have the
data iris.data down below it's a little
confusing without knowing where this
data is coming from so let's look at the
bigger picture and I'm going to go print
I'm just going to change this for a
moment and we're going to print all the
virus and see what that looks like
so when I print all a virus I get this
long list of information and you can
scroll through here and see all the
different titles on there
what's important to notice is that first
off there's a brackets at the beginning
so this is a python dictionary
and in a python dictionary you'll have a
key or a label and this label pulls up
whatever information comes after it so
feature names which we actually used
over here under columns is equal to an
array of simple length simple width
petal length petal width these are the
different names they have for the four
different columns and if you scroll down
far enough you'll also see data down
here oh goodness it came up right
towards the top and data is equal to the
different data we're looking at
now there's a lot of other things in
here like Target we're going to be
pulling that up in a minute and there's
also the names the target names which is
further down and we'll show you that
also in a minute let's go ahead and set
that back
to the Head
and this is one of the neat features of
pandas and Panda data frames
is when you do df.head or the panda
dataframe dot head it will print the
first five lines of the data set in
there along with the headers if you have
them in this case we have the column
header set to Iris features and in here
you'll see that we have 0 1 2 3 4 in
Python most arrays always start at zero
so when you look at the first five it's
going to be zero one two three four not
one two three four five so now we've got
our IRS data imported into a data frame
let's take a look at the next piece of
code in here and so in this section here
of the code we're going to take a look
at the Target and let's go ahead and get
this into our notebook this piece of
code so we can discuss it a little bit
more in detail so here we are in our
jupyter notebook I'm going to put the
code in here and before I run it I want
to look at a couple things going on so
we have DF species and this is
interesting because right here you'll
see where I have DF species in Brackets
which is the key code for creating
another column and here we have
iris.target now these are both in the
pandas setup on here so in pandas we can
do either one I could have just as
easily done Iris and then in Brackets
Target depending on what I'm working on
both are acceptable let's go ahead and
run this code and see how this changes
and what we've done is we've added the
target from the iris data set as another
column on the end
now what species is this is what we're
trying to predict so we have our data
which tells us the answer for all these
different pieces and then we've added a
column with the answer that way when we
do our final setup we'll have the
ability to program our our neural
network to look for these this different
data and know what a setosa is or a Vera
color which we'll see in just a minute
or virginica those are the three that
are in there and now we're going to add
one more column I know we're organizing
all this data over and over again it's
kind of fun there's a lot of ways to
organize it what's nice about putting
everything onto one data frame is I can
then do a printout and it shows me
exactly what I'm looking at and I'll
show you that where you where that's
different where you can alter that and
do it slightly differently but let's go
ahead and put this into our script up to
that now and here we go we're going to
put that down here
and we're going to run that
and let's talk a little bit about what
we're doing now we're exploring data
and one of the challenges is knowing how
good your model is did your model work
and to do this we need to split the data
and we split it into two different parts
they usually call it the training and
the testing and so in here we're going
to go ahead and put that in our database
so you can see it clearly and we've set
it DF remember you can put brackets this
is creating another column is train so
we're going to use part of it for
training and this equals NP remember
that stands for numpy DOT random.uniform
so we're generating a random number
between 0 and 1 and we're going to do it
for each of the rows that's where the
length DF comes from so each row gets a
generated number and if it's less than
0.75 it's true and if it's greater than
0.75 it's false this means we're going
to take 75 percent of the data roughly
because there's a Randomness involved
and we're going to use that to train it
and then the other 25 percent we're
going to hold off to the side and use
that to test it later on on so let's
flip back on over and see what the next
step is so now that we've labeled our
database for which is training and which
is testing let's go ahead and sort that
into two different variables train and
test and let's take this code and let's
bring it into our project and here we go
let's paste it on down here and before I
run this let's just take a quick look at
what's going on here is we have up above
we created remember there's our def dot
head which prints the first five rows
and we've added a column is train at the
end and so we're going to take that
we're going to create two variables
we're going to create two new data
frames one's called train one's called
test 75 in train 25 in test
and then to sort that out
we're going to do that by doing DF our
main original data frame with the iris
data in it and if DF is trained equals
true
that's going to go in the train and if
DF is train equals false it goes in the
test and so when I run this
we're going to print out the number in
each one let's see what that looks like
and you'll see that it puts 118 in the
training module and it puts 32 in the
testing module which lets us know that
there was 150 lines of data in here so
if you went and looked at the original
data you could see that there's 150
lines and that's roughly 75 percent in
one and 25 percent for us to test our
model on afterward so let's jump back to
our code and see where this goes in the
next two steps
we want to do one more thing with our
data and let's make it readable to
humans I don't know about you but I hate
looking at zeros and ones so let's start
with the features and let's go ahead and
take those and make those readable to
humans and let's put that in our code
let's see here we go paste it in and
you'll see here we've done a couple very
basic things we know that the columns in
our data frame again this is a panda
thing the DF columns
and we know the first four of them 0 1 2
3 that'd be the first four are going to
be the features or the titles of those
columns and so when I run this
you'll see down here that it creates an
index sepa length sepa width petal
length and petal width and this should
be familiar because if you look up here
here's our column titles going across
and here's the first four
one thing I want you to notice here is
that when you're in a command line
whether it's Jupiter notebook or you're
running command line in the terminal
window if you just put the name of it
it'll print it out this is the same as
doing print
features
and the shorthand is you just put
features in here if you're actually
writing a code
and saving the script and running it by
remote you really need to put the print
in there but for this when I run it
you'll see it gives me the same thing
but for this we want to go ahead and
we'll just leave it as features because
it doesn't really matter and this is one
of the fun thing about Jupiter notebooks
is I'm just building the code as we go
and then we need to go ahead and create
the labels for the other part so let's
take a look and see what that for our
final step in prepping our data before
we actually start running the training
and the testing is we're going to go
ahead and convert the species on here
into something the computer understands
so let's put this code into our script
and see where that takes us
all right here we go we've set y equal
to PD dot factorize train species of
zero so let's break this down just a
little bit we have our pandas right here
PD factorize what's factorize doing I'm
going to come back to that in just a
second let's look at what train species
is and why we're looking at the group 0
on there
and let's go up here and here is our
species
remember this on that we created this
whole column here for species
and then it has setosis cytosis cytosis
cytosa and if you scroll down enough
you'd also see virginica and Vera color
we need to convert that into something
the computer understands zeros and ones
so the trained species of zero because
this is in the format of a of an array
of arrays so you have to have the zero
on the end and then species is just that
column factorize goes in there looks at
the fact that there's only three of them
so when I run this you'll see that y
generates an array that's equal to in
this case it's the training set and it's
zeros ones and twos representing the
three different kinds of flowers we have
so now we have something the computer
understands and we have a nice table
that we can read and understand and now
finally we get to actually start doing
the predicting so here we go we have two
lines of code oh my goodness that was a
lot of work to get to two lines of code
but there is a lot in these two lines of
code so let's take a look and see what's
going on here and put this into our full
script that we're running and let's
paste this in here and let's take a look
and see what this is we have we're
creating a variable clf and we're going
to set this equal to the random forest
classifier and we're passing two
variables in here and there's a lot of
variables you can play with as far as
these two are concerned they're very
standard in jobs all that does is to
prioritize it not something to really
worry about usually when you're doing
this on your own computer you do in jobs
equals two if you're working in a larger
or big data and you need to prioritize
it differently this is what that number
does is it changes your priorities and
how it's going to across the system and
things like that and then the random
state is just how it starts zero is fine
for here
but let's go ahead and run this
we also have clf.fit train features
comma Y and before we run it let's talk
about this a little bit more clf dot fit
so we're fitting we're training it we
are actually creating our random Forest
classifier right here this is a code
that does everything and we're going to
take our training set remember we kept
our test off to the side and we're going
to take our training set with the
features and then we're going to go
ahead and put that in and here's our
Target the Y so the Y is 0 1 and 2 that
we just created and the features is the
actual data going in that we put into
the training set let's go ahead and run
that
and this is kind of an interesting thing
because it printed out the random Force
classifier
and everything around it
and so when you're running this in your
terminal window or in a script like this
this automatically treats this like just
like when we were up here and I typed in
y and it printed out y instead of print
y
this does the same thing it treats this
as a variable and prints it out but if
you're actually running your code that
wouldn't be the case and what is printed
out is it shows us all the different
variables we can change and if we go
down here you can actually see in jobs
equals two you can see the random State
equals zero those are the two that we
sent in there you would really have to
dig deep to find out all these the
different meanings of all these
different settings on here some of them
are self-explanatory if you kind of
think about it a little bit like Max
features is auto so all the features
that we're putting in there is just
going to automatically take all four of
them whatever we send it it'll take some
of them might have so many features
because you're processing words there
might be like 1.4 million features in
there because you're doing legal
documents and that's how many different
words are in there at that point you
probably want to limit the maximum
features that you're going to process in
leaf nodes that's the end notes remember
we had the fruit and we're talking about
the leaf notes like I said there's a lot
in this we're looking at a lot of stuff
here so you might have in this case
there's probably only think three leaf
nodes maybe four you might have
thousands of leaf nodes at which point
you do need to put a cap on that and say
okay you can only go so far and then
we're going to use all of our resources
on processing this and that really is
what most of these are about is limiting
the process and making sure we don't
overwhelm a system and there's some
other settings in here again we're not
going to go over all of them warm start
equals false alarm start is if you're
programming it one piece at a time
externally since we're not we're not
going to have like we're not going to
continually to train this particular
Learning Tree and again like I said
there's a lot of things in here that
you'll want to look up more detail from
the SK learn and if you're digging in
deep and running a major project on here
for today though all we need to do is
fit or train our features and our Target
y so now we have our training model
what's next if we're going to create a
model
we now need to test it remember we set
aside the test feature test group 25 of
the data so let's go ahead and take this
code and let's put it into our script
and see what that looks like okay here
we go
and we're going to run this
and it's going to come out with a bunch
of zeros ones and twos which represents
the three type of flowers the setosa the
virginica and the Versa color and what
we're putting into our predict is the
test features and I always kind of like
to know what it is I am looking at so
real quick we're going to do test
features and remember features is an
array
of simple length sepal width pedal
length pedal width so when we put it in
this way it actually loads all these
different columns that we loaded into
features so if we did just features let
me just do features in here seeing so
what features looks like this is just
playing with the with pandas data frames
you'll see that it's an index so when
you put an index in like this
into test features into test it then
takes those columns and creates a panda
data frames from those columns and in
this case
we're going to go ahead and put those
into our predict
so we're going to put each one of these
lines of data
the 5.0 3.4 1.5.2 and we're going to put
those in and we're going to predict what
our new Forest classifier is going to
come up with and this is what it
predicts it predicts uh zero zero zero
one two one one two two two and and
again this is the flower type sotosa
virginica and Versa color so now that
we've taken our test features let's
explore that let's see exactly what that
data means to us so the first thing we
can do with our predicts is we can
actually generate a different prediction
model when I say different we're going
to view it differently it's not that the
data itself is different so let's take
this next piece of code and put it into
our script
so we're pasting it in here and you'll
see that we're doing uh predict and
we've added underscore proba for
probability so there's our clf DOT
predict probability so we're running it
just like we ran it up here but this
time with this we're going to get
slightly different result and we're only
going to look at the first 10.
so you'll see down here instead of
looking at all of them uh which was what
27 you'll see right down here that this
generates a much larger field on the
probability and let's take a look and
see what that looks like and what that
means
so when we do the predict underscore
praba for probability it generates three
numbers so we had three leaf nodes at
the end and if you remember from all the
theory we did this is the predictors the
first one is predicting a one for setosa
it predicts a zero for virginica and it
predicts a zero for Versa color and so
on and so on and so on and let's uh you
know what I'm going to change this just
a little bit let's look at 10
to 20 just because we can
and we start to get a little different
of data and you'll see right down here
it gets to this one this line right here
and this line has 0 0.5 0.5 and so if
we're going to vote and we have two
equal votes it's going to go with the
first one so it says uh satosha gets
zero votes virginica gets 0.5 votes
Versa color gets 0.5 votes but let's
just go with the virginica since these
two are equal and so on and so on down
the list you can see how they vary on
here so now we've looked at both how to
do a basic predict of the features and
we've looked at the predict probability
let's see what's next on here so now we
want to go ahead and start mapping names
for the plants we want to attach names
so that it makes a little more sense for
us and this we're going to do in these
next two steps we're going to start by
setting up our predictions and mapping
them to the name so let's see what that
looks like
and let's go ahead and paste that code
in here and run it and this goes along
with the next piece of code so we'll
skip through this quickly and then come
back to it a little bit so here's Iris
dot Target names
and uh if you remember correctly this
was the the names that we've been
talking about this whole time the setosa
virginica versus color and then we're
going to go ahead and do the prediction
again we've run we could have just hit a
variable equal to this instead of
re-running it each time but we'll go
ahead and run it again clf dot predict
test features remember that Returns the
zeros the ones and the twos and then
we're going to set that equal to
predictions so this time we're actually
putting it in a variable and when I run
this
it distributes it it comes out as an
array and the array is setosis cytosis
cytosa we're only looking at the first
five we could actually do let's do the
first 25 just so we can see a little bit
more on there and you'll see that it
starts mapping it to all the different
flower types the Versa color and the
virginica in there and let's see how
this goes with the next one so let's
take a look at the top part of our
species in here and we'll take this code
and put it in our script
and let's put that down here and paste
it there we go and we'll go ahead and
run it and let's talk about both these
sections of code here
and how they go together the first one
is our predictions and I went ahead and
did predictions through 25 let's just do
five
and so we have cytosis cytosis cytosis
cytosis that's what we're predicting
from our test model
and then we come down here we look at
test species I remember I could have
just done
test.species.head and you'll see it says
cytosis cytosis cytosa and they match so
the first one is what our forest is
doing
and the second one is what the actual
data is now is we need to combine these
so that we can understand what that
means we need to know how good our
forest is how good it is at predicting
the features so that's where we come up
to the next step which is lots of fun
we're going to use a single line of code
to combine our predictions and our
actuals so we have a nice chart to look
at and let's go ahead and put that in
our script in our Jupiter notebook here
let's see let's go ahead and paste that
in and then I'm going to because I'm on
the jupyter notebook I can do a control
minus you can see the whole line there
there we go resize it and let's take a
look and see what's going on here we're
going to create in pandas remember PD
stands for pandas and we're doing a
cross tab this function takes two sets
of data and creates a chart out of them
so when I run it you'll get a nice chart
down here and we have the predicted
species
so across the top you'll see the Sentosa
versus color virginica and the actual
species sistosa versicolor virginica and
so the way to read this chart and let's
go ahead and take a look on how to read
this chart here when you read this chart
you have setosa where they meet you have
versicolor where they meet and you have
virginica where they meet and they're
meeting where the actual and the
predicted agree so this is the number of
accurate predictions so in this case it
equals 30. if you had 13 plus 5 plus 12
you get 30. and then we notice here
where it says virginica but it was
supposed to be versacolor this is
inaccurate so now we have two two
inaccurate predictions and 30 accurate
predictions so I will say that the model
accuracy is 93 that's just 30 divided by
32 and if we multiply it by a hundred we
can say that it is 93 accurate so we
have a 93 accuracy with our model I did
want to add one more quick thing in here
on our scripting before we wrap it up so
let's flip back on over to my script in
here we're going to take this line of
code from up above I don't know if you
remember it but predicts equals the iris
dot Target underscore names so we're
going to map it to the names
and we're going to run the prediction
and we read it on test features but you
know we're not just testing it we want
to actually deploy it so at this point I
would go ahead and change this and this
is an array of arrays this is really
important when you're running these to
know that
so you need to double brackets and I
could actually create data maybe let's
let's just do two flowers so maybe I'm
processing more data coming in and we'll
put two flowers in here
and then I actually want to see what the
answer is so let's go ahead and type in
preds and print that out and when I run
this
you'll see that I've now predicted two
flowers so maybe I measured in my front
yard as versacolor and versacolor
not surprising since I put the same data
in for each one
this would be the actual end product
going out to be used on data that you
don't know the answer for
so that's going to conclude our
scripting part of this if you are an
aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simple learns professional certification
program in Ai and machine learning from
Purdue University in collaboration with
IBM should be a right choice for more
details use the link in the description
box below with that in mind by now we
all know machine learning models make
predictions by learning from the past
data available so we have our input
values our machine learning model Builds
on those inputs of what we already know
and then we use that to create a
predicted output is that a dog a little
kid looking over there and watching the
black cat cross their path no dear you
can differentiate between a cat and a
dog based on their characteristics
cats cats have sharp claws uses to climb
smaller length of ears meows and purrs
doesn't love to play around dogs they
have dull claws bigger length of ears
barks loves to run around you usually
don't see a cat running around people
although I do have a cat that does that
where dogs do and we can look at these
we can say we can evaluate their
sharpness of the claws how sharp are
their claws and we can evaluate the
length of the ears and we can usually
sort out cats from dogs based on even
those two characteristics now tell me if
it is a cat or a dog an odd question
usually little kids no cats and dogs by
now unless you live in a place where
there's not many cats or dogs so if we
look at the sharpness of the claws the
length of the ears and we can see that
the cat has a smaller ears and sharper
claws than the other animals its
features are more like cats it must be a
cat sharp claws length of ears and goes
in the cat group because KNN is based on
feature similarity we can do
classification using kn and classifier
so we have our input value the picture
of the black cat it goes into our
trained model and it predicts that this
is a cat coming out so what is KNN what
is the k n algorithm K nearest neighbors
is what that stands for it's one of the
simplest supervised machine learning
algorithms mostly used for
classification so we want to know is
this a dog or is not a dog is it a cat
or not a cat it classifies a data point
based on how his neighbors are
classified KNN stores all available
cases and classifies new cases based on
a similarity measure and here we've gone
from cats and dogs right into wine
another favorite of mine k n stores all
available cases and classifies new cases
based on a similarity measure and here
you see we have a measurement of sulfur
dioxide versus the chloride level and
then the different wines they've tested
and where they fall on that graph based
on how much sulfur dioxide and how much
chloride K and K N is a perimeter that
refers to the number of nearest
neighbors to include in the majority of
the voting process and so if we add a
new glass of wine there red or white we
want to know what the neighbors are in
this case we're going to put k equals
five we'll talk about K in just a minute
a day point is classified by the
majority of votes from its five nearest
neighbors here the unknown point would
be classified as red since four out of
five neighbors are red so how do we
choose K how do we know k equals five I
mean that's was the value we put in
there so we're going to talk about it
how do we choose the factor K and N
algorithm is based on feature similarity
choosing the right value of K is a
process called parameter tuning and is
important for better accuracy so at k
equals three we can classify we have a
question mark in the middle as either a
as a square or not is it a square or is
it in this case a triangle and so if we
set k equals to 3 we're going to look at
the three nearest neighbors we're going
to say this as a square and if we put k
equals to 7 we classify as a triangle
depending on what the other data is
around it you can see as the K changes
depending on where that point is that
drastically changes your answer and we
jump here we go how do we choose the
factor of K you'll find this in all
machine learning choosing these facts
factors that's the face you get he's
like oh my gosh you say choose the right
K did I set it right my values in
whatever machine learning tool you're
looking at so that you don't have a huge
bias in One Direction or the other and
in terms of knnn the number of K if you
choose it too low the bias is based on
it's just too noisy it's it's right next
to a couple things and it's going to
pick those things and you might get a
skewed answer and if your K is too big
then it's going to take forever to
process so you're going to run into
processing issues and resource issues so
what we do the most common use and
there's other options for choosing K is
to use the square root of n so N is a
total number of values you have you take
the square root of it in most cases you
also if it's an even number so if you're
using uh like in this case squares and
triangles if it's even you want to make
your K value odd that helps it select
better so in other words you're not
going to have a balance between two
different factors that are equal so
usually take the square root of n and if
it's even you add one to it or subtract
one from it and that's where you get the
K value from that is the most common use
and it's pretty solid it works very well
when do we use KNN we can use K N when
data is labeled so you need a label on
it we know we have a group of pictures
with dogs dogs cats cats data is Noise
free and so you can see here when we
have a class and we have like
underweight 140 23 Hello Kitty normal
that's pretty confusing we have a high
variety of data coming in so it's very
noisy and that would cause an issue data
set is small so we're usually working
with smaller data sets where you might
get into a gig of data if it's really
clean it doesn't have a lot of noise
because K N is a lazy learner I.E it
doesn't learn a discriminative function
from the training set so it's very lazy
so if you have very complicated data and
you have a large amount of it you're not
going to use the k n but it's really
great to get a place to start even with
large data you can sort out a small
sample and get an idea of what that
looks like using the knnn and also just
using for smaller data sets k n works
really good how does the K N algorithm
work consider a data set having two
variables height in centimeters and
weight in kilograms and each point is
classified as normal or underweight so
we can see right here we have two
variables you know true false are either
normal or they're not they're
underweight on the basis of the given
data we have to classify the below set
as normal or underweight using k n n so
if we have new data coming in this says
57 kilograms and 177 centimeters is that
going to be normal or underweight to
find the nearest neighbors we'll
calculate the euclidean distance
according to the euclidean distance
formula the distance between two points
in the plane with the coordinates x y
and a b is given by distance D equals
the square root of x minus a squared
plus y minus B squared and you can
remember that from the two edges of a
triangle we're Computing the third Edge
since we we know the X side and the Y
side let's calculate it to understand
clearly so we have our unknown point and
we placed it there in red and we have
our other points where the data is
scattered around the distance D1 is the
square root of 170 minus 167 squared
plus 57 minus 51 squared which is about
6.7 and distance 2 is about 13 and
distance 3 is about 13.4 similarly we
will calculate the euclidean distance of
unknown data point from all the points
in the data set and because we're
dealing with small amount of data that's
not that hard to do and it's actually
pretty quick for a computer and it's not
a really complicated Mass you can just
see how close is the data based on the
euclidean distance hence we have
calculated the euclidean distance of
unknown data point from all the points
as shown where X1 and y1 equal 57 and
170 whose class we have to classify so
now we're looking at that we're saying
well here's the euclidean distance who's
going to be their closest neighbors now
let's calculate the nearest neighbor at
k equals three and we can see the three
closest neighbors put some at normal and
that's pretty self-evident when you look
at this graph it's pretty easy to say
okay what you know we're just voting
normal normal three votes for normal
this is going to be a normal weight so
majority of neighbors are pointing
towards normal hence as per k n
algorithm the class of 57 170 should be
normal so a recap of knnn positive
integer K is specified along with a new
sample we select the K entries in our
database which are closest to the new
sample we find the most common
classification of these entries this is
the classification we give to the new
sample so as you can see it's pretty
straightforward we're just looking for
the closest things that match what we
got so let's take a look and see what
that looks like in a use case in Python
so let's dive into the predict diabetes
use case so use case predict diabetes
the objective predict whether a person
will be diagnosed with diabetes or not
we have a data set of 768 people who
were or were not diagnosed with diabetes
and let's go ahead and open that file
and just take a look at that data and
this is in a simple spreadsheet format
the data itself is comma separated very
common set of data and it's also a very
common way to get the data and you can
see here we have columns a through I
that's what one two three four five six
seven eight eight columns with a
particular attribute and then the ninth
column which is the outcome is whether
they have diabetes as a data scientist
the first thing you should be looking at
is insulin well you know if someone has
insulin they have diabetes that's why
they're taking it and that could cause
issue in some of the machine learning
packages but for a very basic setup this
works fine for doing the KNN and the
next thing you notice is it didn't take
very much to open it up I can scroll
down to the bottom of the data there's
768. it's pretty much a small data set
you know at 769 I can easily fit this
into my ram on my computer computer I
can look at it I can manipulate it and
it's not going to really tax just a
regular desktop computer you don't even
need an Enterprise version to run a lot
of this so let's start with importing
all the tools we need and before that of
course we need to discuss what IDE I'm
using certainly can use any a particular
editor for python but I like to use for
doing very basic visual stuff the
Anaconda which is great for doing demos
with the jupyter notebook and just a
quick view of the Anaconda Navigator
which is the new release out there which
is really nice you can see under home I
can choose my application we're going to
be using python36 I have a couple
different versions on this particular
machine if I go under environments I
create a unique environment for each one
which is nice and there's even a little
button there where you can install
different packages so if I click on that
button and open the terminal I can then
use a simple pip install to install
different packages I'm working with
let's go ahead and go back under home
and we're going to launch our notebook
and I've already you know kind of like
the old cooking shows I've already
prepared a lot of my stuff so we don't
have to wait for it to launch because it
takes a few minutes for it to open up a
browser window in this case I'm going
it's going to open up Chrome because
that's my default that I use and since
the script is pre-done you'll see I have
a number of windows open up at the top
the one we're working in and since we're
working on the KNN predict whether a
person will have diabetes or not let's
go and put that title in there and I'm
also going to go up here and click on
Cell actually we want to go ahead and
first insert a cell below and then I'm
going to go back up to the top cell and
I'm going to change the cell type to
markdown that means this is not going to
run as python it's a markdown language
so if I run this first one it comes up
in nice big letters which is kind of
nice minus what we're working on and by
now you should be familiar with doing
all of our Imports we're going to import
the pandas as PD import numpy is NP
pandas is the pandas data frame and
numpy is a number array very powerful
tools to use in here so we have our
Imports so even brought in our pandas
are numpy our two general python tools
and then you can see over here we have
our train test split by now you should
be familiar with splitting the data we
want to split part of it for training
our thing and then training our
particular model and then we want to go
ahead and test the remaining data to see
how good it is pre-processing a standard
scalar preprocessor so we don't have a
bias of really large numbers remember in
the data we had like number of
pregnancies isn't going to get very
large where the amount of insulin they
take and get up to 256 so 256 versus 6
that will skew results so we want to go
ahead and change that so they're all
uniform between
-1 and 1. and then the actual tool this
is the K neighbors classifier we're
going to use
and finally the last three are three
tools to test all about testing our
model how good is it we just put down
test on there and we have our confusion
Matrix our F1 score and our accuracy so
we have our two general python modules
we're importing and then we have our six
modules specific from the SK learn setup
and then we do need to go ahead and run
this so these are actually imported
there we go and then move on to the next
step and so in this set we're going to
go ahead and load the database we're
going to use pandas remember pandas is
PD and we'll take a look at the data in
Python we looked at it in a simple
spreadsheet but usually I like to also
pull it up so we can see what we're
doing so here's our data set equals
pd.read CSV that's a pandas command and
the diabetes folder I just put in the
same folder where my IPython script is
if you put in a different folder you
need the full length on there we can
also do a quick Links of the data set
that is a simple python on command Len
for length we might even let's go ahead
and print that we'll go print and if you
do it on its own line link that data set
in the jupyter notebook it'll
automatically print it but when you're
in most of your different setups you
want to do the print in front of there
and then we want to take a look at the
actual data set and since we're in
pandas we can simply do data set head
and again let's go ahead and add the
print in there if you put a bunch of
these in a row you know the data set one
head data set two head it only prints
out the last one so I usually always
like to keep the print statement in
there but because most projects only use
one data frame pandas data frame doing
it this way it doesn't really matter the
other way works just fine and you can
see when we hit the Run button we have
the 768 lines which we knew and we have
our pregnancies it's automatically given
a label on the left remember the head
only shows the first five lines so we
have zero through four and just a quick
look at the data you can see it matches
what we looked at before we have
pregnancy glucose blood pressure sure
all the way to H and then the outcome on
the end and we're going to do a couple
things in this next step we're going to
create a list of columns where we can't
have zero there's no such thing as zero
skin thickness or zero blood pressure
zero glucose any of those you'd be dead
so not a really good Factor if they
don't if they have a zero in there
because they didn't have the data and
we'll take a look at that because we're
going to start replacing that
information with a couple of different
things and let's see what that looks
like so first we create a nice list as
you can see we have the values talked
about glucose blood pressure skin
thickness and this is a nice way when
you're working with columns is to list
the columns you need to do some kind of
transformation on a very common thing to
do and then for this particular setup we
certainly could use the there's some
Panda tools that will do a lot of this
where we can replace the N A but we're
going to go ahead and do it as a data
set column equals datasetc column dot
replace this is this is still pandas you
can do a direct there's also that's that
you look for your nan a lot of different
options in here but the Nan num pnan is
what that stands for is none it doesn't
exist so the first thing we're doing
here is we're replacing the zero with a
numpy none there's no data there that's
what that says that's what this is
saying right here so put the 0 in and
we're going to play zeros with no data
so if it's a zero that means a person's
well hopefully not dead hope they just
didn't get the data the next thing we
want to do is we're going to create the
mean which is the integer from the data
set from the column dot mean where we
skip in A's we can do that and that is a
pandas command there the skip n a so
we're going to figure out the mean of
that data set and then we're going to
take that data set column and we're
going to replace all the npnan with the
means why did we do that and we could
have actually just taken this step and
gone right down here and just replace 0
and Skip anything where except you can
actually there's a way to skip zeros and
then just replace all the zeros but in
this case we want to go ahead and do it
this way so you can see that we're
switching this to a non-existent value
then we're going to create the mean well
this is the average person so if we
don't know what it is if they did not
get the data and the data is missing one
of the tricks is you replace it with the
average what is the most common data for
that this way you can still use the rest
of those values to do your computation
and it kind of just brings that
particular value of those missing values
out of the equation let's go ahead and
take this and we'll go ahead and run it
doesn't actually do anything so we're
still preparing our data if you want to
see what that looks like we don't have
anything the first few lines just not
going to show up but we certainly could
look at a row let's do that let's go
into our data set with the printed data
set and let's pick in this case let's
just do glucose and if I run this this
is going to print all the different
glucose levels going down and we
thankfully don't see anything in here
that looks like missing data at least on
the ones shows you can see I skipped a
bunch in the middle because that's what
it does we have too many lines in
Jupiter notebook it'll skip a few and go
on to the next in a data set let me go
and remove this and we'll just zero out
that and of course before we do any
processing before proceeding any further
we need to split the data set into our
train and testing data that way we have
something to train it with and something
to test it on and you're going to notice
we did a little something here with the
pandas database code there we go my
drawing tool we've added in this right
here of the data set and what this says
is that the first one in pandas this is
from the PD pandas it's going to say
within the data set we want to look at
the eye location and it is all rows
that's what that says so we're going to
keep all the rows but we're only looking
at zero columns 0 To 8. Remember column
nine here it is right up here we printed
it in here is outcome well that's not
part of the training data that's part of
the answer yes column nine but it's
listed as eight number eight so zero to
eight is nine columns so eight is the
value and when you see it in here 0 this
is actually zero to seven it doesn't
include the last one and then we go down
here to Y which is our answer and we
want just the last one just column eight
and you can do it this way with this
particular notation and then if you
remember we imported the train test
split that's part of the SK learn right
there and we simply put in our X and our
y we're going to do random State equals
zero you don't have to necessarily seed
it that's a seed number I think the
default is one one when you seated I
have to look that up and then the test
size test size is 0.2 that simply means
we're going to take 20 percent of the
data and put it aside so that we can
test it later that's all that is and
again we're going to run it not very
exciting so far we haven't had any
printout other than to look at the data
but that is a lot of this is prepping
this data once you prep it the actual
lines of code are quick and easy and
we're almost there with the actual
running of our k n we need to go ahead
and do a scale the data if you remember
correctly we're fitting the data in a
standard scalar which means instead of
the data being from you know 5 to 303 in
one column and the next column is one to
six we're going to set that all so that
all the data is between minus one and
one that's what that standard scalar
does keeps it standardized and we only
want to fit the scalar with the training
set but we want to make sure the testing
set is the X test going in is also
transformed so it's processing it the
same so here we go with our standard
scalar we're going to call it SC
underscore X for the scalar and we're
going to import the standard scalar into
this variable and then our X train
equals SC underscore x dot fit transform
so we're creating the scalar on the X
train variable and then our X test we're
also going to transform it so we've
trained and transformed the X train and
then the X test isn't part of that
training it isn't part of the of
training the Transformer it just gets
transformed that's all it does and again
we're going to run this if you look at
this we've now gone through these steps
all three of them we've taken care of
replacing our zeros for key columns it
shouldn't be zero and we replace that
with the means of those columns that way
that they fit right in with our data
models we've come down here and we split
the data so now we have our test data
and our training data and then we've
taken and we've scaled the data so all
of our data going in now no we don't TR
we don't train the Y part though y train
and Y test that never has to be trained
it's only the data going in that's what
we want to train in there then Define
the model using K Nabors classifier and
fit the train data in the model so we do
all that data prep and you can see down
here we're only going to have a couple
lines of code where we're actually
building our model and training it
that's one of the cool things about
Python and how far we've come it's such
an exciting time to be in machine
learning because there's so many
automated tools let's see before we do
this let's do a quick Links of and let's
do y we want let's just do length of Y
and we get 768 and if we import math we
do math dot square root let's do y train
there we go it's actually supposed to be
X train before we do this let's go ahead
and do import math and do math square
root length of Y test and when I run
that we get 12.409 I want to see show
you where this number comes from we're
about to to use 12 is an even number so
if you know if you're ever voting on
things remember the neighbors all vote
don't want to have an even number of
neighbors voting so we want to do
something odd and let's just take one
away we'll make it 11. let me delete
this out of here that's one of the
reasons I love Jupiter notebook because
you can flip around and do all kinds of
things on the fly so we'll go ahead and
put in our classifier we're creating our
classifier now and it's going to be the
K neighbors classifier and neighbors
equal 11. remember we did 12 minus 1 for
11. we have an odd number of neighbors P
equals 2 because we're looking for is it
are they diabetic or not and we're using
the euclidean metric there are other
means of measuring the distance you
could do like square square means value
there's all kinds of measure this but
the euclidean is the most common one and
it works quite well it's important to
evaluate the model let's use the
confusion Matrix to do that and we're
going to use the confusion Matrix
wonderful tool and then we'll jump into
the F1 score
and finally accuracy score which is
probably the most commonly used quoted
number when you go into a meeting or
something like that so let's go ahead
and paste that in there and we'll set
the cm equal to confusion Matrix why
test why predict so those are the two
values we're going to put in there and
let me go ahead and run that and print
it out and the way you interpret this is
you have the Y predicted which would be
your title up here we could do let's
just do p r e d
predicted across the top and actual
going down actual
it's always hard to to write in here
actual that means that this column here
down the middle that's the important
column and it means that our prediction
said 94 and prediction in the actual
agreed on 94 and 32. this number here
the 13 and the 15 those are what was
wrong so you could have like three
different if you're looking at this
across three different variables instead
of just two you'd end up with the third
row down here in the column going down
the middle so in the first case we have
the the and I believe the zero has a 94
people who don't have diabetes the
prediction said that 13 of those people
did have diabetes and were at higher
risk and the 32 that had diabetes it had
correct but our prediction said another
15 out of that 15 it classified as
incorrect so you can see where that
classification comes in and how that
works on the confusion Matrix then we're
going to go ahead and print the F1 score
let me just run that and you see we get
a 0.69 in our F1 score the F1 takes into
account both sides of the balance of
false positives where if we go ahead and
just do the accuracy account and that's
what most people think of is it looks at
just how many we got right out of how
many we got wrong so a lot of people
when you're a data scientist and you're
talking to other data scientists they're
going to ask you what the F1 score the F
score is if you're talking to the
general public or the decision makers in
the business they're going to ask what
the accuracy is and the accuracy is
always better than the F1 score but the
F1 scores more telling it lets us know
that there's more false positives than
we would like on here but 82 percent not
too bad for a quick flash look at
people's different statistics and
running an sklearn and running the k n n
the K nearest neighbor on it so we have
created a model using KNN which can
predict whether a person will have
diabetes or not or the very least East
whether they should go get a checkup and
have their glucose checked regularly or
not the print accuracy score we got the
0.818 was pretty close to what we got
and we can pretty much round that off
and just say we have an accuracy of 80
percent tells us it is a pretty fair fit
in the model if you are an aspiring
machine learning engineer looking for
online training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learns
professional certification program in Ai
and machine learning from Purdue
University in collaboration with IBM
should be a right choice for more
details use the link in the description
box below with that in mind and find out
why support Vector machine so in this
example last week my son and I visited a
fruit shop dad is that an apple or a
strawberry so the question comes up what
fruit do they just pick up from the
Fruit Stand after a couple of seconds
you could figure out that it was a
strawberry so let's take this model a
step further and let's uh why not build
a model which can predict an unknown
data and in this we're going to be
looking at some sweet strawberries or
crispy apples we wanted to be able to
label those two and decide what the
fruit is and we do that by having data
already put in so we already have a
bunch of strawberries we know our
strawberries and they're already labeled
as such we already have a bunch of
apples we know our apples and are
labeled as such then once we train our
model that model then can be given the
new data and the new data is this image
in this case you can see a question mark
on it and it comes through and goes it's
a strawberry in this case we're using
the support Vector Machine model svm is
a supervised learning method that looks
at data and sorts it into one of two
categories and in this case we're
sorting the strawberry into the
strawberry side at this point you should
be asking the question how does the
prediction work before we dig into an
example with numbers let's apply this to
our fruit scenario we have our support
Vector machine we've taken it and we've
taken labeled sample of data
strawberries and apples and we draw on a
line down the middle between the two
groups this split now allows us to take
new data in this case an apple and a
strawberry and place them in the
appropriate group based on which side of
the line they fall in and that way we
can predict the unknown as colorful and
tasty as the food example is let's take
a look at another example with some
numbers involved and we can take a
closer look at how the math Works in
this example we're going to be
classifying men and women and we're
going to start with a set of people with
a different height and a different
weight and to make this work we'll have
to have a sample data set a female where
you have their height weight 174 65 174
88 and so on and we'll need a sample
data set of the male they have a height
179 90 180 to 80 and so on let's go
ahead and put this on a graph so we have
a nice visual so you can see here we
have two groups based on the height
versus the weight and on the left side
we're going to have the women on the
right side we're going to have them in
now if we're going to create a
classifier let's add a new data point
and figure out if it's male or female so
before we can do that we need to split
our data first we can split our data by
choosing any of these lines in this case
we draw in two lines through the data in
the middle that separates some men from
the women but to predict the gender of a
new data point we should split the data
in the best possible way and we say the
best possible way because this line has
a maximum space that separates the two
classes here you can see there's a clear
split between the two different classes
and in this one there's not so much a
clear split this doesn't have the
maximum space it separates the two that
is why this line best splits the data we
don't want to just do this by eyeballing
it and before we go further we need to
add some technical terms to this we can
also say that the distance between the
points and the line should be as far as
possible in technical terms we can say
the distance between the support vector
and the hyperplane should be as far as
possible and this is where the support
vectors are the extreme points in the
data set and if you look at this data
set they have circled two points which
seem to to be right on the outskirts of
the women and one on the outskirts of
the men and hyperplane has a maximum
distance to the support vectors of any
class now you'll see the line down the
middle and we call this the hyperplane
because when you're dealing with
multiple Dimensions it's really not just
a line but a plane of intersections and
you can see here where the support
vectors have been drawn in dashed lines
the math behind this is very simple we
take D plus the shortest distance to the
closest positive point which would be on
the Min side and D minus is the shortest
distance to the closest negative point
which is on the women's side the sum of
D plus and D minus is called the
distance margin or the distance between
the two support vectors that are shown
in the dashed lines and then by finding
the largest distance margin we can get
the optimal hyperplane once we've
created an optimal hyperplane we can
easily see which side the new data fits
in and based on the hyperplane we can
say the new data point belongs to the
male gender hopefully that's clear and
how that works on a visual level as a
data scientist you should also be asking
what happens if the hyperplane is not
optimal if we select a hyperplane having
low margin then there is a high chance
of misclassification this particular svm
model the one we discussed so far is
also called referred to as the ls VM so
far so clear but a question should be
coming up we have our sample data set
but instead of looking like this what if
it looked like this where we have two
sets of data but one of them occurs in
the middle of another set you can see
here where we have the blue and the
yellow and then blue again on the other
side of our data line in this data set
we can't use a hyperplane so when you
see data like this it's necessary to
move away from a 1D view of the data to
a two-dimensional view of the data and
for the transformation we use what's
called a kernel function the kernel
function will take the 1D input and
transfer it to a two-dimensional output
as you can see in this picture here the
1 D when transferred to a
two-dimensional makes it very easy to
draw a line between the two data sets
what if we make it even more complicated
how do we perform an svm for this type
of data set here you can see we have a
two-dimensional data set where the data
is in the middle surrounded by the green
data on the outside in this case we're
going to segregate the two classes we
have our sample data set and if you draw
a line through it's obviously not an
optimal hyperplane in there so to do
that we need to transfer the 2D to a 3D
array and when you translate it into a
three-dimensional array using the kernel
you can see where you can place a
hyperplane right through it and easily
split the data before we start looking
at a programming example and dive into
the script let's look at the advantage
of the support Vector machine we'll
start with high dimensional input space
or sometimes referred to as the curse of
dimensionality we looked at earlier one
dimension two Dimension three dimension
when you get to a thousand dimensions a
lot of problems start occurring with
most algorithms that have to be adjusted
for for the svm automatically does that
in high dimensional space one of the
high dimensional space one high
dimensional space that we work on is
sparse document vectors this is where we
tokenize the words and documents so we
can run our machine learning algorithms
over them I've seen ones get as high as
2.4 million different tokens that's a
lot of vectors to look at and finally we
have regularization parameter the
realization parameter or Lambda is a
parameter that helps figure out whether
we're going to have a bias or
overfitting of the data whether it's
going to be overfitted to a very
specific instance or it's going to be
biased to a high or low value with the
svm it naturally avoids the overfitting
and bias problems that we see in many
other algorithms these three advantages
of the support Vector machine make it a
very powerful tool to add to your
repertoire of machine learning tools now
we did promise you a used case study
we're actually going to dive into some
Python Programming and so we're going to
go into a problem statement and start
off with the zoo so in the zoo example
we have of family members going to the
zoo we have the young child going dad is
that a group of crocodiles or alligators
well that's hard to differentiate and
zoos are a great place to start looking
at science and understanding how things
work especially as a young child and so
we can see the parents in here thinking
well what is the difference between a
crocodile and an alligator well one
crocodiles are larger in size alligators
are smaller in size snout width the
crocodiles have a narrow snout and
alligators have a wider snout and of
course in the modern day and age the
father's hitting there is thinking how
can I turn this into a lesson for my son
and he goes let a support Vector machine
segregate the two groups I don't know if
my dad ever told me that but that would
be funny now in this example we're not
going to use actual measurements and
data we're just using that for imagery
and that's very common in a lot of
machine learning algorithms and setting
them up but let's roll up our sleeves
and we'll talk about that more in just a
moment as we break into our python
script so here we arrive in our actual
coding and I'm going to move this into a
python editor in just a moment but let's
talk a little bit about what we're going
to cover first we're going to cover in
the code the setup how to actually
create our svm and you're going to find
that there's only two lines of code that
actually create it and the rest of it is
done so quick and fast that it's all
here in the first page and we'll show
you what that looks like as far as our
data so we're going to create some data
I talked about creating data just a
minute ago and so we'll get into the
creating data here and you'll see this
nice correction of our two blobs and
we'll go through that in just a second
and then the second part is we're going
to take this and we're going to bump it
up a notch we're going to show you what
it looks like behind the scenes but
let's start with actually creating our
setup I like to use the Anaconda Jupiter
notebook because it's very easy to use
but you can use any of your favorite
python editors or setups and go in there
but let's go ahead and switch over there
and see what that looks like so here we
are in the Anaconda python notebook or
anaconda jupyter notebook with python
we're using python3 I believe this is
3.5 but it should be work in any of your
3x versions and and you'd have to look
at the SK learn and make sure if you're
using a 2X version an earlier version
let's go and put our code in there and
one of the things I like about the
jupyter notebook is I go up to view and
I'm going to go ahead and toggle the
line numbers on to make it a little bit
easier to talk about and we can even
increase the size because this is edited
in in this case I'm using Google Chrome
Explorer and that's how it opens up for
the editor although anyone any like I
said any editor will work now the first
step is going to be our Imports and
we're going to import four different
parts the first two I want you to look
at are line one and line two are numpy
as NP and
matplotlibrary.pi plot as PLT now these
are very standardized Imports when
you're doing work the first one is the
numbers python we need that because part
of the platform we're using uses that
for the numpy array and I'll talk about
that in a minute so you can understand
why we want to use a numpy array versus
a standard python array and normally
it's pretty standard setup to use NP for
numpy the matplot library is how we're
going to view our data so this has you
do need the NP for the sklearn module
but the matplot library is purely for
our use for visualization and so you
really don't need that for the svm but
we're going to put it there so you have
a nice visual aid and we can show you
what it looks like that's really
important at the end when you finish
everything so you have a nice display
for everybody to look at and then
finally we're going to I'm going to jump
one ahead to line number four that's the
sklearn.datasets dot samples generator
import make blobs and I told you that we
were going to make up data and this is a
tool that's in the SK learning makeup
data I personally don't want to go to
the zoo get in trouble for jumping over
the fence and probably get eaten by the
crocodiles or alligators as I work on
measuring their snouts and width and
length instead we're just going to make
up some data and that's what that make
blobs is It's a Wonderful tool if you're
ready to test your your setup and you're
not sure about what data you're going to
put in there you can create this blob
and it makes it real easy to use and
finally we have our actual svm the
sklearn import svm on line three so that
covers all our Imports we're going to
create remember I used the make blobs to
create data and we're going to create a
capital x and a lowercase y equals make
blobs in samples equals 40. so we're
going to make 40 lines of data it's
going to have two centers with a random
State equals 20. so each each group is
going to have 20 different pieces of
data in it and the way that lux is that
we'll have under X an X Y plane so I
have two numbers under X and Y will be
zero one that's the two different
centers so we have yes or no in this
case alligator a crocodile that's what
that represents and then I told you that
the actual SK learner the svm is in two
lines of code and we see it right here
with clf equals
svm.svc kernel equals linear and I set
SQL to 1 although in this example since
we are not regularizing the data because
we want to be very clear and easy to see
I went ahead you can set it to a
thousand a lot of times you're not doing
that but for this thing linear because
it's a very simple linear example we
only have the two dimensions and it'll
be a nice linear hyperplane it'll be a
nice linear line instead of a full plane
so we're not dealing with a huge amount
of data and then all we have to do is do
clf dot fit X comma Y and that's it clf
has been created and then we're going to
go ahead and display it and I'm going to
talk about this display here in just a
second but let me go ahead and run this
code and this is what we've done is
we've created two blobs you'll see the
blue on the side and then kind of an
orangish on the other side that's our
two sets of data they represent one
represents crocodiles and one represents
alligators and then we have our
measurements in this case we have like
the width and length of the snout and I
did say I was going to come up here and
talk just a little bit about our plot
and you'll see PLT that's what we
imported we're going to do a scatter
plot that means we're just putting dots
on there and then look at this notation
I have the capital x and then in
brackets I have a colon comma zero
that's from numpy if you did that in a
regular array you'll get an error in a
python array you have to have that in a
numpy array it turns out that our make
blobs returns a numpy array and this
notation is great because what it means
is the first part is the colon means
we're going to do all the rows that's
all the data in our blob we created
under capital x and then the second part
has a comma zero we're only going to
take the first value and then if you
notice we do the same thing but we're
going to take the second value remember
we always start with zero and then one
so we have column 0 and column one and
you can look at this as our X Y plots
the first one is the X plot and the
second one is the Y plot so the first
one is on the bottom 0 2 4 6 8 and 10
and then the second one X of the one is
the four five six seven eight nine ten
going up the left hand side s equals 30
is just the size of the dot so we can
see them instead of little tiny dots and
then C map equals plt.cm dot paired and
you'll also see the C equals y That's
the color we're using two colors zero
one and that's why we get the nice blue
and the two different colors for the
alligator and the crocodile now you can
see here that we did this the actual fit
was done in two lines of code a lot of
times there'll be a third line where we
regularize the data we set it between
like minus one and one and we reshape it
but for this it's not necessary and it's
also kind of nice because you can
actually see what's going on and then if
we wanted to we wanted to actually run a
prediction let's take a look and see
what that looks like and to predict some
new data and we'll show this again as we
get towards the end of digging in deep
you can simply assign your new data in
this case I am giving it a width and
length three four and a with the length
five six and note that I put the data as
a set of brackets and then I have the
brackets inside and the reason I do that
is because when we're looking at data
it's designed to process a large amount
of data coming in we don't want to just
process one line at a time and so in the
this case I'm processing two lines and
then I'm just going to print and you'll
see clf.predict new data so the clf and
the dot predict part is going to give us
an answer and let's see what that looks
like and you'll see 0 1 so predicted the
first one the 3 4 is going to be on the
one side and the 5 6 is going to be on
the other side so one came out as an
alligator and one came out as a
crocodile now that's pretty short
explanation for this setup but really we
want to dug in and see what's going on
behind the scenes and let's see what
that looks like so the next step is to
dig in deep and find out what's going on
behind the scenes and also put that in a
nice pretty graph we're going to spend
more work on this and we did actually
generating the original model and you'll
see here that we go through a few steps
and I'll move this over to our editor in
just a second we come in we create our
original data it's exactly identical to
the first part and I'll explain why we
redid that and show you how not to redo
that and then we're going to go in there
and add in those lines we're going to
see what those lines look like and how
to set those up and finally we're going
to plot all that on here and show it and
you'll get a nice graph with the what we
saw earlier when we were going through
the theory behind this where it shows
the support vectors and the hyper plane
and those are done where you can see the
support vectors as the dashed lines and
the solid line which is the hyperplane
let's get that into our Jupiter notebook
before I scroll down to a new line I
want you to notice line 13 it has Plot
show and we're going to talk about that
here in just a second but let's scroll
down to a new line down here and I'm
going to paste that code in and you'll
see that the plot show has moved down
below let's scroll up a little bit and
if you look at the top here of our new
section one two three and four is the
same code we had before and let's go
back up here and take a look at that
we're going to fit the values on our svm
and then we're going to plot scatter it
and then we're going to do a plot show
hey there learner Sim linen brings you a
comprehensive professional certificate
program in Ai and machine learning from
Purdue University that is in
collaboration with IBM that will cover a
wide range of topics that will Empower
you with the knowledge and skills needed
to excel in the field of AI to learn
more about this course you can find the
course link mentioned in the description
box
five days let's start with a basic
introduction to the base there
first coinness in the western literature
naive Bayes classifier works on the
principle of conditional probability as
given by the Bayes theorem before we
move ahead let us go through some of the
simple Concepts in the probability that
we will be using let us consider the
following example of tossing two coins
here we have two quarters and if we look
at all the different possibilities of
what they can come up as we get that
they can come up as head heads they come
up as head tell tell head and Telltale
when doing the math on probability we
usually denote probability as a p a
capital P so the probability of getting
two heads equals one-fourth you can see
in our data set we have two heads and
this occurs once out of the four
possibilities and then the probability
of at least one tail occurs three
quarters of the time you'll see in three
of the toin tosses we have tails in them
and out of four that's three fourths and
then the probability of the second coin
being head given the first coin is tail
is one half and the probability of
getting two heads given the first coin
is a head is one half we'll demonstrate
that just a minute and show you how that
math works now when we're doing it with
two coins it's easy to see but when you
have something more complex you can see
where these Pro these formulas really
come in and work so the Bayes theorem
gives us a conditional probability of an
event a given another event B has
occurred in this case the first coin
toss will be B and the second coin toss
a this could be confusing because we've
actually reversed the order of them and
go from B to a instead of a to B you'll
see this a lot when you work in
probabilities the reason is we're
looking for event a we want to know what
that is so we're going to label that a
since that's our focus and then given
another event B has occurred in the
Bayes theorem as you can see on the left
the probability of a occurring given B
has occurred equals the probability of B
occurring given a has occurred times the
probability of a over the probability of
B this simple formula can be moved
around just like any algebra formula and
we could do the probability of a after a
given B times probability of a b equals
the probability of B given a times
probability of a you can easily move
that around and multiply it and divide
it out let us apply Bayes theorem to our
example here we have our two quarters
and we'll notice that the first two
probabilities of getting two heads and
at least one tail we compute directly
off the data so you can easily see that
we have one example HH out of four one
fourth and we have three with tails in
them giving us three quarters or
three-fourths seventy-five percent the
second condition the second set three
and four we're gonna explore a little
bit more in detail now we stick to a
simple example with two coins because
you can easily understand the math the
probability of throwing a tail doesn't
matter what comes before it and the same
with the head so still going to be fifty
percent or one half but when that com
when that probability gets more
complicated let's say you have a D6 dice
or some other instance then this formula
really comes in handy but let's stick to
the simple example for now in this
sample space let a be the event that the
second coin is head and B be the event
that the first coin is tells again we
reversed it because we want to know what
the second event is going to be so we're
going to be focusing on a and we write
that out as a probability of a given B
and we know this from our formula that
that equals the probability of B given a
times the probability of a over the
probability of B and when we plug that
in we plug in the probability of the
first coin being tells given the second
coin is heads and the probability of the
second coin being heads given the first
coin being over the probability of the
first coin being Tails when we plug that
data in and we have the probability of
the first coin being Tails given the
second coin is heads times the
probability of the second coin being
heads over the probability of the first
coin being tails you can see it's a
simple formula to calculate we have one
half times one half over one-half or
one-half equals 0.5 or 1 4. so the Bayes
theorem basically calculates the
conditional probability of the
occurrence of an event based on prior
knowledge of conditions that might be
related to the event we will explore
this in detail when we take up an
example of online shopping further in
this tutorial understand standing naive
Bayes and machine learning like with any
of our other machine learning tools it's
important to understand where the naive
Bayes fits in the hierarchy so under the
machine learning we have supervised
learning and there is other things like
unsupervised learning there's also
reward system This falls under the
supervised learning and then under the
supervisor's learning there's
classification there's also a regression
but we're going to be in the
classification side and then under
classification is your naive Bayes let's
go ahead and glance into where is naive
Bayes used let's look at some of the use
scenarios for it as a classifier we use
it in face recognition is this Cindy or
is it not Cindy or whoever or it might
be used to identify parts of the face
that they then feed into another part of
the face recognition program this is the
eye this is the nose this is the mouth
weather prediction is it going to be
rainy or sunny medical recognition news
prediction it's also used in medical
diagnosis we might diagnose somebody as
either as high risk or not as high risk
for cancer or heart disease fees or
other elements and news classification
when you look at the Google news and it
says well is this political or is this
world news or a lot of that's all done
with the naive Bayes understanding naive
Bayes classifier now we already went
through a basic understanding with the
coins and the two heads and two tells
and head tail tail heads Etc we're going
to do just a quick review on that and
remind you that the naive Bayes
classifier is based on the Bayes theorem
which gives a conditional probability of
event a given event B and that's where
the probability of a given b equals the
probability of B given a times
probability of a over probability of B
remember this is an algebraic function
so we can move these different entities
around we can multiply by the
probability of B so it goes to the left
hand side and then we could divide by
the probability of a given B and just as
easy come up with a new formula for the
probability of B to me staring at these
algebraic functions kind of gives me a
slight headache
it's a lot better to see if we can
actually understand how this data fits
together in a table and let's go ahead
and start applying it to some actual
data so you can see what that looks like
so we're going to start with the
shopping demo problem statement and
remember we're going to solve this first
in table form so you can see what the
math looks like and then we're going to
solve it in Python and in here we want
to predict whether the person will
purchase a product are they going to buy
or don't buy very important if you're
running a business you want to know how
to maximize your profits or at least
maximize the purchase of the people
coming into your store and we're going
to look at a specific combination of
different variables in this case we're
going to look at the day the discount
and the free delivery and you can see
here under the day we want to know
whether it's on the weekday you know
somebody's working they come in after
work or maybe they don't work weekend
you can see the bright colors coming
down there celebrating not being in work
or holiday and did we offer a discount
that day yes or no did we offer free
delivery that day yes or no and from
this we want to know whether the person
is going to buy based on these traits so
we can maximize them and find out the
best system for getting somebody to come
in and purchase our goods and products
from our store now having a nice visual
is great but we do need to dig into the
data so let's go ahead and take a look
at the data set we have a small sample
data set of 30 rows we're showing you
the first 15 of those rows for this demo
now the actual data file you can request
just type in below under the comments on
the YouTube video and we'll send you
some more information and send you that
file as you can see here the file is
very simple columns and rows we have the
day the discount the free delivery and
did the person purchase or not and then
we have under the day whether it was a
weekday a holiday was it the weekend
this is a pretty simple set of data and
long before computers people used to
look at this data and calculate this all
by hand so let's go ahead and walk
through this and see what that looks
like when we put that into tables also
note in today's world we're not usually
looking at three different variables in
30 rows nowadays because we're able to
collect data so much we're usually
looking at 27 30 variables across
hundreds of rows the first thing we want
to do is we're going to take this data
and based on the data set containing our
three inputs Day discount and free
delivery we're going to go ahead and
populate that to frequency tables for
each attribute so we want to know if
they had a discount how many people buy
and did not buy did they have a discount
yes or no do we have a free delivery yes
or no on those dates how many people
made a purchase how many people didn't
and the same with the three days of the
week was it a weekday a weekend a
holiday and did they buy yes or no as we
dig in deeper to this table for our
Bayes theorem let the event buy ba now
remember we looked at the coins I said
we really want to know what the outcome
is did the person buy or not and that's
usually event a is what you're looking
for and the independent variables
discount free delivery and day BB so
we'll call that probability of B now let
us calculate the likelihood table for
one of the variables let's start with
day which includes weekday weekend and
holiday and let us start by summing all
of our our rows so we have the weekday
row and out of the weekdays there's nine
plus two so it's 11 weekdays there's
eight weekend days and eleven holidays
well it's a lot of holidays and then we
want to sum up the total number of days
so we're looking at a total of 30 days
let's start pulling some information
from our chart and see where that takes
us and when we fill in the chart on the
right you can see that 9 out of 24
purchases are made on the weekday 7 out
of 24 purchases on the weekend and 8 out
of 24 purchases on a holiday and out of
all the people who come in 24 out of 30
purchase you can also see how many
people do not purchase on the weekdays
two out of six didn't purchase and so on
and so on we can also look at the totals
and you'll see on the right we put
together some of the formulas the
probability of making a purchase on the
weekend comes out at 11 out of 30. so
out of the 30 people who came into the
store throughout the weekend weekday and
holiday 11 of those purchases were made
on the weekday and then you can also see
the probability of them not making a
purchase and this is done for doesn't
matter which day of the week so we call
that probability of no buy would be 6
over 30 or 0.2 so there's a twenty
percent chance that they're not going to
make a purchase no matter what day of
the week it is and finally we look at
the probability of B of A in this case
we're going to look at the probability
of the weekday and not buying two of the
no buys were done on the weekend out of
the six people who did not make
purchases so when we look at that
probability of the week day without a
purchase is going to be 0.33 or 33
percent let's let's take a look at this
at different probabilities and based on
this likelihood table let's go ahead and
calculate conditional probabilities as
below the first three we just did the
probability of making a purchase on the
weekday is 11 out of 30 or roughly 36 or
37 percent 0.367 the probability of not
making a purchase at all doesn't matter
what day of the week is roughly 0.2 or
20 percent and the probability of a
weekday no purchase is roughly two out
of six so two out of six of our no
purchases were made on the weekday and
then finally we take our P of a b if you
look we've kept the symbols up there we
got P of probability of B probability of
a probability of B if a we should
remember that the probability of a if B
is equal to the first one times the
probability of no buys over the
probability of the weekday so we could
calculate it both off the table we
created we can also calculate this by
the formula and we get the 0.36 7 which
equals or 0.33 times 0.2 over 0.367
which equals 0.179 or roughly 17 to 18
percent and that'd be the probability of
no purchase done on the weekday and this
is important because we can look at this
and say as the probability of buying on
the weekday is more than the probability
of not buying on the weekday we can
conclude that customers will most likely
buy the product on a weekday now we've
kept our chart simple and we're only
looking at one aspect so you should be
able to look at the table and come up
with the same information or the same
conclusion that should be kind of
intuitive at this point next we can take
the same setup we have the frequency
tables of all three independent
variables now we can construct the
likelihood tables for all three of the
variables we're working with we can take
our day like we did before we have
weekday weekend and holiday and we
filled in this table and then we can
come in and also do that for the
discount yes or no did they buy yes or
no and we fill in that full table so now
we have our probabilities for a discount
and whether the discount leads to a
purchase or not and the probability for
free delivery does that lead to a
purchase or not and this is where it
starts getting really exciting let us
use these three likelihood tables to
calculate whether a customer will
purchase a product on a specific
combination of Day discount and free
delivery or not purchase here let us
take a combination of these factors day
equals holiday discount equals yes free
delivery equals yes let's dig deeper
into the math and actually see what this
looks like and we're going to start with
looking for the probability of them not
purchasing on the following combinations
of days we're actually looking for the
probability of a equal no buy no
purchase and our probability of B we're
going to set equal to is it a holiday do
they get a discount yes and was it a
free delivery yes before we go further
let's look at the original equation the
probability of a if B equals the
probability of B given the condition a
and the probability times the
probability of a over the probability of
B occurring now this is basic algebra so
we can multiply this information
together so when you see the probability
of a given B in this case the condition
is b c and d or the three different
variables we're looking at and when you
see the probability of B that would be
the conditions we're actually going to
multiply those three separate conditions
out probability of you'll see that just
a second in the formula times the full
probability of a over the full
probability of B so here we are back to
this and we're going to have let a equal
no purchase and we're looking for the
probability of B on the condition a
where a sets for three different things
remember that equals the probability of
a given the condition B and in this case
we just multiply those three different
variables together so we have the
probability of the discount times the
probability of freedom delivery times
the probability is the day equal a
holiday those are our three variables of
the probability of a if B and then that
is going to be multiplied by the
probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day two out of six or a no
purchase on a free delivery day and
three out of six or a no purchase on a
holiday those are our three
probabilities of a of B multiplied out
and then that has to be multiplied by
the probability of a no purchase and
remember the probability of a no buy is
across all the data so that's where we
get the six out of 30. we divide that
out by the probability of each category
over the total number so we get the 20
out of 30 had a discount 23 3 out of 30
had a yes for free delivery and 11 out
of 30 were on a holiday we plug all
those numbers in we get
0.178 so in our probability math we have
a 0.178 if it's a no buy for a holiday a
discount and a free delivery let's turn
that around and see what that looks like
if we have a purchase I promise this is
the last page of math before we dig into
the python script so here we're
calculating the probability of the
purchase using the same math we did to
find out if they didn't buy now we want
to know if they did buy and again we're
going to go by the day equals a holiday
discount equals yes free delivery equals
yes and let a equal buy now right about
now you might be asking why are we doing
both calculations why why would we want
to know the no buys and buys for the
same data going in well we're going to
show you that in just a moment but we
have to have both of those pieces of
information so that we can figure it out
as a percentage as opposed to a
probability equation and we'll get to
that normalization here in just a moment
let's go ahead and walk through this
calculation and as you can see here the
probability of a on the condition of b b
being all three categories did we have a
discount with a purchase do we have a
free delivery with a purchase and did we
is a day equal to Holiday and when we
plug this all into that formula and
multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall
probability of it being a purchase
divided by again multiplying the three
variables out the full probability of
there being a discount the full
probability of being a free delivery and
the full probability of there being a
day equal holiday and that's where we
get this 19 over 24 times 21 over 24
times 8 over 24 times the P of a 24 over
30 divided by the probability of the
discount the free delivery times a day
or 20 over 30 23 over 30 times 11 over
30 and that gives us our
0.986 so what are we going to do with
these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals 0.986 we have a
probability of no purchase equals 0.178
so finally we have a conditional
probabilities of purchase on this day
let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking these sum of probabilities which
equals
0.98686 plus 0.178 and that equals the
1.164 if we divide each probability by
the sum we get the percentage and so the
likelihood of a purchase is 84.71
percent and the likelihood of no
purchase is 15.29 percent given these
three different variables so it's if
it's on a holiday if it's a with a
discount and has free delivery then
there's an 84.71 percent chance that the
customer is going to come in and make a
purchase hooray they purchased our stuff
we're making money if you're owning a
shop that's like is the bottom line is
you want to make some money so you keep
your shop open and have a living now I
promised you that we were going to be
finishing up the math here with a few
pages so we're going to move on and
we're going to do two steps the first
step is I want you to understand why you
went under why you want to use the naive
Bayes what are the advantages of naive
bays and then once we understand those
advantages we just look at that briefly
then we're going to dive in and do some
python coding advantages of naive Bayes
classifier so let's take a look at the
six advantages of the naive Bayes
classifier and we're going to walk
around this lovely wheel looks like an
origami folded paper the first one is
very simple and easy to implement
certainly you could walk through the
tables and do this by hand you got to be
a little careful because the notations
can get confusing you have all these
different probabilities and I certainly
mess those up as I put them on you know
is it on the top of the bottom got to
really pay close attention to that when
you put it into python it's really nice
because you don't have to worry about
any of that you let the python handle
that the python module but understanding
it you can put it on a table and you can
easily see how it works and it's a
simple algebraic function it needs less
training data so if you have smaller
amounts of data this is great powerful
tool for that handles both continuous
and discrete data it's highly scalable
with number of predictors and data
points so as you can see just keep
multiplying different probabilities in
there and you can cover not just three
different variables or sets you can now
expand this to even more categories
number five it's fast it can be used in
real time predictions this is so
important this is why it's used in a lot
of our predictions on online shopping
carts referrals spam filters is because
there's no time delay as it has to go
through and figure out a neural network
or one of the other mini setups where
you're doing classification and
certainly there's a lot of other tools
out there in the machine learning that
can handle these but most of them are
not as fast as the naive Bayes and then
finally it's not sensitive to irrelevant
features so it picks up on your
different probabilities and if you're
short on date on one probability you can
kind of it automatically adjust for that
those formulas are very automatic and so
you can still get a very solid
predictability even if you're missing
data or you have overlapping data for
two completely different areas we see
that a lot in doing census and studying
of people and habits where they might
have one study that covers one aspect
another one that overlaps and because of
two overlap they can then predict the
unknowns for the group that they haven't
done the second study on or vice versa
so it's very powerful in that it is not
sensitive to the irrelevant features and
in fact you can use it to help predict
features that aren't even in there so
now we're down to my favorite part we're
going to roll up our sleeves and do some
actual programming we're going to do the
use case text classification now I would
challenge you to go back and send us a
note on the notes below underneath the
video and request the data for the
shopping cart so you can plug that into
python code and do that on your own time
so you can walk through it since we walk
through all the information on it but
we're going to do a python code doing
text classification very popular for
doing the naive Bayes so we're going to
use our new tool to perform a text
classification of news headlines and
classify news into different topics for
a News website as you can see here we
have a nice image of the Google news and
then related on the right subgroups I'm
not sure where they actually pulled the
actual data we're going to use from it's
one of the standard sets but certainly
this can be used on any of our news
headlines and classification so let's
see how it can be done using the naive
Bayes classifier now we're at my
favorite part we're actually going to
write some python script roll up our
sleeves and we're going to start by
doing our Imports these are very basic
Imports including our news group and
we'll take a quick glance at the Target
names then we're going to go ahead and
start training our data set and putting
it together we'll put together a nice
graph because it's always good to have a
graph to show what's going on and once
we've traded it and we've shown you a
graphical what's going on then we're
going to explore how to use it and see
what that looks like now I'm going to
open up my favorite editor or inline
editor for python you don't have to use
this you can use whatever your editor
that you like whatever interface IDE you
want this just happens to be the
Anaconda Jupiter notebook and I'm going
to paste that first piece of code in
here so we can walk through it let's
make it a little bigger on the screen so
you have a nice view of what's going on
and we're using Python 3 in this case
3.5 so this would work in any of your 3x
if you have it set up correctly should
also work in a lot of the 2x you just
have to make sure all of the versions of
the modules match your python version
and in here you'll notice the first line
is your percentage matplot library in
line now three of these lines of code
are all about plotting the graph this
one lets the notebook notes and this is
an inline setup do we want the graphs to
show up on this page without it in a
notebook like this which is an Explorer
interface it won't show up now a lot of
Ides don't require that a lot of them
like on if I'm working on one of my
other setups it just has a pop-up and
the graph pops up on there so you have a
that set up also but for this we want
the matplot library in line and then
we're going to import numpy as NP that's
number python which has a lot of
different formulas in it that we use for
both of our sklearn module and we also
use it for any of the upper math
functions in Python and it's very common
to see that as NP numpy is NP the next
two lines are all about our graphing
remember I said three of these were
about graphing well we need our matplot
library.pi plot as PLT and you'll see
that PLT is a very common setup as is
the SNS and just like the NP and we're
going to import Seaborn as S and S and
we're going to do the sns.set now
Seaborn sits on top of Pi plot and it
just makes a really nice heat map it's
really good for heat maps and if you're
not familiar with heat maps that just
means we give it a color scale term
comes from the brighter red it is the
hotter it is in some form of data and
you can set it to whatever you want and
we'll see that later on so those you'll
see that those three lines of code here
are just importing the graph function so
we can graph it and as a data science
test you always want to graph your data
and have some kind of visual it's really
hard just to shove numbers in front of
people and they look at it and it
doesn't mean anything and then from the
sklearn.data sets we're going to import
the fetch 20 news groups very common one
for analyzing tokenizing words and
setting them up and exploring how the
words work and how do you categorize
different things when you're dealing
with documents and then we set our data
equal to fetch 20 news groups so our
data variable will have the data in it
and we're going to go ahead and just
print the target names data.target names
and let's see what that looks like and
you'll see here we have alt atheism comp
Graphics comp osms windows.miscellaneous
and it goes all the way down to talk
politics.miscellaneous talk
religion.missella genius these are the
categories we've already assigned to
this news group and it's called fetch20
because you'll see there's I believe
there's 20 different topics in here or
20 different categories as we scroll
down now we've gone through the 20
different categories and we're going to
go ahead and start defining all the
categories and set up our data so we're
actually going to here going to go ahead
and get it get the data all set up and
take a look at our data and let's move
this over to our Jupiter notebook and
let's see what this code does
first we're going to set our categories
now if you noticed up here I could have
just as easily set this equal to
data.target underscore names because
it's the same thing but we want to kind
of spell it out for you so you can see
the different categories it kind of
makes it more visual so you can see what
your data is looking like in the
background once we've created the
categories
we're going to open up a train set so
this training set of data is going to go
into fetch 20 news groups and it's a
subset in there called train and
categories equals categories so we're
pulling out those categories that match
and then if you have a train set you
should also have the testing set we have
test equals fetch 20 News Group subset
equals test and categories equals
categories let's go down one side so it
all fits on my screen there we go and
just so we can really see what's going
on let's see what happens when we print
out one part of that data so it creates
train and under train it creates
train.data and we're just going to look
at data piece number five and let's go
ahead and run that and see what that
looks like and you can see when I print
train dot data number five under train
it prints out one of the Articles this
is article number five you can go
through and read it on there and we can
also go in here and change this to test
which should look identical because it's
splitting the data up into different
groups train and test and we'll see test
number 5 is a different article but it's
another article in here and maybe you're
curious and you want to see just how
many articles are in here we could do
Links of train dot data and if we run
that you'll see that the training data
has 11
314 articles so we're not going to go
through all those articles that's a lot
of articles but we can look at one of
them just you can see what kind of
information is coming out of it and what
we're looking at and we'll just look at
number five for today and here we have
it rewarding the Second Amendment IDs
vtt line 58 lines 58 in article
Etc and you can scroll all the way down
and see all the different parts to there
now we've looked at it and that's pretty
complicated when you look at one of
these articles to try to figure out how
do you weight this if you look down here
we have different words and maybe the
word from well from is probably in all
the Articles so it's not going to have a
lot of meaning as far as trying to
figure out whether this article fits one
of the categories or not so trying to
figure out which category it fits in
based on these words is where the
challenge comes in now that we've viewed
our data we're going to dive in and do
the actual predictions this is the
actual naive Bayes and we're going to
throw another model at you or another
module at you here in just a second we
can't go into too much detail but it
deals specifically working with words
and text and what they call tokenizing
those words so let's take this code and
let's skip on over to our Jupiter
notebook and walk through it and here we
are in our jupyter notebook let's paste
that in there and I can run this code
right off the bat it's not actually
going to display anything yet but it has
a lot going on in here so the top we
have the print module from the earlier
one I didn't know why that was in there
so we're going to start by importing our
necessary packages and from the sklearn
features extraction dot text we're going
to import TF IDF vectorizer I told you
we're going to throw a module at you we
can't go too much into the math behind
this or how it works you can look it up
the notation for the math is usually
tf.idf
and that's just a way of weighing the
words and it weighs the words based on
how many times are used in a document
how many times or how many documents are
used in and it's a well used formula
it's been around for a while it's a
little confusing to put this in here but
let's let them know that it just goes in
there and waits the different words in
the document for us that way we don't
have to wait and if you put a weight on
it if you remember I was talking about
that up here earlier if these are all
emails they probably all have the word
from in them from probably has a very
low weight it has very little value in
telling you what this document's about
same with words like in an article in
articles in cost of on maybe cost might
or where words like criminal weapons
destruction these might have a heavier
weight because we describe a little bit
more what the article's doing well how
do you figure out all those weights in
the different articles that's what this
module does that's what the TF IDF
vectorizer is going to do for us and
then we're going to import our
sklearn.naive Bays and that's our
multinomial in B multinomial naive base
pretty easy to understand that where
that comes from and then finally we have
the skylearn pipeline import make
pipeline now the make pipeline is just a
cool piece of code because we're going
to take the information we get from the
TF IDF vectorizer and we're going to
pump that into the multinomial in B so a
pipeline is just a way of organizing how
things flow it's used commonly you
probably already guess what it is if
you've done any businesses they talk
about the sales pipeline if you're on a
work crew or project manager you have
your pipeline of information that's
going through or your projects and what
has to be done in what order that's all
this pipeline is we're going to take the
tfid vectorizer and then we're going to
push that into the multinomial NB now
we've designated that as the variable
model we have our pipeline model and
we're going to take that model and this
is just so elegant this is done in just
a couple lines of code model dot fit and
we're going to fit the data and first
the train data and then the train Target
now the train data has the different
articles in it you can see the one we
were just looking at and the train dot
Target is what category they already
categorized that that particular article
as and what's Happening Here is the
trained data is going into the tfid
vectorizer so when you have one of these
articles that goes in there it waits all
the words in there so there's thousands
of words with different weights on them
I remember once running a model on this
and I literally had 2.4 million tokens
go into this so when you're dealing like
large document bases you can have a huge
number of different words it then takes
those words gives them a weight and then
based on that weight based on the words
and the weights and then puts that into
the multinomial in B and once we go into
our naive Bays we want to put the train
Target in there so the train data that's
been mapped to the tfid vectorizer is
now going through the multinomial in B
and then we're telling it well these are
the answers these are the answers to the
different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the talk show or the
article on religion miscellaneous once
we fit that model we can then take
labels and we're going to set that equal
to model dot predict most of the sklearn
use the term dot predict to let us know
that we've now trained the model and now
we want to get some answers and we're
going to put our test data in there
because our test data is the stuff we
held off to the side we didn't train it
on there and we don't know what's going
to come up out of it and we just want to
find out how good our labels are do they
match what they should be now I've
already read this through there's no
actual output to it to show this is just
setting it all up this is just training
our model creating the labels so we can
see how good it is and then we move on
to the next step to find out what
happened to do this we're going to go
ahead and create a confusion Matrix and
a heat map so the confusion Matrix which
is confusing just by its very name is
basically going to ask how confused is
our answer did it get it correct or did
it Miss some things in there or have
some missed labels and then we're going
to put that on a heat map so we'll have
some nice colors to look at to see how
that plots out let's go ahead and take
this code and see how that take a walk
through it and see what that looks like
so back to our jupyter notebook I'm
going to put the code in there and let's
go ahead and run that code take it just
a moment and remember we had the in line
that way my graph shows up on the inline
here and let's walk through the code and
then we'll look at this and see what
that means so make it a little bit
bigger there we go no reason not to use
the whole screen too big so we have here
from sklearnmetrics import confusion
Matrix
and that's just going to generate a set
of data that says I the prediction was
such the actual truth was either agreed
with it or is something different and
it's going to add up those numbers so we
can take a look and just see how well it
worked and we're going to set a variable
matte equal to confusion Matrix we have
our test Target our test data that was
not part of the training very important
in data science we always keep our test
data separate otherwise it's not a valid
model if we can't properly test it with
new data and this is the labels we
created from that test data these are
the ones that we predict it's going to
be so we go in and we create our SN heat
map the SNS is our Seaborn which sits on
top of the pi plot so we create an
sns.heat map we take our confusion
Matrix and it's going to be
met.t and do we have other variables
that go into the sns.heat map we're not
going to go into detail what all the
variables mean The annotation equals
true that's what tells it to put the
numbers here so you have the 166 the one
the zero zero zero one format d and c
bar equals false have to do with the
format if you take those out you'll see
that some things disappear and then the
X tick labels and the y t labels those
are our Target names and you can see
right here that's the alt atheism comp
graphics composms windows.miscellaneous
and then finally we have our plt.x label
remember the SNS or the Seaborn sits on
top of our matplot library our PLT and
so we want to just tell it X label
equals a true is is true the labels are
true and then the Y label is prediction
label so when we say a true this is what
it actually is and the prediction is
what we predicted and let's look at this
graph because that's probably a little
confusing the way we rattled through it
and what I'm going to do is I'm going to
go ahead and flip back to the slides
because they have a black background
they put in there that helps it shine a
little bit better so you can see the
graph a little bit easier so in reading
this graph what we want to look at is
how the color scheme has come out and
you'll see a line right down the middle
diagonally from upper left to bottom
right what that is is if you look at the
labels we have our predicted label on
the left and our true label on the right
those are the numbers where the
prediction and the true come together
and this is what we want to see is we
want to see those lit up that's what
that heat map does is you can see that
it did a good job of finding those data
and you'll notice that there's a couple
of red spots on there where it missed
you know it's a little confused we talk
about talk religion miscellaneous versus
talk politics miscellaneous social
religion Christian versus Alt atheism it
mislabeled some of those and those are
very similar topics you could understand
why it might mislabel them but overall
it did a pretty good job if we're going
to create these models we want to go
ahead and be able to use them so let's
see what that looks like to do this
let's go ahead and create a definition a
function to run and we're going to call
this function let me just expand that
just a notch here there we go I like
mine in big letters predict categories
we want to predict the category we're
going to send it as a string and then
we're sending it train equals train we
have our training model and then we had
our pipeline model equals model this way
we don't have to resend these variables
each time the definition knows that
because I said train equals train and I
put the equal for model and then we're
going to set the prediction equal to the
model dot predict s so it's going to
send whatever string we send to it it's
going to push that string through the
pipeline the model pipeline it's going
to go through and tokenize it and put it
through the TF IDF convert that into
numbers and weights for all the
different documents and words and then
I'll put that through our naive Bayes
and from it we'll go ahead and get our
prediction we're going to predict what
value it is and so we're going to return
train.target namespredict of zero and
remember that the train.target names
that's just categories I could have just
as easily put categories in there dot
predict of zero so we're taking the
prediction which is a number and we're
converting it to an actual category
we're converting it from I don't know
what the actual number numbers are let's
say 0 equals alt atheism so we're going
to convert that 0 to the word or one
maybe it equals comp Graphics so we're
going to convert number one into comp
Graphics that's all that is and then we
got to go ahead and and then we need to
go ahead and run this so I load that up
and then once I run that we can start
doing some predictions I'm going to go
ahead and type in predict category and
let's just do predict category Jesus
Christ and it comes back and says it's
social religion Christian that's pretty
good now note I didn't put print on this
one of the nice things about the Jupiter
notebook editor and a lot of inline
editors is if you just put the name of
the variable out as returning the
variable train.target underscore names
it'll automatically print that for you
in your own ID you might have to put in
print let's see where else we can take
this and maybe you're a space science
buff so how about sending load to
International
Space Station
and if we run that we get science space
or maybe you're a automobile buff and
let's do um they were going to tell me
Audi is better than BMW but I'm going to
do BMW is better than an Audi so maybe
you're a car buff and we run that and
you'll see it says recreational I'm
assuming that's what Rec stands for
Autos so I did a pretty good job
labeling that one how about uh if we
have something like a caption running
through there president of India and if
we run that it comes up and says talk
politics miscellaneous
so when we take our definition or our
function and we run all these things
through Kudos we made it we were able to
correctly classify texts into different
groups based on which category they
belong to using the naive Bayes
classifier now we did throw in the
pipeline the TF idea vectorizer we threw
in the graphs those are all things that
you don't necessarily have to know to
understand the naive Bayes setup or
classifier but they're important to know
one of the main uses for the naive Bayes
is with the TF IDF tokenizer vectorizer
where it tokenizes a word and has labels
and we use the pipeline because you need
to push all that data through and it
makes it really easy and fast you don't
have to know those to understand naive
Bayes but they certainly help for
understanding the industry and data
science and we can see their categorizer
our naive Bayes classifier we were able
to predict the category religion space
motorcycles Autos politics and properly
classify all these different things we
pushed into our prediction in our
trained model if you are an aspiring
machine learning engineer looking for
online training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simple lens professional
certification program in Ai and machine
learning from Purdue University in
collaboration with IBM should be a right
choice for more details use the link in
the description box below with that in
mind
so what is Gaming's clustering k-means
clustering is an unsupervised learning
algorithm in this case you don't have
labeled data unlike in supervised
learning so you have a set of data and
you want to group them and as the name
suggests you want to put them into
clusters which means objects that are
similar in nature similar in
characteristics need to be put together
so that's what K means clustering is all
about the term k is basically is a
number so we need to tell the system how
many clusters we need to perform so if K
is equal to 2 there will be two clusters
if K is equal to 3 3 clusters and so on
and so forth that's what the k stands
for and of course there is a way of
finding out what is the best or Optimum
value of K for a given data we will look
at that so that is K means cluster so
let's take an example K means clustering
is used in many many scenario use but
let's take an example of Cricket the
game of cricket let's say you received
data of a lot of players from maybe all
over the country or all over the world
and this data has information about the
runs scored by the people or by the
player and the wickets taken by the
player and based on this information we
need to Cluster this data into two
clusters batsman and Bowlers so this is
an interesting example let's see how we
can perform this so we have the data
which consists of primarily two
characteristics which is the runs and
the wickets so the bowlers basically
take wickets and the batsman score runs
there will be of course a few Bowlers
who can score some runs and similarly
there will be some batsmen who will Who
would have taken a few wickets but with
this information we want to Cluster
those players into batsmen and Ballers
so how does this work let's say this is
how the data is so there are information
there is information on the y-axis about
the runs code and on the x-axis about
the wickets taken by the players so if
we do a quick plot this is how it would
look and when we do the clustering we
need to have the Clusters like shown in
the third diagram out here so we need to
have a cluster which consists of people
who have scored High runs which is
basically the batsman and then we need a
cluster with people who have taken a lot
of wickets which is typically the
bowlers there may be a certain amount of
overlap but we will not talk about it
right now so with cayman's clustering we
will have here that means K is equal to
2 and we will have two clusters which is
batsman and Bowlers so how does this
work the way it works is the first step
in k-means clustering is the allocation
of two centroids randomly so two points
are assigned as so-called centroids so
in this case we want two clusters which
means K is equal to two so two points
have been randomly assigned as centroids
keep in mind these points can be
anywhere there are random points they
are not initially they are not really
the centroids centroid means it's a
central point of a given data set but in
this case when it starts off it's not
really the centroid okay so these points
though in our presentation here we have
shown them one point closer to these
data points and another closer to these
data points they can be assigned
randomly anywhere okay so that's the
first step the next step is to determine
the distance of each of the data points
from each of the randomly assigned
centroids so for example we take this
point and find the distance from this
centroid and the distance from this
centroid this point is taken and the
distance is found from this centroid and
the center and so on and so forth so for
every point the distance is measured
from both the centroids and then
whichever distance is less that point is
assigned to that centroid so for example
in this case visually it is very obvious
that all these data points are assigned
to this centroid and all these data
points are assigned to this centroid and
that's what is represented here in blue
color and in this yellow color the next
step is to actually determine the
central point or the actual centroid for
these two clusters so we have this one
initial class star this one initial
cluster but as you can see these points
are not really the centroid centroid
means it should be the central position
of this data set Central position of
this data set so that is what needs to
be determined as the next step so the
central point of the actual centroid is
determined and the original randomly
allocated centroid is repositioned to
the actual centroid of this new clusters
and this process is actually repeated
now what might happen is some of these
points may get reallocated in our
example that is not happening probably
but it may so happen that the distance
is found between each of these data
points once again with these centroids
and if there is if it is required some
points may be reallocated we will see
that in a later example but for now we
will keep it simple so this process is
continued till the centroid
repositioning stops and that is our
final cluster so this is our so after
iteration we come to this position this
situation where the centroid doesn't
need any more repositioning and that
means our algorithm has converged
convergence has occurred and we have the
cluster two clusters we have the
Clusters with a centroid so this process
is repeated the process of calculating
the distance and repositioning the
centroid is repeated till the
repositioning stops which means that the
algorithm has converged and we have the
final cluster with the data points and
the centroids so this is what you're
going to learn from this session we will
talk about the types of clustering what
is k-means clustering application of
k-means clustering k-means clustering is
done using distance measure so we will
talk about the common distance measures
and then we will talk about how cayman's
clustering works and go into the details
of K Mains clustering algorithm and then
we will end with the demo and a use case
for k-min's clustering so let's begin
first of all what are the types of
clustering there are primarily two
categories of clustering hierarchical
clustering and then partitional
clustering and each of these categories
are further subdivided into
agglomerative and divisive clustering
and k-means and fuzzy Siemens clustering
let's take a quick look at what each of
these types of clustering are in
hierarchical clustering the Clusters
have a tree like structure and
hierarchical clustering is further
divided into agglomerative and divisive
agglomerative clustering is the
bottom-up approach we begin with each
element as a separate cluster and merge
them into successively larger clusters
so for example we have ABCDEF we start
by combining B and C Form 1 cluster DNA
form one more then we combine d e and f
one more bigger cluster and then add BC
to that and then finally a to it
compared to that divisive clustering or
divisive clustering is a top-down
approach we begin with the whole set and
proceed to divide it into successfully
smaller clusters so we have a b c d e f
we first take that as a single cluster
and then break it down into a b c d e
and f
then we have partitional clustering
split into two subtypes k-means
clustering and fuzzy c means in k-means
clustering the objects are divided into
the number of clusters mentioned by the
number K that's where the K comes from
so if we say k is equal to 2 the objects
are divided into two clusters C1 and C2
and the way it is done is the features
or characteristics are compared and all
objects having similar characteristics
are clubbed together so that's how K
means clustering is done we will see it
in more detail as we move forward and
fuzzy c means is very similar to K means
in the sense that it clubs objects that
have similar characteristics together
but while in k-min's clustering two
objects cannot belong to or any object a
single object cannot belong to two
different clusters in c means objects
can belong to more than one cluster so
that is the primary difference between k
means and fuzzy c means so what are some
of the applications of k-means
clustering k-means clustering is used in
a variety of examples or variety of
business cases in real life starting
from academic performance diagnostic
system search engines and wireless
sensor networks and many more so let us
take a little deeper look at each of
these examples academic performance So
based on the scores of the students
students are categorized into a b c and
so on clustering forms a backbone of
search engines when a search is
performed the search results need to be
grouped together the search engines very
often use clustering to do this and
similarly in case of wireless sensor
networks the clustering algorithm plays
the role of finding the cluster heads
which collects all the data in its
respective cluster so clustering
especially k-means clustering uses
distance measure so let's take a look at
what is distance pressure so while these
are the different types of clustering in
this video we will focus on k-means
clustering so distance measure tells how
similar some objects are so the
similarity is measured using what is
known as distance measure and what are
the various types of distance measures
there is euclidean distance there is
Manhattan distance then we have squared
euclidean distance measure and cosine
distance measure these are some of the
distance measures supported by k-means
clustering let's take a look at each of
these what is euclidean distance measure
this is nothing but the distance between
two points so we have learned in high
school how to find the distance between
two points this is a little
sophisticated formula for that but we
know a simpler one is square root of Y2
minus y1 whole square plus X2 minus X1
whole Square so this is an extension of
this formula so that is the euclidean
distance between two points what is the
squared euclidean distance measure it's
nothing but the square of the euclidean
distance as the name suggests so instead
of taking the square root we leave the
square as it is and then we have
Manhattan distance measure in case of
Manhattan distance it is the sum of the
distances across the x-axis and the
y-axis and note that we are taking the
absolute value so that the negative
values don't come into play so that is
the Manhattan distance measure then we
have cosine distance measure in this
case we take the angle between the two
vectors formed by joining the points
from the origin so that is the cosine
distance measure okay so that was a
quick overview about the various
distance measures that are supported by
k-means now let's go and check how
exactly K means clustering works okay so
this is how k-means clustering works
this is like a flowchart of the whole
process there is a starting point and
then we specify the number of clusters
that we want now there are a couple of
ways of doing this we can do by trial
and error so we specify a certain number
maybe K is equal to 3 or 4 or 5 to start
with and then as we progress we keep
changing until we get the best clusters
or there is a technique called elbow
technique whereby we can determine the
value of K what should be the best value
of K how many clusters should be formed
so once we have the value of K we
specify that and then the system will
assign that many centroid so it picks
randomly that to start with randomly
that many points that are considered to
be the centroids of these clusters and
then it measures the distance of each of
the data points from these centroids and
assigns those points to the
corresponding centroid from which the
distance is minimum so each data point
will be assigned to the centroid Which
is closest to it and thereby we have K
number of initial clusters however this
is not the final clusters The Next Step
it does is for the new groups for the
Clusters that have been formed it
calculates the main position thereby
calculates the new centroid position the
position of the centroid moves compared
to the randomly allocated one so it's an
iterative process once again the
distance of each point is measured from
this new centroid point and if required
the data points are reallocated to the
new centroids and the mean position or
the new centroid is calculated once
again if the centroid moves then the
iteration continues which means the
convergence has not happened the
clustering has not converged so as long
as there is a movement of the centroid
this iteration keeps happening but once
the centroid stops moving which means
that the cluster has converged or the
clustering process has converged that
will be the end result so now we have
the final position of the centroid a and
the data points are allocated
accordingly to the closest centroid I
know it's a little difficult to
understand from this simple flowchart so
let's do a little bit of visualization
and see if we can explain it better
let's take an example if we have a data
set for a grocery shop so let's say we
have a data set for a grocery shop and
now we want to find out how many
clusters this has to be spread across so
how do we find the optimum number of
clusters there is a technique called the
elbow method so when these clusters are
formed there is a parameter called
within sum of squares at the lower this
value is the better the cluster is that
means all these points are very close to
each other so we use this within sum of
squares as a measure to find the optimum
number of class Masters that can be
formed for a given data set so we create
clusters or we let the system create
clusters of a variety of numbers maybe
of 10 10 clusters and for each value of
K the within SS is measured and the
value of K which has the least amount of
within SS or WSS that is taken as the
optimum value of K so this is the
diagrammatic representation so we have
on the y-axis the within sum of squares
or WSS and on the x-axis we have the
number of clusters so as you can imagine
if you have K is equal to 1 which means
all the data points are in a single
cluster the witnesses value will be very
high because they are probably scattered
all over the moment you split it into
two there will be a drastic fall in the
within SS value and that's what is
represented here but then as the value
of K increases the decrease the rate of
decrease will not be so high it will
continue to decrease but probably the
rate of decrease will not be high so
that gives us an idea so from here we
get an idea for example the optimum
value of K should be either 2 or 3 or at
the most 4 but beyond that increasing
the number of clusters is not
dramatically changing the value in WSS
because that pretty much gets stabilized
okay now that we have got the value of K
and let's assume that these are our
delivery points the next step is
basically to assign two centroids
randomly so let's say C1 and C2 are the
centroids assigned randomly now the
distance of each location from the
centroid is measured and each point is
assigned to the centroid Which is
closest to it so for example these
points are very obvious that these are
closest to C1 whereas this point is far
away from C2 so these points will be
assigned which are close to C1 will be
assigned to C1 and these points are
locations which are close to C2 will be
assigned to C2 and then so this is the
how the initial grouping is done this is
part of C1 and this is part of C2 then
the next step is to calculate the actual
centroid of this data because remember
C1 and C2 are not the centroids they've
been randomly assigned points and only
thing that has been done was the data
points which are closest to them have
been assigned but now in this step the
actual centroid will be calculated which
may be for each of these data sets
somewhere in the middle so that's like
the main point that will be calculated
and the centroid will actually be
positioned or repositioned there
same with C2
so the new centroid for this group is C2
in this new position and C1 is in this
new position once again the distance of
each of the data points is calculated
from these centroids now remember it's
not necessary that the distance Still
Remains the or each of these data points
still remain in the same group by
recalculating the distance it may be
possible that some points get
reallocated like so you see this so this
point earlier was closer to C2 because
C2 was here but after recalculating
repositioning it is observed that this
is closer to C1 than C2 so this is the
new grouping so some points will be
reassigned and again the centroid will
be calculated and if the centroid
doesn't change so that is the repetitive
process iterative process and if the
centroid doesn't change once the
centroid stops changing that means the
algorithm has converged and this is our
final cluster with this as the centroid
C1 and C2 as the centroids these days
data points as a part of each cluster so
I hope this helps in understanding the
whole process iterative process of K
means clustering so let's take a look at
the K means clustering algorithm let's
say we have X1 X2 X3 and number of
points as our inputs and we want to
split this into K clusters or we want to
create K clusters so the first step is
to randomly pick K points and call them
centroids they are not real centroids
because centroid is supposed to be a
center point but they are just called
centroids and we calculate the distance
of each and every input point from each
of the centroids so the distance of X1
from C1 from C2 C3 each of the distances
we calculate and then find out which
distance is the lowest and assign X1 to
that particular random centroid repeat
that process for X2 calculate its
distance from each of the centroid c 1 C
to C3 up to c k a and find which is the
lowest distance and assign X2 to that
particular centroid same with X3 and so
on so that is the first round of
assignment that is done now we have K
groups because there are we have
assigned the value of K so there are K
centroids and so there are K groups all
these inputs have been split into K
groups however remember we picked the
centroids randomly so they are not real
centroids so now what we have to do we
have to calculate the actual centroids
for each of these groups which is like
the mean position which means that the
position of the randomly selected
centroids will now change and they will
be the main positions of this newly
formed K groups and once that is done we
once again repeat this process of
calculating the distance right so this
is what we are doing as part of Step 4
we repeat step two and three so we again
calculate the distance of X1 from the
centroid C1 C2 C3 and then C which is
the lowest value and assign X1 to that
calculate the distance of X2 from C1 C
to C3 or whatever up to c k and find
whichever is the lowest distance and
assign X2 to that centroid and so on in
this process there may be some
reassignment X1 Pro was probably
assigned to Cluster C2 and after doing
this calculation maybe now X1 is
assigned to C1 so that kind of
reallocation may happen so we repeat the
steps 2 and 3 till the position of the
centroids don't change or stop changing
and that's when we have convergence so
let's take a detailed look at it at each
of these steps so we randomly pick K
cluster centers we call them centroids
because they are not initially they are
not really the centroids so we let us
name them C1 C2 up to CK and then step
two we assign each data point to the
closest Center so what we do we
calculate the distance of each x value
from each C value so the distance
between X1 C1 distance between X1 C2 X1
C3 and then we find which is the lowest
value right that's the minimum value we
find and assign X1 to that particular
centroid then we go next to X2 find the
distance of X2 from C1 X2 from C2 X2
from C3 and so on up to c k and then
assign it to the point or to the
centroid which has the lowest value and
so on so that is Step number two in Step
number three We Now find the actual
centroid for each group so what has
happened as a part of Step number two we
now have all the points all the data
points grouped into K groups because we
we wanted to create K clusters right so
we have K groups each one may be having
a certain number of input values they
need not be equally distributed by the
way based on the distance we will have K
groups but remember the initial values
of the C1 C2 were not really the
centroids of these groups right we
assign them randomly so now in step 3 we
actually calculate the centroid of each
group which means the original point
which we thought was the centroid will
shift to the new position which is the
actual centroid for each of these groups
okay and we again calculate the distance
so we go back to step 2 which is what we
calculate again the distance of each of
these points from the newly positioned
centroids and if required we reassign
these points to the new centroids so as
I said earlier there may be a
reallocation so we now have a new set or
a new group we still have K groups but
the number of items and the actual
assignment may be different from what
was in Step 2 here okay so that might
change then we perform step 3 once again
to find the new centroid of this new
group so we have again a new set of
clusters new centroids and new
assignments we repeat this step two
again once again we find and then it is
possible that after iterating through
three or four or five times the centroid
will stop moving in the sense that when
you calculate the new value of the
centroid that will be same as the
original value or there will be very
marginal change so that is when we say
convergence has occurred and that is our
final cluster that's the formation of
the final cluster hey there learner Sim
linen brings you a comprehensive
professional certificate program in Ai
and machine learning from Purdue
University that is in collaboration with
IBM that will cover a wide range of
topics that will Empower you with the
knowledge and skills needed to excel in
the field of AI to learn more about this
course you can find the course link
mentioned in the description box alright
so let's see a couple of demos of
k-means clustering we will actually see
some live Demos in Python notebook using
python notebook but before that let's
find out what's the problem that we are
trying to solve the problem statement is
let's say Walmart wants to open a chain
of stores across the State of Florida
and it wants to find the optimal store
locations now the issue here is if they
open too many stores close to each other
obviously the they will not make profit
but if they if the stores are too far
apart then and they will not have enough
sales so how do they optimize this now
for an organization like Walmart which
is an e-commerce giant they already have
the addresses of their customers in
their database so they can actually use
this information or this data and use
k-means clustering to find the optimal
location now before we go into the
python notebook and show you the Live
code I wanted to take you through very
quickly a summary of the code in the
slides and then we will go into the
python notebook so in this block we are
basically importing all the required
libraries like numpy
matplotlib and so on and we are loading
the data that is available in the form
of let's say the addresses for
simplicity's sake we will just take them
as some data points then the next thing
thing we do is quickly do a scatter plot
to see how they are related to each
other with respect to each other so in
the scatter plot we see that there are a
few distinct groups already being formed
so you can actually get an idea about
how the cluster would look and how many
clusters what is the optimal number of
clusters and then starts the actual
k-means clustering process so we will
assign each of these points to the
centroids and then check whether they
are the optimal distance which is the
shortest distance and assign each of the
points data points to the centroids and
then go through this iterative process
till the whole process converges and
finally we get an output collectors so
we have four distinct clusters and
which is if we can say that this is how
the population is probably distributed
across Florida State and the centroids
are like the location where the store
should be the optimum location where the
store should be so that's the way we
determine the best locations for the
store and that's how we can help Walmart
find the best locations for the stores
in Florida so now let's take this into
python notebook let's see how this looks
when we are learning running the code
live all right so this is the code for
k-means clustering in Jupiter notebook
we have a few examples here which we
will demonstrate how k-means clustering
is used and even there is a small
implementation of k-means clustering as
well okay so let's get started okay so
this block is basically importing the
various libraries that are required like
matplotlib and pi and so on and so forth
which would be used as a part of the
code then we are going and creating
blobs which are similar to clusters now
this is a very neat feature which is
available in scikit-learn make blobs is
a nice feature which creates clusters of
data sets so that's a wonderful
functionality that is readily available
for us to create some test data kind of
thing
so that's exactly what we are doing here
we are using make blobs and we can
specify how many clusters we want so
centers we are mentioning here so it
will go ahead and so we just mentioned
four so it will go ahead and create some
test data for us
and this is how it looks as you can see
visually also we can figure out that
there are four distinct classes or
clusters in this data set and that is
what make blobs actually provides now
from here onwards we will basically run
the standard k-means functionality that
is readily available so we really don't
have to implement k-means itself the
cayman's functionality or the or the
function is readily available you just
need to feed the data and we'll create
the Clusters so this is the code for
that we import k-means and then we
create an instance of k-means and we
specify the value of K this n underscore
clusters is the value of K remember K
means in K means K is basically the
number of clusters that you want to
create and it is a integer value so this
is where we are specifying that so we
have K is equal to 4. and so that
instance is created we take that
instance and as with any other machine
learning functionality fit is what we
use the function or the method rather
fit is what we use to train the model
here there is no real training kind of
thing but that's the call okay so we are
calling fit and what we are doing here
we are just passing the data so X has
these values the data that has been
created right so that is what we are
passing here and this will go ahead and
create the Clusters and then we are
using
after doing uh fit We Run The predict
which basically assigns for each of
these observations which cluster it
belongs to all right so it will name the
Clusters maybe this is cluster one this
is two three and so on or I will
actually start from zero cluster 0 1 2
and 3 maybe and then for each of the
observations it will assign based on
which cluster it belongs to it will
assign a value so that is stored in y
underscore K means when we call predict
that is what it does and we can take a
quick look at these y underscore K means
or the cluster numbers that have been
assigned for each observation so this is
the cluster number assigned for
observation one maybe this is for
observation two observation three and so
on so we have how many about I think 300
samples right so all the 300 samples
there are 300 values here each of them
the cluster number is given and the
class faster number goes from 0 to 3 so
there are four clusters so the numbers
go from 0 1 2 3 so that's what is seen
here okay now so this was a quick
example of generating some dummy data
and then clustering that okay and this
can be applied if you have proper data
you can just load it up into X for
example here and then run the game so
this is the central part of the k-means
clustering program example so you
basically create an instance and and you
mentioned how many clusters you want by
specifying this parameter and underscore
clusters and that is also the value of K
and then pass the data to get the values
now the next section of this code is the
implementation of a k means now this is
kind of a rough implementation of the
k-means algorithm so we will just walk
you through I will walk you through the
code at each step what it is doing and
then we will see a couple of more
examples of how k-means clustering can
be used in maybe some real life examples
real life use cases all right so in this
case here what we are doing is basically
implementing k-means clustering and
there is a function for a library it
calculates for a given two pairs of
points it will calculate the the
distance between them and see which one
is the closest and so on so this is like
this is pretty much like what k-means
does right so it calculates the distance
of each point or each data set from
predefined centroid and then based on
whichever is the lowest this particular
data point is assigned to that percent
right so that is basically available as
a standard function and we will be using
that here so as explained in the slides
the first step that is done in case of
k-means clustering is to randomly assign
some centroids so as a first step we
randomly allocate a couple of centroids
which we call here we are calling as
centers
and then we put this in a loop and we
take it through an iterative process
for each of the data points we first
find out using this function pairwise
distance argument for each of the points
we find out which one which Center or
which randomly selected centroid is the
closest and accordingly we assign that
data or the data point to that
particular centroid or cost staff and
once that is done for all the data
points we calculate the new centroid by
finding out the mean position with the
center position right so we calculate
the new centroid and then we check if
the new centroid is the coordinates or
the position is the same as the previous
centroid the positions we will compare
and if it is the same that means the
process has converged so remember we do
this process till the centroids or that
centroid doesn't move anymore right so
the centroid gets relocated each time
this reallocation is done so the moment
it doesn't change anymore the position
of the centroid doesn't change anymore
we know that convergence has occurred so
till then so you see here this is like
an infinite Loop while true is an
infinite Loop it only breaks when the
centers are the same the new center and
the old Center positions are the same
and once that is done we return the
centers and the labels now of course as
explained this is not a very
sophisticated and advanced
implementation very basic implementation
because one of the flaws in this is that
sometimes what happens is the centroid
the position will keep moving but in if
the change will be very minor so in that
case also with that is actually
convergence right so for example the
change is 0.0001 we can consider that as
convergence otherwise what will happen
is this will either take forever or it
will be never ending so that's small
flaw here so that is something
additional checks may have to be added
here but again as mentioned this is not
the most sophisticated implementation
this is like a kind of a rough
implementation of the k-means clustering
okay so if we execute this code this is
what we get as the output so this is the
definition of this particular function
and then we call that find underscore
clusters and we pass our data X and the
number of clusters which is 4 and if we
run that and plot it this is the output
that we get so this is of course each
cluster is represented by a different
color so we have a cluster in green
color yellow color and so on and so
forth and these big points here these
are the centroids this is the final
position of the centroids and as you can
see visually also this appears like a
kind of a center of all these points
here right similarly this is like the
center of all these points here and so
on so this is the example or this is an
example of a implementation of K means
clustering and next we will move on to
see a couple of examples of how k-means
clustering is used in maybe some real
life scenarios or use cases in the next
example or demo we are going to see how
we can use k-means clustering to perform
color compression we will take a couple
of images so there will be two examples
and we will try to use cayman's
clustering to compress the colors this
is a common situation in image
processing when you have an image with
millions of colors but then you cannot
render it on some devices which may not
have enough memory so that is the
scenario where where something like this
can be used so before again we go into
the python notebook let's take a look at
quickly the the code as usual we import
the libraries and then we import the
image and then we will flatten it so the
reshaping is basically we have the image
information stored in the form of pixels
and if the image is like for example 427
by 640 and it has three colors so that's
the overall dimension of the of the
initial image we just reshape it and
feed this to our algorithm and this will
then create clusters of only 16 clusters
so this this colors there are millions
of colors and now we need to bring it
down to 16 colors so we use K is equal
to 16 and this is uh when we visualize
this is how it looks there are these are
all about 16 million possible colors the
input color space has 16 million
possible colors and we just
sub compressor to 16 color so this is
how it would look when we compress it to
16 colors and this is how the original
image looks and after compression to 16
colors this is uh the new image looks as
you can see there is not a lot of
information that has been lost though
the image quality is definitely reduced
a little bit
so this is an example which we are going
to now see in Python notebook let's go
into the python node and once again as
always we will import some libraries and
load this image called
flower.jpg okay so let me load that and
this is how it looks this is the
original image which has I think 16
million colors and this is the shape of
this image which is basically what is
the shape is nothing but the overall
size right so this is 427 pixel by 640
pixel and then there are three layers
which is this three basically is for RGB
which is red green blue so color image
will have that right so that is the
shape of this now what we need to do is
data let's take a look at how data is
looking so let me just create a new cell
and show you what is in data basically
we have captured this information
so data is what let me just show you
here
all right so let's take a look at China
what are the values in China and if we
see here this is how the data is stored
this is nothing but the pixel values
okay so this is like a matrix and each
one has about for for this 427 by 640
pixel side so this is how it looks now
the issue here is these values are large
the numbers are large so we need to
normalize them to between 0 and 1 right
so that's why we will basically create
one more variable which is data which
will contain the values between 0 and 1
and the way to do that is divide by 255
so we divide China by 255 and we get the
new values in data so let's just run
this piece of code and this is the shape
so we now have also yeah what we have
done is we changed using reshape we
converted into the three-dimensional
into a two-dimensional data set and let
us also take a look at how
let me just insert
probably a cell here and take a look at
how data is looking all right so this is
how data is looking and now you see this
is the values are between 0 and 1 right
so if you earlier noticed in case of
china the values were large numbers now
everything is between 0 and 1. this is
one of the things we
all right so after that the next thing
that we need to do is to visualize this
and we can take random set of maybe 10
000 points and plot it and check and see
how this looks so let us just plot this
yeah so this is how the original the
color the pixel distribution is these
are two plots one is red against Green
and another is red against Blue and this
is the original distribution of
so then what we will do is we will use
k-means clustering to create just 16
clusters for the various colors and then
apply that to the image now what will
happen is since the data is large
because there are millions of colors
using regular K means maybe a little
time consuming so there is another
version of k-means which is called mini
batch came in so we will use that which
is which processes in the overall
concept Remains the Same but this
basically processes it in smaller
batches that's the only thing okay so
the results will pretty much be the same
so let's go ahead and execute this piece
of code and also visualize this so that
we can see that there are this this is
how the 16 colors would look so this is
red again screen and this is red against
Blue there is quite a bit of similarity
between this original color schema and
the new one right so it doesn't look
very very completely different or
anything like that now we apply this the
newly created colors to the image and we
can take a look how this is looking now
we can compare both the images so this
is our original image and this is our
new image so as you can see there is not
a lot of information that has been lost
it pretty much looks like the original
image yes we can see that for example
here there is a little bit it appears a
little dullish compared to this one
right because we kind of took off some
of the finer details of the color but
overall the high level information has
been maintained at the same time the
main advantage is that now this can be
this is an image which can be rendered
on a device which may not be that very
sophisticated now let's take one more
example with a different image in the
second example we will take an image of
the Summer Palace in China and we repeat
the same process this this is a high
definition color image with millions of
colors and also three-dimensional
and now we will reduce that to 16 colors
using k-means clustering and we do the
same process like before we reshape it
and then we cluster the colors to 16 and
then we render the image once again and
we will see that the color the quality
of the image slightly deteriorates as
you can see here this has much finer
details in this which are probably
missing here but then that's the
compromise because there are some
devices which may not be able to handle
this kind of a high density images so
let's run this chord in Python notebook
all right so let's apply the same
technique for another picture which is
even more intricate and has probably
much complicated color schema so this is
the image now once again we can take a
look at the shape which is 427 by 640 by
3 and this is the new data would look
somewhat like this compared to the
flower image so we have some new values
here and we will also bring this as you
can see the numbers are much bigger so
we will much bigger so we will now have
to scale them down to values between 0
and 1 and that is done by dividing by
255 so let's go ahead and do that
and reshape it okay so we get a
two-dimensional Matrix and we will then
as a next step we will go ahead and
visualize this how it looks the 16
colors and this is basically how it
would look 16 million colors and now we
can
create the Clusters out of this the 16
k-means clusters we will create so this
is how the distribution of the pixels
would look with 16 colors and then we go
ahead and apply this
visualize how it is looking for with the
new just the 16 color so once again as
you can see this looks much richer in
color but at the same time and this
probably doesn't have as we can see it
doesn't look as rich as this one but
nevertheless the information is not lost
the shape and all that stuff and this
can be also rendered on a slightly a
device which is probably not that
sophisticated okay so that's pretty much
it so we have seen two examples of how
color compression can be done using
k-means clustering and we have also seen
in the previous examples of how to
implement k-means the code to roughly
how to implement k-means clustering and
we use some sample data using blob to
just execute the cayman's clustering if
you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simply learns professional certification
program in Ai and machine learning from
Purdue University in collaboration with
IBM should be a right choice for more
details use the link in the description
box below with that in mind so let's go
a little deeper into hierarchical
clustering let's consider we have a set
of cars and we have a group similar we
want to group similar ones together so
below we have you'll see four different
cars down there and we get two clusters
of car types sedan and SUV so if you're
just looking at it you can probably
think oh yeah we'll put the sedans
together and the SUVs together and then
at last we can group everything into one
cluster so we just have just cars so you
can see as we have this we make a nice
little tree this is very common when you
see anybody talks about hierarchical
clustering this is usually what you see
and what comes out of it we terminate
when we are left with only one cluster
so we have as you can see we bring them
all together we have one cluster we
can't bring it together anymore because
we're together hierarchical clustering
is separating data into different groups
based on some measure of similarity so
we have to find a way to measure what
makes them alike and what makes them
different agglomerative clustering is
known as a bottom-up approach remember I
said think of that as bringing things
together do you see I think the Latin
term aglo is together because you have
your conglomerate rocks where all the
different pieces of rocks are in there
so we want to bring everything together
that's a bottom up and then divisive is
we're going to go from the top down so
we take one huge cluster and we start
dividing it up into two clusters into
three four five and so on digging even
deeper into how hierarchical clustering
Works let's consider we have a few
points on a plane so or this plane is 2D
so we have an X Y coordinates kind of
makes it easy we're going to start with
measure the distance so we want to
figure a way to compute the distance
between each point each data point is a
cluster of its own remember if we're
going from the bottom up a glomerative
then we have each point being its own
cluster we try to find the least
distance between two data points to form
a cluster and then once we find those
with the least distance between them we
start grouping them together so we start
forming clusters of multiple points this
is represented in a tree-like structure
called dendogram so there's another key
word dendogram and you can see it is
it's just a branch we've looked at
before and we do the second group the
same so it gets its own dendrogram and
the third gets its own denigram and then
we might group two groups together so
now those two groups are all under one
dendogram because they're closer
together other than the P1 and P2 and
then we terminate when we are left with
one cluster so we finally bring it all
together you can see on the right how
we've come up all the way up to the top
whoops and we have the gray hierarchical
box coming in there and connecting them
so we have just one cluster and that's a
good place to terminate because there is
no way we can bring them together any
further so how do we measure the
distance between the data points I mean
this is really where it starts getting
interesting up until now you can kind of
eyeball it and say hey these look
together but when you have thousands of
data points how are we going to measure
those distances and there is a lot of
ways to get the distance measure so
let's go and take a look at that
distance measures will determine the
similarity between two elements and it
will influence the shape of the Clusters
and we have euclidean distance measure
we have squared euclidean distance
measure which is almost the same thing
but with less computations and we have
the Manhattan distance measure which
will give you slightly different results
and we have the cosine distance measure
which again is very similar to the
euclidean playing with triangles and
sometimes it can compute faster
depending on what kind of data you're
looking at so let's start with the
clitian distance measure the most common
is we want to know the distance between
the two points so if we have Point p and
point Q the euclidean distance is the
ordinary straight line it is the
distance between the two points in
euclidean space and you should recognize
D equals in this case we're going to sum
all the points so if there was more than
one point we could figure out the
distance to the not more than one points
this is the sum of more than two
Dimensions so we can have the distance
between each of the different dimensions
squared and that will give us and then
take the square root of that and that
gives us the actual distance between
them and they should look familiar from
euclidean geometry maybe you haven't
played too much with multiple Dimensions
so the summation symbol might not look
familiar to you but it's pretty
straightforward as you add the distance
between each of the two different points
squared so if your y difference was 2
minus 1 squared would be two and then
you take the difference between the X
again squared and if there was a z
coordinates it would be you know Z1
minus Z2 squared and then take the
square root of that and sum it all or
sum it all together and then take the
square root of it so to make it compute
faster since the difference in distances
whether one is farther apart or closer
together than the other we can do What's
called the squared euclidean distance
measurement this is identical to the
euclidean measurement but we don't take
the square root at the end there's no
reason to it certainly gives us the
exact distance but as far as doing
calculations as to which one's bigger or
smaller than the other one it won't make
a difference so we'll just go with the
so we just get rid of that that final
square root computes faster and it gives
us the pretty much the euclidean squared
distance on there now if the Manhattan
distance measurement is a simple sum of
horizontal and vertical components or
the distance between two points measured
along axes at right angles now this is
different because you're not looking at
the direct line between them and in
certain cases the individual distances
measured will give you a better result
now generally that's not true most times
you go with euclidean squared method
because that's very fast and easy to see
but the Manhattan distance is you
measure just the Y value and you take
the absolute value of it and you measure
just the X difference you take the
absolute value of it and just the Z and
if you had more you know different
dimensions in their a b c d e f however
many dimensions you would just take the
absolute value of the difference of
those dimensions and then we have the
cosine distance similarity measures the
angle between the two vectors and as you
can see as the two vectors get further
and further apart the cosine distance
gets larger so it's another way to
measure the distance very similar to the
euclidean so you're still looking at the
same kind of measurement so it should
have a similar result as the first two
but keep in mind the Manhattan will have
a very different result and you can end
up with a bias with the Manhattan if
your data is very skewed if one set of
values is very large and another set of
values is very small but that's a little
bit beyond the scope of this it's just
important to know that about the
Manhattan distance so let's dig into the
agglomerative clustering and a glometric
clustering begins with each element as a
separate cluster and then we merge them
into a larger cluster how do we
represent a cluster of more than one
point so we're going to kind of mix the
distance together with the actual
agglomerative and see what that looks
like and we're actually going to have
three key questions that are going to be
answered here so how do we represent a
cluster of more than one point so we
want to look at the math what it looks
like mathematically and geometrically
how do we determine The Nearness of
clusters when to stop combining clusters
always important to have your computer
script or your whatever you're working
on have a termination point so it's not
going on eternally we've all done that
if you do any kind of computer
programming or writing a script let's
assume that we have six data points in a
euclidean space so again we're dealing
with X Y and Z in this case just X and Y
so how do we represent a cluster of more
than one point let's take a look at that
and first we're going to make use of
centroids very common terminology in a
lot of machine learning languages when
we're grouping things together so we're
going to make use of centroids which is
the average of its points
and you can see here we're going to take
the 1 2 and the 2 1 and we're going to
group them together because they're
close I mean if we were looking at all
the points we'd look for those that are
closest and start with those and we're
going to take those two we're going to
compute a point in the middle and we'll
give that point 1 5 1.5 1.5 and that's
going to be the centroid of those two
points and next we start measuring like
another group of points we got four one
five zero when they're pretty close
together so we'll go ahead and set up a
centroid of those two points in this
case it would be the 4.5 and 0.5 would
be the measurements on those two points
and once we have the centroid of the two
groups we find out that the next closest
point to a centroid is over on the left
and so we're going to take this and say
oh 0 0 is closest to the 1.5 1.5
centroid so let's go ahead and group
that together and we compute a new
centroid based on those three points so
now we have a centroid of 1.1 or 1 comma
1. and then we also do the this again
with the last point the five three and
it computes into the first group and you
can see our dendogram on the right is
growing so we have each of these points
are become connected and we start
grouping them together and finally we
get a centroid of that group two and
then finally the last thing we want to
do is combine the two groups by their
centroids and you can see here we end up
with one large group and you'll have its
own centroid although usually they don't
compute the last centroid we just put
them all together so when do we stop
combining clusters well hopefully it's
pretty obvious to you in this case when
they all got to be one but there are
actually many approaches to it so first
pick a number of clusters K up front and
this is done in the fact that we don't
want to look at 200 in clusters we only
want to look at the top five clusters or
something like that so we decide the
number of clusters required in the
beginning and we terminate when we reach
the value K so if you looked back on our
clustering let me just go back a couple
screens you'll see how we clustered
these all together and we might want
just the two clusters and so we look at
just the top two or maybe we only want
three clusters and so we would compute
which one of these has a wider spread to
it or something like that there's other
computations to know how to connect them
and we'll look at that in just a minute
but to note that when we pick the K
value we want to limit the information
that's coming in so that can be very
important especially if you're feeding
it into another algorithm that requires
three values or you set it to four
values and you need to know that value
coming in so we might take the
clustering and say okay only three
clusters that's all we want for K so the
possible challenges this only makes
sense when we know the data well so when
you're clustering with K clusters you
might already know that domain and know
that that makes sense but if you're
exploring brand new data you might have
no idea how many clusters you really
need to explore that data with let's
consider the value of K to be 2. so in
this case in our previous example we
stop and we are left with two clusters
and you can see here that this is where
they came together the best whilst still
keeping separate the data the second
approach is stop when the next merge
would create a cluster with low cohesion
so we keep clustering till the next
merge of clusters creates a bad cluster
low cohesion setup on there that means
the point is so close to being between
two clusters it doesn't make sense to
bring them together but how is cohesion
defined oh let's dig a little deeper
into cohesion the diameter of a cluster
so we're looking at the actual diameter
of our cluster and the diameter is the
maximum distance between any pair of
points in the cluster we terminate when
the diameter of a new cluster exceeds
the threshold so as that diameter gets
bigger and bigger we don't want the two
circles or clusters to overlap and we
have radius of a cluster radius is the
maximum distance of a point from
centroid we terminate when the radius of
a new cluster exceeds the threshold
again we're not we don't want things to
overlap so when it crosses that
threshold and it's overlapping with
other data we stop so let's look at
divisive clustering remember we went
from the bottom up now we want to go
from the top down divisive clustering
approach begins with a whole set and
proceeds to divide it into smaller
clusters so we start with a single
cluster composed of all the data points
we split it into different clusters this
can be done using monothetic divisive
methods what is a monothetic divisive
method and we'll go backwards and let's
consider the example we took in the
agglomerative clustering to understand
this so we consider a space with six
points in it just like we did before
same points we had before and we name
each point in the cluster so we have in
this case we just gave it a letter value
a b c d e f since we follow top-down
approach in divisive clustering obtain
all possible splits into two columns so
we want to know where you could split it
here and we could do like a a b split
and a cdef split we could do BCE ADF and
you can see this starts generating a
huge amount of data a b c t e f and so
for each split we can compute cluster
sum of squares and we can see here the
we actually have the formula out for us
B J 12 equals N1 of absolute value of x
minus absolute value of x squared so
again we're Computing all the different
distances in there squared back to your
kind of euclidean distances on that and
so we can actually compute B A J between
clusters one and two and we have the
mean of the cluster and the grand mean
depending on the number of members in
the cluster and we select the cluster
with the largest sum of squares let's
assume that the sum of squared distance
is largest for the third split we had up
above and that's where we split the ABC
out and if we split the a b c out we're
left with the def on the other side we
again find the sum of squared distances
and split it into clusters so we go from
ABC we might find that the a is splits
into BC and D into EF and again you
start to see that hierarchical dendogram
coming down as we start splitting
everything apart and finally we might
have a splits in b and c and then each
one gets their own d e f and it
continues to divide until we get little
nodes at the end and every data has its
own point or until we get to K if we
have set a k value so we've kind of
learned a little bit about the
background and some of the math in
hierarchical clustering let's go ahead
and dive into a demo and our demo today
is going to be for the problem statement
we're going to look at U.S oil so a U.S
oil organization needs to know its cells
in various States in U.S and cluster of
the states based on the cells so what
are the steps involved in setting this
problem up so the steps we're going to
look at and this is really useful for
just about about any processing of data
although I believe we're going to be
doing this in R today we're going to
import the data set so we'll explore our
data a little bit there create a scatter
plot it's always good to have a visual
if you can once you have a visual you
can know if you're really far off in the
model you choose to Cluster the data in
or how many splits you need and then
we're going to normalize the data so
we're going to fix the data up so it
processes correctly we'll talk more
detail about normalization when we get
there and then calculate the euclidean
distance and finally we'll create our
dendogram so it looks nice and pretty
and we have something we can show to our
shareholders so that they have something
to go on and know why they gave us all
that money and salary for the year so we
go ahead and open up R and we're
actually using our studio which is the
really it has some nice features in it
it automatically sets up the three
Windows where we have our script file on
the upper left
and then we can execute that script and
it'll come down and put it into the
console bottom left and execute it and
then we have our plots off to the right
and I've got it zoomed in hopefully not
too large a font but large enough that
you can see it
and let's just go ahead and take a look
at some of the script going in here
it's clustering analysis and we're going
to work we'll call it my data and we're
going to assign it in R this is a symbol
for assigning and we're going to go read
CSV
read CSV file
and we'll put that in Brackets and let's
before we go any further let's just look
at the data outside of R it's always
nice to do if you can
and the file is going to be called
utilities.csv this would also be the
time to get the full path so you have
the right path to your file and remember
that you can always Post in the comments
down below
and when you post down there just let us
know you want to connect up with simply
learn so that they can get you this file
so you can get the same file we're
working on and you can repeat it and see
how this works this is utilities.csv
It's a comma separated variable file so
it's pretty straightforward and you can
see here they have the city fixed charge
and a number of different features to
the data and so we have our RoR cost low
demand cells
nuclear fuel cost on here and then going
down the other side we have U.S cities
Arizona Boston they have Central U.S I
guess they're grouping a number of areas
together the Commonwealth area you can
see down here in Nevada New England
Northern us Oklahoma the Pacific region
and so on so we have a nice little chart
of different data they brought in
and so I'm going to take that complete
path that ends in the utilities.csv
and we're going to import that file let
me just enlarge this all the way upside
an extra set of brackets here somehow or
maybe I missed a set of brackets
this can happen if you're not careful
you can get brackets on one side and not
the other or in this case I got double
brackets on each side there we go and
then the magic hotkeys in this case are
your control enter which will let me go
ahead and run the script
and so I've now loaded the data and as
you can see I went ahead and shrunk the
plot since we're going to be looking at
the window down below
and we can simply convert the data to a
string
now all of us do this automatically the
first time we say hey just print it all
out as a string and then we get this
huge mess of stuff
that doesn't make a whole lot of sense
so and you can see here they have you
can probably kind of pull it together as
looking at it but let's go ahead and
just do the head
now we'll do the head of my data
there we go and control enter on that
and the head shows the first five rows
you'll see this in a lot of different
scripts in R it's you type in head and
then in Brackets you put your data and
it comes through and this is the first
five rows as you can see below
and it shows Arizona Boston Central and
it has the same columns we just looked
at so we have the fixed charge the RoR
the cost the load the D demand I'm not
an expert in oil so I'm not even sure
what D demand is cells I'm guessing
nuclear how much of it's supplied by
nuclear and the actual fuel cost
and then the different states that it's
in or different areas
and one of the wonders of R is all these
cool easy to use tools that are so quick
so we'll do pairs
and pairs creates a nice graph so let me
go ahead and run this
uh whoops the reason it gave me an error
is because I forgot to resize it so let
me bring my plot way out so we can see
it and let's run that again
and you'll see here that we have a nice
graph of the different data and how it
plots together how the different points
kind of come together
this is neat because if you look at this
I would look at this and say hey this is
a great candidate for some kind of
clustering and the reason is is when I
look at any two pairs let's go down to
say cells and fuel cost towards the
bottom right
and when you look at them where they
cross over you sort of see things how
they group together but it's a little
confusing you can't really pinpoint how
they group together you could probably
look at these two and say yeah there's
pretty good commonalities there and if
you look at any of the other pairs
you'll start seeing some patterns there
also and so we really want to know what
are the patterns on all of them put
together not just any two of them the
whole setup
let me go ahead and Shrink my this down
for just a second just a notch here and
let's create a scatter plot oops
and this is simply just use the term
plot in Brackets and then which values
do we want to plot and if we remember
when we looked at the data earlier let
me just go back this way
in this case let's go ahead and compare
two just two values to see how they look
and we'll do fuel costs and cells
and it's in my data so we gotta let it
know which two columns we're looking at
next to each other
and it will open up our plot thing and
then go ahead and execute that
and we can see on those close-up what
we're just looking at in the pairs
and if I was eyeballing this I would say
oh look there's a kind of a cluster up
here of five items and this one is hard
to tell which cluster to be with but
maybe it's six you go in the top one and
you have a middle cluster and a bottom
cluster maybe two different clusters so
you can sort of group them together fuel
cost and the sales and see how they
connect with each other
and again that's only two different
values we're looking at so in the long
run we want to look at all of them
and then the people in the back
they sent me this grip so we can go
ahead and add labels so with my data
text fuel cost cells the labels equal
City
position four these are numbers you can
kind of play with till they look nice
and oops again I forgot to resize my
plot it doesn't like having it too small
and we'll run that
I must type something in here
oh I did a lowercase dnated capital D
there we go so now we can go in here and
do this with my data and you can see a
little hard to see on my screen with all
the the different things in there hey
plus the actual cities so we can now see
where all the cities are in connection
with in this case fuel cost and sales so
you have a nice label to go with the
graph
and then we can also go ahead and plot
in this case let's do oh the RoR oops
and we'll do that
also with the cells and do my data
remember to leave it lowercase this time
so we plot those two
we'll come over here and it's going to
I'm surprised it can give me an error
and then we'll also add in the with
statement so we put some nice labels in
there
and it's going to be the same as before
except instead of doing the fuel cost
cells we want the r cells and we'll
execute that
oops and of course it gives me an error
because I shrunk it down so let's redo
those two again
there we go and we can see now we have
the RoR with cells and we probably get a
slightly different clustering here if I
was looking at these cities they're
probably different than we had before
but you could probably look at this and
say yeah these kind of go together and
those kind of go together but again
we're going to be looking at all of them
instead of just one or two
and so at this point we want to dive
into the next step where we're going to
start looking at a little bit of coding
or scripting here
this is very important because we're
going to be looking at normalization
we put that in there normalization and
if you've done any of the other machine
learning skills and setup this should
start to look very normal in your
pre-processing of data whether you're in
r or python or any of these scripts we
really want to make sure you normalize
your data so it's not biased
remember we're dealing with distances
and if I have let's say the RoR is even
look at this graph here on the right
you can see where my RoR varies between
8 and 14. that's a very small variable
and our cells varies between four
thousand and sixteen thousand so you can
imagine the distance between four
thousand and eight thousand which is the
distance of four thousand versus 8 to 10
versus 2 the cells is going to dominate
so if we do any kind of special work on
this it's going to look at cells and
it's going to Cluster them just by the
cells alone and then RoR might have a
little tiny effect of two versus four
thousand we want to level the playing
field
turns out there's actually a number of
ways in script to normalize
so I'm just going to put in the code
that they put together in the back for
me and let's talk about it a little bit
so we have Z we're going to assign it to
my data
and let's go ahead and
we're going to do a little reshaping
across all rows
or I mean across all columns so each of
the rows is going to have a little
reshaping there
and then we're going to get M which
stands for means
and we're going to apply it to my data
so again we want to go ahead and create
a the most common variable in there
and then s is going to be the SD stands
for standard deviation
so instead of just doing a lot of times
what they do with normalization of data
is we just reshape the data everything
between 0 and 1. so that if the lower
end is eight that now becomes zero and
the upper end is 14 that now becomes 1.
that doesn't help if it is not a linear
set of data so with this we're going to
look for the means and the standard
deviation for reshaping the data
and that way the most common values now
become the kind of like the center point
and then the standard deviation is how
big the spread so we want the standard
deviation to be equal amongst all of
them and then finally we go ahead and
take Z and with the Z we're going to
reassign it and we're going to scale the
original my data which we re-shaped
based on M and based on the standard
deviation
and the 2 in here that just means we're
looking at everything in kind of a x y
kind of plot
and we can quickly run these control
enter control enter control enter
control enter so now we have Z which is
a scaled version of my data
and now we can go ahead and calculate
the euclidean distance
oops calcu
there we go
and in R this is so easy once you've
gotten to here we've done all that
pre-data processing
we'll call it distance
and we'll assign this to
dist so d-i-s-t
is the computation for getting the
euclidean distance and we can just put Z
in there because we've already
reformatted and scaled Z to fit what we
want let me go ahead and just hit enter
on that
and I'm going to widen my left hand side
again
I'm always curious what does this data
look like so let's just type in distance
which will print the variable down below
oops
you have to hit Ctrl enter
and this prints out a huge amount of
information
as you can see just kind of streams down
there
and let's go ahead and enlarge this
and I don't know about you but when I
look at something like this it doesn't
mean a whole lot to me other than I see
two three four five six
and then you kind of have the top part 6
17 18
so I imagine this is like a huge chart
is what we're looking at
and we can go ahead and use print oh
distance
digits
equal three and let's run that
oops I keep forgetting that it has to go
through the graph on the right and we
see a different slightly different
output in here let me just open this up
so we can see we're looking at and by
cutting down the distance you can start
to see the patterns here if it's looking
at the different distances
so if I go to the top we have the
distance between one and two one and
three one and four one and five one and
six and then two and three and so on
obviously the distance between itself is
zero and it doesn't repeat the data so
we don't care to see two versus one
again because we already know the
distance between one and two
and so we have a nice view of all the
distances in the chart and that's what
we're looking at right here and it's a
little easier to read that's why we did
the print statement up here to do digits
equals three make it a little bit
smaller we could even just do digits
well let's just do two see what that
looks like we might lose some data on
this one if it's uh if something's way
off
but we have a nice setup and we can see
the different distances and that's what
we were computed here between each of
the points
and then the whole reason we're doing
this is to get ourselves a nice
dendogram going a nice clustering
dendogram we'll do a couple of these
looking at different things
we'll take a variable hc.l and we're
going to assign it
H cluster
and then distance
that easy we've already computed the
distances so the H clustering does all
the work for us
and let me hit enter on there so now we
have our HCL which is assigned the H
clustering computation based on
distances and apart and then I'm going
to expand my graph because we would like
to go ahead and see what this looks like
we can simply plot that
and hit the control enter so it runs
and look at that we have a really nice
clustering dendrogram except when I look
at it the first thing I notice is it
really shows like numbers down below
now if you were a shareholder in some
data scientist came up to you and said
look at this this is what it means you'd
be looking at that going what the heck
does 3 9 14 19 1 18 mean so let's give
it some words there
so let's do the same plot with our HCL
HC there we go
and let's add in labels and this is just
one of the commands and plots
so we have labels equals my data
and then under my data we want to know
the city
and we'll have it hang minus one that's
just the instructions to make it pretty
so we'll run that
oops I accidentally ran just to hang my
one let me try that again
there we go okay so now you can see what
is done in the hang my one turns it
sideways that way we can see Central
which is Central us and Kentucky
and we start to actually get some
information off our clustering setup and
the information you start looking at is
that when we put all the information
together you probably want to look at
Central America and Kentucky together
Oklahoma and Texas has a lot of
commonality as does Arizona and Southern
us and you can even group all five of
those Florida Oklahoma Texas Arizona and
Southern U.S these regions for some
reason share a lot of similarities and
so we want to start asking what those
similarities are
but this gives us a place to look it
says hey these things really go together
you should be grouping these together as
far as your cells and what's what you're
doing
and then one of the things you might
want to do is there's also we can do the
dendrogram average
this changes how we do the clustering
so it looks very similar like we had
before and those are HCL we're going to
assign it we're going to do an H cluster
we're still doing it on distance oops
distance
and this time we're going to set the
method to average so we can change the
methodology in which it computes the
values
and before if you remember correctly we
did median median's a little bit
different than means we did the most
common one and then we want the average
of the median
and let's go ahead and run that
and then we can plot
and here's our HCL
oops there we go here's our HCL and I
can run that plot and you can see this
changed a little bit so our way it
computes and groups things looks a
little different than it did before and
let's go and put the cities back in
there and do the hang
control copy let me just really quickly
copy that down here
because we want the same labels
and again you can see Nevada Idaho pujet
I remember we were looking at Southern
U.S and Arizona Texas and Oklahoma
Florida so the grouping really hasn't
changed too much so we still have a very
similar grouping
it's almost kind of flipped it as far as
the distance based on average has
but this is something you could actually
take to the shareholders and say hey
look these things are connected and at
which point you want to explore a little
deeper as to why they're connected
because they're going to ask you okay
how are they connected and why do we
care
that's a little bit beyond the actual
scope of what we're working on today
but we are going to cover membership
what's called a clustering membership on
there
and let's create a member we'll just
call it Member One oops member dot one
and we're going to assign to this
we're going to do cut tree
and
cut tree
it limits it so what that means is I
take my HC Dot
l in here
oops there we go dot l and so I'm taking
the cluster I just built and we want to
take that cluster
and we want to limit it to just a depth
of three so we go ahead and do that and
run that one oops let me go run
there we go so now I've created my
Member One
and then whoops let me just move this
out of the way we're going to do an
Aggregate and we're going to use Z
remember Z from above and we're going to
turn Member One into a list and then
we're going to aggregate that together
based on the mean let me go ahead and
enter run that
Oops I did remember L it's actually
remember one
there we go
and if we take a look at this
we now have our group one fixed charge
and then all your different
columns listed there and most of them
should come up kind of looking between
zero and one but you'll see a lot of
variation because we're varying it based
on the means so it's a means the
standard deviation not just forcing it
in between 0 and 1.
which is a much better way usually to
normalize your data than just doing the
zero one setup
and finally we can actually look at the
actual values
and the same chart we just did
oops
I made a mistake on there with my data
there we go okay
and again we now have our actual data
instead of looking at just the if you
looked up here it's all between 0 and 1
and when we look down here
we now have some actual connections and
how far distance this different data is
again because more of a domain issue and
understanding the oil company and what
these different values means and you can
look at these as being the distances
between different items
so a little bit different View and you
have to really dig deep into this data
we really want you to take away from
this is the dendigram and the charts
that we did earlier
and that is the cluster output and our
nice dendogram so this would be stage
one in data analysis of the cells again
you'd have to have a lot more domain
experience to find out what all the
individual numbers we looked at mean and
what the distance is and what's
difference between Central America and
Kentucky and why they're similar and why
it groups all of Central Kentucky
Florida Oklahoma Texas Arizona and
Southern us together into one larger
group so it'd be Way Beyond the scope of
this but you can see how we start to
explore data and we start to see things
in here where things are grouped
together and ways we might not have seen
before and this is a good start for
understanding and giving advice for
cells and marketing maybe Logistics City
development there's all kinds of things
that kind of come together in the
hierarchical clustering as you begin to
explore data and we just want to point
out that we get three clusters of
regions with the highest cells region
with average cells region with the
lowest cells again those are some of the
things that they clustered it around and
you could actually see where things are
going on or lacking you know in this
case if you're the lowest cells no one
wants to be in the region of the lowest
cells if you are an aspiring machine
learning engineer looking for online
training and certifications from
prestigious universities and in
collaboration with leading experts then
search no more simply learns
professional certification program in Ai
and machine learning from Purdue
University in collaboration with IBM
should be a right choice for more
details use the link in the description
box below with that in mind the
regression algorithms is used to find
out the connection between dependent and
independent variables dependent
variables are nothing but a variable
that we are trying to predict or
forecast and independent variables are
the factor that influence the analysis
it is usually used to make projections
medical researchers frequently utilize
linear regression to study the
connection between patient blood
pressure and dosage researcher may give
patient different quantities of a
specific medication and track the how
their blood pressure changes a
regression model might use dosage as a
predictor variable and blood pressure as
a response variable
there are some popular regression
algorithms that come under supervised
machine learning first one is linear
regression regression trees non-linear
regression basal linear regression after
discussing what is supervised learning
and its types let's move forward to see
what actually regression analysis is
a stat technique known as iteration
analysis is used to simulate the
relationship between dependent and
independent variables using one or more
independent variables regression
analysis more precisely enables us to
comprehend and when other independent
variables are held constant changes in
the dependent variables value that
correspond to an independent variables
are changing prediction include values
that are continuous or real such as
temperature age pay and cost
there are some following concept or
terminologies that must be understood in
order to completely grasp regression
analysis the first one is dependent
variable in a regression analysis the
primary variable we wish to predict or
comprehend is the dependent variable
here dependent variables are also known
as Target variable the next one is
independent variable
the term independent variables also
known as predictor refers to the
elements that impact the dependent
variables or are used to forecast their
values
and the third one is outliers an
observation that is extremely high or
extremely low in relation to the other
observed values is referred as an
outlier an outlier should be avoided as
it might affect the outcomes the fourth
one is multicolinarity the situation is
set to as occurring when independent
variables have a higher correlation with
one another than other variables it
shouldn't be included in the data set
because it causes issues when
determining which variable has the
greatest impact and the fifth one is
overfitting and underfitting overfitting
is the problem that occurs whenever
system problems well well with the
training data set but poorly with the
test data set under fitting is a term
use when an algorithm does not perform
well even with the training data set
after understanding what regression
analysis is and its terminologies let's
move forward and see why we use with
regression analysis
including those involving the weather
sales marketing Trends and other factors
in this situation we need regression
analysis that can make forecast more
precisely so let's understand the
concept of regression analysis using an
example
business frequently use linear
regression to understand the
relationship between advertising
spending and income for instance they
might run a simple linear model with
advertising as a response variable and
revenue as a predictor variable the
direction model would look like this if
n equal to beta0 plus B1 is AD spending
where there are no advertisement the
coefficient 0 would indicate the whole
expected Revenue
when advertising expenditures are
increased by one unit coefficient will
represent the typical change in the
total revenue that is one dollar if beta
1 is negative then higher advertising
expenditure will result in lower Revenue
if beta 1 is nearly zero advertising
expenditures have little impact on
revenue and if beta 1 is positive it
would imply that more advertising
expenditures are linked to higher income
the amount of beta 1 will determine
whether a corporation decide to or
decrease its advertising budget
regression means creating a graph that
connects variable the best fit the given
data point the direction analysis model
can then make prediction about data
using the plot regression analysis is
defined as showing a line or a curve
that goes through all the data points on
the target predictive graph in such a
way that vertical distance between the
data points and the regression is
smallest whether a model has captured a
strong link between determined by the
distance between data points and the
line there are some more use case of
regression analysis like we can use
temperature another variable to predict
or forecast drain regression analysis
can be used for identification of Market
threads
for predicting traffic accidents caused
by reckless driving we can use
regression analysis for the same and
many more
so by understanding why we need
regression analysis moving forward let's
see some popular regression algorithms
in detail
the first one is linear regression
linear deviation is one of the most
famous and straightforward machine
learning algorithm utilized for
predictive analysis linear regression
shows the linear connection between
dependent and independent factors the
line of the equation is y equals to MX
plus b here y stands for the response
variable or a dependent variable where
the x is for the predictor variable or
an independent variable here m is the
estimated slope and B is for the
estimated intercept
and the next one is regression trees a
regression trees is worked through a
cycle known as binary recursive which is
iterative interaction that divides the
information into segments of branches
and afterwards keep splitting the data
into a smaller group as the technique
claims each plant this tree is used for
the dependent variable with continuous
values for example a regression trees
name food which is divided into segments
veg and non veg further it keeps
splitting into smaller groups
and the third one is non-linear
regression nonlinear suggestion is a
type of regression examination wherein
information is to fit a model and
afterward communicated with the
numerical function simple linear
regression relates to factor X and Y
with a straight line Y equals to MX plus
b while non-linear regression relates
the two factor in a non-linear
relationship nonlinear reaction can be
predict population growth over time or
the relationship between GDP and a
country's time and the fourth one is
patient linear regression the algorithm
is a way to deal with a linear
regression in which statical examination
is attempted inside the setting obvious
and inference linear regression and
patient regression can generate the same
reduction and with the help of Bayesian
processing we can retrieve the complete
variety of inferential solution instead
of a point estimate
after seeing some regression analysis
algorithm let's move forward and see
advantages and disadvantages of
different regression analysis models
and the first model is linear regression
model the advantages are work value is
respective of data set size and the
second one is gives information about
the relevance of features and the
disadvantages are the assumptions of
linear regression and the next one is
polynomial regression
the advantages are works on any size of
the data set and the second one is works
very well on non-linear problems and
disadvantages are we need to choose the
right polynomial degree for the variance
trade-off the next one is support Vector
regression algorithms and the advantages
are easily adaptable works very well on
non-linear problems and disadvantages
are difficult to understand and not well
known and the next one is decision tree
regression the advantages of decision
trees are no need to apply scaling and
the second one is works very well on
both linear and non-linear problems and
disadvantages are poor results on small
leadership and overfitting can easily
occur and the last one is random Forest
regression the advantages are powerful
and second one is accurate and
disadvantages are no interpretably and
the second one is overfitting can easily
occur
after seeing advantages and
disadvantages of different types of
regression analysis models let's move
forward and see some applications of
regression analysis
the first one is forecasting regression
analysis is frequently used in business
to predict potential opportunities and
dangers for example demand analysis
predicts how many items a buyer is
likely to purchase demand however is not
only the dependent variables when it
comes to business much more than just
direct income can be predicted using
regression analysis and the second one
is capm capital asset pricing model the
linear regression model is a key
component of the capital asset pricing
model capm which determines the
relationship between an asset expected
return and the associated Market risk
premium Financial analysts typically use
it for forecast corporate return and
operational performance when doing
financial analysis and the third one is
comparing with competition it can be
used to evaluate how well a business is
doing financially in comparison to a
certain rival
it can also be used to figure out how
the stock prices of two companies are
related to one another this can be
extended to find the correlation between
two competing companies it might help
the company identifying the effectors
affecting the sales in contrast to the
comparable company
these methods can help small business
succeed quickly within a short period of
time and the last one is identifying
problem regression is a beneficial for
identifying incorrect judgment as well
as for providing factual support for the
management for instance a retail store
manager might believe that extending the
hours of operation will greatly increase
sales
if you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simple learns professional certification
program in Ai and machine learning from
Purdue University in collaboration with
IBM should be a right choice for more
details use the link in the description
box below with that in mind
so let's go ahead and talk about
overfitting when we talk about
overfitting it's a scenario where the
machine learning model tries to learn
from the details along with the noise
and the data tries to fit each data
point on the curve you can see that
um
if you plug in your coordinates you're
just going to get the whatever is fitted
every point on the data stream there's
no average there's no two points that
might have that you know y might have
two different answers because uh if the
wind blows a certain way
um and your efficiency of your car maybe
you have a headwind so your car might
alter how efficient it is as it goes and
so there's going to be this variance on
here and this says no you can't have any
variance what's you know the this is
it's going to be exactly this there
can't be any you can't be the same speed
or the same car and have a slightly
different efficiency
so as the model has very less
flexibility it fails to predict new data
points and thus the model rejects every
new data point during the prediction
uh so you'll get like a really high
error on here
and so uh reasons for overfitting data
used for training is not cleaned and
contains noise garbage value is in it
you can spend so much time cleaning your
data and it's so important it's so
important that if you have if you have
some kind of something wrong with the
data coming in it needs to be addressed
whether it's the source of the data
maybe they use in medical different
measuring tools
so you now have to adjust for data that
came in from hospital a versus Hospital
b or even off of machine a and machine B
that's testing something and those those
numbers are coming in wrong
the model has a high variance again wind
is a good example I was talking about
that with the car you may have 100 tests
but because the wind's blowing it's all
over the place
size of training data used is not enough
so a small amount of data is going to
also cause this problem you only have a
few points and you try to plot
everything
the model is too complex this comes up a
lot
we put too many pieces together and how
they interact can't even be tracked and
so you have to go back break it up and
find out actually what correlates and
what doesn't
so what is underfitting a scenario where
machine learning models can neither
learn
the relationship between the data points
nor predict or classify a new data point
and you can see here we have our
efficiency of our car and our line drawn
and it's just going to be way off for
both the training and the predicting
data
as the model doesn't fully learn the
patterns it accepts every new data point
during the prediction
so instead of looking for a general
pattern we just kind of accept
everything
data used for training is not cleaned
and contains noise garbage and values
again under fitting and overfitting same
issue you got to clean your data
the model has a high bias
we've seen this in all kinds of things
from
the mod the most common is the driving
cars to facial identification or
whatever it is the model itself when
they build it might have a bias towards
one thing and this would be an
underfitted model but have that bias
because it's averaged it out so if you
have five people from India and 10
people from
Africa and 20 people from the U.S you've
created a bias because it's looking at
the 20 people and you only have a small
amount of data to work with
size of training data used is not enough
that goes with the size I was just
talking about
so we have a model with a high bias we
have size of training data used it's not
enough the model is too simple
again this is one straight line through
all the data when it needs as a slight
shift to it for other reasons
so what is a good fit
a linear curve that best fits the data
is neither overfitting or underfitting
models but is just right
and of course we have the nice examples
here where we have overfitting lines
going up and down every point is trying
to be include included under fitting
the line really is off from where the
data is and then a good fit is got to
get rid of that minimize that error
coming through
so this is all exciting but what does
this look like so we really need to jump
in and put a code together and see what
this looks like when we're programming
for this demo we'll bring up our trusty
anaconda and go into Jupiter notebook
for python
and move myself out of the way here
uh and so we're going to start off this
is going to be a demo on overfitting and
underfitting using python
and let's start with our Imports
now if you've been through enough of
these tutorials we don't want to spend a
huge amount of time on what we're
bringing in and what we're doing so you
should be up on doing this with python
and how to bring in your different
modules
uh we're going to bring in the SK learn
or the scikit processing sklin
learn.neural Network Import in a MLP
regressor
so there's our regressor model right
there
that's going to be our linear regression
model
and we have our metrics mean absolute
error if you remember we had our that's
how we figure out how well it fits is
how far off that error is based on the
um mean square error value MSE
and then of course numpy because we just
like to work with numpy it's a great
data array we always import it as MP
that's the most common way of doing it
and then we have SK learn model
selection import validation curve so
we're going to look at a validation
curve to see how good our models are
and then we have the data set we'll use
the very famous Iris data set and that's
embedded in the sci kit so the S key
learn data sets have a load Iris in
there
and then we have the matplot library
because if you're doing any kind of demo
or showing this off to your shareholders
we want to have something nice to
display it on
and then we have SK learn model
selection we're going to import import K
fold and we'll talk about that when we
get to it and then we're going to go
ahead and do just for our numpy we're
going to do like a random seed for
random numbers and then for our plot
style we'll use the GG plot that's just
some back end setup you could even
probably leave the plot style out
depending on what version of
um
you're using of
depending on what version you're using a
matplot library
and then we'll go ahead and run this
it's not going to do anything that we
can visibly see because it's just
loading those modules and then we also
want to load our Iris data in here and
the IRS data has an iris data and Iris
Target
we're going to load that as X and Y
and just so you can have an idea what
we're talking about we're going to go
ahead and print
X
and just the first bit of X we'll just
do the top of X and we'll also print y
so you can see what the top of Y looks
like print y
and since we're in numpy we're going to
go ahead and do our own thing if this
was uh of course some pandas we could
just do the head of it and see what it
looks like and you can see here we've
loaded this up and in X we have these
different measurements that they take of
the flower the iris flowers from this
particular data set and what kind of
flower it is it's going to be a zero one
or a two is actually what the target
comes out of even though that doesn't
show in here
and so we're going to come in here and
we're going to use the k-folds cross
validation with 20 folds
and a good catch that this was a model
selection we're we're going through and
we're selecting different parts of the
data in here here we use k-fold
cross-validation with 20 folds k equals
20 to evaluate the generalization
efficiency of the model within each fold
we will then estimate the training and
test error using the training and test
sets respectfully
so here we have our KF equals KF k-fold
here's our splits on the top
and then we need to go ahead and have
our list training error we're going to
create an array for that we're going to
list our testing error
and for train index and test index in k
f dot splitx X train y uh X train and X
test
we're going to go ahead and split up our
our data our X values and the same thing
with the Y values so now we have an X
train and X test a y train and a y test
and then here's our model our
MLP regressor
not your linear regressor model in there
and we have used a multi-layer
precipitron MLP so this is a neural
network a multi-layer precipitron that's
what the MLP is for regressor means that
it is dealing with numbers we're not
categorizing things
um
and then let's go ahead uh kind of went
off the screen here we'll just go ahead
and bring that down it's a class of feed
forward artificial neural networks and
they kind of loosely call it a n n don't
get caught up in the a n n n
there's NN is neural network and then
everybody puts their own flavor on it
depending on what they're doing uh so if
you see the NN you know you're dealing
with a neural network
so we go ahead and fit our data here's
our model.fit we have X train and Y
train
and the Y train data we're going to
predict equals model dot predict X train
so here's our prediction of what it's
going to be so we've trained it and
we've predicted it we've traded our the
train data and then we have our y train
and then we have our y test
and the Y test equals a model predict X
test
now notice what we did here is we're
going to use our model to predict what
we think y should be but this is the
training set so we've trained it with
this data and now we want to see how
good our model fits our training data
and then we want to see how well it fits
our testing data
so we take our fold training error mean
absolute error y train
y train data predict and we're going to
do our full testing error the mean
absolute error of Y test and why test
data predict
and we do this and here's our mean
absolute error there's our a little bit
different connotation but that's that's
taking the Square value and finding the
in this case it's using the absolute
value so instead of the Square value we
get rid of the minus and pluses by using
an absolute value and we find the
average of that and that works the same
way as doing the squared value
then we take our list training error and
we're going to just append it for each
each one of these runs we go through so
every time we fold the data
think of it like this uh we want to go
ahead and take a piece of data that's
going to be one piece of the data and
we're going to look at
each section and we want to go through
each section to see how well it does and
splits it up this way we have a nice
picture when we're looking at it from a
distance I do this a lot when I do X and
when I split my X train and my y train
I'll take two-thirds of the data and
then one third of the data and then I'll
switch it and I'll do three different
models so I can really see how well it
tests out and how that averages out
this is the same thing but with the with
the k-fold we're doing and we're doing
it across 20 sections
we'll go ahead and run this
and we run this it's not too exciting
because we're just loading up the data
and appending it into our list
and so we want to take with this uh is
we're going to go ahead and plot it and
this is where we can really see what's
going on this is where it gets exciting
uh so we take it we're going to create a
couple subplots that way we have a nice
setup down here we're splitting it up
into a couple different graphs
and let's go ahead and run this and then
we'll walk through it a little bit so
our subplot comes in
um
there's our subplot and then our PLT
plot we're going to do in there range
one we're going to ahead and do the
splits plus one in pra list training
error uh Ravel
this is of course just a code to how we
properly set it up on there so that it
sees it correctly
and then we have our X label which is
our number fold plot the Y label
training error plot the title training
error across Folds
plot the tight layout plot the subplot
so we're going to move on to this is one
two one one two two there's our one and
just one and two
it has to do with how it how it layers
it on there for doing multiple plots
because you can do all kinds of cool
things with our plot our PI plot Library
uh and again we're going to go ahead and
do the same thing for the error and we
end up with our training error cross
folds and our testing error across Folds
and so you can see these different folds
how they kind of Spike and how they look
and so we're talking about overfitting
or under fitting we're comparing these
two graphs
and if one of them
is more off than the other one
if you're looking at these two graphs
you got to say hey is this one over fit
or under fit and this is always a good
question to ask I mean what do we got
going here is that over fit or is that
under fit and I would say based on these
two graphs and the training data the
training data is more sporadic than the
testing data
so I would look at this and say hey this
might need to be fit a little bit better
maybe we don't have enough data with the
iris we probably don't
something else is going on here so it's
a little underfit maybe a different
model would fit better I would not use a
neural network model for this I would
actually use just a basic linear linear
model on this
a lot of different choices but this
gives you an idea what we're looking at
is how chaotic are these two is it
getting better or is it getting worse
if at some point the training data gets
so much better than the testing data you
know you've overfit it and that's where
you start running into the overfitting
this to me looks like it's under fit so
that concludes under fitting and
overfitting if you are an aspiring
machine learning engineer looking for
online training and certifications from
prestigious universities and in
collaboration with leading experts then
search Nomo simply learns professional
certification program in Ai and machine
learning from Purdue University in
collaboration with IBM should be a right
choice for more details use the link in
the description box below with that in
mind the ideal way to display your
machine learning skill is in the form of
portfolio of data science and machine
learning projects a solid portfolio of
projects will illustrate that you can
utilize those machine learning skills in
your profile as well projects like movie
recommendation system fake news
detection and many more are the best way
to improve your early programming skills
you may have the knowledge but putting
it to the use what is keep you
competitive here are 10 machine learning
projects that can increase your
portfolio and enable you to acquire a
job as a machine learning engineer at
number 10 we have loan approval
prediction system in this machine
learning project we will analyze and
make prediction about the loan approval
process of any person this is a
classification problem in which we must
determine whether or not the loan will
be approved a classification problem is
a predictive modeling problem that
predict a class label for a given
example of input data some
classification problem include spam
email cancer detection sentiment
analysis and many more you can check the
project link from the description box
below to understand classification
problem and how to build a loan
prediction system at number 9 we have
fake news detection system do you
believe in everything you read in social
media isn't it true that not all news is
true but how will you recognize fake
news I'm always the answer you will able
to tell the difference between real and
fake news by practicing this project of
detecting face new this ml projects for
detecting fake news is concerned with
the fake news and the true news on our
data set we create a tfid vectorizer
with SQL the model is then fitted using
a passive aggressive classifier that has
been initialized finally the accuracy
score and the confusion Matrix indicate
how well our model performs the link for
the projects in the description box
below at number 8 we have personality
prediction system the idea is based on
determining an individual personality
using machine learning techniques a
person personality influences both his
personal and professional life nowadays
many companies are shortlisting
applicant based on their personality
which increases job efficiency because
the person is working on what he is good
at rather than what is compelled to do
in our study we are tempted to combine
personality prediction system using
machine learning techniques such as SPD
name based and logistic regression to
predict a personal principality and
talent prediction using phrase frequency
method this model or method allows users
to recognize their personality and
Technical abilities easily to learn
about more this project check the link
in the description box below at number
seven we have Parkinson disease system
Parkinson disease is a progressive
central nervous system element that
affects movement and cause tremors and
stiffness it comprises five stages and
effects more than 1 million worldwide
each other in this machine learning
project we will develop an svm model
using python modules scikit-learn numpy
and pandas and svm we will import the
data extract the features and label and
scale the features split the data set
design and NCS model and calculate the
model accuracy and at the end we will
check the Parkinson's disease for the
individual to learn about more this
project check the link in the
description box below at number six we
have text to speech converter
application the machine learning domain
of audio is undoubtedly cutting as right
now the majority of the application
available today are the commercial the
community is building several audio
specific open source framework and
algorithm other text-to-speech apis are
available for this project we will
utilize pytt sx3 pydt access 3 is a
python text to speech conversion
liability it operates offline unlike
other libraries and is compatible with
python 2 and python 3. before API
various pre-trained models were
accessible in Python but changing the
voice of volume was often difficult it
also needed additional computational
power to learn more about this project
check the link in the description box
below at number 5 we have speech
recognition system speech recognition
often known as speech two types is the
capacity of a machine or programmed to
recognize and transfer word is spoken
allowed into readable text mlsp's
recognition uses algorithm that models
speech in terms of both language and
sound to extract the more important
parts of the speech such as words
sentences and acoustic modeling is used
to identify the finance and the
phonetics on the speech for this project
we will utilize pyts X3 pyts X3 is a
python text to speech conversion Library
it operates offline unlike other
libraries and is compatible with python
2 and python 3. to learn more about this
project check the link in the
description box below at number 4 we
have sentiment analysis sentiment
analysis also known as opinion mining is
a state forward process of determining
the author's feeling about attacks what
was the user intention when he or she
wrote something to determine what could
be personal information we employ a
variety of natural language processing
and text analysis technology we must
detect extract and quantify such
information from the text to enable
classification and data management
station in this project we will use the
Amazon customer review data set for the
sentiment analysis check the link in the
description box below at number 3 we
have image classification using CNN deep
learning is a booming field currently
most projects and problem statement use
deep learning is and any sort of work
many of you like myself would choose a
conventional neural network as a deep
learning technique for answering any
computer vision problem statement in
this project we will use CNN to develop
an image processing project and learn
about its capabilities and why it has
become so popular we will go over each
stage of creating our CNN model and our
first spectacular project we will use
the CFI 10 data set for image
conspiration in this project to learn
more about this project check the link
in the description box below at number 2
we have face recognition system
currently technology absolutely amazes
people with Incredible invention that
makes life easier and more comfortable
face recognition has shown to be the
least intrusive and fastest form of the
biometric verification over time this
project will use opencv and face
recognition libraries to create a phase
detection system opencv provides a
real-time computer vision tool library
and Hardware we can create amazing
real-time projects using opencv to learn
how to create face recognition system
for you check the link in the
description box below and last but not
the least we have movie recommendation
system almost everyone today use
technology to stream movies and
television show while figuring out what
to stream next can be disheartening
recommendations are often made based on
a viewer history and preferences this is
done through a machine learning and can
be a fun and the easy projects for the
beginners new programmers can practice
by coding in either python or R and with
the data from the movie lens dataset
generated by the more than 6000 users to
learn how to create movie recommendation
system for yourself or for your loved
ones check the project in the
description box below
import
on us as speedy
and import
numpy as NP
then
import
c bond
that's SNS
and import
ask you learn
dot model selection
quote
train underscore test underscore split
before that I will import
matplotlib
Dot pipelot
as PLT
okay then
I will write here from sqlarn
Dot Matrix
import
accuracy
or
then from
Escalon dot matrix
import
classification
to report
re then import string
okay
and press enter
so it is saying
okay here I have to write from
everything seems good
loading let's see
okay till that number is a python
Library used for working with arrays
which also has function for working with
the domain of lineal algebra and
matrices
it is an open source project and you can
use it freely
remember I stand for numerical python
pandas so panda is a software Library
written for Python programming language
for data manipulation and Analysis in
particular it offers data structure and
operation for manipulating numerical
tables and Time series
then c bond an open source python
Library based on Metro Link is called C1
it is utilized for data exploration and
data visualization with data frames and
the pandas Library c bond functions with
ease
than metrolip for Python and external
extension numpy Metro type is a cross
platform for the data visualization and
graphical charting package
as a result it presents a strong open
source suitable format lab
the apis for my problem allow
programmers to incorporate graphs into
GUI applications then this training test
split we may build our training data and
the test data with the aid of SQL and
trim test split function
this is so because the original data set
often serves as both the training data
and the test data starting with a single
data set we divide it into two data sets
to obtain the information needed to
create a model like home address
accuracy score the accuracy score is
used to watch the model's Effectiveness
by calculating the ratio of total true
positive to Total to negative across all
the model prediction
expression the function in the model
allow you to determine whether a given
tax rate a given regular occupation or
not which is known as re
okay then string a collection of letters
words or other characters is called a
string it is one of the basic data
structure that serves as the foundation
of manipulating data
the Str class is a built-in string class
in Python because python since are
immutable they cannot be modified after
they have been formed okay so now let's
import the data set we will be going to
import two data set one for the fake
news and one for the True News or you
can say not fake news okay so I will
write here we have underscore
fake question
PD Dot
read underscore CSV
oh what can I say TF fake okay
okay
then
fake dot CSV you can download this data
set from the description box below
and data Dot
true
foreign
so this is defined data okay then
data underscore row
Dot
and this is the two data
okay this is not fake so if you want to
see your top five rows of the particular
data set you can use head
and if you want to see the last five
rows of the data set you can use tail
instead of head
okay
so let me give some space for the better
visual
so now we will insert column class as a
Target feature okay then I will write
here data let's go fake
Plus
equals to zero
and beta underscore true
and
class
question one
okay
I will write here data underscore fake
dot shape and data underscore true
dot ship
okay then press enter
so the shape method return the shape of
an array the shape is a tuple of
integers these number represent the
length of the corresponding area
dimension in other words a tuple
containing the quantities of entries on
each axis is an array shift Dimension so
what's the meaning of shape in the fake
world
in this data set we have two three four
eight one rows and five columns and in
this data set row we have 2 1 4 1 7 rows
and five column okay so these are the
rows column rows column for the
particular data set
so now let's move
and let's remove the last 10 rows for
the manual testing okay then I will add
here data underscore speak
let's go manual
this thing
equals to data underscore fake
dot tail
what it last and those I have to write
here 10.
okay so for I
in range
root 3 4
8 1.
so zero
comma 2 3
four seven zero
comma minus one
and
EF underscore not DF data
underscore fake
dot drop
one
here instead of one I can write here I
comma
this equals to zero
it is
equal to true
then
data
not yet
data underscore
same I will write four I will copy from
here
and I will paste it here and I will make
the particular changes
so here I can write row
where I can write true
then I have to change your number
two one
one six
right
two one four zero six
minus one
same
so press enter
X is equal to zero
relations X maybe you mean double zero
or so of this
okay we will put here double of course
that I'm putting this
take the drop i x is equal to 0 in place
okay
here also write equals to a question
yeah
so
okay access is not defined
so now it's working so
let me see
you know data underscore
Big Dot shape
okay
and data dot true
and data underscore true
dot ship
as you can see
10 rows are deleted from each data set
yeah so I will write here data
underscore fake underscore manual
testing
Plus
equals to zero
and data underscore
true
let's go
manual underscore testing
class equals to
what
just ignore this warning
and let's see
data underscore
fake underscore manual
testing dot head
as you can see we have this and then
data dot sorry I'll just go true
underscore manual
testing
dot at
this is this is the true data set
so here I will merge data let us go
merge
real soon
PD Dot
concat
contact is used for the concatenation
data underscore fake
data underscore
axis
equals to zero
then data underscore merge
dot head
the top 10 rows
yeah
as you can see the data is merged here
first it will come for the fake news and
then with it for the True News
and let's merge true and fake data
frames okay
we did this and
let's merge the column then data dot
merge
Dot columns or let's see the columns
it has not defined but a data underscore
much
these are the column same title tag
subject date class okay
now
let's remove those columns which are not
required for the further process so here
I will write data underscore
or request of data underscore merge
prop
ER title I don't need
that
subject we don't need then
so one
so let's check some null values
because of this
that's good then data
dot is none
so
Center
so no null values okay then
let's do the random shuffling of the
data frames okay
for that we have to write here data
equals to
data dot sample
and
data okay data
Dot
now you can see here the random shifting
is done
and 140
true data reset and zero for the fake
news one okay
then
let me write your data Dot
reset
underscore index
is
because you
through
data dot drop
comma X's equals to 1
then comma in place
equals to true
okay
and let me see columns now data dot
columns
so here we have two columns only rest we
have deleted okay
let me see data Dot it
yeah
everything seems good
let's proceed further and let's create a
function to process the text okay
for that I will write here
but
okay
you can use any name
text
and text equal to
text Dot lower
okay
and text
for the substring
remove these things
from the
data okay
so for that I'm writing here
comma
okay
then text equals to re Dot substring
comma
comma text
okay
then I have to add text equals to
r e Dot substring
www
Dot
S Plus
comma
from our text
okay then text equals to
early Dot substring
uh
comma
yeah
then x equals to
re Dot substring
and
percentage
as
again percentage or r u dot SK function
it has string
our punctuation
comma
then comma then text
right
and text equal to Rd Dot substring
and and
comma
x equals Rd Dot
substring
right here
and again d
and again
then comma
then again
texture
okay then add dnf right here return text
so everything like this these type of
special character will be removed from
the data set okay let's run this let's
see
yeah so here I will write DF sorry not
DF data
data
then
next
muscle
data
dot apply
to the function name what what word opt
okay
the presenter yeah
so now let's define the dependent and
independent variables okay x equals to
data
text
and Y equals U
data
Plus
okay
then splitting training and testing data
okay sorry
so here I will write X underscore train
comma X underscore test
then y underscore train
comma y underscore test was to
train underscore just underscore split
then X comma y
Comma just
let's go size equals to 0.25
okay press enter
so now let's convert x two vectors for
that I have to write here
that is X
so here I will write from Escalon
Dot teacher
extraction
dot text import
d vectorizer
okay
then vectorization
goes to gfid
factorizer
okay then
three underscore
train
equals to
factorization
factorization dot fit
that transform
X underscore trade
okay
then XV underscore test equals to
factorization
dot transform
X underscore test
okay representative
[Music]
so now let's see our first model
logistic regression
so here I will write from
sqlan Dot
linear underscore model
okay import
logistic
creation
then Allah
logistic
regression
and I have to write here LR Dot
wait
then XV Dot
dot dot so dot train comma
test
okay at the center
there's a lot of XP Doctrine
yeah I have to write y chain
and press enter
we walk so here I will write prediction
underscore linear regression
question
l r dot predict
three I'll just go test
okay let's see the accuracy score
for that I have to write LR DOT score
then XV underscore test
comma y underscore test
okay
let's see the accuracy so here as you
can see accuracy is quite good 98
percent
now let's print
the classification mission
port
underscore test comma
prediction of linear regression okay
so this is you can see Precision score
then F1 is code then support value
accuracy
okay
so now we will do this same for the
decision three gradient boosting
classifier random Forest classifier okay
then we will do model testing then we
will predict this code
yeah so now for the decision entry
classification so for that I have to
report from SK learn
Dot 3
import
decision
free
classifier
okay then at the short formal is right
here I can copy it from here
and
okay
then I have to write the same as this so
I will copy it from here
and
yeah
let's change linear regression
classification
okay
then I will write here same
GT
question
GT dot predict
see let's go
test
let's be
the loading is it will take time
okay
till then let me write here for the
accuracy
PT DOT score
we underscore test
comma y
let's wait okay
accuracy so as you can see accuracy is
good than this linear regression okay
logistic regression
yeah so let me
show you
let me predict
print
so this is the accuracy score this is
the all the report
yeah
now let's move for the gradient boosting
classifier okay for that I've read from
sqlan
dot ensemble
quote
gradient
boosting
classifier
let's see first
I will write here GB
equals to let me copy it from here
I will give here random
let's go state
equals to zero
wait wait wait so I will write here g b
Dot fit
see underscore train
comma y underscore train okay then press
enter
here I will write predict underscore GB
also
GB Dot wait sorry
Reddit
three
DOT test
dot dot UNESCO test
till then it's loading so I will write
here uh it's for the score then I'll add
GB DOT score
then three underscore test Java
y underscore test
okay so let's wait it is running this
part
and then let me write for the printing
this
case taking time
taking time still taking time
but if I will run this
it's not coming because of this
yeah it's done now so you can see the
accuracies
not good then
decision tree but yeah it is also good
99
.4 something okay so
now let's check for the last one random
Forest
first I will do
what did random for us we have to write
from sqlan Dot
symbol
import
random
Forest
classifier
okay
and here I will write RF
pursue right I will copy it from here
and
random
state
question
and
RF Dot switch
so we underscore train
comma y underscore
okay then press enter
and predict
underscore
RC
or F
equals to
other thoughts licked
three underscore tests okay
till then I will write here still
loading it will take time so till then I
will write for the score score accuracy
score
x v underscore test comma y underscore
test
okay
then I will write here till then print
classification
port
and Y underscore test
comma
it will take time little bit
so
it runs the accuracy score is 99 it is
also good
so now I will add the code for the model
testing so I will get back to you after
writing the code so
so I have made two functions one for the
output level and one for the manual
testing okay so it will predict the all
the from the all models from the
repeat so it will predict
the the news is fake or not from all the
models okay
so for that let me write
here news
what's your strength
but
okay
then I'll be right here manual
underscore
testing
so
here I will you can add any news from
the you can copy it from the Internet or
whatever from wherever you want so I'm
just copying from the internet
okay probably Google
the news
which is not fake okay I am adding which
is not fake because I already know I
searched on Google so I'm entering this
so just run it let's see what is showing
okay string into object is not callable
okay let me check this first
okay I have to give here Str only
yeah let's check
you have to add here again the script
yeah
manual testing is not defined
let me see manual testing
okay I have to edit something
it is just GB and it is just RF
in GPS is not defined okay okay
so what I have to do
this
this
everything seems sorted
now
as I said to you I just copied this news
from the internet I already know the
news is not fake so it is showing not a
fake news okay so now what I will do I
will copy
one fake news from the internet
and let's see it is detecting it or not
okay
so let me run this
and let me add the news for this
so
all the models are predicting right it
is a fake news or you can add your own
script like this is the fake news okay
I hope you guys understand
so I hope you guys must have understand
how to detect a fake news using machine
learning you can you can copy it any
news from the internet and you can check
it is fake or not okay or if your model
is predicting right or not if you are an
aspiring machine learning engineer
looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then search no more
simple lens professional certification
program in Ai and machine learning from
Purdue University in collaboration with
IBM should be a right choice for more
details use the link in the description
box below with that in mind
opencv open source computer vision
library is an open source computer
vision and machine learning software
Library it is written in C plus plus but
as a binding for various programming
languages such as python Java Matlab
opencv was designed with a goal of
providing a common infrastructure for
computer vision applications and to
accelerate the use of machine learning
perception in commercial product opencv
is widely used in the variety of indices
including robotics automotive and
Healthcare it's supported by a large
community of developers researchers and
users who contribute to its development
and provide supports to its users it is
supported by a large community of
developers researchers and users who
contribute to its development and
provide supports to its users so now
let's see what is object detection
object detection is a computer with a
technology that involves identifying and
localizing the object of interest within
an image or a video it is a challenging
task as it involves not only recognizing
the presence of an object but also
detecting its precise location and size
within the image or video object
detection algorithm typically used deep
learning techniques such as CNN to
analyze the image or video for
identifying the objects these algorithm
can also determine the boundaries of the
object by drawing a bounding box around
them so after understanding what is
object detection now let's move on to
the programming part so this is Kernel
here we will change this name
so here I will write object detection
demo okay
so
the first we will import some major
Library like opencv so for that we will
write import CV2 and the next one is
import mat.ly
dot Pi plot
as PLT
so why we are writing PLT because we
can't write again and again micro.lib
dot Pi plot okay it's a long one so we
can write a short form pld
so yeah
let's I'm just so what is opacity opencv
is an open source software library for
computer vision and machine learning
your opencp full form is open source
computer vision Library it was created
to provide a shared infrastructure for
application for computer vision and to
speed up the use of machine learning
perception in consumer products opencv
has a PSD license software make it
simple for companies to use and change
the code okay so there are some pretty
different packages and leverages that
make our life simple and open series is
one of them again
easy to use and an amazing visualized
library in Python that is built on numpy
array and designed to work with broader
sci-fi stack and consists of several
dots like line bar scatter histogram and
many others okay so moving forward we
will import our file okay so here I will
write config
file equals to this is our file name SSD
underscore
mobile net
for V3
large
Coco
22
.3b
okay
so you can find this file in the
description box below
inference
graph dot PB okay
so let me run it first Mobile land as a
name applied the mobile ad model is
designed to use in mobile application
and its tensorflow first mobile
subdivision model mobile net used depth
wise separable convolutions it
significantly reduces the numbers of
parameter when compared to the network
with the regular convolutions with the
same depth in the rights this is
actually the lightweight of the deep
neural network so mobile net is a class
of CNN that was open source by Google
and therefore this gives us an excellent
starting point for training over
classifiers that are insanely small and
insanely fast okay so what is this large
cocoa this is a data set Coco data set
language applications such as object
detection segmentation and captioning
the cocoa data set is widely understood
by the state of the art of neural
network it's versatility and the
multipurpose scene variation so best to
drain a computer vision model and
Benchmark its performance okay so what
is cocoa the common object in context is
one of the most popular large scale
level images data set available for
public use it represents a handful of
object we encounter on a daily basis and
contains image Innovations in 80
categories I will show you the
categories I have before 1.5 million
object instances okay so modern day AI
different solution are still not capable
of producing absolute accuration result
this comes down to the fact that Coco
data set is a major Benchmark for CV to
train test and polish refine models for
faster scaling of The annotation
Pipeline on the top of that the code
data set is a supplement to transfer
learning where the data used for one
model serves as a starting point for the
another so what is chosen Insurance
graph like freezing is a process to
identify and save all the required
graphs like weights and many others in a
single file that you can usually use a
typical tensorflow model contains
profile and this contains a complete
graph okay so forward let's create one
model here I've got model
and
explosion
and
then config file
so here I am giving the parameters to
parameters like frozen model and config
file
let's go here yeah we run it first okay
there is
cv2.dnn interaction model return is
result
with an exception set the question comes
what is the meaning of detection model
or DNN detection model so this class
represents a high level API for object
detection networks detection model
allows to set parameters for
pre-processing input image detection
model creates net from files with
trained weights and config such a
processing input runs forward pass and
Returns the result detection
okay moving forward let's set the class
levels
class
levels
file name
also labels Dot txt
I will put this file on the description
box below you a number from there
open
file name
pass labels
District
good
so here I created one array of name
class labels so this is the file name
what I'm doing I'm putting this label
file into this class label okay so here
if I will print
class labels
so these are the 80 categories in the
cocoa data set
okay this person bicycle car motorbike
airplane bus drain these all are theater
categories
I will put this file label.txt in the
description box below you can download
from them okay fine
so let's print the length of the cocoa
data set or you can see class labels
this is 80 as you can see I have already
told you
the lens will be 80. so here let's
set this some model input size scaling
mean and all right here model Dot
set good
guys
320
dot set
input
scale
1.0
slash 127
.5
okay I will explain you though sorry
model dot set
input
mean
127.5
from my 127.5
comma 127.5
okay and then model dot set
with
what is set input size
okay
so set input size is a size of new frame
the shape of the new blob Less Than Zero
okay so this is the size of the new
frame the second one is set input scale
the set input scale is a scale factor of
the value for the frame or you can say
the perimeter will be the multiplier of
the frame values or you can say multiply
4 different values okay so at input V so
it set the mean value for the frame the
frame in which the photo will come the
video will come or my webcam will come
so it sets the mean value for the frame
so the four parameters mean scalar with
the mean values which are subtracted
from the channels you can see and the
last one is that input swap RV so let's
set the flag so power before the every
frame we don't have to put every time a
single frame for a particular image it
will be set the two for the all the
images okay so parameters will be sure
if I'll be flagged which indicates the
swipe first and the last sentence so
moving forward we will forward one image
okay
I am read
by dot jpg
D dot
I am sure
so this is the size of 320 by 320 okay
so first thing is
you can download this the random picture
from the Google I took from Google
itself so now what we will do we will
set the class index
the confidence value
box is the boundary box which I will
create for that particular person's
cycle motorbike
okay equals to model
that
confidence threshold
threshold is used for
if my model will confirm its the
particular image which is the texting is
correct it will print the name okay
so let me print
and
Plus
cos index is coming one two three four
okay so one means person
two means bicycle
three means car and four means motorbike
this is the class index and therefore
particular level I will do I will print
the boxes
font
here
equals to 3 and the font equals to CV2
Dot
want Hershey
again
one
class
index and
confidence that the boxes
next Dot
confidence
box the boundary box
okay
and I will write here CB2 dot rectangle
to make the rectangle
so that means
and obviously it's
knife from a zero comma zero this is the
color of the box and this will be the
thickness
okay
and I will write CB2 dot fourth
text
image
class labels
I will write class index minus 1
because all this index start is 0 that's
why now the box is
0
and
boxes
and
body
okay
I want
comma
scale
assume font
scale
okay
hello
question
this will be the text column 0 from 255
comma zero
and the thickness
three let me run it
okay now PLT dot I am sure
then CV2 dot CVT
color
then
our series T2
dot color
and BGR
G
that is why we wrote
swap RB equals to true because every
time we will convert PCR to brg
GB
RGB so we don't have to write again and
again it will count out all the files
into RGB okay
okay as you can see the motorbike is
coming bicycle is coming the person is
coming the car will car is coming okay
so it's detecting the right
for the image now we will do this for
the video and for the webcam
we are done with this image one and then
now I will write here
okay
so this is we'll do for the video
for the video I will write here cap
equal to capture you can write any name
so CV2 Dot
video
capture
so you can take any random video I took
this pixels
George
you can comment down
share the link
F dot is open
so here I will add cap
equals to
be two
so in CV2
start video
capture
Leo
and if not
cap dot is open
then
and this is
output
hello
and
open the video
and open the video
here everything will be the same font
scale
equals to 3
okay if you want equal to CV2 Dot
Dot
so here I will write y true
from a frame
foreign
[Music]
confidence
from a boundary box
for Zoom model
dot detect
game
and the confidence
threshold
equals to 0.55
okay everything is the same we did
before so I will print
class
index
okay so here I will write if
and
the class index
does not equals to zero
then what to perform is here after X4
plus index
comma confidence
comma boxes
in zip
uh
x dot flattened
flatten is a layers okay
quantities
flatten
ebox
and if
class
index is greater than equals to A D
and
what to do
the copy from here
okay the same thing I have to write here
so here I will write CV2 Dot
I am sure
this will be the return in the frame
object detection
by singular
and frame
so if CB2 Dot weight key
to
and 0
the FFX
first two
Ord
Q okay then
right link
will you break when
will be 2. okay I will tell you what is
the weight key
so here I will write cap dot release
and CV2 Dot
destroy
or Windows
okay
so now let me run
it's here a little bit better okay
modules
let me run it again
okay
video is here
the video is here as you can see see
bicycle the person the person the bus
car traffic light the person person so
it our object detection for the video is
coming right okay person okay person
traffic light Plus
so this is how you can do for the video
okay
so now let's we will do for the webcam
with live
so this is for the video so
if
we want to do for the webcam we
okay so we need to just change
one one thing only we have to change
instead of giving the file we have to
write one here okay the rest will be the
same
got it so I have to just shut down my
webcam
so let me shut down the webcam and get
back to you
so as you can see
this is a 320 by 320 box so
so this is coming right okay so if I
will show this the mobile phone is
coming right now okay so this is how you
can do the correct project detection if
you are an aspiring machine learning
engineer looking for online training and
certifications from prestigious
universities and in collaboration with
leading experts then such no more
supplements professional certification
program in Ai and machine learning from
Purdue University in collaboration with
IBM should be a right choice for more
details use the link in the description
box below with that in mind so first we
will open command prompt to write a
command to open Jupiter notebook so here
we will write Jupiter
notebook
then press enter so it will take some
time yeah
so this is the landing page of Jupiter
notebook and here we have to name that
python 3.
so this is how the Jupiter notebook UI
look likes so at first we will import
some major libraries of python which
help in creating a mass detection system
so here I will write Imports
CV2 comma OS
and press enter then I will give the
path the data path to assume
see
and Slash
users
then slash
SLP
zero nine three seven five
and Slash
desktop
then slash and face
mask
detection
then slash
data set
okay
so here it will be slash my bad sorry
and Slash
okay so fine
so here I would like categories
equals to OS Dot
list
directory is dir
then I will assign data underscore path
then I will create some labels
equals to
here I will write
and write here I
in range
then here I will give length
of categories
yeah
then press enter Then here I will write
label
then text directory to assume
directory then
zip then categories
comma labels
so here I will write print
then label directory
okay then I will print
categories
then I will print labels
okay then press enter
yeah
so here CD means capturing video the cp2
function in opencv can read video video
capture by using pass zero as a function
parameter we can access our webcam we
may pass rtsp URL in the function
parameter to capture CCTV footage which
is quite helpful for video analysis then
OS this this OS module the OS module in
Python has function for adding and
deleting folders retrieving their
contents changing the directory locating
the current directory and more before
you can communicate with the underlying
operation system you must import the OS
module
and this OS list directory
to retrieve a list of all files and
folders in the given directory use
Python OS dot list directory method the
list of files or director in the current
working directory will be returned if no
directory is specified
then label the the ticket into visit
class called a label is used to show
text or an image
the user views the label they cannot be
indirect with it so moving forward I
will write code so just stay with me
after that I will explain you likewise
okay
so here
I will write
that
image underscore size
pursue 100.
then I will create two classes data
then dot it
okay
at least
then I will write here for category
in categories
folder
underscore
but
equals to OS dot path
then dot join
then data
underscore path
comma yeah
okay then IMG
image names equals to OS dot list
directory then folder path
just stay with me I will explain you
line wise
okay so here I will write for
IMG name
in IMG
names
IMG underscore path
equals to OS Dot Plot
Dot
join
then I will write here folder
I repeat
I'll repeat so here you write folder
underscore path
then IMG underscore name
so IMG equals to CV2
dot I'll eat
then IMG underscore path
so here I will try
that gray equals to cd2 dot CVT
and capital c color
then IMG comma CV2
Dot
then color
then underscore B gr to rgd okay
so I will go for the gray one
pgr
yeah
so I will write here this size okay not
in capital letter G size
equals to CV2
dot resize
then three
comma IMG
size
drama I understand
why these two I am Jesus because like
length and like width and breath will be
shame
like hundred so in the center
so I will write Theta Dot
append
then resized
then Target
dot append
then label
then directory
of category
okay
so
I will write here except
exception
as e
so here I will print
and
exception
comma e
okay then press enter let's see there
should be no error
so this is loading let's wait
okay no I don't
so image size should be like 100 by 100
so that is why I wrote here 100 the
enemy size 100 by 100. and I made two
arrays like for one for data then one
for Target
so
this for this gray also cb2.70 color
image so converting the image into
grayscale okay then this line resize
equals to CO2 dot resize gray IMG so
resizing gray scale into like 100 by 100
since we need a fixed common size for
all the images in the data set okay
then they started a DOT append labeled
electrical so appending the image like
and the label categories into the list
list what is list data set
so like for the last print exception e
so if any exception raised the exception
will be printed here and the pass to the
next image okay
so moving forward let's import numpy and
save this data and the target okay so I
will write here
import
right
then I will write here data
equals to NP dot array
then again data
by 255
.0
okay
then again data equals to NP Dot
reshape
then data
comma
data
dot shift
then 0
comma empty size
I don't know mg size
comma one
okay and press enter Then for Target
let me put those things like this then
Target equals to NV dot array
then start it
okay
okay
so here I will write from
Eros Dot
details import
and putas
okay so here I will write new underscore
Target
equals to NP underscore utils
dot two
underscore categories
categorical I guess categorical
then Target
okay
then press enter
so here I will save empty dot save
this data
comma data
okay and NP dot safe
this target
okay
so press enter
here
it also has a function for working with
in the domain of linear algebra and
matrices it is an open source project
and you can use it freely numpy stand
for numerical Python and this Keras a
python interface for artificial neutral
network is provided by the open source
software package known as Keras the
tensorflow library interface is provided
by the Keras a number of backends were
supported by KRS up until version 2.3
including tensorflow Microsoft cognitive
so this is the part of the data
processing this all this part of the
data processing so let me write here
data
pre-processing three processor
so the data preprocessing part is done
now we will create the another file of
this python for the training CNN CNN
means convocational neural network okay
that's data
comma
train underscore Target
comma test underscore sounds good
because to train
underscore split
then we will split on data and Target
comma yes I should be
0.1
okay so now I will give a presenter of
the train script drain script is not
defined
okay
it is change
test strip
yeah
so now I will write checkpoint
equals to model
checkpoint
then again
model
light model
comma
epoch
than 0 3D
dot model
okay
so here I have to write comma monitor
equals to Value loss
underscore
loss
comma
bubbles equals to zero comma save
best
only
consume
through drama mode
a consumer should be Auto
okay
then press enter
then here I will write history
equals to model Dot
with
to train
underscore Theta
comma
okay chain
underscore Target
comma
apoc is just equals to 20
comma called X
call x equals to
foreign
comma validation
validation underscore split
because so 0.2 the best ratio okay so
let me press enter okay there is one
error model dot print code then X
okay this should be a spelling mistake
apoch
pochs
220
.
dot model it is fine
then monitor
because the value loss
equals to zero
save best only positive truth and more
Okay so
here I have to write
maybe it will so it will take some time
to go till 20 okay so it will download
one by one so we will wait for a while
so the model checkpoints are completed
so here I will write print
then model
dot evaluate
okay
here I will write test underscore data
comma test
underscore Target
and press enter
okay so I hope you guys understand it
here if you have any question or any
query regarding any code just put us in
comments our team will shortly provide
you the correct solution okay so moving
forward we will do we will create
another file for detecting mask okay so
I will go here and then
new file
python
so I will write here detecting
all right okay
yeah so I will import some libraries
here here as dot models
import load
models
we will import
CV2
and we will import
numpy
as NP
so press enter
okay number five
okay so press enter so what I will do I
will write here model equals to load
underscore model
then
okay
Keras model from chaos cannot import
name
Okay so
it is model only yeah
so here I will add model equal to load
model
so I will write a model
017
dot model
okay
okay
so here I will write face underscore
classifier
equals to CV2 Dot Cascade
Cascade classifier
so this is one file for frontal phase
default so you can find this file in the
description box below so I will write
here
our Cascade
underscore
content
phase
minus 44
dot XML
okay
so source
equals to
CV2
Dot
video
capture
zero it will open our camera
so labels underscore directory
question
0
must
and one for
no mask
okay
so colors
directory because you
like no mask for red and mask for green
okay so here at zero
then 0 comma 2
55 comma 0
then again comma equals one there should
be 0 comma zero comma 251
okay
so I hope you guys understand it here so
if you have any question or any query
regarding any code so just put as in
comments our team will surely provide
you the correct solution
so
the code is written
so let me run this for this first I have
to like
so first I have to close my this person
on a screen camera
so the code is any fine
now it is showing no Mass
now it is showing off
so I hope you guys understand if you
have any queries any push hey there
learner simply learn brings you a
comprehensive professional certificate
program in Ai and machine learning from
Purdue University that is in
collaboration with the IBM that will
cover a wide range of topics that will
Empower you with the knowledge and
skills needed to excel in the field of
AI to learn more about this course you
can find the course link mentioned in
the description box we'll talk about
interview questions
for machine learning now this video will
probably help you when you're attending
interviews or machine learning positions
and the attempt here is to probably
consolidate 30 most commonly asked
questions and to help you in answering
these questions we tried our best to
give you the best possible answers but
of course what is more important here is
rather than the theoretical knowledge
you need to kind of add to the answers
or supplement your answers with your own
experience so the responses that we put
here are a bit more generic in nature so
that if there are some Concepts that you
are not clear this video will help you
in kind of getting those Concepts
cleared up as well but what is more
important is that you need to supplement
these responses with your own practical
experience okay so with that let let's
get started so one of the first
questions that you may face is what are
the different types of machine learning
now what is the best way to respond to
this there are three types of machine
learning you read any material you will
always be told there are three types of
machine learning but what is important
is you would probably be better of
emphasizing that there are actually two
main types of emotional learning which
is supervised and unsupervised and then
there is a third type which is
reinforcement learn so supervised
learning is where you have some
historical data and then you feed that
data to your model to learn now you need
to be aware of a keyword that they will
be looking for which is labeled data
right so if you just say pass data or
historical data the impact may not be so
much you need to emphasize on labeled
data so what is labeled data basically
let's say if you are trying to do train
your model for classification you need
to be aware of for your existing data
which class each of the observations
belonged right so that is what is
labeling so it is nothing but a fancy
name you must be already aware but just
make it a point to throw in that keyword
labeled so that will have the right
impact
so that is what is supervised learning
when you have existing labeled data
which you then use to train your model
that is known as supervised learning and
unsupervised learning is when you don't
have this labeled data so you have data
it is not labeled so the system has to
figure out a way to do some analysis on
this okay so that is unsupervised
learning and you can then add a few
things like what are the ways of
performing a supervised learning and
unsupervised learning or what are some
of the techniques so supervised learning
we we perform or we do regression and
classification and unsupervised learning
video clustering and clustering can be
of different types similarly regression
can be of different types but you don't
have to probably elaborate so much if
they are asking for just the different
types you can just mention these and
just at a very high level but if they
want you to elaborate give examples then
of course I think there is a different
question
then the third so we have supervised and
we have unsupervised and then
reinforcement you need to provide a
little bit of information around as well
because it is sometimes a little
difficult to come up with a good
definition for reinforcement
so you may have to little bit elaborate
on how reinforcement learning
right so reinforcement learning works in
in such a way that it basically has two
parts to it one is the agent and the
environment and the agent basically is
working inside of this environment and
it is given a Target that it has to
achieve and every time it is moving in
the direction of the target so the agent
basically has to take has some action
and every time it takes an action which
is moving uh the agent towards the
Target right towards a goal a Target is
nothing but a goal then it is rewarded
and every time it is going in a
direction where it is away from the goal
then it is punished so that is the way
you can little bit explain and this is
used primarily or very very impactful or
teaching the system to learn games and
so on examples of this are basically
used in alphago you can throw that as an
example where alphaco used reinforcement
learning to actually learn to play the
game of Go and finally it defeated the
go world champion all right this much of
information that would be good enough
then there could be a question on
overfitting uh so the question could be
what is overfitting and how can you
avoid it so what is overfitting so let's
first try to understand the concept
because sometimes overfitting Maybe
overfitting is a situation where the
model has kind of memorized the data so
this is an equivalent of memorizing the
data so we can draw an analogy so that
it becomes
it becomes plainness now let's say
you're teaching a child about
recognizing some fruits or something
like that and you're teaching this child
about recognizing let's say three fruits
apples oranges and pineapples okay so
this is a small child and for the first
time you're teaching the child to
recognize fruits then so what will
happen so this is very much like that is
your training data set so what you will
do is you will take a basket of fruits
which consists of apples oranges and
pineapples
and you take this basket to this child
and there may be let's say hundreds of
these fruits so you take this basket to
this child and keep showing each of this
fruit and then first time obviously the
child will not know what it is so you
show an apple and you say hey this is
Apple then you show maybe an orange and
say this is orange and so on and so and
then again you keep repeating that right
so till that basket is over this is
basically how training work in machine
learning also that's how training works
so till the basket is completed maybe
100 fruits you keep showing this child
and then the process what has happened
the child has pretty much memorized
these so even before you finish that
basket right by the time you are halfway
through the child has learned about
recognizing the Apple orange and
pineapple now what will happen after
halfway through initially you remember
it made mistakes in recognizing but
halfway through now it has learned so
every time you show a fruit it will
exactly 100 accurately it will identify
it will say the child will say this is
an apple this is an orange and if you
show a pineapple it will say this is a
pineapple Apple so that means it has
kind of memorized this data now let's
say you bring another basket of fruits
and it will have a mix of maybe apples
which were already there in the previous
set but it will also have in addition to
Apple it will probably have a banana or
maybe another fruit like a jackfruit
right so this is an equivalent of your
test data set which the child has not
seen before some parts of it it probably
has seen like the apples it has seen but
this banana and Jackfruit it has not
seen so then what will happen in the
first round which is an equivalent of
your training data set towards the end
it has 100 percent it was telling you
what the fruits are right Apple was
accurately recognized orange or I was
accurately recognized and pineapples
were accurately recognized right so that
is like a hundred percent accuracy but
now when you get another a fresh set
which were not a part of the original
one what will happen all the apples
maybe it will be able to recognize
correctly but all the others like the
Jackfruit or the banana will not be
recognized by the child right so this is
an analogy this is an equivalent of
overfitting so what has happened during
the training process it is able to
recognize or reach 100 accuracy maybe
very high accuracy okay and we call that
as very low loss right so that is the
technical term so the loss is pretty
much zero and accuracy is pretty much
hundred percent whereas when you use
testing there will be a huge error which
means the loss will be pretty high and
therefore the accuracy will be also low
okay this is known as overfitting this
is basically a process where training is
done training processes it goes very
well almost reaching 100 accuracy but
while testing it really drops down now
how can you avoid it so that is the
extension of this question there are
multiple ways of avoiding overfitting
there are techniques like what you call
regularization that is the most common
technique that is used for avoiding
overfitting and within regularization
there can be a few other subtypes like
drop out in case of neural networks and
a few other examples but I think if you
give example or if you give
regularization as the technique probably
that should be
easy so so there will be some questions
where the interviewer will try to test
your fundamentals and your knowledge and
depth of knowledge and so on and so
forth and then there will be some
questions which are more like quick
questions that will be more to stop you
okay then the next question is around
the methodology so when we are
performing machine learning training we
split the data into training and test
right so this question is around that so
the question is what is training set and
test set machine learning model and how
is the split done that the question can
be
so in machine learning when we are
trying to train the model so we have a
three-step process we train the model
and then we test the model and then once
we are satisfied with the test only then
we deploy the model so what happens in
the train and test is that you remember
the labeled data so let's say you have
thousand records with labeling
information now one way of doing it is
you use all the Thousand records for
training and then maybe right which
means that you have exposed all this
thousand records during the training
process and then you take a small set of
the same data and then you say okay I
will test it with this okay and then you
probably what will happen you may get
some good results right it but there is
a flaw there what is the flaw this is
very similar to human beings it is like
you are showing this model the entire
data as a part of training okay so
obviously it has become familiar with
the entire data so when you're taking a
part of that again and you're saying
that I want to test it obviously you
will get good results so that is not a
very accurate way of testing so that is
the reason what we do is we have the
label data of this thousand records or
whatever we set aside before starting
the training process we set aside a
portion of that data and we call that
test set and the remaining we call as
training set and we use only this for
training our model now the training
process remember is not just about
passing one round of this data set so
let's say now your training set has 800
records it is not just one time you pass
this 800 records what you normally do is
you actually as a part of the training
you may ask this data through the model
multiple times so this thousand records
may go through the model maybe 10 15 20
times still the training is perfect till
the accuracy is high till the errors are
minimized okay now so which is fine
which means is that here that is what is
known as the model has seen your data
and gets familiar with your data and now
when you bring your test data what will
happen is this is like some new data
because that is where the real test is
now you have trained the model and now
you are testing the model with some data
which is kind of new that is like a
situation like a realistic situation
because when the model is deployed that
is what will happen it will receive some
new data not the data that it has
already seen right so this is a
realistic test so you put some new data
so this data which you have set aside is
for the model it is new and if it is
able to accurately predict the values
that means your training has worked okay
the model got trained properly but let's
say while you are testing this with this
test data you're getting lot of errors
that means you need to probably either
change your model or retrain with more
data and takes
now coming back to the question of how
do you split this what should be the
ratio there is no fixed number again
this is like individual preferences some
people split it into 50 50 test and 50
training Some people prefer to have a
larger amount for training and a smaller
amount for test so they can go by either
60 40 or 70 30 or some people even go
with some odd numbers like 65 35 or
63.33 and 33 which is like one third and
two-thirds so there is no fixed rule
that it has to be something that issue
has to be this you can go by your
individual preferences all right then
you may have questions around data
handling data manipulation or
data management preparation so these are
all some questions around that area
there is again no one answer one single
good answer to this it really varies
from situation to situation and
depending on what exactly is the problem
what kind of data it is how critical it
is what kind of data is missing and what
is the type of corruption so there are a
whole lot of things this is a very
generic question and therefore you need
to be little careful about responding to
this as well so probably have to
illustrate this again if you have
experience in doing this kind of work in
handling data you can illustrate with
examples saying that I was on one
project where I received this kind of
data these were the columns where data
was not filled or these were this many
rows where the data was missing that
would be in fact a perfect way to
respond to this question but if you
don't have that obviously you have to
provide some good answer I think it
really depends on what exactly the
situation is and there are multiple ways
of handling the missing data or corrupt
data now let's take a few examples now
let's say you have data where some
values in some of the columns are
missing and you have pretty much half of
your data having these missing values in
terms of number of rows okay that could
be one situation another situation could
be that you have records or data missing
but when you do some initial calculation
how many records are corrupt or how many
rows or observations as we call it has
this missing data let's assume it is
very minimal like a 10 percent okay now
between these two cases how do we so
let's assume that this is not a mission
critical situation and in order to fix
this 10 percent of the data the effort
that is required is much higher and
obviously effort means also time and
money right so it is not so Mission
critical and it is okay to let's say get
rid of these records so obviously one of
the easiest ways of handling the data
part or missing data is remove those
records or remove those observations
from your analysis so that is the
easiest way to do but then the downside
is as I said in as in the first case if
let's say percent of your data is like
that because some column or the other is
missing so it is not like every in every
place in every Row the same column is
missing but you have in maybe 10 percent
of the records column one is missing and
another ten percent column two is
missing another ten percent column three
is missing and so on and so forth so it
adds up to maybe half of your data set
so you cannot completely remove half of
your data set then the whole purpose is
lost okay so then how do you handle then
you need to come up with ways of filling
up this data with some meaningful value
right that is one way of handling so
when we say meaningful value what is
that meaningful value let's say for a
particular column you might want to take
a mean value for that column and fill
wherever the data is missing fill up
with that mean value so that when you're
doing the calculations your analysis is
not completely way off so you have
values which are not missing first of
all so your system will work number two
these values are not so completely out
of whack that your whole analysis goes
for
right there may be situations where if
the missing values instead of putting
mean may be a good idea to fill it up
with the minimum value or with a zero so
or with a maximum value again as I said
there are so many possibilities so there
is no like one correct answer for this
you need to basically talk around this
and illustrate with your experience as I
said that would be the best otherwise
this is how you need to handle this
okay so then the next question can be
how can you choose a classifier based on
a training set data size so again this
is one of those questions uh where you
probably do not have like a
one-size-fits-all answer first of all
you may not let's say decide your
classifier based on the training set
size maybe not the best way to decide
the type of the classifier and even if
you have to there are probably some
thumb rules which we can use but then
again every time so in my opinion the
best way to respond to this question is
you need to try out few classifiers
irrespective of the size of the data and
you need to then decide on your
particular situation which of these
classifiers are the right ones this is a
very generic issue so you will never be
able to just buy if somebody defines a
problem to you and somebody even if they
show the data to you or tell you what is
the data or even the size of the data I
don't think there is a way to really say
that yes this is the classifier that
will work here no that's not the right
way so you need to still you know test
it out get the data try out a couple of
classifiers and then only you will be in
a position to decide which classifier to
use you try out multiple classifiers see
which one gives the best accuracy and
only then you can decide then you can
have a question around confusion Matrix
so the question can be explained
confusing Matrix
Matrix so confusion Matrix I think the
best way to explain it is by taking an
example and drawing like a small diagram
otherwise it can really become tricky so
my suggestion is to take a piece of pen
and paper and explain it by drawing a
small Matrix and confusion Matrix is
about to find out this is used
especially in classification learning
process and when you get the results
when the our model predicts the results
you compare it with the actual value and
try to find out what is the accuracy
okay so in this case let's say this is
an example of a confusion Matrix and it
is a binary Matrix so you have the
actual values which is the labeled data
right and which is so you have how many
yes and how many know so you have that
information and you have the predicted
values how many yes and how many you
know right so the total actual values
the total yes is 12 plus 130 and they
are shown here and the actual value
knows are 9 plus 3 12 okay so that is
what this information here is so this is
about the actual and this is about the
predicted similarly the predicted values
there are yes are 12 plus 3 15 yeses and
no are 1 plus 9 10 nodes okay so this is
the way to look at this confusion Matrix
okay and uh out of this what is the
meaning convey so there are two or three
things that needs to be explained out
right the first thing is for a model to
be accurate the values across the
diagonal should be high like in this
case right that is one number two the
total sum of these values is equal to
the total observations in the test data
set so in this case for example you have
12 plus 3 15 plus 10 25 so that means we
have 25 observations in our test data
set okay so these are the two things you
need to First explain that the total sum
in this Matrix the numbers is equal to
the size of the test data set and the
diagonal values indicate the accuracy so
by just by looking at it you can
probably have a idea about is this an
accurate model is the model being
accurate if they're all spread out
equally in all these four boxes that
means probably the accuracy is not very
good okay now how do you calculate the
accuracy itself right how do you
calculate the accuracy itself so it is a
very simple mathematical calculation you
take some of the diagonals right so in
this case it is 9 plus 12 21 and divide
it by the total so in this case what
will it be let me take a pen so your
your diagonal values is equal if I say t
is equal to 12 plus 9 so that is 21
right and the total data set is equal to
right we just calculated it is 25 so
what is your accuracy it is 21 by your
accuracy is equal to 21 by 25 and this
turns out to be about 85 percent right
so this is 85 percent so that is our
accuracy okay so this is the way you
need to explain raw diagram Give an
example and maybe it may be a good idea
to be prepared with an example so that
it becomes easy for you you don't have
to calculate those numbers on the fly
right so a couple of hints are that you
take some numbers which are with which
add up to 100 that is always a good idea
so you don't have to really do this
complex calculations so the total value
will be 100 and then diagonal values you
divide once you find the diagonal values
that is equal to your percentage okay
all right so the next question can be a
related question about false positive
and false negative so what is false
positive and what is false negative now
once again the best way to explain this
is using a piece of paper and then
otherwise it will be pretty difficult to
twice so we use the same example of the
confusion Matrix
and we can explain that so A confusion
Matrix looks somewhat like this and when
we just
come out like this and we continue with
the previous example where this is the
actual value this is the predicted value
and in the actual value we have 12 plus
1 13 yeses and three plus nine twelve
nose and the predicted values there are
12 plus the 15 years and one plus nine
ten loss okay now this particular case
which is the false positive what is a
false positive first of all the second
word which is positive okay is referring
to the predicted value so that means the
system has predicted it as a positive
but the real value so this is what the
false comes from but the real value is
not positive okay that is the way you
should understand this term false
positive or even false negative so false
positive so positive is what your system
has predicted so where is that system
predicted this is the one positive is
what yes so you basically consider this
row okay now if you consider this row so
this is this is all positive values this
entire row is positive values okay now
the false positive is the one which
where the value actual value is negative
predicted value is positive but the
actual value is negative so this is a
false positive right and here is a true
positive so the predicted value is
positive and the actual value is also
positive okay I hope this is making
sense now let's take a look at what is
false negative false negative so
negative is the second term that means
that is the predicted value that we need
to look for so which are the predicted
negative values this row corresponds to
predicted negative values all right so
this row corresponds to predicted
negative values and what they are asking
for false so this is the row for
predicted negative values and the actual
value is this one right this is
predicted negative and the actual value
is also negative therefore this is a
true negative so the false negative is
this one predicted is negative but
actual is positive right so this is the
false negative so this is the way to
explain and this is the way to look at
false positive and false negative same
way there can be true positive and true
negative as well so again positive the
second term you will need to use to
identify the predicted row right so if
we say true positive positive we need to
take for the predicted part so predicted
positive is here okay and then the first
term is for the actual so true positive
so true in case of actual is yes right
so true positive is this one okay and
then in case of actual the negative now
we are talking about let's say true
negative true negative negative is this
one and the true comes from here so this
is true negative right 9 is true
negative the actual value is also
negative and the predicted value is also
negative okay so that is the way you
need to explain this the terms false
positive false negative and true
positive true negative then uh you might
have a question like what are the steps
involved in the machine learning process
or what are the three steps in the
process of developing a machine learning
model right so it is around the
methodology that is applied so basically
the way a you can probably answer in
your own words but the way the model
development of the machine learning
model
first of all you try to understand the
problem and try to figure out whether it
is a classification problem or a
regression problem based on that you
select a few algorithms and then you
start the process of training these
models
so you can either do that or you can
after due diligence you can probably
decide that there is one particular
algorithm that which is more suitable
usually it happens through trial and
error process but at some point you will
decide that okay this is the model we
are going to use
so in that case we have the model
algorithm and the model decided and then
you need to do the process of training
the model and testing the model and this
is where if it is supervised learning
you split your data the label data into
training data set and test data set and
you use the training data set to train
your model and then you use the test
data set to check the accuracy whether
it is working fine or not so you test
the model before you actually put it
into production right so once you test
the model you're satisfied it's working
fine then you go to the next level which
is putting it for production and then in
production obviously new data will come
and
so the model is readily available and
only thing that happens is new data
comes and the model redicts the values
you know so this can be an iterative
process so it is not a straightforward
process where you do the training
through the testing and then you move it
to production now so during the training
and test process there may be a
situation where because of either
overfitting or things like that the test
doesn't go through which means that you
need to put that back into the training
process so that can be a an iterative
process not only that even if the
training and test goes through properly
and you deploy the model in production
there can be a situation that the data
that actually comes the real data that
comes with that this model is failing so
in which case you may have to once again
go back to the drawing board or
initially it will be working fine but
over a period of time maybe due to the
change in the nature of the data once
again the accuracy will let it rearrate
so that is again a recursive process so
once in a while you need to keep
checking whether the model is working
fine or not and if required you need to
tweak it and modify it and so on
so let net this is a continuous process
of tweaking the model and testing it and
making sure it is up to date then you
might have question around deep learning
so because deep learning is now
associated with AI artificial
intelligence and so on so it can be as
simple as what is deep learning so I
think the best way to respond to this
could be deep learning is a part of
machine learning and then obviously the
question would be then what is the
difference right so deep learning you
need to mention there are two key parts
that interviewer will be looking for
when you are defining deep learning so
first is of course deep learning is a
subset of machine learning so machine
learning is still the bigger let's say
scope and deep learning is one part of
so then what exactly is the difference
deep learning is primarily when we are
implementing these our algorithms or
when we are using neural networks for
doing our training and classification
and regression and all that right so
when we use neural network then it is
considered as deep learning and the term
deep comes from the fact that you can
have several layers of neural networks
and these are called Deep neural
networks and therefore the term deep you
know deep learning uh the other
difference between machine learning and
deep learning which the interviewer may
be wanting to hear is that in case of
machine learning the feature engineering
is done manually what do we mean by
feature engineering basically when we
are trying to train our model we have
our training data right so we have our
training label data and this data has
several let's say if it is a regular
table it has several columns now each of
these columns actually has information
about a feature right so if we are
trying to predict the height weight and
so on and so forth so these are all
features of human beings let's say we
have this data and we have only so those
are the features now there may be
probably 50 or 100 in some cases there
may be 100 such features now all of them
do not contribute to our model right so
we as a data scientist we have to decide
whether we should take all of them all
the features or we should throw away
some of them because again if we take
all of them number one of course your
accuracy will probably get affected but
also there is a computational part so if
you have so many teachers and then you
have so much data it becomes very tricky
so in case of machine learning we
manually take care of identifying the
features that do not contribute to the
learning process and thereby we
eliminate those features and so on right
so this is known as feature engineering
and in machine learning we do that
manual whereas in deep learning where we
use neural networks the model will
automatically determine which features
to use and which to not use and
therefore feature engineering is also
learn automatically so this is a
explanation these are two key things
probably will add value to your response
all right so the next question is what
is the difference between or what are
the differences between machine learning
and deep learning so here this is a
quick comparison table between machine
learning and deep learning and in
machine learning learning enables
missions to take decisions on their own
based on past data so here we are
talking primarily of supervised learning
and it needs only a small amount of data
for training and then works well on low
end system so you don't need a large
machine and most features need to be
identified in advance and manually coded
so basically the feature engineering
part is done manually and the problem is
divided into parts and solved
individually and then combined so that
is about the machine learning part in
deep learning deep learning basically
enables machines to take decisions with
the help of artificial neural network so
here in deep learning we use neural line
so that is the key differentiator
between machine learning and deep
learning and usually deep learning
involves a large amount of data and
therefore the training also requires
usually the training process requires
high-end machines as needs a lot of
computing power and the Machine learning
features are the or the feature
engineering is done automatically so the
neural networks takes care of doing the
feature engineering as well and in case
of deep learning therefore it is said
that the problem is handled end to end
so this is a quick comparison between
machine learning and deep learning in
case you have that kind of a question
then you might get a question around the
uses of machine learning or some real
life applications of machine learning in
modern business the question may be
worded in different ways but the meaning
is how exactly is machine learning used
or actually supervised machine learning
it could be a very specific question
around supervised decision learning so
this is like give examples of supervised
machine learning use of supervised
machine learning in modern business so
that could be the next question so there
are quite a few examples or quite a few
use cases if you will for supervised
machine learning the very common one is
email spam detection so you want to
train your application or your system to
detect between spam and non-spam so this
is a very common business application of
a supervised machine learning so how
does this work the way it works is that
you obviously have historical data of
your emails and they are categorized as
spam and not spam that is what is the
labeled information and then you feed
this information or the all these emails
as an input to your model right and the
model will then get trained to detect
which of the emails are to detect which
is Spam and which is not so that is the
training process and this is supervised
machine learning because you have
labeled data you already have emails
which are tagged as spam or not spam and
then you use that to train your model
right so this is one example now there
are a few industry specific applications
for supervised machine learning one of
the very common ones is a healthcare
diagnostic
Diagnostics you have these images and
you want to train models to detect
whether from a particular image whether
it can find out if the person is sick or
not whether a person has cancer or not
right so this is a very good example of
supervised machine learning here the way
it works is that existing images it
could be x-ray images it will be MRI or
any of these images are available and
they are saying that okay this x-ray
image is deflective or the person as an
illness or it could be cancer whichever
illness right so data stacked as
effective or clear or good image and
different something like that so we come
up with the binary or it could be
multi-class as well saying that this is
defective to 10 percent this is 25 and
so on but let's keep it simple you can
give an example of just a binary
classification that would be good enough
so you can say that in healthcare
Diagnostics using image we need to
detect whether a person is ill or
whether a person is having cancer or not
so here the way it works is you feed
labeled images and you allow the model
to learn from that so that when New
Image is fed it will be able to predict
whether this person is having that
illness or not having cancer or not
right so I think this would be a very
good example for supervised machine
learning in modern business all right
then we can have a question like so
we've been talking about supervised and
on rise then so there can be a question
around semi-supervised machine learning
so what is semi-supervised machine now
semi-supervised learning as the names
such as it falls between supervised
learning and unsupervised learning but
for all practical purposes it is
considered as a art of supervised
learning and the reason this has come
into existence is that in supervised
learning you need labeled data so all
your data for training your model has to
be labor now this is a big problem in
many Industries or in many under many
situations getting the label data is not
that easy because there's a lot of
effort in labeling this data let's take
an example of diagnostic images
just
three images now there are actually
millions of x-ray images available all
over the world but the problem is they
are not labeled so their images are
there but whether it is effective or
whether it is good and information is
not available along with it right in a
form that it can be used by a machine
which means that somebody has to take a
look at these images and usually it
should be like a doctor and then say
that okay yes this image is clean and
this image is cancerous and so on and so
forth now that is a huge effort by
itself so this is where semi-supervised
learning comes into play so what happens
is there is a large amount of data maybe
a part of it is labeled then we try some
techniques to label the remaining part
of the data so that we get completely
labeled data and then we train our model
so I know this is a little long winding
explanation but unfortunately there is
no quick and easy definition for
semi-supervised machine learning this is
the only way probably to explain this
concept
we may have another question as what are
unsupervised machine learning techniques
or what are some of the techniques used
for performing unsupervised machine
learning so it can be worded in
different ways so how do we answer this
question so unsupervised learning you
can say that there are two types
clustering and Association and
clustering is a technique where similar
objects are put together and there are
different ways of finding similar
objects so their characteristics can be
measured and if they have in most of the
characteristics if they are similar then
they can be able to
a string then Association you can I
think the best way to explain
Association is with an example in case
of Association you try to find out how
the items are linked to each other
for example if somebody bought a maybe a
laptop or the person has also purchased
a mouse so this is more in an e-commerce
scenario for example so you can give
this as an example so people who are
buying and laptops are also buying a
mouse so that means there is an
association between laptops and
people who are buying red are also
buying
buying so that is the association that
can be created so this is unsupervised
learning one of the techniques okay all
right then we have very fundamental
question what is the difference between
supervised and unsupervised machine
learning so machine learning these are
the two main types of machine learning
supervised and unservice and in case of
supervised and again here probably the
key word that the person may be wanting
to hear is labeled data now very often
people say we have historical data and
if we run it it is supervised and if we
don't have historical data yes but you
may have historical data but if it is
not labeled then you cannot use it for
supervised learning so it is it's very
key to understand that we put in that
keyword labeled so when we have labeled
data for training our model then we can
use supervised learning and if we do not
have labeled data then we use
unsupervised learning and there are
different algorithms available to
perform both of these
so there can be another question a
little bit more theoretical and
conceptual in nature this is about
inductive machine learning and
machine learning so the question can be
what is the difference between inductive
machine learning and deductive machine
learning or somewhat in that manner so
that the exact phrase or exact question
can vary they can ask for examples and
things like that but that could be the
question so let's first understand what
is inductive and deductive training
inductive training is induced by
somebody and you can illustrate that
with a small example I think that always
helps so whenever you're doing some
explanation try as much as possible as I
said to give examples from your work
experience or give some analogies and
that will also help a lot in explaining
as well and for the interviewer also to
understand so here we'll take an example
or rather we will use an analogy so
inductive training is when we induce
some knowledge or the learning process
into a person without the person
actually experiencing it what can be an
example so we can probably tell the
person or show a person a video that
fire can burn the thing burn his finger
or fire can cause damage so what is
happening here this person has never
probably seen a fire or never seen
anything getting damaged by fire but
just because he has seen this video he
knows that okay fire is dangerous and if
a fire can cause damage right so this is
inductive learning compared to that what
is deductive learning so here you draw a
conclusion or the person draws
conclusion out of experience so we will
stick to the analogy so compared to the
showing a video Let's assume a person is
allowed to play with fire right and then
he figures out that if he puts his
finger it's burning or you throw
something into the fire it burns so he
is learning through experience so this
is known as deductive learning okay so
you can have applications or models that
can be trained using inductive learning
or deductive learning all right I think
probably that explanation
sufficient the next question is rknn and
K means clustering similar to one
another or are they same right because
the letter K is kind of common between
them okay so let us take a little while
to understand what these two are one is
KNN another is K means a KNN stands for
K nearest neighbors and K means of
course is the clustering mechanism now
these two are completely different
except for the letter K being common
between them KN is completely different
K means clustering is
KNN is a classification process and
therefore it comes under supervised
learning whereas k-means clustering is
actually unsupervised okay when you have
K and N and you want to implement k n n
which is basically K nearest neighbors
the value of K is a number so you can
say k is equal to 3 you want to
implement k n n with K is equal to 3 so
which means that it performs the
classification in such a way that how
does it perform the classification so it
will take three nearest objects and
that's why it's called nearest neighbor
so basically uh based on the distance it
will try to find out its nearest objects
that are let's say three of the nearest
objects and then it will check whether
the class they belong to which class
right so if all three belong to one
particular class obviously this new
object is also classified as that
particular
but it is possible that they may be from
two or three different classes okay so
let's say they are from two classes and
then they are from two classes now
usually you take a odd number you assign
a odd number two so if there are three
of them and two of them belong to one
class and then one belongs to another
class so this new object is assigned to
the class to which the two of them
belong now the value of K is sometimes
tricky whether should you use three
should you use five should you use seven
it can be tricky because the ultimate
classification can also vary so it's
possible that if you're taking K as3 the
object is probably in one particular
class but if you take K is equal to 5
maybe the object will belong to a
different class because when you're
taking three of them probably two of
them belong to a class one and one
belong to class two whereas when you
take five of them it is possible that
only two of them belong to class one and
the three of them belong to Class 2 so
which means that this object will belong
to class 2 right so you see that so it
is the class allocation can vary
depending on the value of K now K means
on the other hand is a clustering
process and it is unsupervised where
what it does is the system will
basically identify how the objects are
how close the objects are with respect
to some of their features okay and but
personality of course is the the letter
K and in case of K means also we specify
its value and it could be 3 or 5 or 7
there is no technical limit as such but
it can be any number of clusters that
you can create okay so based on the
value that you provide the system will
create that many clusters of similar
objects so there is a similarity to that
extent that K is a number in both the
cases but actually these two are
completely different processes
we have what is known as naive based
classifier and people often get confused
thinking that naive base is the name of
the person who found this classifier or
who developed this classifier which is
not 100 True base is the name of the
person bais is the name of the person
but naive is not the name of the person
right so naive is basically an English
word and that has been added here
because of the nature of this particular
classifier an ibase classifier is a
probability based classifier and it
makes some assumptions that presence of
one feature of a class is not related to
the presence of any other feature of
maybe other classes right so which is
not a very strong or not a very what do
you say accurate assumption because
these features can be related and so on
but
then if we go with this assumption this
whole algorithm works very well even
with this assumption and that is the
good side of it but the term comes from
there so that so the explanation that
you can
then there can be question around
reinforcement learning it can be
paraphrased in multiple ways one could
be can you explain how a system can play
a game of chess using reinforcement or
it can be any game so the best way to
explain this is again to talk a little
bit about what reinforcement learning is
about and then elaborate on that to
explain the process so first of all
reinforcement learning has an
environment and an agent and the agent
is basically performing some actions in
order to achieve a certain goal and this
goals can be anything either it is
related to game then the goal could be
that you have to score very high score
or it could be that your number of lives
should be as high as possible don't lose
life so this could be some of them a
more advanced examples could be for
driving the automotive industry
self-driving cars they actually also
make use of reinforcement learning to
teach the car how to navigate through
the roads and so on and forth that is
also another example now how does it
work so if a system is basically there
is an agent and environment and every
time the agent takes a step or performs
a task which is taking it towards the
goal the final goal let's say to
maximize the score or to minimize the
number of lives and so on or minimize
that
well it is rewarded and every time it
takes a step which goes against that
core right contrary or in the reverse
Direction it is penalized okay so it is
like a keratin as
now how do you use this to create a game
of chess so to create a system to play a
game of chess now the way this works is
and this could probably go back to this
alphago example where alphaco defeated a
human Champion so the way it works is in
reinforcement learning the system is
allowed for example in this case we are
talking about Chess so we allow the
system to first of all watch playing a
game of chess so it could be with a
human being or it could be the system
itself there are computer games of Chess
right so either this new learning system
has to watch that game or watch a human
being play the game because this is
reinforcement learning is pretty much
all visual so when you're teaching the
system to play a game the system will
not actually go behind the scenes to
understand the logic of your software of
this game or anything like that it is
just visually watching the screen and
then it learns okay so reinforcement
learning to a large extent it works on
that so you need to create a mechanism
whereby your model will be able to watch
somebody playing the game and then you
allow the system also to start playing
the game so it pretty much starts from
scratch okay and as it moves forward it
it's at right at the beginning the
system really knows nothing about the
game of chess okay so initially it is a
clean slate it just starts by observing
how you are playing so it will make some
random moves and keep losing badly but
then what happens is over a period of
time so you need to now allow the system
or you need to play with this system not
just one two three four or five times
but hundreds of times thousands of times
maybe even hundreds of thousands of
times and that's exactly how alphago has
done it played millions of games between
itself and the system right so for the
game of chess also you need to do
something like that you need to allow
the system to play chess and and learn
on its own over a period of repetition
so I think you can probably explain it
to this much to this extent and I
sufficient
now this is another question which is
again somewhat similar but here the size
is not coming into picture so the
question is how will you know which
machine learning algorithm to choose for
your classification problem now this is
not only classification problem it could
be a regression problem I would like to
generalize this question so if somebody
asks you how will you choose how will
you know which algorithm to use the
simple answer is there is no way you can
decide exactly saying that this is the
algorithm I am going to use in a variety
of situations there are some guidelines
like for example you will obviously
depending on the problem you can say
whether it is a classification problem
or a regression problem and then in that
sense you are kind of restricting
yourself to if it is a classification
problem there are you can only apply a
classification algorithm right to that
extent you can probably let's say limit
the number of algorithms but now within
the classification algorithms you have
decision trees you have SPM you have
logistic regression is it possible to
outright say yes so for this particular
problem since you have explained this
now this is the exact algorithm that you
can use that is how
okay so we have to try out a bunch of
algorithms see which one gives us the
best performance and best accuracy and
then decide to go with that particular
algorithm so in machine learning a lot
of it happens through trial and error
there is no real possibility that
anybody can just by looking at the
problem or understanding the problem
tell you that okay in this particular
situation this is exactly the algorithm
that you should use then the questions
may be around application of machine
learning and this question is
specifically around how Amazon is able
to recommend other things to buy so this
is around recommendation engine how does
it work how does a recommendation engine
work so is basically the question is all
so the recommendation engine again Works
based on various inputs that are
provided obviously something like you
know Amazon a website or e-commerce site
like Amazon collects a lot of data
around the customer Behavior who is
purchasing what and if somebody is
buying a particular thing they're also
buying something else so this kind of
Association right so this is the
unsupervised learning we talked about
they use this to associate and Link or
relate items and that is one part of it
so they kind of build association
between items saying that somebody
buying this is also buying this that is
one part of it then they also profile
the users right based on their age their
gender their geographic location they
will do some profiling and then when
somebody is logging in and when somebody
is shopping kind of a mapping of these
two things are done they try to identify
obviously if you have logged in then
they know who you are and your
information is available like for
example your age maybe your agenda and
where you're located what you purchased
earlier right so all this is taken and
the recommendation engine basically uses
all this information and comes up with
recommendations for a particular user so
that is how the recommendation engine
work all right then the question can be
something very basic like when will you
go for classification versus regression
right when do you do classification
instead of instead of regression or when
will you use classification instead of
regression now yes so so this is
basically going back to the
understanding of the basics of
classification and regression so
classification is used when you have to
identify or categorize things into
discrete classes so the best way to
respond to this question is to take up
some examples and use it otherwise it
can become a little tricky the question
may sound very simple but explaining it
can sometimes be very tricky in case of
regression the use of course there will
be some keywords that they will be
looking for so just you need to make
sure you use those keywords one is the
discrete values another is the
continuous values so regression if we
are trying to find some continuous
values you use regression whereas if you
are trying to find some discrete values
you use classification and then you need
to illustrate what are some of the
examples so classification is like let's
say there are images and you need to put
them into classes like cat dog elephant
tiger something like that so that is a
classification problem or it can be that
is a multi-class classification problem
it could be binary classification
problem like for example whether a
customer will buy or he will not buy
that is a classification binary
classification it can be in the weather
forecast area now weather forecast is
again combination of regression and
classification because on the one hand
you want to predict whether it's going
to rain or not it's a classification
problem that's a binary classification
right whether it's going to rain or not
rain however you also have to predict
what is going to be the temperature
tomorrow right now temperature is a
continuous value you can't answer the
temperature in a yes or no kind of a
response right so what will be the
temperature tomorrow so you need to give
a number which can be like 20 degrees 30
degrees or whatever right so that is
where you use regression one more
example is stock price prediction so
that is where again you will use
regression so these are the various
examples so you need to illustrate with
examples and make sure you include those
keywords like discrete and continuous so
the next question is more about a little
bit of a design related question to
understand your constant
so it is how will you design a spam
filter so how do you basically design
our developers
so I think the main thing here is he's
looking at probably understanding your
Concept in terms of what is the
algorithm you will use or what is your
understanding about difference between
classification and regret
regular and things like that right and
the process of course the methodology
and the process so the best way to go
about responding to this is we say that
okay this is a classification problem
because we want to find out whether an
email is a spam or not spam so that we
can apply the filter accordingly so
first thing is to identify what type of
a problem it is so we have identified
that it is a classification then the
second step may be to find out what kind
of algorithm to use now since this is a
binary classification problem logistic
regression is a very common very common
algorithm but however right as I said
earlier also we can never say that okay
for this particular problem this is
exactly the algorithm that we can use so
we can also probably try decision trees
or even support Vector machines for
example svm so we will kind of list down
a few of these algorithms and we will
say okay we want to we would like to try
out these algorithms and then we go
about taking your historical data which
is the labeled data which are marked so
you will have a bunch of emails and then
you split that into training and test
data sets you your training data set to
train your model that or your algorithm
that you have used or rather the model
actually so and you actually will have
three models let's say you are trying to
test out three algorithms so you will
obviously have three models so you need
to try all three models and test them
out as well see which one gives the best
accuracy and then you decide that you
will go with that model okay so training
and test will be done and then you zero
in on one particular model and then you
say okay this is the model will you use
we will use and then go ahead and
Implement that or put that in production
so that is the way you design a spam the
next question is about random Forest
what is random form so this is a very
straightforward question however the
response you need to be again a little
careful while we all know what is random
Forest explaining this can sometimes be
tricky so one thing is random Forest is
kind of in one way it is an extension of
decision trees because it is basically
nothing but you have multiple decision
trees and trees will basically we will
use for doing if it is classification
mostly it is classification you will use
the trees for classification and then
you use voting for finding that the
final class so that is the underlying
but how will you explain this how will
you respond to this so first thing
obviously we will say that random Forest
is one of the algorithms and the more
important thing that you need to
probably the interviewer is is waiting
to here is Ensemble learner right so
this is one type of Ensemble learner
what is Ensemble learner Ensemble
learner is like a combination of
algorithms so it is a learner which
consists of more than one algorithm or
more than one or maybe models okay so in
case of random Forest the algorithm is
the same but instead of using one
instance of it we use multiple instances
of it and we use so in a way that is a
random Forest is an ensemble learner
there are other types of Ensemble
Learners where we have like reuse
different algorithms itself so you have
one maybe logistic regression and a
decision tree combined together and so
on and so forth there are other ways
like for example splitting the data in a
certain way and so on so that's all
about Ensemble we will not go into that
but random Forest itself I think the
interviewer will be happy to do with
this word answer
and so then you go and explain how the
random Forest works so if the random
Forest is used for classification then
we use what is known as a voting
mechanism so basically how does it work
let's say your random Forest consists of
100 trees okay and each observation you
pass through this forest and each
observation let's say it is a
classification problem binary
classification zero or one and you have
100 trees now if 90 trees say that it is
a zero and ten of the trees say it is a
one you take the majority you may take a
vote and since 90 of them are saying
zero you classify this as zero then you
take the next observation and so on so
that is the way random Forest works for
classification if it is a regression
problem it's somewhat similar but only
thing is instead of what what we will do
is sorry in regression remember what
happens you actually calculate a value
right so for example you're using
regression to predict the temperature
and you have 100 trees and each tree
obviously will probably predict a
different value of the temperature they
may be close to each other but they may
not be exactly the same value so these
hundred trees so how do you now find the
actual value the output for the entire
Forest right so you have outputs of
individual trees which are a part of
this Forest but then you need to find
the final output of the forest itself so
how do you do that so in case of
regression you take like an average or
the mean of all the 100 trees right so
this is also a way of reducing the error
so maybe if you have only one tree and
that one tree makes a header it is
basically hundred percent wrong or 100
right right but if you have on the other
hand if you have a bunch of trees you
are basically medicating that reducing
that error okay so that is the way
random Forest works so the next question
is considering the long list of machine
learning algorithms how will you decide
on which one to use so once again here
there is no way to outright say that
this is the algorithm that we will use
for a given data set this is a very good
question but then the response has to be
like again there will not be a
one-size-fits all so we need to first of
all you can probably shorten the list in
terms of by saying okay whether it is a
classification problem or it is a
regression problem to that extent you
can probably shorten the list because
you don't have to use all of them if it
is a classification problem you only can
pick from the classification algorithm
right so for example if it's a
classication you cannot use linear
regression algorithm or if it is a
regression problem you cannot use svm or
maybe now you can use svm but maybe a
logistic regression right so to that
extent you can probably shorten the list
but still you will not be able to 100
decide on saying that this is the exact
algorithm that I am going to use so the
way to go about is you choose a few
algorithms based on what the problem is
you try out your data you train some
models of these algorithms check which
one gives you the lowest error or the
highest accuracy and based on that you
choose that particular algorithm
all right then they can be questions
around bias and variants so the question
can be what is bias and variance in
machine learning uh so you just need to
give out a definition for each of these
for example a bias in machine learning
it occurs when the predicted values are
far away from the actual value so that
is the bias okay and whereas they are
all all the values are probably they are
far off but they are very near to each
other though the predicted values are
close to each other right while they are
far off from the actual value but they
are close to each other you see the
difference so that is bias and then the
other part is your variance now variance
is when the predicted values are all
over the place right so the variance is
high that means it may be close to the
Target but it is kind of very scattered
so the points the predicted values are
not close to each other right in case of
bias the predicted values are close to
each other but they are not close to the
Target but here they may be close to the
Target but they may not be close to each
other so they are a little bit more
scattered so that is what in case of a
variance okay then next question is
about again related to bias and variance
what is the trade-off between bias and
variance yes I think this is a
interesting question because these two
are heading in different directions so
for example if you try to minimize the
bias variance will keep going high and
if you try to minimize the variance bias
will keep going high and there is no way
you can minimize both of them so you
need to have a trade-off saying that
okay this is the level at which I I will
have my bias and this is the level at
which I will have variance so the
trade-off is that pretty much uh attack
you you decide what is the level you
will tolerate for your bias and what is
the level you will tolerate for variance
and a combination of these two in such a
way that your final results are not way
off and having a trade-off will ensure
that the results are consistent right so
that is basically the output is
consistent and which means that they are
close to each other and they are also
accurate which that means they are as
close to the Target as possible right so
if either of these is high then one of
them will go off the track define
precision and Recall now again here I
think it would be best to draw a diagram
and take up in the confusion Matrix and
it is very simple the definition is like
a formula your Precision is true
positive by true positive plus false
positive and your recall is true
positive by true positive plus false
negative okay so that's you can just
show it in a mathematical way that's
pretty much you know
that's the easiest way to define so the
next question can be about decision tree
what is decision tree pruning and why is
it so basically decision trees are
really simple to implement and
understand but one of the drawbacks of
decision trees is that it can become
highly complicated as it grows right and
the rules and conditions can become very
it and this can also lead to overfitting
which is basically that during training
you will get 100 accuracy but when
you're doing testing you will get a lot
of Errors so that is the reason pruning
to be done so the purpose or the reason
for doing decision tree pruning is to
reduce overfitting or to cut down on
overfitting and
and what is decision tree rolling it is
basically that you reduce the number of
branches because as you may be aware a
tree consists of the root node and then
there are several internal nodes and
then you have the leaf nodes now if
there are too many of these internal
nodes that is when you face the problem
of overfitting and pruning is the
process of reducing those internal nodes
all right so the next question can be
what is logistic regression uh so
basically logistic regression is one of
the techniques used for performing
classification especially binary
classification now there is something
special about logistic regression and
there are a couple of things you need to
be careful about first of all the name
is a little confusing it is called
logistic regression but it is used for
classification so this can be sometimes
confusing so you need to probably
clarify that to the interviewer if and I
think divide and they can also ask this
like a request
the question thing is the term logistic
has nothing to do with the usual
Logistics that we talk about but it is
derived from the log so that the
mathematical derivation was log and
therefore the name
so what is logistic regression and how
is it used so logistic regression is
used for binary classification and the
output of a logistic regression is
either a zero or a one and it varies so
it's basically it calculates a
probability between 0 and 1 and we can
set a threshold that can vary typically
it is 0.5 so any value above 0.5 is
considered as 1 and if the probability
is below 0.5 it is considered as 0. so
that is the way we calculate the
probability of the system calculates the
probability and based on the threshold
it sets a value of 0 or 1 which is like
a binary classification zero or one okay
then we have a question around K nearest
neighbor algorithm so explain K nearest
neighbor algorithm so first of all what
is the K nearest neighbor algorithm this
is a classification algorithm so that is
the first thing we need to mention and
we also need to mention that the K is a
number it is an an integer and this is
variable and we can Define what the
value of K should be it can be 2 3 5 7
and usually it is an odd number so that
is something we need to mention
technically it can be even number also
but then typically it would be odd
number and we will see why that is okay
so based on that we need to classify
objects okay we need to classify objects
so again it will be very helpful to draw
a diagram you know if you're explaining
I think that will be the best way so
draw some diagram like this and let's
say we have three clusters or three
classes existing and now you want to
find for a new item that has come you
want to find out which class this
belongs right so you go about as the
name suggests if you go about finding
the nearest neighbors right the points
which are closest to this and how many
of them you will find that is what is
defined by K now let's say our initial
value of K was 5 okay so you will find
find the K the five nearest data points
so in this case as it is Illustrated
these are the five nearest data points
but then all five do not belong to the
same class or cluster so there are one
belonging to this cluster one the second
one belonging to this cluster two three
of them belonging to this third class
okay so how do you decide that's exactly
the reason we should as much as possible
try to assign an odd number so that it
becomes easier to assign this so in this
case you see that the majority actually
if there are multiple classes then you
go with the majority so since three of
these items belong to this class we
assign which is basically the in in this
case the green or the tennis or the
third cluster as I was talking
right so we assign it to this third
class so in this case it is uh that's
how it is decided okay so K nearest
neighbors the first thing is to identify
the number of neighbors that are
mentioned as K so in this case it is K
is equal to 5 so we find the five
nearest points and then find out out of
these five which class has the maximum
number in that and and then the new data
point is assigned to that class okay so
that's pretty much how K nearest
neighbors and this was all for this
machine learning bootcamp hope you guys
found it informative and helpful if you
like this session then like share and
subscribe if you have any questions then
you can drop them in the comment section
below thanks for watching and stay tuned
for more from Simply learn
hi there if you like this video
subscribe to the simply learned YouTube
channel and click here to watch similar
videos turn it up and get certified
click here
foreign