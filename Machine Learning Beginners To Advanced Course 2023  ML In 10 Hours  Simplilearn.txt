hello everyone welcome to this fantastic
machine learning beginner to Advance
full course by simply learn but before
we begin if you enjoy watching these
videos and find them interesting
subscribe to our Channel as we bring you
the best videos daily also hit the Bell
icon to never miss any updates from
Simply learn so let's get started we
will brief you with machine learning
introduction after that we will see the
types of machine learning forward we
will cover machine learning road map
skills required for machine learning
engineers and the top applications and
projects for machine learning we will
walk you through some fantastic Concepts
like AI versus ml versus DL by
proceeding with the course we will teach
you different machine learning
algorithms like linear regression
logistic regression and many more with
Hands-On demo explanation ations in
Python we will walk you through unique
projects like fake news detection object
detection and many more these projects
will serve you as the finest portfolios
for your future interviews speaking of
interviews we have covered you long with
most frequently Asked machine learning
interview questions to help you crack
the most challenging interviews but
before we begin to understand what
machine learning is if you want to
become an AI expert and gain handsome
salary packages look at the wide range
of AIML courses by simply learn in
collaboration with top universities
across the globe by enrolling in any of
these certification programs you will
gain expertise in skills like generative
AI prompt engineering chat GPT
explainable AI machine learning
algorithms supervised and unsupervised
learning model training and optimization
and there's much more on the list with
hands-on experience in the tools like
chat GPT di p python open CV and tens
oflow you will catch the eyes of top
recruiters so what are you waiting for
hurry up and enroll now an year of
experience is preferred to enroll in
these courses find the course Link in
the description box with that in mind
over to our training experts here we
have our um looks a little bit like
Frankenstein our Frankenstein looking
robot today let me tell you what is
machine learning machine learning works
on the development of computer programs
that caness access data and use it to
automatically learn and improve from
experience watch a robot builder
construct house in 2 days this was back
in July 29th
2016 so that's pretty impressive this
amount of time to continue to grow in
its development and it's smart enough to
leave spaces in the brick work for
wiring and plumbing and can even cut and
shape bricks to size Amazon Echo relies
on machine learning and with more data
it becomes more accurate play your
favorite music order order pizza from
dominoes voice control your home request
rides from Uber have you ever wondered
the difference between AI machine
learning and deep learning artificial
intelligence a technique which enables
machines to mimic human behavior this is
really important because this is how we
are able to gauge how well our
computations or what we're working on
works is the fact that we're mimicking
human behavior we're using this to
replace human work and make it more
efficient and make it more streamlined
and more accurate and so the center of
artificial intelligence is the big
picture of all this put together IBM
deep blue chess electronic game
characters those are just a couple
examples of artificial intelligence
machine learning a technique which uses
statistical methods enabling machines to
learn from their past data so this means
if you have your input from last time
and you have your answer you use that to
help prove the next guess it makes for
the correct answer IBM Watson Google
search algorithm email spam filters
these are all part of machine learning
and then deep learning which is a subset
of machine learning composing algorithms
that allow a model to trade itself and
perform tasks Alpha go natural speech
recognition these are a couple examples
deep learning is associated with tools
like neural networks where it's kind of
a black BLX as it learns it changes all
these things that are as a human we'd
have a very hard time tracking and it's
able to come up with an answer from that
now let's see how machine learning works
first we start with training the data
once we've trained the data the train we
go into the machine learning algorithm
which then puts the data into a
processing which then goes down to
machine another machine learning
algorithm and then we take new data
because you have to test whatever you
did and make sure it works correctly and
we put that into the same algorithm once
we do that we check our prediction we
check our results and from the
prediction if we've set aside some
training data and we find out it didn't
do a good job predicing it and it getss
a thumbs down as you see then we go back
to the beginning and we retrain the
algorithm and a lot of times it's not
just about getting the wrong answer it's
about continually trying to get a better
answer so you'll see the first time you
might be like oh this is not the answer
I want depending on what domain you're
working in whether it's medical
economical business stocks whatever you
try out your model and if it's not
giving you a good answer you retrain it
if you think you can get a better answer
you retrain it and you keep doing that
until you get the best answer you can
now let's look into the types of machine
learning machine learning is primarily
of three types first one is supervised
machine learning as the name suggest you
have to supervise your machine learning
while you train it to work on its own it
requires labeled training data next up
is unsupervised learning wherein there
will be training data but it won't be
labeled finally there's reinforcement
learning wherein the system learns on
its own let's talk about all these types
in detail let's let's try to understand
how supervised Learning Works look at
the pictures very very carefully the
monitor depicts the model or the system
that we are going to train this is how
the training is done we provide a data
set that contains pictures of a kind of
a fruit say an apple then we provide
another data set which lets the model
know that these pictures wear that of a
fruit called
Apple this ends the training phase now
what we will do is we provide a new set
of data which only contains pictures of
apple now here comes the fun part the
system can actually tell you what fruit
it is and it will remember this and
apply this knowledge in future as well
that's how supervised Learning Works you
are training the model to do a certain
kind of an operation on its own this
kind of a model is generally used into
filtering spam mails from your email
accounts as well yes surprise aren't you
so let's move on to unsupervised
learning now let's say we have a a data
set which is cluttered in this case we
have a collection of pictures of
different fruits we feed this data to
the model and the model analyzes the
data to figure out patterns in it in the
end it categorizes the photos into three
types as you can see in the image based
on their
similarities so you provide the data to
the system and let the system do the
rest of the work simple isn't it this
kind of a model is used by flip card to
figure out the products that are well
suited for you honestly speaking this is
my favorite type of machine learning out
of all the three and this type has been
widely shown in most of the Sci-Fi
movies lately let's find out how it
works imagine a newborn baby you put a
burning candle in front of the baby the
baby does not know that if it touches
the flame its fingers might get burned
so it does that anyway and gets hurt the
next time you put that candle in front
of the baby it will remember what
happened the last time and would not
repeat what it did that's exactly how
reinforcement Learning Works we provide
the machine with a data set wherein we
ask it to identify a particular kind of
a fruit in this case an Apple so what it
does as a response it tells us that it's
a mango but as we all know it's a
completely wrong answer so as a feedback
we tell the system that it's wrong it's
not a mango it's an apple what it does
it learns from the feedback and keeps
that in mind when the next time when we
ask a same question it gives us the
right answer if is able to tell us that
it's actually an apple that is a
reinforced response so that's how
reinforcement learning works it learns
from its mistakes and experiences this
model is used in games like Prince of
Persia or Assassin's Creed or FIFA where
in the level of difficulty increases as
you get better with the games just to
make it more clear for you let's look at
a comparison between supervised and
unsupervised learning firstly the data
involved in case of supervised learning
is labeled as we mentioned in the
examples previously we provide the
system with a photo of an apple and let
the system know that this is actually an
apple that is called label data so the
system learns from the label data and
makes future
predictions now unsupervised learning
does not require any kind of label data
because its work is to look for patterns
in the input data and organize it the
next point is that you get a feedback in
case of supervised learning that is what
once you get the output the system tends
to remember that and uses it for the
next operation that does not happen for
unsupervised learning and the last point
is that supervised learning is mostly
used to predict data whereas
unsupervised learning is used to find
out hidden patterns or structures in
data I think this would have made a lot
of things clear for you regarding
supervised and unsupervised learning in
recent years artificial intelligence and
machine learning have permeated every
aspect of Our Lives if you are a
programmer chances are you have begun
utilizing github's co-pilot an AI tool
that transforms natural language PRS
into coding suggestions streamlining the
programming process as a writer you may
have encountered open AIS gp3 or similar
Auto regressive language models that
leverage deep learning to generate text
resembling human language many of us
dedicated a few hours to experimenting
with dalu the AIML power text to image
generator capable of producing intricate
visuals based on the most unusual
request not only is d 2 exponentially
more powerful but it also has the
potential to revolutionize the field of
digital art consider its impact on a
digital artist illustrator or graphic
designer career imagine creating a
highly realistic image within seconds
using an app these technologies have
significant real world applications with
far-reaching implications let me tell
you some fascinating facts about machine
learning according to recent studies
machine learning related job postings
have increased bi a staggering 344 per
in the past 5 years companies across the
globe are actively seeking professionals
who can harness the power of data and
build intelligence systems the average
salary is
$190,000 in us and 26 lakhs perom in
India accelerate your career in Ai and
ml with our comprehensive postgraduate
program in Ai and machine learning gain
expertise in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
a prestigious certificate exclusive
alumni membership and hackathons and ask
me anything sessions by IBM with three
Capstone and 25 plus industry projects
using real data sets from Twitter Uber
and more you will gain practical
experience master classes by Purdue
faculty and IBM experts ensure Top Notch
education simply learns job assist helps
you get noticed by Leading companies
live sessions on AI trends like chat GPD
generative Ai and explainable AI this
program covers statistics python
supervised and unsupervised learning NLP
neural networks computer vision G caras
tensor flow and many more skills enroll
now and unlock exciting a and ml
opportunities the link is mentioned in
the description box below with that
having said now here is a comprehensive
step-by-step guide to learning machine
learning number one is skills required
number two job rolls in ml number third
companies hiring ml engineers and the
number four is future scope of ml now
starting with the number one that is
fundamentals of machine learning
familiarize yourself with the
fundamentals of machine learning in the
initial stages of learning to drive we
are familiarized with the different
elements varieties and regulations
pertaining to operating a car it is
essential to delve into the fundamentals
of machine learning learning to
understand what lies ahead and the
required knowledge now moving to number
two that is acquiring Proficiency in
python or our programming language to
excel in the field of artificial
intelligence and machine learning it is
crucial to develop a strong command of
python or R programming language both
Python and R are widely used in the data
science Community due to their
versatility and extensive libraries for
scientific Computing so now moving on to
the third step that is gain knowledge of
essential python libraries for machine
learning once you have acquired
Proficiency in python as part of our
machine Learning Journey the subsequent
step involves familiarizing yourself
with essential python libraries crucial
for working with data and implementing
machine learning Solutions the key
python libraries that you should learn
from machine learning are numpy pandas
MPL lip and pyit Lear now moving to step
four
that is learn and Implement various
machine learning algorithms after
gaining Proficiency in python as part of
our machine Learning Journey the
subsequent Milestone involves learning
various machine learning algorithms and
their implementation using python listed
below are some of the key machine
learning algorithms that are essential
to learning number one linear regression
number two logistic regression number
three passive agression number four na
Base number five support Vector machines
now moving to step five Master the
concepts and implementation of neural
networks once you have acquired
knowledge of python and machine learning
algorithms the next significant step in
the machine learning road map is to
learn neural network architecture and
their implementations using python
outlined below are several crucial
neural network architectures that are
essential to learning number one
artificial neural networks number two
CNN number three
RNN number four long shortterm memory
now moving to step six that is engage
Hands-On projects to apply your
knowledge and reinforce your
understanding after acquiring knowledge
of python machine learning algorithms
and neural network architectures the
next crucial step is the machine
learning road map and is to gain
practical experience by working on
projects that allow you to apply what
you have learned the first project is
Iris flower classification number two
California house price prediction number
three stock price prediction number four
customer segmentation now we'll move to
job roles in machine learning machine
learning as highlighted earlier has
gained immense popularity for its
ability to elevate human efforts and
enhance machine performance through
autonomous learning this popularity has
resulted in lucrative and sought after
career options within the field
including roles like machine learning
engineer so the duty of a machine
learning engineer encompasses the
creation construction and deployment of
machine learning models collaborating
closely with data scientist and software
Engineers they participate in the
development and execution of machine
Learning Systems according to glasto ml
Engineers can earn up to
$150,000 in us and 11 lakhs perom in
India now moving to data scientist the
role of a data scientist involves
Gathering scrutinizing and interpreting
extensive data sets leveraging machine
learning algorithms they uncover
patterns and glean insights from the
data utilizing this knowledge to inform
decisions and address challenges
according to glast data scientist earn
$145,000 in us and 13 lakhs perom in
India now moving to NLP Engineers the
specific duties may vary based on the
role and sector but as outlined by
freelancer map an NLP engineer typically
engage in tasks such as designing
natural language processing system sys
and addressing speech patterns and AI
speech recognition according to glass
door n LP engineer can earn
$120,000 in us and 10 lakhs perom in
India now moving to computer vision
engineer Engineers specializing in
computer vision operate within the realm
of computer vision employing machine
learning to empower computers to
comprehend and interpret visual data
from the surroundings the
responsibilities include task t such as
image and video analysis and object
detection according to glasto CV
Engineers earn
$156,000 in us and 8 lakhs perom in
India now moving to business
intelligence developer the main role of
a bi developer is to develop deploy and
maintain bi tools and interfaces they
are also responsible for simplifying
highly technical language and complex
information into layman's terms for
everyone else in the company to
understand according to Glass Ro
business intelligence developers earn
$15,000 in us and 7 lakhs perom in India
now we'll see the top companies hiring
for machine learning engineer number one
is Amazon then we have as centure Google
Apple Intel so these are the top hiring
companies that hire machine learning
Engineers now we'll talk about future of
machine learning machine learning has a
bright future but faces several
difficulties ml is predicted to grow
increasingly pervasive as technology
develops revolutionizing sectors
including Health Care Banking and
transportation the work Market will
change due to AI driven automation
necess setting new position and skills
and with this we have come to the end of
this video I hope you found this video
informative and interesting hello
everyone and Welcome to our video on
skills required for an ml engineer
machine learning has been the Talk of
the Town lately
every organization has realized the
potential of machine learning in
improving their business objectives and
attaining the Enterprise goals this
expanding demand has led to a lot of
people applying for machine learning
jobs and upskilling them themselves in
the field of machine learning you can
take up this growing opportunity in the
field of machine learning and utilize it
to land yourself a very challenging
fulfilling and high ping job in this
video we will be breaking down in
complete detail each and every skill
that you would need in order to crack
the machine learning engineer job
interview well ml is not just a passing
Trend it's a sismic shift that is
reshaping our world and creating new
avenues for Innovation and Discovery so
by embracing a career in ml you become
part of dynamic field that thrives on
solving complex problem pushing
boundaries and making a profound impact
on society so the demand for ML
professional is skyrocketing across
industries from Healthcare and finance
to entertainment and transportation
organization are actively seeking
talented individuals who can harness the
power of AI to drive their business
forward but what skill does it takes to
become an ml engineer how can you embark
on this thrilling Journey we have the
answers to all your questions also
accelerate your career in Ai and ml with
our comprehensive post-graduate program
in Ai and machine learning gain
expertise in machine learning deep
learning NLP computer vision and
reinforcement learning you will receive
a prestigious certificate exclusive
alumini members ship and ask me anything
sessions by IBM with three Capstone
projects and 25 plus industry projects
using real data set from Twitter Uber
and more you will gain practical
experience master classes by keltech
faculty and IBM experts ensure top-notch
education simply learns job assist help
you get notice by Leading companies this
program covers python supervised and
unsupervised learning NLP neural
networks computer Visions g k tflow and
many more other skills so enroll now and
unlock exciting Ai and MLM opportunities
the link is in the description box below
so without any further delay let's get
started to become a machine learning
engineer you need a combination of
technical skills non-technical skills
and some bonus skills so here are some
essential skills required to pursue a
career as an ml engineer so first we
will talk about some technical skills to
become a ml engineer so first one in the
list is programming languages strong
programming skills are essential you
should be proficient in at least one one
programming languages such as python or
R python is widely used in the ml
Community due to its Rich libraries that
is numai pandas tensorflow and pytorch
that supports ml task and the second on
the list is machine learning algorithms
and techniques you should have a solid
understanding of various ml algorithm
such as linear regression logistic
regression decision trees random Forest
neural network and deep learning
familiarize yourself with the principles
Behind These algorithms their Pros and
cons and when to use them so third one
on the list is data pre-processing ml
models require clean and well prepared
data you should know how to handle
missing data deal with normalized and
standardized data and the final is
perform feature engineering
understanding data pre-processing
technique is crucial for Effective ml
model training and the fourth one is
data manipulation and Analysis data is
the foundation of ml model you should be
skilled in the data manipulation and
Analysis using libraries like n and
pandas this includes cleaning and
transforming data exploratory data
analysis which is Eda and understanding
the statical properties of the data and
the fifth one is machine learning
libraries and Frameworks familiarity
with popular ml libraries and framework
is essential some some commonly used one
include numai pandas tensorflow and pyot
these Library provide pre-implemented ml
algorithms neural network architectures
and tools for model training and
evaluation now that we have seen the
Technical Machine learning engineering
skills let us have a look at the
non-technical machine learning skills so
the first one is industry knowledge
machine learning projects that
effectively tackle genuine challenges
are likely to achieve great success
regardless of the industry you are
involved in it is crucial to have a
comprehensive understanding of its
operation and identify ways to optimize
business outcomes and the second one on
the list is effective communication
effective communication plays a crucial
role in facilitating these interaction
seeking skilled ml engineer value
candidates who can effectively Cove
technical discoveries to non-technical
terms like marketing or sales
demonstrating Clarity and fluency in
their explanation moving forward let's
see some bonus skills to become ml
engineer the first one on the list is
reinforcement learning in 2023
reinforcement learning emerg as a
catalyst for numerous captivating
advancement in deep learning and
artificial intelligence to pursue a
career in robotics self-driving cars or
any other AI related feed it is crucial
to comprehend this concept and the
second on the list is computer vision
computer vision and machine learning are
fundamental branches of computer science
that can independently fuel highly
Advanced system relying on CV and ml
algorithm however the combination has a
potential to unlock greater even
possibilities and achievements so
remember that the field of ml is
constantly evolving to continuous
learning and staying updated with the
latest development and the research
papers r essential to be the Practical
machine learning has improved our lives
in a number of wonderful ways today
let's talk about some of these I'm Rahul
from Simply learn and these are the top
10 applications of machine learning
first let's talk about virtual personal
assistants Google Assistant Alexa
Cortana and Siri now we've all used one
of these at least at some point in our
lives now these help improve our lives
in a great number of ways for example
you could tell them to call someone you
could tell them to play some music you
could tell them to even schedule an
appointment so how do these things
actually work first they record whatever
you're saying send it over to a server
which is usually in a cloud decode it
with the help of machine learning and
neural networks and then provide you
with an output so if you ever noticed
that these systems don't work very well
without the internet that's because the
server couldn't be contacted next let's
talk about traffic predictions now say I
wanted to travel from Buckingham Palace
to LS cricket ground the first thing I
would probably do is to get on Google
Maps so search
it
and let's put it
here so here we have the path you should
take to get to Lodge cricket ground now
here the map is a combination of red
yellow and blue now the blue regions
signify a clear road that is you won't
encounter traffic TR there yellow
indicate that they're slightly congested
and red means they're heavily congested
so let's look at the map a different
version of the same map and here as I
told you before red means heavily
congested yellow means slow moving and
blue means clear so how exactly is
Google able to tell you that the traffic
is clear slow moving or heavily
congested so this is with the help of
machine learning and with the help of
two important measures first is the
average time that's taken on specific
days at specific times on that route the
second one is the realtime location data
of vehicles from Google Maps and with
the help of sensors some of the other
popular map services are Bing Maps
maps.me and here we go next up we have
social media personalization so say I
want to buy a drone and I'm on Amazon
and I want to buy a DJI mavic Pro the
thing is it's close to one lap so I
don't want to buy it right now but the
next time I'm on Facebook I'll see an
advertisement for the product next time
I'm on YouTube I'll see an advertisement
even on Instagram I'll see an
advertisement so here with the help of
machine learning Google has understood
that I'm interest interested in this
particular product hence it's targeting
me with these advertisements this is
also with the help of machine learning
let's talk about email spam filtering
now this is a spam that's in my inbox
now how does Gmail know what's spam and
what's not spam so Gmail has an entire
collection of emails which have already
been labeled as spam or not spam so
after analyzing this data Gmail is able
to find some characteristics like the
word lottery or winner from then on any
new email that comes to your inbox goes
through a few spam filters to decide
whether it's spam or or not now some of
the popular spam filters that Gmail uses
is content filters header filters
General Blacklist filters and so on next
we have online fraud detection now there
are several ways that online fraud can
take place for example there's identity
theft where they steal your identity
fake accounts where these accounts only
last for how long the transaction takes
place and stop existing after that and
man in the middle attacks where they
steal your money while the transaction
is taking place the feed forward neural
network helps determine whether a
transaction is genuine or fraudulent so
what happen happens with feed for in
unit networks are that the outputs are
converted into hash values and these
values become the inputs for the next
round so for every real transaction that
takes place there's a specific pattern a
fraudulent transaction would stand out
because of the significant changes that
it would cause with the hash values
Stock Market trading machine learning is
used extensively when it comes to Stock
Market trading now you have stock market
indices like nikai they use long
shortterm memory neural networks now
these are used to classify process and
predict data when there are time lags of
unknown size and duration now this is
used to predict stock market trends
assisted medical technology now medical
technology has been innovated with the
help of machine learning diagnosing
diseases has been easier from which we
can create 3D models that can predict
where exactly there are lesions in the
brain it works just as well for brain
tumors and Ice kemic stroke lesions they
can also be used in fetal Imaging and
cardiac analysis now some of the medical
fields that machine learning will help
assist in his disease identification
personalized treatment drug Discovery
clinical research and ideology and
finally we have automatic translation
now say you're in a foreign country and
you see Billboards and signs that you
don't understand that's where automatic
translation comes of help now how does
automatic translation actually work the
technology behind it is the same as the
sequence to sequence learning which is
the same thing that's used with chat
Bots here the image recognition happens
using convolutional neural networks and
the text is identified using optical
character recognition furthermore the
sequence to sequence algorithm is also
used to translate the text from one
language to the other finding a suitable
job in the field of machine learning is
becoming increasingly difficult the
ideal way to display your machine
learning skill is in the form of
portfolio of data science and machine
learning projects a solid portfolio of
projects will illustrate that you can
utilize those machine learning skills in
your profile as well projects like movie
reccommendation system fake news
detection and many more are the best way
to improve your early programming skills
you may have the knowledge but putting
it to the use what is keep you
competitive here are 10 machine learning
projects that can increase your
portfolio and enable you to acquire a
job as a machine learning engineer at
number 10 we have loan approval
prediction s system in this machine
learning project we will analyze and
make prediction about the loan approval
process of any person this is a
classification problem in which we must
determine whether or not the loan will
be approved a classification problem is
a predictive modeling problem that
predict a class label for a given
example of input data some
classification problem include spam
email cancer detection sentiment
analysis and many more you can check the
project link from the description box
below to understand classification
problem and how to build a loan
prediction system at number nine we have
fake news jaction system do you believe
in everything you read in social media
isn't it true that not all news is true
but how will you recognize fake news ml
is the answer you will able to tell the
difference between real and fake news by
practicing this project of detecting
fake news this ml project for detecting
fake news is concerned with the fake
news and the true news on our data set
we create a tfid vectorizer with escalan
the model is then fitted using a passive
aggressive classifier that has been
initialized finally the accuracy score
and the confusion Matrix in indicate how
well our model performs the link for the
project is in the description box below
at number eight we have personality
prediction system the idea is based on
determining an individual personality
using machine learning techniques a
person personality influences both his
personal and professional life nowadays
many company are shortlisting applicant
based on their personality which
increases job efficiency because the
person is working on what he is good at
rather than what is compelled to do in
our study we attempted to combine
personality prediction system using
machine learning techniques such as SVD
na base and logistic regession to
predict a person personality and talent
prediction using phase frequency method
this model or method allows users to
recognize their personality and
Technical abilities easily to learn
about Moree this project check the link
in the description box below at number
seven we have Parkinson disease system
Parkinson disease is a progressive
central nervous system element that
affects movement and cause tremors and
stiffness it comprises five stages and
affects more than 1 million worldwide
each other in this machine Lear learning
project we will develop an svm model
using python modules psychic learn numpy
and pandas and svm we will import the
data extract the features and label and
scale the features split the data set
design an N model and calculate the
model accuracy and at the end we will
check the Parkinson disease for the
individual to learn about more this
project check the link in the
description box below add number six we
have text to speech converter
application the machine learning domain
of audio is undoubtly cutting as right
now the majority of the application
available today are the commercial the
community is building several audio
specific open source framework and
algorithm other text to speech apis are
available for this project we will
utilize pyttsx3 pyttsx3 is a python text
to speech conversion Library it operates
offline unlike other libraries and is
compatible with python 2 and Python 3
before API various pre-trained models
were accessible in Python but changing
the voice of volume was often difficult
it also needed additional computational
power to learn more about this project
check the link in the description box
below at number five we have speech
recognition system speech recognition
often known as speech to text is the
capacity of a machine or program to
recognize and transfer word spoken
allowed into readable text MLS speech
recognition uses algorithm that model
speech in terms of both language and
sound to extract the more important
parts of the speech such as words
sentences and acostic modeling is used
to identify the phenomes and the
phonetics on the speech for this project
we will utilize pyttsx3 pyttsx3 is a
python text to speech conversion Library
it operates offline unlike other
libraries and is compatible with python
2 and Python 3 to learn more about this
project check the link in the
description box below at number four we
have sentiment analysis sentiment
analysis also known as opinion mining is
a straightforward process of determining
the author's feeling about a text what
was the user intention when he or she
wrote something to determine what could
be personal information we employ a
variety of natural language processing
and text analysis technology we must
detect extract and quantify such
information from the text to enable
classification and data manipulation in
this project we will use the Amazon
customer review data set for the
sentiment analysis check the link in the
description box below at number three we
have image classification using CNN deep
learning is a booving field currently
most projects and problem statement used
deep learning is and any sort of work
many of you like myself would choose a
conventional neural network as a deep
learning technique for answering any
computer vision problem statement in
this project we will use CNN to develop
an image processing project and learn
about its capabilities and why it has
become so popular we will go over each
stage of creating our CNN model and our
first spectacular project we will use
the CFI 10 data set for image
classification in this project to learn
more about this project check the link
in the description box below at number
two we have face recognition system
currently technology absolutely amazes
people with Incredible invention that
makes life easier and more comfortable
face recognition has shown to be the
least intrusive and fastest form of the
biometric verification over time this
project will use open CV and face
recognition libraries to create a phas
detection system open CV provides a
realtime computer vision tool library
and Hardware we can create amazing
realtime projects using open CV to learn
how to create face recognition system
for you check the link in the
description box below and last but not
the least we have movie recommendation
system almost everyone today use
technology to stream movies and
television show while figuring out what
to stream next can be disheartening
recommendation are often made based on a
viewer history and preferences this is
done through a machine learning and can
be a fun and the easy project for the
beginners new programmers can practice
by coding in either python or R and with
the data from the movie lens data set
generated by the more than 6,000 users
to learn how to create movie
recommendation system for yourself or
for your loved ones check the project in
the description box below human versus
artificial
intelligence humans are amazing let's
just face it we're amazing creatures
we're all over the planet we're
exploring every nich and Nook we've gone
to the Moon uh we've got into outer
space we're just amazing creatures we're
able to use the available information to
make decisions to communicate with other
people identify patterns and data
remember what people have said adapt to
new situations so let's take a look at
this so so you can get a picture you're
a human being so you know what it's like
to be human let's take a look at
artificial intelligence versus the human
artificial intelligence develops
computer systems that can accomplish
tasks that require human
intelligence so we're looking at this
one of the things that computers can do
is they can provide more accurate
results this is very important recently
I did a project on cancer where it's
identifying
markers and as a human being you look at
that and you might be uh looking at all
the different images and the data that
comes off of them and say say I like
this person so I want to give them a
very
good um Outlook and the next person you
might not like so you want to give them
a bad Outlook well with artificial
intelligence you're going to get a
consistent prediction of what's going to
come out interacts with humans using
their natural language we've seen that
as probably the biggest development
feature right now that's in the
commercial Market that everybody gets to
use as we saw with the example of Alexa
they learn from their mistakes and adapt
to new environments so we see this
slowly coming in more and more and they
learn from the data and automate
repetitive learning repetitive learning
has a lot to do with the neural networks
you have to program thousands upon
thousands of pictures in there and it's
all automated so as today's computers
evolved it's very quick and easy and
affordable to do this what is machine
learning and deep learning all about
imagine this say you had some time to
waste not that any of us really have a
lot of time anymore to just waste in
today's world
and you're sitting by the road and you
have a whole lot of and a whole lot of
time passes by there's a few hours and
suddenly you wonder how many cars buses
trucks and so on passed by in the six
hours now chances are you're not going
to sit by the road for 6 hours and count
buses cars and trucks unless you're
working for the city and you're trying
to do City Planning and you want to know
hey do we need to add a new truck route
maybe we need a Bicycle Link we have a
lot of bicyclists here that kind of
thing so maybe City Planning would be
great for this machine learning well the
way machine Learning Works is we have
labeled data with
features okay so you have a truck or a
car a motorcycle a bus or a bicycle and
each one of those are labeled it comes
in and based on those labels and
comparing those features it gives you an
answer it's a bicycle it's a truck it's
a motorcycle let's look a little bit
more in depth on this in the model here
it actually the features we're looking
at would be like the tires someone sits
there and figures out what a tire looks
like takes a lot of work if you try to
try to figure the difference between a
car tire a bicycle tire a motorcycle
tire uh so in the machine learning field
this could take a long time if you're
going to do each individual aspect of a
car and try to get a result on there and
that's what they did do that was a a
very this is still used on smaller
amounts of data where you figure out
what those features are and then you
label them deep learning so with deep
learning one of our Solutions is to take
a very large unlabeled data set and we
put that into a training model using
artificial neural networks and then that
goes into the neural network itself and
we create a neural network and you'll
see um the arrows are actually kind of
backward but uh which actually is a nice
point because when we train the neural
network we put the bicycle in and then
it comes back and says if it said truck
it comes back and says well you need to
change that to B bicycle and then it
changes all those weights going backward
they call it back propagation and let it
know it's a bicycle and that's how it
learns once you've trained the neural
network you then put the new data in and
they call this testing the model so you
need to have some data you've kept off
to the side where you know the answer to
and you take that and you provide the
required output and you say okay is this
is this neural network working correctly
did it identify a bike as a bike a truck
is a truck a motorcycle as a motorcycle
let's just take a little closer look at
that
determining what objects are present in
the data so how does deep learning do
this and here we have the image of the
bike it's 28 by 28 pixels that's a lot
of information there um could you
imagine trying to guess that this is a
bicycle image by looking at each one of
those pixels and trying to figure out
what's around it uh and we actually do
that as human beings it's pretty amazing
we know what a bicycle is and even
though it comes in is all this
information and what this looks like is
the image comes in it converts it into
into a bunch of different nodes in this
case there's a lot more than what they
show here and it goes through these
different layers and outcomes and says
okay this is a
bicycle a lot of times they call this
the magic Black Box why because as we
watch it go across here all these
weights and all the math behind this and
it's not it's a little complicated on
the math side you really don't need to
know that when you're programming or
doing working with the Deep learning but
it's like magic you you don't know you
really can't figure out what's going to
come out by looking what's in each one
of those dots and each one of those
lines are firing and what's going in
between them so we like to call it the
magic box uh so that's where deep
learning comes in and in the end it
comes up and you have this whole neural
notwork it comes up and it says okay we
fire all these different pixels and we
connects all these different dots and
gives them different weights and it says
okay this is a bicycle and that's how we
determine what the object is present in
the data with deep learning machine
learning we're going to take a step into
machine learning here and you'll see how
these fit together in a minute minut it
the system is able to make predictions
or take decisions based on past data
that's very important for machine
learning is that we're looking at stuff
and based on what's been there before
we're creating a decision on there we're
creating something out of there we're
coloring a beach ball we're telling you
what the weather is in Chicago what's
nice about machine learning is a very
powerful processing capability it's
quick and accurate outcomes so you get
results right way once you program the
system the results are very fast and the
decisions and predictions are better
they're more accurate they're consistent
you can analyze very large amounts of
data some of these data things that
they're analyzing now are pedabytes and
terabytes of data it would take hundreds
of people hundreds of years to go
through some of this data and do the
same thing that the machine learning can
do in a very short period of time and
it's inexpensive compared to hiring
hundreds of people so it becomes a very
affordable way to move into the future
is to apply the machine learning to
whatever businesses you're working on
and deep Learning Systems think and
learn like humans using artificial
neural networks again it's like a magic
box performance improves with more data
so the more data the Deep learning gets
the more it gives you better results
it's scalability so you can scale it up
you can scale it down you can increase
what you're looking at currently you
know we're Limited by the amount of
computer processing power as to how big
that can get but that envelope
continually gets pushed every day on
what it can do problem solved in an end
to end method so instead of having to
break it apart and you have the first
piece coming in and you identify tires
and the second piece is identifying uh
labeling handlebars and then you bring
that together that if it has handlebars
and tires it's a bicycle and if it has
something that looks like a large Square
it's probably a truck the Neal networks
does this all in one network you don't
really know what's going on in all those
weights and all those little bubbles uh
but it does it pretty much in one
package that's why the neural network
systems are so big nowadays and coming
into their own best features are
selected by the system and it this is
important they kind of put it it's on a
bullet on the side here it's a subset of
machine learning this is important when
we talk about deep learning it is a form
of machine learning there's lots of
other forms of machine learning data
analysis but this is the newest and
biggest thing that they apply to a lot
of different packages and they use all
the other machine learning tools
available to work with it and it's very
fast to test um you put in your
information you then have your group of
uh test and then you held some aside you
see how does it do it's very quick to
test it and see what's going on with
your deep learning and your neural
network are they really all that
different H AI versus machine learning
versus deep learning
concepts of AI so we have concepts of AI
you'll see natural language processing
uh machine learning an approach to
create artificial intelligence so it's
one of the subsets of artificial
intelligence knowledge representation
automated reasoning computer vision
robotics machine learning versus AI
versus deep learning or Ai and machine
learning and deep
learning so we look at this we have ai
with machine learning and deep learning
and so we're going to put them all
together we find out that AI is a big
picture we have a collection of books it
goes through some deep learning the
Digital Data is analyzed text mining
comes through the particular book you're
looking for maybe it's a genre books is
identified and in this case uh we have a
robot that goes and gives a book to the
patron I have yet to be at a library
that has a robot bring me a book but
that will be cool when it happens uh so
we look at some of the pieces here this
information goes into uh as far as this
example the translation of the
handwritten printed data to digital form
that's pretty hard to do that's pretty
hard to go in there and translate
hundreds and hundreds of books and
understand what they're trying to say if
you've never read them so in this case
we use the Deep learning because you can
already use examples where they've
already classified a lot of books and
then they can compare those texts and
say oh okay this is a book on autom mod
of repair this is a book on robotic
building the Digital Data is in analyzed
then we have more text mining using
machine learning so maybe we'd use a
different program to do a basic classify
uh what you're looking for and say oh
you're looking for auto repair and
computers so you're looking for
automated cars once it's identified then
of course it brings you the
book so here's a nice summation of what
we were just talking about AI with
machine learning and deep learning deep
learning learning is a subset of machine
learning which is a subset of artificial
intelligence so you can look at
artificial intelligence as the big
picture how does this compare to The
Human Experience in either uh doing the
same thing as a human we do or it does
it better than us and machine learning
which has a lot of tools uh is something
that learns from data past experiences
it's programmed it's uh comes in there
and it says hey we already had these
five things happen the six one should be
about the same and then uh then there's
a lot of tools in machine learning but
deep learning then is a very specific
tool in machine learning it's the
artificial neural network which handles
large amounts of data and is able to
take huge pools of experiences pictures
and ideas and bring them together real
life
examples artificial intelligence news
generation very common nowadays as it
goes through there and finds the news
articles or generate the news based upon
the news feeds or the backend coming in
and says okay let's give you the actual
news based on this there's all the
different things Amazon Echo they have a
number of different Prime music on there
of course there's also the Google
command and there's also Cortana there's
tons of smart home devices now where we
can ask it to turn the TV on or play
music for us that's all artificial
intelligence from front to back you're
having a human experience with these
computers and these objects that are
connected to the processing machine
learning uh spam detection very common
machine learning doesn't really have the
human interaction part so this is the
part where it goes and says okay that's
a Spam that's not a Spam and it puts it
in your spam folder search engine result
refining uh another example of machine
learning whereas it looks at your
different results and it Go and it uh is
able to categorize them as far as this
had the most hits this is the least
viewed this has five stars um you know
however they want to wait it uh all exam
good examples of machine learning and
then the Deep learning uh deep learning
another example is as you have like a
exit sign in this case is translating it
into French sorti I hope I said that
right um neural network has been
programmed with all these different
words and images and so able to look at
the exit in the middle and it goes okay
we want to know what that is in French
and it's able to push that out in French
French and learn how to do
that and then we have chatbots um I
remember when Microsoft first had their
little paperclip um Bo that was like a
long time ago that came up and you would
type in there and chat with it these are
growing you know it's nice to just be
able to ask a question and it comes up
and gives you the answer and instead of
it being where you just doing a search
on certain words it's now able to start
linking those words together and form a
sentence in that chat box types of AI
and machine
learning types of artificial
intelligence this in the next few slides
are really important so one of the types
of artificial intelligence is reactive
machines systems that only react they
don't form memories they don't have past
experiences they have something that
happens to them and they react to it my
washing machine is one of those if I I
put a ton of clothes in it and they had
all clumped on one side it automatically
adds a weight to reciter it so that my
washing machine is actually a reactive
machine working with whatever the load
is and keeps it nice and so when it
spins it doesn't go thumping against the
side limited memory another form of
artificial intelligence systems look
into the past information is added over
a period of time and information is
shortlived when we're talking about this
and you look at get like a neural
network that's been programmed to
identify cars it doesn't remember all
those pictures it has no memory as far
as the hundreds of pictures you process
through it all it has is this is the
pattern I use to identify cars as the
final output for that neural network we
looked at so when they talk about
limited memory this is what they're
talking about they're talking about I've
created this based on all these things
but I'm not going to remember anyone
specifically theory of Mind systems
being able to understand human emotions
and and how they affect decision-making
to adjust their behaviors according to
their human understanding this is
important because this is our page mark
this is how we know whether it is an
artificial intelligence or not is it
interacting with humans in a way that we
can
understand uh without that interaction
is just an object uh so when we talk
about theory of mind we really
understand how it interfaces that whole
if you're in web development user
experience would be the term I would put
in there so theory of of mind would be
user experience how is the whole UI
connected together and one of the final
things as we get into artificial
intelligence is systems being aware of
themselves understanding their internal
States and predicting other people's
feelings and act appropriately so as
artificial intelligence continues to
progress uh we see ones are trying to
understand well what makes people happy
how would they increase our happiness uh
how would they keep themselves from
breaking down if something's broken
inside they have that self-awareness to
be able to fix fix it and just based on
all that information predicting which
action would work the best what would
help people uh if I know that you're
having a cup of coffee first thing in
the morning is what makes you happy as a
robot I might make you a cup of coffee
every morning at the same time uh to
help your life and help you grow that'd
be the self-awareness is being able to
know all those different things types of
machine learning and like I said on the
last slide this is very important this
is very important if you decide to go in
and get certified in machine learning or
know more about it these are the three
primary types of machine learning the
first one is supervised learning systems
are able to predict future outcome based
on past data requires both an input and
an output to be given to the model for
it to be trained so in this case we're
looking at anything where you have 100
images of a
bicycle and those 100 images you know
are bicycle so it's they're preset
someone already looked at all 100 images
and said these are pictures of bicycles
and so the computer learns from those
and then it's given another picture and
maybe the next picture is a bicycle and
it says oh that resembles all these
other bicycles so it's a bicycle and the
next one's a car and it says it's not a
bicycle that would be supervised
learning because we had to train it we
had to supervise it unsupervised
learning systems are able to identify
hidden patterns from the input data
provided by making the data more
readable and organized the patterns simp
similarities or anomalies become more
evident uh you'll heard the term cluster
how do you cluster things together some
of these things go together some of
these don't this is unsupervised where
can look at an image and start pulling
the different pieces of the image out
because they aren't the same the human
all the parts of the human are not the
same as a fuzzy tree behind them it's
slightly out of focus which is not the
same as the beach ball it's unsupervised
because we never told it what a beach
ball was we never told it what the human
was and we never told it that those were
trees
all we told it was hey separate this
picture by things that don't match and
things that do match and come together
and finally there's reinforcement
learning systems are given no training
it learns on the basis of the reward
punishment it received for performing
its Last Action it helps increase the
efficiency of a tool function or a
program reinforced learning or
reinforcement learning is kind of you
give it a yes or no yes you gave me the
right response no you didn't and then it
looks at that and says oh okay so based
on this data coming in uh what I gave
you was a wrong response so next time
I'll give you a different one comparing
machine learning and deep learning so
remember that deep learning is a
subcategory of machine learning so it's
one of the many tools and so they we're
grouping a ton of machine learning tools
all together linear regression K means
clustering there's all kinds of cool
tools out there you can use in machine
learning enables machines to take
decisions to make decisions on their own
based on past data enables machines to
make decisions with the help of
artificial neural networks so it's doing
the same thing but we're using an
artificial neural network as opposed to
one of the more traditional machine
learning tools needs only a small amount
of training data this is very important
when you're talking about machine
learning they're usually not talking
about huge amounts of data we're talking
about maybe your spreadsheet from your
business and your totals for the end of
the year when you're talking about
neural networks you usually need a large
amount of data to train the data so
there's a lot of training involved if
you have under 500 points of data that's
probably not going to go into machine
learning or maybe have like the case of
one of the things 500 points of data and
30 different fields it starts getting
really confusing there in artificial
intelligence or machine learning and the
Deep learning aspect really shines when
you get to that larger data that's
really
complex works well on a low-end systems
so a lot of the machine learning tools
out there you can run on your laptop
no problem and do the calculations there
where with the machine learning usually
needs a higher end system to work it
takes a lot more processing power to
build those neural networks and to train
them it goes through a lot of data we're
talking about the general machine
learning tools most features need to be
identified in advanced and manually
coded so there's a lot of human work on
here the machine learns the features
from the data it is provided so again
it's like a magic box you don't have to
know what a tire is it figures it out
for you the problem is divided into
parts and solved individually and then
combined so machine learning you usually
have all these different tools and use
different tools for different parts and
the problem is solved in an end to end
manner so you only have one neural
network or two neural networks that is
bringing the data in and putting it out
that's not going through a lot of
different processes to get there and
remember you can put machine learning
and deep learning together so you don't
always have just the Deep learning
solving the problem you might have
soling one piece of the puzzle with
regular machine learning and most of
machine learning tools out there they
they take longer to test and understand
how they work and with the Deep learning
it's pretty quick once you build that
neural network you test it and you know
so we're dealing with very crisp rules
limited resources you have to really
explain how the decision was made when
you use most machine learning tools but
when you use the Deep learning tool
inside the machine learning tools the
system takes care of it based on its own
logic and reasoning and again it's like
a magic Black Box you really don't know
how it came up with the answer you just
know it came up with the right answer a
glimpse into the future so a quick
glimpse into the future artificial
intelligence be using it to detecting
crimes before they happen humanoid AI
helpers which we already have a lot of
there'll be more and more maybe it'll
actually be Androids that'd be cool to
have an Android that comes and get stuff
out of my fridge for me machine learning
increasing efficiency in healthc care
that's really big in all the forms of
machine learning better marketing
techniques any of these things if we get
into the Sciences it's just off the
scale machine learning and artificial
intelligence go everywhere and then the
subcategory Deep learning increased
personalization so what's really nice
about the Deep learning is it's going to
start now catering to you that'll be one
of the things we see more and more of
and we'll have more of a hyper
intelligent personal assistant I'm
excited about that what are the
different types of machine learning
algorithms machine learning algorithms
are broadly classified into three types
the supervised learning unsupervised
learning and reinforcement learning
supervised learning in turn consists of
techniques like regression and
classification and unsupervised learning
we use techniques like Association and
clustering and reinforcement learning is
a recently developed technique and it is
very popular in gaming some of you must
have heard about alphao so this was
developed using reinforcement learning
primary difference between supervised
learning and unsupervised learning
supervised learning is used when we have
historical data and we have labeled data
which means that we know how the data is
classified so we know the classes if we
are doing classification or we know the
values when we are doing regression so
if we have historical data with these
values which are known as labels then we
use supervised learning in case of
unsupervised learning we do not have
past labeled data historical labeled
data so we use techniques like
Association and clustering to maybe form
clusters or new classes maybe and then
we move from there in case of
reinforcement learning the system learns
pretty much from scratch there is an
agent and there is an environment the
agent is given a certain Target and it
is rewarded when it is moving towards
that Target and it is penalized if it is
moving in a direction which is not
achieving that Target so it's more like
a carrot and stick model so what is the
difference between these three types of
algorithms supervised algorithms or
supervised learning algorithms are used
when you have a specific Target value
that you would like to predict the
target could be categorical having two
or more possible outcomes or classes if
you will that is what is classification
or the target could be a a value which
can be measured and that's where we use
regression like for example whether
forecasting you want to find the
temperature whereas in classification
you want to find out whether this is a
fraud or not a fraud or if it is email
spam whether it is Spam or not spam so
that is a classification example so if
you know or this is known as labeled
information if you have the labeled
information then you use supervised
learning in case of unsupervised
learning we have input data but we don't
have the labels or what the output is
supposed to be so that is when we use
unsupervised learning techniques like
clustering and Association and we try to
analyze the data in case of
reinforcement learning it allows the
agent to automatically determine the
ideal Behavior within a specific context
and it has to do this to maximize the
performance like for example playing a
game so the agent is told that you need
to score the maximum score possible
without losing lives so that is a Target
that is given to the agent and it is
allowed to learn from scratch play the
game itself multiple times and slowly it
will learn the behavior which will
increase the score and keep the lives to
the maximum if you want to become an AI
expert and gain handsome salary packages
look at the wide range of AIML courses
by simply learn in collaboration with
top universities across Ross the globe
by enrolling in any of these
certification programs you will gain
expertise in skills like generative AI
prompt engineering chat GPT explainable
AI machine learning algorithms
supervised and unsupervised learning
model training and optimization and
there's much more on the list with
hands-on experience in the tools like
chart GPT di python open CV and tens
oflow you will catch the eyes of top
recruiters so what are you waiting for
hurry up and enroll now an year of
experience is preferred to enroll in
these courses find the course Link in
the description boxs when we look at our
different machine learning algorithms we
can divide them into three areas
supervised
unsupervised reinforcement we're only
going to look at supervised today
unsupervised means we don't have the
answers we're just grouping things
reinforcement is where we give positive
and negative feedback to our algorithm
to program it and it doesn't have the
information till after the fact but
today we're just looking at supervised
because that's where linear regression
fits in in supervised data we have our
data already there and our answers for a
group and then we use that to program
our model and come up with an answer the
two most common uses for that is through
the regression and classification now
we're doing linear regression so we're
just going to focus on the regression
side and in the regression we have
SIMPLE linear regression we have
multiple linear regression and we have
Pol nomial linear regression now on
these three simple linear regression is
the examples we've looked at so far
where we have a lot of data and we draw
a straight line through it multiple
linear regression means we have multiple
variables remember where we had the
rainfall and the crops we might add
additional variables in there like how
much food do we give our crops when do
we Harvest them those would be
additional information add into our
model and that's why it be multiple
linear regression and finally we have
polinomial linear regression that is
instead of drawing a line we can draw a
curved line through it now that you see
where regression model fits into the
machine learning algorithms and we're
specifically looking at linear
regression let's go ahead and take a
look at applications for linear
regression let's look at a few
applications of linear regression
economic growth used to determine the
economic growth of a country or a state
in the coming quarter can also be used
to predict the GDP of a country product
price can be used to predict what would
would be the price of a product in the
future we can guess whether it's going
to go up or down or should I buy today
housing sales to estimate the number of
houses a builder would sell and what
price in the coming months score
predictions Cricut fever to predict the
number of runs a player would score in
the coming matches based on the previous
performance I'm sure you can figure out
other applications you could use linear
regression for so let's jump in and
let's understand linear regression and
dig into the theory understanding linear
regression linear regression is the
statistical model used to predict the
relationship between independent and
dependent variables by examining two
factors the first important one is which
variables in particular are significant
predictors of the outcome variable and
the second one that we need to look at
closely is how significant is the
regression line to make predictions with
the highest possible accuracy if it's
inaccurate we can't use it so it's very
important we find out the most accurate
line we can get
since linear regression is based on
drawing a line through data we're going
to jump back and take a look at some
ukian geometry the simplest form of a
simple linear regression equation with
one dependent and one independent
variable is represented by yal m * x + C
and if you look at our model here we
plotted two points on here uh X1 and y1
X2 and Y2 y being the dependent variable
remember that from before and X being
the independent variable so y depends on
whatever X is m in this case is the
slope of the line where m equals the
difference in the Y 2 - y1 and X2 - X1
and finally we have C which is the
coefficient of the line or where happens
to cross the zero axis let's go back and
look at an example we used earlier of
linear regression we're going to go back
to plotting the amount of crop yield
based on the amount of rainfall and here
we have our rainfall remember we cannot
change rainfall and we have our crop
yield which is dependent on the rainfall
so we have our independent and our
dependent variables we're going to take
this and draw a line through it as best
we can through the middle of the data
and then we look at that we put the red
point on the y axis is the amount of
crop yield you can expect for the amount
of rainfall represented by the Green Dot
so if we have an idea what the rainfall
is for this year and what's going on on
that we can guess how good our crops are
going to be and we've created a nice
line right through the middle to give us
a nice mathematical formula let's take a
look and see what the math looks like
behind this let's look at the intuition
behind the regression line now before we
dive into the math and the formulas that
go behind this and what's going on
behind the scenes I want you to note
that when we get into the case study and
we actually apply some python script
that this math that you're going to see
here is already done automatically for
you you don't have to have it memorized
it is however good to have an idea
what's going on so if people reference
the different terms you'll know what
they're talking about let's consider a
sample data set with five rows and find
out how to draw the regression line
we're only going to do five rows because
if we did like the rainfall with
hundreds of points of data that would be
very hard to see what's going on with
the mathematics so we'll go ahead and
create our own two sets of data and we
have our independent variable X and our
dependent variable Y and when X was 1 we
got Y = 2 when X was uh 2 y was four and
so on and so on if we go ahead and plot
this data on a graph we can see how it
forms a nice line through the middle you
can see where it's kind of grouped going
upwards to the right the next thing we
want to know is what the means is of
each of the data coming in the X and the
Y the means doesn't mean anything other
than the average so we add up all the
number numbers and divide by the total
so 1 + 2 + 3 + 4 + 5 over 5 = 3 and the
same for y we get four if we go ahead
and plot the means on the graph we'll
see we get 3 comma 4 which draws a nice
line down the middle a good estimate
here we're going to dig deeper into the
math behind the regression line now
remember before I said you don't have to
have all these formulas memorized or
fully understand them even though we're
going to go into a little more detail of
how it works and if you're not a math
whz and you don't know if you've never
seen the sigma character before which
looks a little bit like an e that's
opened up that just means summation
that's all that is so when you see the
sigma character it just means we're
adding everything in that row and for
computers this is great because as a
programmer you can easily iterate
through each of the XY points and create
all the information you need so in the
top half you can see where we' broken
that down into pieces and as it goes
through the first two points it computes
the squared value of x the squared value
of y and x * Y and then it takes all of
X and adds them up all of Y adds them up
all of X squ adds them up and so on and
so on and you can see we have the sum of
equal to 15 the sum is equal to 20 all
the way up to x * Y where the sum equals
66 this all comes from our formula for
calculating a straight line where y
equal the slope * X plus the coefficient
C so we go down below and we're going to
compute more like the averages of these
and we're going to explain exactly what
that is in just a minute and where that
information comes from it's called the
square means error but we'll go into
that in detail in a few minutes all you
need to do is look at the formula and
see how we've gone about Computing it
line by line instead of trying to have a
huge set of numbers pushed into it and
down here you'll see where the slope m
equals and in the top part if you read
through the brackets you have the number
of data points times the sum of x * Y
which we computed one line at a time
there and that's just the 66 and take
all that and you subtract it from the
sum of x times the sum of Y and those
have both been computed so you have 15 *
20 and on the bottom we have the number
of lines times the sum of x^2 easily
computed as 86 for the sum minus I'll
take all that and subtract the sum of X
squ and we end up as we come across with
our formula you can plug in all those
num numbers which is very easy to do on
the computer you don't have to do the
math on a piece of paper or calculator
and you'll get a slope of 6 and you'll
get your C coefficient if you continue
to follow through that formula you'll
see it comes out as equal to 2.2
continuing deeper into what's going
behind the scenes let's find out the
predicted values of Y for corresponding
values of X using the linear equation
where m equal 6 and C = 2.2 we're going
to take these values and we're going to
go ahead and plot them we're going to
predict them so y = 6 * or x = 1 + 2.2 =
2.8 so on and so on and here the Blue
Points represent the actual y values and
the brown points represent the predicted
y values based on the model we created
the distance between the actual and
predicted values is known as residuals
or errors the best fit line should have
the least sum of squares of these errors
also known as e square if we put these
into a nice chart where you can see X
and you can see Y where we actual values
were and you can see y predicted you can
easily see where we take Yus y predicted
and we get an answer what is the
difference between those two and if we
square that Yus y prediction squared we
can then sum those squared values that's
where we get the 64 plus the 36 + 1 all
the way down until we have a summation
equals 2.4 so the sum of squared errors
for this regression line is 2.4 we check
this error for for each line and
conclude the best fit line having the
least e Square value in a nice graphical
representation we can see here where we
keep moving this line through the data
points to make sure the best fit line
has the least Square distance between
the data points and the regression line
now we only looked at the most commonly
used formula for minimizing the distance
there are lots of ways to minimize a
distance between the line and the data
points like sum of squared errors sum of
absolute errors root means square error
Etc what you want to take away from this
is whatever formula is being used you
can easily using a computer programming
and iterating through the data calculate
the different parts of it that way these
complicated formulas you see with the
different summations and absolute values
are easily computed one piece at a time
up until this point we've only been
looking at two values X and Y well in
the real world it's very rare that you
only have two values when you're
figuring out a solution so let's move on
to the next topic multiple linear
regression let's take a brief look at
what happens when you have multiple
inputs so in multiple linear regression
we have uh well we'll start with the
simple linear regression where we had y
= m + x + C and we're trying to find the
value of y now with multiple linear
regression we have multiple variables
coming in so instead of having just X we
have X1 X2 X3 and instead of having just
one slope each variable has its own
slope attached to it as you can see here
we have M1 M2 M3 and we still just have
the single coefficient so when you're
dealing with multiple linear regression
you basically take your single linear
regression and you spread it out so you
have yal M1 * X1 + M2 * X2 so on all the
way to m to the nth x to the nth and
then you add your coefficient on there
implementation of linear regression now
we get into my favorite part let's
understand how multiple linear
regression works by implementing it in
Python if you remember before we were
looking at a company and just based on
its R&D trying to figure out its profit
we're going to start looking at the
expenditure of the company we're going
to go back to that we're going to
predict its profit but instead of
predicting it just on the R&D we're
going to look at other factors like
Administration costs marketing costs and
so on and from there we're going to see
if we can figure out what the profit of
that company is going to be to start our
coding we're going to going to begin by
importing some basic libraries and we're
going to be looking through the data
before we do any kind of linear
regression we're going to take a look at
the data to see what we're playing with
then we'll go ahead and format the data
to the format we need to be able to run
it in the linear regression model and
then from there we'll go ahead and solve
it and just see how valid our solution
is so let's start with importing the
basic libraries now I'm going to be
doing this in Anaconda Jupiter notebook
a very popular IDE I enjoy it it's such
a visual to to look at and so easy to
use um just any ID for python will work
just fine for this so break out your
favorite python IDE so here we are in
our Jupiter notebook let me go ahead and
paste our first piece of code in there
and let's walk through what libraries
we're importing first we're going to
import numpy as NP and then I want you
to skip one line and look at import
pandas as PD these are very common tools
that you need with most of your linear
regression the numpy which stands for
number python is usually denoted as NP
and you have to almost have that for
your sklearn toolbox you always import
that right off the beginning pandas
although you don't have to have it for
your sklearn libraries it does such a
wonderful job of importing data setting
it up into a data frame so we can
manipulate it rather easily and it has a
lot of tools also in addition to that so
we usually like to use the pandas when
we can and I'll show you what that looks
like the other three lines are for us to
get a visual of this data and take a
look at it so we're going to import
matplot library. pyplot as PLT and then
caborn as SNS caborn works with the map
plot Library so you have to always
import map plot library and then caborn
sits on top of it and we'll take a look
at what that looks like you could use
any of your own plotting libraries you
want there's all kinds of ways to look
at the data these are just very common
ones and the caborn is so easy to use it
just looks beautiful it's a nice
representation that you can actually
take and show somebody and the final
line is the Amber sign map plot library
in line that is only because I'm doing
an inline IDE my interface in the
Anaconda Jupiter notebook requires I put
that in there or you're not going to see
the graph when it comes up let's go
ahead and run this it's not going to be
that interesting because we're just
setting up variables in fact it's not
going to do anything that we can see but
it is importing these different
libraries and setup the next step is
load the data set and extract
independent and dependent variables now
here in the slide you'll see companies
equals pd. read CSV and it has a long
line there with the file at the end
1,000 companies. CSV you're going to
have to change this to fit whatever
setup you have and the file itself you
can request just go down to the
commentary below this video and put a
note in there and simply learn we try to
get in contact with you and Supply you
with that file so you can try this
coding yourself so we're going to add
this code in here and we're going to see
that I have companies equals pd. reader
CSV and I've changed this path to match
my computer c/s simply learn
/1000 companies. CSV and then below
there we're going to set the x equals to
companies under the iocation and because
this is companies as a PD data set I can
use this nice notation that says take
every row that's what the colon the
first colon is comma except for the last
column that's what the second part is
where we have a colon minus one and we
want the values set into there so X is
no longer a data set a pandas data set
but we can easily extract the data from
our pandas data set with this not
notation and then y we're going to set
equal to the last row well the question
is going to be what are we actually
looking at so let's go ahead and take a
look at that and we're going to look at
the companies. head which lists the
first five rows of data and I'll open up
the file in just a second so you can see
where that's coming from but let's look
at the data in here as far as the way
the panda sees it when I hit run you'll
see it breaks it out into a nice setup
this is what pandas one of the things
pandas is really good about is it looks
just like an Excel spreadsheet you have
your rows and remember when we're
programming we always start with zero we
don't start with one so it shows the
first five rows 0 1 2 3 4 and then it
shows your different columns R&D spend
Administration marketing spend State
profit it even notes that the top are
column names it was never told that but
pandas is able to recognize a lot of
things that they're not the same as the
data rows why don't we go ahead and open
this file up in a CSV so you can
actually see the raw data so here I've
opened it up as a text editor and you
can see at the top we have R&D spin
comma Administration comma marketing
spin comma State comma profit carriage
return I don't know about you but I go
crazy trying to read files like this
that's why we use the pandas you could
also open this up in an Excel and it
would separate it since it is a comma
separated variable file but we don't
want to look at this one we want to look
at something we can read rather easily
so let's flip back and take a look at
that top part the first five row now as
nice as this format is where I can see
the data to me it doesn't mean a whole
lot maybe you're an expert in business
and Investments and you understand what
$165,300
120 compared to the administration cost
of
$136,800 so on so on helps to create the
profit of
192,25
183 that makes no sense to me whatsoever
no pun intended so let's flip back here
and take a look at our next set of code
where we're going to graph it so we can
get a better understand understanding of
our data and what it mean so at this
point we're going to use a single line
of code to get a lot of information so
we can see where we're going with this
let's go ahead and paste that into our
uh notebook and see what we got going
and so we have the visualization and
again we're using SNS which is pandas as
you can see we imported the map plot
library. pyplot as PLT which then the
Seaborn uses and we imported the caborn
as SNS and then that final line of code
helps us show this in our um inline
coding without this it wouldn't display
and you can display it to a file and
other means and that's the mat plot
Library inline with the Amber sign at
the beginning so here we come down to
the single line of code caborn is great
because it actually recognizes the panda
data frame so I can just take the
companies. core for coordinates and I
can put that right into the Seaborn and
when we run this we get this beautiful
plot and let's just take a look at what
this plot means if you look at this plot
on mine the colors are probably a little
bit more purplish and blue than the
original one uh we have the columns and
the rows we have R and D spending we
have Administration we have marketing
spending and profit and if you cross
index any two of these since we're
interested in profit if you cross index
profit with profit it's going to show up
if you look at the scale on the right
way up in the dark why because those are
the same data they have an exact
correspondence so R&D spending is going
to be the same as R&D spending and the
same thing with ad ministration Coster
right down the middle you get this dark
row or dark um diagonal row that shows
that this is the highest corresponding
data that's exactly the same and as it
becomes lighter there's less connections
between the data so we can see with
profit obviously profit is the same as
profit and next it has a very high
correlation with R&D spending which we
looked at earlier and it has a slightly
less connection to marketing spending
and even less to how much money we put
into the administration so so now that
we have a nice look at the data let's go
ahead and dig in and create some actual
useful linear regression models so that
we can predict values and have a better
profit now that we've taken a look at
the visualization of this data we're
going to move on to the next step
instead of just having a pretty picture
we need to generate some hard data some
hard values so let's see what that looks
like we're going to set up our linear
regression model in two steps the first
one is we need to prepare some of our
data so it fits correctly and let's go
ahead and paste this code into our
jupyter notebook and what we're bringing
in is we're going to bring in the
sklearn pre-processing where we're going
to import the label encoder and the one
hot encoder to use the label encoder
we're going to create a variable called
label encoder and set it equal to
capital L label capital E encoder this
creates a class that we can reuse for
transferring the labels back and forth
now about now you should ask what labels
are we talking about let's go take a
look at the data we processed before and
see what I'm talking about about here if
you remember when we did the companies.
head and we printed the top five rows of
data we have our columns going across we
have column zero which is R&D spending
column one which is Administration
column two which is marketing spending
and column three is State and you'll see
under State we have New York California
Florida now to do a linear regression
model it doesn't know how to process New
York it knows how to process a number so
the first thing we're going to do is
we're going to change that New York
California in Florida and we're going to
change those to numbers that's what this
line of code does here x equals and then
it has the colon comma 3 in Brackets the
first part the colon comma means that
we're going to look at all the different
rows so we're going to keep them all
together but the only row we're going to
edit is the third row and in there we're
going to take the label coder and we're
going to fit and transform the X also
the third row so we're going to take
that third row we're going to set it
equal to a transformation and that
transformation basically tells it that
instead of having a uh New York it has a
zero or one or a two and then finally we
need to do a one hot encoder which
equals one hot encoder categorical
features equals three and then we take
the X and we go ahead and do that equal
to one hot encoder fit transform X to
array this final transformation preps
our data for us so it's completely set
the way we need it as just a row of
numbers even though it's not in here
let's go ahead and print X and just take
a look at what this data is doing you'll
see you have an array of arrays and then
each array is a row of numbers and if I
go ahead and just do row zero you'll see
I have a nice organized row of numbers
that the computer now understands we'll
go ahead and take this out there because
it doesn't mean a whole lot to us it's
just a row of numbers next on setting up
our data we have avoiding dummy variable
trap this is very important why because
the computer's automatically transformed
our header into the setup and it's
automatically transformed all these
different variables so when we did the
encoder the encoder created two columns
and what we need to do is just have the
one because it has both the variable and
the name that's what this piece of code
does here let's go ahead and paste this
in here and we have xal X colon comma 1
colon all this is doing is removing that
one extra column we put in there when we
did our one hot encoder and our label
encoding let's go ahead and run that and
now we get to create our linear
regression model and let's see what that
looks like here and we're going to do
that in two steps the first step is
going to be in splitting the data now
whenever we create a uh predictive model
of data we always want to split it up so
we have a training set and we have a
testing set that's very important
otherwise we'd be very unethical without
testing it to see how good our fit is
and then we'll go ahead and create our
multiple linear regression model and
train it and set it up let's go ahead
and paste this next piece of code in
here and I'll go ahead and Shrink it
down a size or two so it all fits on one
line so from the sklearn module
selection we're going to import train
test split and you'll see that we've
created four completely different
variables we have capital x train
capital X test smaller case y train
smaller case y test that is the standard
way that they usually reference these
when we're doing different uh models you
usually see that a capital x and you see
the train and the test and the lowercase
y with this is is X is our data going in
that's our R&D spin our Administration
our marketing and then Y which we're
training is the answer that's the profit
because we want to know the profit of an
unknown entity so that's what we're
going to shoot for in this tutorial the
next part train test split we take X and
we take y we've already created those X
has the columns with the data in it and
Y has a column with profit in it and
then we're going to set the test size
equals 0 2 that basically means 20% So
20% of the rows are going to be tested
we're going to put them off to the side
so since we're using a th lines of data
that means that 200 of those lines we're
going to hold off to the side to test
for later and then the random State
equals zero we're going to randomize
which ones it picks to hold off to the
side we'll go ahead and run this it's
not overly exciting it's setting up our
variables but the next step is the next
step we actually create our linear
regression model now that we got to the
linear regression model we get that next
piece of the puzzle let let's go ah and
put that code in there and walk through
it so here we go we're going to paste it
in there and let's go ahead and since
this is a shorter line of code let's
zoom up there so we can get a good look
and we have from the SK learn. linear
model we're going to import linear
regression now I don't know if you
recall from earlier when we were doing
all the math let's go ahead and flip
back there and take a look at that do
you remember this where we had this long
formula on the bottom and we were doing
all this suiz and then we also looked at
setting it up with the different lines
and then we also looked all the way down
to multiple linear regression where
we're adding all those formulas together
all of that is wrapped up in this one
section so what's going on here is I'm
going to create a variable called
regressor and the regressor equals the
linear regression that's a linear
regression model that has all that math
built in so we don't have to have it all
memorized or have to compute it
individually and then we do the
regressor doet in this case we do X
train and Y train because we're using
the training data X being the data in
and Y being profit what we're looking at
and this does all that math for us so
within one click and one line we've
created the whole linear regression
model and we fit the data to the linear
regression model and you can see that
when I run the regressor it gives an
output linear regression it says copy
xals True Fit intercept equals true in
jobs equal one normalize equals false
it's just giving you some general
information on what's going on with that
regressor model now that we've created
our linear regression model let's go
ahead and use it and if you remember we
kept a bunch of data aside so we're
going to do a y predict variable and
we're going to put in the X test and
let's see what that looks like scroll up
a little bit paste that in here
predicting the test set results so here
we have y predict equals regressor do
predict X test going in and this gives
us y predict now because I'm in Jupiter
in line I can just put the variable up
there and when I hit the Run button
it'll print that array out I could have
just as easily done print y predict so
if you're in a different IDE that's not
an inline setup like the Jupiter
notebook you can do it this way print
why predict and you'll see that for the
200 different test variables we kept off
to the side it's going to produce 200
answers this is what it says the profit
are for those 200 predictions but let's
don't stop there let's keep going and
take a couple look we're going to take
just a short detail here and calculating
the coefficients and the intercepts this
gives us a quick flash at what's going
on behind the line we're going to take a
short detour here and we're going to be
calculating the coefficient and
intercepts so you can see what those
look like what's really nice about our
regressor we created is it already has
the coefficients for us and we can
simply just print regressor do
coefficient uncore when I run this
you'll see our coefficients here and if
we can do the regressor coefficient we
can also do the regressor intercept and
let's run that and take a look at that
this all came from the multiple
regression model and we'll flip over so
you can remember where this is going
into and where it's coming from you can
see the formula down here where y = M1 *
X1 + M2 * X2 and so on and so on plus C
the coefficient so these variables fit
right into this formula y equal slope 1
* column 1 variable plus slope 2 *
column 2 variable all the way to the m
into the n and x to the n + C the
coefficient or in this case you have -
8.89 to the power of two etc etc times
the First Column and the second column
and the third column and then our
intercept is the minus
1039 Point boy it gets kind of
complicated when you look at it this is
why we don't do this by hand anymore
this is why we have the computer to make
these calculations easy to understand
and calculate now I told you that was a
short detour and we're coming towards
the end of our script as you remember
from the beginning I said if we're going
to divide this information we have to
make sure it's a valid model that this
model works and understand how good it
works so calculating the r squ value
that's what we're going to use to
predict how good our prediction is and
let's take a look at what that looks
like in code and so we're going to use
this from SK learn. metric we're going
to import R2 score that's the R squar
value we're looking at the error so in
the R2 score we take our y test versus
our y predict y test is the actual
values we're testing that was the one
that was given to us so we know our true
the Y predict of those 200 values is
what we think it was true and when we go
ahead and run this we see we get a
9352 that's the R2 score now it's not
exactly a straight percentage so it's
not saying it's 93% correct but you do
want that in the upper 90s oh and higher
shows that this is a very valid
prediction based on the R2 score and if
r s value of0 91 or 92 as we got on our
model remember it does have a random
generation involved this proves the
model is a good model which means
success yay we successfully trained our
model with certain predictors and
estimated the profit of the companies
using linear regression so now that we
have a successful linear regression
model all right what is logistic
regression as I mentioned earlier
logistic regression is an algorithm for
performing binary classification so
let's take an example and see how this
works let's say your car has not been
serviced for for quite a few years and
now you want to find out if it is going
to break down in the near future so this
is like a classification problem find
out whether your car will break down or
not so how are we going to perform this
classification so here's how it looks if
we plot the information along the X and
Y AIS X is the number of years since the
last service was performed and why is
the probability of your car breaking
down and let's say this
was this data rather was collected from
several car users it's not just your car
but several car user so that is our
labeled data so the data has been
collected and um for for the number of
years and when the car broke down and
what was the probability and that has
been plotted along X and Y AIS so this
provides an idea or from this graph we
can find out whether your car will break
down or not we'll see how so first of
all the probability can go from zero to
one as you all aware probability can be
between Zer and one and as we can
imagine it is intuitive as well as the
number of years are on the Lower Side
maybe one year 2 years or 3 years till
after the service the chances of your
car breaking down are very limited right
so for example chances of your car
breaking down or the probability of your
car breaking down within two years of
your last service are 0.1 probability
similarly three years is maybe3 and so
on but as the number of years increases
let's say if it was six or seven years
there is almost a certainty that your
car is going to break down that is what
this graph shows so this is an example
of a application of the classification
algorithm and we will see in little
details how exactly logistic regression
is applied here one more thing needs to
be added here is that the dependent
variables outcome is discrete so if we
are talking about whether the car is
going to break down or not so that is a
discrete value the Y that we are talking
about the dependent variable that we are
talking about what we are looking at is
whether the car is going to break down
or not yes or no that is what we are
talking about so here the outcome is
discrete and not a continuous value so
this is how the logistic regression
curve looks let me explain exp a little
bit what exactly and how exactly we are
going to uh determine the class at the
outcome rather so for a logistic
regression curve a threshold has to be
set saying that because this is a
probability calculation remember this is
a probability calculation and the
probability itself will not be zero or
one but based on the probability we need
to decide what the outcome should be so
there has to be a threshold like for
example point .5 can be the threshold
let's say in this case so any value of
the probability below 0.5 is considered
to be zero and any value above 0.5 is
considered to be one so an output of
let's
say8 will mean that the car will break
down so that is considered as an output
of one and let's say an output of 0. 29
is considered as zero which means that
the car will not break down so that's
the way logistic regression works now
let's do a quick comparison between
logistic regression and linear
regression because they both have the
term regression in them so that can
cause confusion so let's try to remove
that confusion so what is linear
regression linear regression is a
process is once again an algorithm for
supervised learning however here you're
going to find a continuous value you're
going to determine a continuous value it
could be the price of a real estate
property it could could be your hike how
much hike you're going to get or it
could be a stock price these are all
continuous values these are not discrete
compared to a yes or a no kind of a
response that we are looking for in
logistic regression so this is one
example of a linear regression let's say
at the HR team of a company tries to
find out what should be the salary hike
of an employee so they collect all the
details of their existing employees
their ratings and their salary hikes
what has been given and that is the
labeled information that is available
and the system learns from this it is
trained and it learns from this labeled
information so that when a new employees
information is fed based on the rating
it will determine what should be the
height so this is a linear regression
problem and linear regression example
now salary is a continuous value you can
get 5,000
5,500 5,600 it is not discrete like a
cat or a dog or an apple or a banana
these are discrete or a yes or a no
these are discrete values right so this
where you are trying to find continuous
values is where we use linear regression
so let's say just to extend on this
scenario we now want to find out whether
this employee is going to get a
promotion or not so we want to find out
that is a discrete problem right a yes
or no kind of a problem in this case we
actually cannot use linear regression
even though we may have labeled data so
this is the labeled data So based on the
employee rating these are the ratings
and then some people got the promotion
and this is the ratings for which people
did not get promotion that is a no and
this is the rating for which people got
promotion we just plotted the data about
whether a person has got an employee has
got promotion or not yes no right so
there is nothing in between and what is
the employees rating okay and rating
things can be continuous that is not an
issue but the output is discrete in this
case whether employee got promotion yes
no okay so if we try to plot that and we
try to find a straight line this is how
it would look and as you can see doesn't
look very right because looks like there
will be lot of Errors the root mean
square error if you remember for linear
regression would be very very high and
also the the values cannot go beyond
zero or Beyond one so the graph should
probably look somewhat like this clipped
at 0 and one but still the straight line
doesn't look right therefore instead of
using a linear equation we need to come
up with something different and
therefore the logistic regression model
looks somewhat like this so we calculate
the probability and if we plot that
probability not in the form of a
straight line but we need to use some
other equation we will see very soon
what that equation is is then it is a
gradual process right so you see here
people with some of these ratings are
not getting any promotions and then
slowly uh at certain rating they get
promotion so that is a gradual process
and U this is how the math behind
logistic regression looks so we are
trying to find the odds for a particular
event happening and this is the formula
for finding the odd so the probability
of an event happening divided by the
probability of the event not happening
so P if it is the probability of the
event happening probability of the
person getting a promotion and divided
by the probability of the person not
getting a promotion that is 1 minus
P so this is how you measure the odds
now the values of the odds range from 0o
to Infinity so when this probability is
zero then the odds will the value of the
odds is equal to zero and when the
probability becomes 1 then the value of
the odds is 1 by 0 that will be Infinity
but the probability itself remains
between 0 and 1 now this is how an
equation of a straight line Looks So Y
is equal to Beta 0 + beta 1 x where beta
0 is the Y intercept and beta 1 is the
slope of the line if we take the odds
equation and take a log of both sides
then this would look somewhat like this
and the term logistic is actually
derived from the fact that we are doing
this we take a log of PX by 1 - PX this
is an extension of the calculation of
odds that we have seen right and that is
equal to Beta 0 + beta 1 x which is the
equation of the straight line and now
from here if you want to find out the
value of PX you will see we can take the
exponential on both sides and then if we
solve that equation we will get the
equation of PX like this PX is equal to
1 by 1 + e^ of minus beta 0 + beta 1 x
and recall this is nothing but the
equation of the line which is equal to y
y is equal to Beta 0 + beta 1 x so that
this is the equation also known as the
sigmoid function and this is the
equation of the logistic regression Al
all right and if this is plotted this is
how the sigmoid curve is obtained so
let's compare linear and logistic
regression how they are different from
each each other let's go back so linear
regression is solved or used to solve
regression problems and logistic
regression is used to solve
classification problems so both are
called regression but linear regression
is used for solving regression problems
where we predict continuous values
whereas logistic regression is used for
solving classification problems where we
have have to predict discrete values the
response variables in case of linear Reg
regression are continuous in nature
whereas here they are categorical or
discrete in nature and um linear
regression helps to estimate the
dependent variable when there is a
change in the independent variable
whereas here in case of logistic
regression it helps to calculate the
probability or the possibility of a
particular event happening and linear
regression as the name suggests is a
straight line that's why it's called
linear regression whereas logistic
regression is a sigmoid function and and
the curve is the shape of the curve is s
it's an s-shaped curve this is another
example of application of logistic
regression in weather prediction whether
it's going to rain or not rain now keep
in mind both are used in weather
prediction if we want to find the
discrete values like whether it's going
to rain or not rain that is a
classification problem we use logistic
regression but if we want to determine
what is going to be the temperature
tomorrow then we use linear regression
so this keep in mind that in weather
prediction we actually use both but
these are some examples of logistic
regression so we want to find out
whether it's going to be rain or not
it's going to be sunny or not whether
it's going to snow or not these are all
logistic regression examples a few more
examples classification of objects this
is a again another example of logistic
regression now here of course one
distinction is that these are multiclass
classification so logistic regression is
not used in its original form but it is
used in a slightly different form so we
say whether it is a dog or not a dog I
hope you understand so instead of saying
is it a dog or a cat or a elephant we
convert this into saying so because we
need to keep it to Binary classification
so we say is it a dog or not a dog is it
a cat or not a cat so that's the way
logistic regression can be used for
classifying objects otherwise there are
other techniques which can be used for
performing multiclass classification in
healthcare logistic regression is used
to find the survival rate of a patient
so they take multiple parameters like
trauma score and age and so on and so
forth and they try to predict the rate
of survival all right now finally let's
take an example and see how we can apply
logistic regression to predict the
number that is shown in the image so
this is actually a live demo I will take
you into jib a notebook and um show the
code but before that let me take you
through a couple of slides to explain
what we're trying to do so let's say you
have an 8 by8 image and the the image
has a number 1 2 3 4 and you need to
train your model to predict what this
number is so how do we do this so the
first thing is obviously in any machine
learning process you train your model so
in this case we are using logistic
regression so and then we provide a
training set to train the model and then
we test how accurate our model is with
the test data which means that like any
machine learning process we split our
initial data into two parts training set
and test set with the training set we
train our model and then with the test
set we we test the model till we get
good accuracy and then we use it for for
inference right so that is typical
methodology of uh uh training testing
and then deploying of machine learning
models so Let's uh take a look at the
code and uh see what we are doing so
I'll not go line by line but just take
you through some of the blocks so first
thing we do is import all the libraries
and then we basically take a look at the
images and see what is the total number
of images we can display using mat plot
lip some of the images or a sample of
these images and um then we split the
data into training and test as I
mentioned earlier and we can do some
exploratory analysis and uh then we
build our model we train our model with
the training set and then we test it
with our test set and find out how
accurate our model is using the
confusion Matrix the heat map and use
heat map for visualizing this and I will
show you in the code what exactly is the
confusion Matrix and how it can be used
for finding the accuracy in our example
we got we get an accuracy of about .94
which is pretty good or 94% which is
pretty good all right so what is a
confusion Matrix this is an example of a
confusion Matrix and uh this is used for
identifying the accuracy of a a
classification model or like a logistic
regression model so the most important
part in a confusion Matrix is that first
of all this as you can see this is a
matrix and the size of the Matrix
depends on how many outputs uh we are
expecting right so the the most
important part here is that the model
will be most accurate when we have the
maximum numbers in its diagonal like in
this case that's why it has almost 93
94% because the diagonals should have
the maximum numbers and the others other
than diagonals the cells other than the
diagonal should have very few numbers so
here that's what is happening so there
is a two here there are there's a one
here but most of them are along the
diagonal this what does this mean this
means that the number that has been fed
is zero and the number that has been
detected is also zero so the predicted
value and the actual value are the same
so along the diagonals that is true
which means that let's let's take this
diagonal right if if the maximum number
is here that means that like here in
this case it is 34 which means that 34
of the images that have been fed or
rather actually there are two
misclassifications in there so 36 images
have been fed which have number four and
out of which 34 have been predicted
correctly as number four and one has
been predicted as number eight and
another one has been predicted as number
nine so these are two
misclassifications okay so that is the
meaning of saying that the maximum
number should be in the diagonal so if
you have all of them so for an ideal
model which has let's say 100% accuracy
everything will be only in the diagonal
there will be no numbers other than zero
in all other cells so that is like a
100% accurate model okay so that's uh
just of how to use this Matrix how to
use this uh confusion Matrix I know the
name uh is a little funny sounding
confusion Matrix but actually it is not
very confusing it's very straightforward
so you are just plotting what has been
predicted and what is the labeled
information or what is the actual data
that's also known as the ground truth
sometimes okay these are some fancy
terms that are used so predicted label
and the actual label that's all it is
okay yeah so we are showing a little bit
more information here so 38 have been
predicted and here you will see that all
of them have been predicted correctly
there have been 38 zeros and the
predicted value and actual value is is
exactly the same whereas in this case
right it has there are I think 37 + 5
yeah 42 have been fed the images 42
images are of Digit three and uh the
accuracy is only 37 of them have been
accurately predicted three of them have
been predicted as number seven and two
of them have been predicted as number
eight and so on and so forth okay all
right so with that let's go into Jupiter
notebook and see how the code looks so
this is the code in in Jupiter notebook
for logistic regression in this
particular demo what we are going to do
is train our model to recognize digits
which are the images which have digits
from let's say 0 to five or 0 to 9 and
um and then we will see how well it is
trained and whether it is able to
predict these numbers correctly or not
so let's get started so the first part
is as usual we are importing some
libraries that are required and uh then
the last line in this block is to load
the digits so let's go ahead and run
this code then here we will visualize
the shape of these uh digits so we can
see here if we take a look this is how
the shape is
1797 by 64 these are like 8 by8 images
so that's that's what is reflected in
this shape now from here onwards we are
basically once again importing some of
the libraries that are required like
numpy and map plot and we will take a
look at uh some of the sample images
that we have loaded so this one for
example creates a figure uh and then we
go ahead and take a few sample images to
see how they look so let me run this
code and so that it becomes easy to
understand so these are about five
images sample images that we are looking
at 0 1 2 3 4 so this is how the images
this is how the data is okay and uh
based on this we will actually train our
logistic regression model and then we
will test it and see how well it is able
to recognize so the way it works is the
pixel information so as you can see here
this is an 8 by 8 pixel kind of a image
and uh the each pixel whether it is
activated or not activated that is the
information available for each pixel now
based on the pattern of this activation
and non-activation of the various pixels
this will be identified as a zero for
example right similarly as you can see
so overall each of these numbers
actually has a different pattern of the
pixel activation and that's pretty much
that our model needs to learn uh for
which number what is the pattern of the
activation of the pixels right so that
is what we are going to train our model
okay so the first thing we need to do is
to split our data into training and test
data set right so whenever we perform
any training we split the data into
training and test so that the training
data set is used to train the system so
we pass this probably multiple times uh
and then we test it with the test data
set and the split is usually in the form
of there and there are various ways in
which you can split this data it is up
to the individual preferences in our
case here we are splitting in the form
of 23 and 77 so when we say test size as
2023 that means 23% of the entire data
is used for testing and the remaining
77% is used for training so there is a
readily available function which is uh
called train test split so we don't have
to write any special code for the
splitting it will automatically split
the data based on the proportion that we
give here which is test size so we just
give the test size automatically
training size will be determined and uh
we pass the data that we want to split
and the the results will be stored in
xcore train and Yore train for the
training data set and what is xcore
train this are these are the features
right which is like the independent
variable and Yore train is the label
right so in this case what happens is we
have the input value which is or the
features value which is in xor train and
since this is labeled data for each of
them each of the observations we already
have the label information saying
whether this digit is a zero or a one or
a two so that this this is what will be
used for comparison to find out whether
the the system is able to recognize it
correctly or there is an error for each
observation it will compare with this
right so this is the label so the same
way xcore train Yore train is for for
the training data set xcore test Yore
test is for the test data set okay so
let me go ahead and execute this code as
well and then we can go and check
quickly what is the how many entries are
there and in each of this so xcore train
the shape is
1383 by 64 and Yore train has 1383
because there is uh nothing like the
second part is not required here and
then xcore test shape we is 414 so
actually there are 414 observations in
test and 1383 observations in train so
that's basically what these four lines
of code are are saying okay then we
import the uh logistic regression
library and uh which is a part of
psychic learn so we we don't have to
implement the logistic regression
process itself we just call these the
function and uh let me go ahead and
execute that so that uh we have the
logistic regression Library imported now
we create an instance of logistic
regression right so logistic RR is a is
an instance of logistic regression and
then we use that for training our model
so let me first execute this code so
these two lines so the first line
basically creates an instance of
logistic regression model and then the
second line where is where we are
passing our data the training data set
right this is our the the predictors and
uh this is our Target we are passing
this data set to train our model all
right so once we do this in this case
the data is not large but by and large
uh the training is what takes usually a
lot of time so we spend in machine
learning activities in machine learning
projects we spend a lot of time for the
training part of it okay so here the
data set is relatively small so it was
pretty quick so all right so now our
model has been trained using the
training data set and uh we want to see
how accurate this is so what we'll do is
we will test it out in probably faces so
let me first try out how well this is
working for uh one image okay I will
just try it out with one image my the
first entry in my test data set and see
whether it is uh correctly predicting or
not so and in order to test it so for
training purpose we use the fit method
there is a method method called fit
which is for training the model and once
the training is done if you want to test
for a particular value new input you use
the predict method okay so let's run the
predict method and we pass this
particular image and uh we see that the
shape is or the prediction is four so
let's try a few more let me see for the
next 10 uh seems to be fine so let me
just go ahead and test the entire data
set okay that's basically what we will
do so now we want to find out how
accurately this has U performed so we
use the score method to find what is the
percentage of accuracy and we see here
that it has performed up to 94% Accurate
okay so that's uh on this part now what
we can also do is we can um also see
this accuracy using what is known as a
confusion Matrix so let us go ahead and
try that as well uh so that we can also
visualize how well uh this model has uh
done so let me execute this piece of
code which will basically import some of
the libraries that are required and um
we we basically create a confusion
Matrix an instance of confusion matrix
by running confusion Matrix and passing
these uh values so we have so this
confusion underscore Matrix method takes
two parameters one is the Yore test and
the other is the prediction so what is
the Yore test these are the labeled
values which we already know for the
test data set and predictions are what
the system has predicted for the test
data set okay so this is known to us and
this is what the system has uh the model
has generated so we kind of create the
confusion Matrix and we will print it
and this is how the confusion Matrix
looks as the the name suggests it is a
matrix and um the key point out here is
that the accuracy of the model is
determined by how many numbers are there
in the diagonal the more the numbers in
the diagonal the better the accuracy is
okay and first of all the total sum of
all the numbers in this whole Matrix is
equal to the number of observations in
the test data set that is the first
thing right so if you add up all these
numbers that will be equal to the number
of observations in the test data set and
then out of that the maximum number of
them should be in the diagonal that
means the accuracy is pretty good if the
the numbers in the diagonal are less and
in all other places there are a lot of
numbers uh which means the accuracy is
very low the diagonal indicates a
correct prediction that this means that
the actual value is same as the
predicted value here again actual value
is same as the predictive value and so
on right so the moment you see a number
here that that means the actual value is
something and the predicted value is
something else right similarly here the
actual value is something and the
predicted value is something else so
that is basically how we read the
confusion Matrix now how do we find the
accuracy you can actually add up the
total values in the diagonal so it's
like 38 + 44 + 43 and so on and divide
that by the total number of test
observations that will give you the
percentage accuracy using a confusion
Matrix now let us visualize this
confusion Matrix in a slightly more
sophisticated way uh using a heat map so
we will create a heat map with some
We'll add some colors as well it's uh
it's like a more visually visually more
appealing so that's the whole idea so if
we let me run this piece of code and
this is how the heat map looks uh and as
you can see here the diagonals again uh
are or all the values are here most of
the values so which means reasonably
this seems to be reasonably accurate and
yeah basically the accuracy score is 94%
this is calculated as I mentioned by
adding all these numbers divided by the
total test values so the total number of
observations in test data set okay so
this is the confusion Matrix for
logistic
regression all right so now that we have
seen the confusion Matrix let's take a
quick sample and see how well uh the
system has classified and we will take a
few examples of the data so if we see
here we we picked up randomly a few of
them so this is uh number four which is
the actual value and also the predicted
value both are four this is an image of
zero so the predicted value is also zero
actual Valu is of course zero then this
is the image of nine so this has also
been predicted correctly 9 and actual
value is n and this is a image of one
and again this has been predicted
correctly as like the actual value okay
so this was a quick demo of logistic
regression how to use logistic
regression to identify images there's
all kinds of regression models that come
out of this so we put them side to by
side we have our linear regression which
is a predictive number used to predict a
dependent output variable based on
Independent input variable accuracy is a
measured uh using least squares
estimation so that's where you take uh
you could also use absolute value uh the
least squares is more popular there's
reasons for that mathematically and also
for computer
runtime uh but it does give you an an
accuracy based on the the least Square
estimation the best fit line is a
straight line and clearly that's not
always used in all the regression models
there's a lot of variations on that the
output is a predicted integer value
again this is what we're talking about
we're talking about linear regression
and we're talking about regression it
means the numberers coming out linear
usually means we're looking for that
line versus a different model and it's
used in business domain forecasting
stocks uh it's used as a basis of of
most um uh predictions with numbers so
if you're looking at a lot of numbers
you're probably looking at a a linear
regression
model uh for instance if you do just the
high lows of the stock exchange and
you're you're going to take a lot more
of that if you want to make money off
the stock you'll find that the linear
regression model fits uh probably better
than almost any of the other models even
you know highend neural networks and all
these other different machine learning
and AI models because they're numbers
they're just a straight set of numbers
you have a high value a low value volume
uh that kind of thing so when you're
looking at something that straight
numbers um and are connected in that way
usually you're talking about a linear
regression model and that's where you
want to start a logistic regression
model used to classify dependent output
variable based on Independent input
variable so just like the linear
regression model and like all of our
machine learning tools you have your
features coming in uh and so in this
case you might have uh label you know an
image or something like that is is
probably the very popular thing right
now labeling broccoli and vegetables or
whatever accuracy is measured using
maximum likelihood estimation the best
fit is given by a curve and we saw that
um we're talking about linear regression
you definitely are talking about
straight line although there is other
regression models that don't use
straight lines and usually when you're
looking at a logistic regression the
math as you saw was still kind of a
ukian line but it's now got that sigmoid
activation which turns it into a a a
heavily weighted curve and the output is
a binary value between zero and one and
it's used for classification image
processing as I mentioned is is what
people usually think of um although they
use it for classification of um like a
window of things so you could take a
window of stock history and you could
CLA generate classifications based on
that and separate the data that way if
it's going to be that this particular
pattern occurs it's going to be upward
trending or downward
trending in fact a number of stock uh uh
Traders use that not to tell them how
much to bid or what to bid uh but they
use it as to whether it's worth looking
at the stock or not whether the Stock's
going to go down or go up and it's just
a 01 do I care or do I even want to look
at it so let's do a demo so you can get
a picture of what this looks like in
Python code let's predict the price at
which insurance should be sold to a
particular customer based on their
medical history we will also classify on
a mushroom data set to find the
poisonous and nonpoisonous
mushrooms and when you look at these two
datas the first one uh we're looking at
the price so the price is a number um so
let's predict the price which the
insurance should be sold to and the
second one is we're looking at either
it's poisonous or it's not poisonous so
first off before we begin the demo I'm
in the Anaconda Navigator and this one
I've looked Ed the python
3.6 and using the Jupiter notebook and
you can use jupyter notebook by itself
um you can use the Jupiter lab which
allows multiple tabs it's basically the
notebook with tabs on it uh but the
jupyter notebook is just fine and it'll
go into uh Google Chrome which is what
I'm using for my Internet Explorer and
from here we open up new and you'll see
Python 3 and again this is loaded with
python
3.6 and we're doing the linear versus
say logic uh regression or logit you'll
see L git T um is one of the one of the
names that kind of pops up when you do a
search on here uh but it is a logic
we're looking at the logistic regression
models and we'll start with the linear
regression uh because it's easy to
understand you draw a line through stuff
um and so in programming uh we got a lot
of stuff to unfold here in our in our uh
startup as we preload all of our
different
parts and let's go ahead and break this
up we have at the beginning import uh
pandas so this is our data frame uh it's
just a way of storing the data think of
a uh when you talk about data frame
think of a spreadsheet we have rows and
columns it's a nice way of viewing the
data and then we have uh we're going to
be bringing in our pre-processing label
en Co coder I'll show you what that is
uh when we get down to it it's easier to
see in the data but there's some data in
here like um sex it's male or female so
it's not like an actual number it's
either your one or the other that kind
of stuff ends up being encoded that's
what this label encoder is right here we
have our test split
model if you're going to build a model
uh you do not want to use all the data
you want to use some of the data and
then test it to see how good it is and
if it can't have seen the data you're
testing on until you're ready to test it
on there and see how good it is and then
we have our logistic regression model
our categorical one and then we have our
linear regression model these are the
two these right here let me just um um
clear all that there we go uh these two
right here are what this is all about
logistic versus uh linear is it
categorical are we looking for a true
false or are we looking for um a
specific
number and then finally um usually at
the very end we have to take and just
ask how accurate is our model did it
work um if you're trying to predict
something in this case we're going to be
doing um uh Insurance costs uh how close
to the insurance cost does it measure
that we expect it to be you know if
you're an insurance company you don't
want to promise to pay everybody's
medical bill and not be able
to and in the case of the mushrooms
probably want to know just how much at
risk you are for following this model uh
as to far as whether you're going to get
a eat a poisonous mushroom and die or
not um so we'll look at both of those
and we'll get talk a little bit more
about the shortcomings and the um uh
value of these different processes so
let's go ahead and run this this has
loaded the data set on here and then
because we're in Jupiter notebook I
don't have to put the print on there we
just do data set and by and it prints
out all the different data on here and
you can see here for our insurance CU
that's what we're starting with uh we're
loading that with our pandas and it
prints it in a nice format where you can
see the age sex uh body mass index
number of children smoker so this might
be something that the insurance company
gets from the doctor it says hey we're
going to this is what we need to know to
give you a quote for what we're going to
charge you for your
insurance and you can see that it has uh
1,338 rows seven columns you can count
the columns 1 2 3 4 five six seven so
there's seven columns on
here and the column we're really
interested in is charges um I want to
know what the charges are going to be
what can I expect not a very good Arrow
drawn
um what to expect them to charge on
there uh so is this going to be you know
is this person going to cost me uh
$16,814.23
or is this person only going to cost me
uh
3,866 how do we guess that so that we
can guess what the minimal charge is for
their
insurance and then there's one other
thing you really need to notice on this
data um and I mentioned it before but
I'm going to mention it again because
pre-processing data is so much of the
work in data science um sex well how do
you how do you deal with female versus
male um are you a smoker yes or no what
does that mean region how do you look at
Region it's not a number how do you draw
a line between Southwest and
Northwest um you know they they're
objects it's either your Southwest or
your Northwest it's not like I'm
southwest I guess you could do longitude
and latitude but the data doesn't come
in like that it comes in as true false
or whatever you know it's either your
Southwest or your
Northwest so we need to do a little bit
of pre-processing of the data on here to
make this
work oops there we go okay so let's take
a look and see what we're doing with
pre-processing and again this is really
where you spend a lot of time with data
Sciences trying to understand how and
why you need to do that and so we're
going to do uh you'll see right up
here label uh and then we're going to do
the do a label encoder one of the
modules we brought in so this is SK
learns uh label
encoder I like the fact that it's all
pretty much automated uh but if you're
doing a lot of work with the label
encoder you should start to understand
how that
fits um and then we have uh label. fit
right here where we're going to go ahead
and do the data set uh. sex. drop
duplicates and then for data set sex
we're going to do the label transform
the data sex and so we're looking right
here at um Mel or female and so it
usually just converts it to a 01 because
there's only two choices on
here same thing with the smoker it's Z
or one so we're going to transfer the
trans change the smoker uh 01 on this
and then finally we did region down here
region does it a little bit different
we'll take a look at that and um it it's
I think in this case it's probably going
to do it because we did it on this label
transform um with this particular setup
it gives each region a number like 0 1 2
3 so let's go a and take a look and see
what that looks like go and run
this and you can see that our new data
set um has age that's still a number uh
Sex Is Zero or one uh so zero is female
one is male number of children we left
that alone uh smoker one or zero it says
no or yes on there we actually just do
one for no zero or no yeah yeah one for
no I'm not sure how it organized them
but it turns the smoker into zero or one
yes or no uh and then region it did this
as uh 0 1 2 three so it's three
regions now a lot of times in in when
you're working with data science and
you're dealing with uh regions or even
word
analysis um instead of doing one column
and labeling it 0 one two three a lot of
times you increase your features and so
you would have region north West would
be one column yes or no region Southwest
would be one column yes or no true
01 uh but for this this this particular
setup this will work just fine on here
now that we spent all that time getting
it set up uh here's the fun part uh
here's the part where we're actually
using our setup on this and you'll see
right here we have our um y linear
regression uh data set drop the charges
because that's what we want to
predict and so our X I'm sorry our X
linear data set drop the charges because
that's where we're going to predict
we're predicting charges right here so
we don't want that as our input for our
features and our y output is charges
that's what we want to guess we want to
guess what the charges
are and then what we talked about
earlier is we don't want to do all the
data at once so we're going to take um
three means 30% we're going to take 30%
of our data and it's going to be as the
train as the test T in site so here's
our y test and our X test down there um
and so that part our model will never
see it until we're ready to test to see
how good it is and then of course right
here you'll see our um training set and
this is what we're going to train it
we're going to trade it on 70% of the
data and then finally the big ones uh
this is where all the magic happens this
is where we're going to create our magic
setup and that is right here our linear
model we're going to set it equal to the
linear regression model and then we're
going to fit the data on
here and then at this point I always
like to pull up um if you if you if
you're working with a new model it's
good to see where it comes from and this
comes from the pyit uh learn and this is
the sklearn linear model linear
regression that we imported earlier and
you can see they have different
parameters the basic parameter works
great if you're dealing with just
numbers uh mentioned that earlier with
stock high lows this model will do as
good as any other model out there for do
if you're doing just the very basic high
lows and looking for a linear fit a
regression model fit um and what you one
of the things when I looking at this is
I look for
methods and you'll see here's our fit
that we're using right now and here's
our
predict and we'll actually do a little
bit in the middle here as far as looking
at some of the parameters hidden behind
it the math that we talked about
earlier and so we go in this we go ahead
and run this you'll see it loads the
linear regression model and just has a
nice output that says hey I loaded the
linear regression model and then the
second part is we did the fit and so
this model is now trained our linear
model is now trained on the training
data and so one of the things we can
look at is the um um for idx and call a
name and enumerate X linear train
columns come an interesting thing this
prints out the coefficients uh so when
you're looking at the back end of the
data you remember we had that formula uh
BX X1 plus bxx2 plus the plus the uh
intercept uh and so forth these are the
actual coefficients that are in here
this is what it's actually multiplying
these numbers
by and you can see like region Gets A
minus value so when it adds it up I
guess a region you can read a lot into
these numbers uh it gets very
complicated there's ways to mess with
them if you're doing a basic linear
regression model you usually don't look
at them too closely uh but you might
start looking in these and saying hey
you know what uh smoker look how smoker
impacts the cost um it's just massive uh
so this is a flag that hey the value of
the smoker really affects this model and
you can see here where the body mass
index uh so somebody who is overweight
is probably less healthy and more likely
to have cost money and then of course
age is a factor um and then you can see
down here we have uh sex is than a
factor also and it just it changes as
you go in there negative number it
probably has its own meaning on there
again it gets really complicated when
you dig into the um workings in how the
linear model works on that and so um we
can also look at the intercept this is
just kind of fun um so it starts at this
negative number and then adds all these
numbers to it that's all that means
that's our intercept on there and that
fits the data we have on that and so you
can see right here we can go back and
oops give me just a second there we go
we can go ahead and predict the unknown
data and we can print that out and if
you're going to create a model to
predict something uh we'll go ahead and
predict it here's our y prediction value
linear model
predict and then we'll go ahead and
create a new data frame in this case
from our X linear test group we'll go
ahead and put the cost back into this
data frame and then the predicted cost
we're going to make that equal to our y
prediction and so when we pull this up
uh you can see here that we have uh the
actual cost and what we predicted the
cost is going to
be there's a lot of ways to measure the
accuracy on there uh but we're going to
go go ahead and jump into our mushroom
data and so in this you can see here we
we've run our basic model we've built
our coefficients you can see the
intercept the back end you can see how
we're generating a number here uh now
with mushrooms we want to yes or no we
want to know whether we can eat them or
not and so here's our mushroom file
we're going to go and run this take a
look at the data and again you can ask
for a copy of this file uh send a note
over to Simply
learn.com and you can see here that we
have a class um the cap shape cap
surface and so forth so there's a lot of
feature in fact there's 23 different
columns in here going
across and when you look at this um I'm
not even sure what these particular like
PE PE I don't even know what the class
is on
this I'm going to guess by the notes
that the class is uh poisonous or
edible
so if you remember before we had to do a
little precoding on our data uh same
thing with here uh we have our cap shape
which is b or X or k um we have cap
color uh these really aren't numbers so
it's really hard to do anything with
just a a single number so we need to go
ahead and turn those into a label
encoder which again there's a lot of
different encoders uh with this
particular label encoder it's just
switching it to 01 1 two three and
giving it an integer
value in fact if you look at all the
columns all of our columns are labels
and so we're just going to go ahead and
uh loop through all the columns and the
data and we're going to transform it
into a um label encoder and so when we
run this you can see how this gets
shifted from uh xbxx K to 0 1 2 3 4 5 or
whatever it is class is 01
one being poisonous zero looks like it's
editable and so forth on here so we're
just encoding it if you were doing this
project depending on the results you
might encode it differently like I
mentioned earlier you might actually
increase the number of features as
opposed to laboring at 0 1 2 3 4 five um
in this particular example it's not
going to make that big of a difference
how we encode
it and then of course we're looking for
the class whether it's poisonous or
edible so we're going to drop the class
in our X Logistics model and we're going
to create our y Logistics model is based
on that class so here's our
XY and just like we did before we're
going to go ahead and split it uh using
30% for
test 70% to program the model on
here and that's right here whoops there
we go we go there's our U train and
test and then you'll see here on this
next setup um this is where we create
our model all the magic happens right
here uh we go ahead and create a
logistics model I've up the max
iterations if you don't change this for
this particular problem you'll get a
warning that says this has not
converged um because then that what it
does is it goes through the math and it
goes hey can we minimize the error and
it keeps finding a lower and lower error
and it still is changing that number so
that means it hasn't conversed yet it
hasn't find the lowest amount of error
it can and the default is 100 uh there's
a lot of settings in here so when we go
in here to let me pull that up from the
sklearn uh so we pull that up from the
sklearn
model you can see here we have our
logistic it has our different settings
on here that you can mess with most of
the these work pretty solid on this
particular setup so you don't usually
mess a lot usually I find myself
adjusting the um iteration and it'll get
that warning and then increase the
iteration on there and just like the
other model you can go just like you did
with the other model we can scroll down
here and look for our
methods and you can see there's a lot of
methods uh available on here and
certainly there's a lot of different
things you can do with it uh but the
most basic thing we do is we fit our
model make sure it's set right uh and
then we actually predict something with
it so those are the two main things
we're going to be looking at on this
model is fitting and predicting there's
a lot of cool things you can do that are
more advanced uh but for the most part
these are the two which um I use when
I'm going into one of these models and
setting them
up so let's go ahead and close out of
our sklearn setup on there and we'll go
ahead and run this and you can see here
it's now loaded this up there we now
have a uh uh logistic model and we've
gone ahead and done a predict here also
just like I was showing you earlier uh
so here is where we actually predicting
the data so we we've done our first two
lines of code as we create the model we
fit the model to our training data and
then we go ahead and predict for our
test data now in the previous model we
didn't dive into the test score um I
think I just showed you a graph and we
can go in there and there's a lot of
tools to do this we're going to look at
the uh model score on this one and let
me just go ahead and run the model
score and it says that it's pretty
accurate we're getting a roughly 95%
accuracy well that's good one 95%
accuracy 95% accuracy might be good for
a lot of
things but when you look at something as
far as whether you're going to pick a
mushroom on the side of the trail and
eat it we might want to look at the
confusion Matrix and for that we're
going to put in our y listic test the
actual values of edible and unedible and
we're going to put in our prediction
value and if you remember on here um
let's see I believe it's poisonous was
one uh zero is edible so let's go ahead
and run that 01 zero is good so here is
um a confusion Matrix and this is if
you're not familiar with these we have
true true true
false true false false false so says out
of the edible mushrooms we correctly
labeled 121 mushrooms edible that were
edible and we correctly measured
1,113 poisonous mushrooms as
poisonous but here's the
kicker I labeled uh 56 edible mushrooms
as being um poisonous well that's not
too big of a deal we just don't eat them
but I measured 68 mushrooms as being
edible that were poisonous so probably
not the best choice to use this model to
predict whether you're going to eat a
mushroom or not and you'd want to dig a
Little Deeper before you uh U start eat
picking mushrooms off the side of the
trail so a little warning there when
you're looking at any of these data
models looking at the error and how that
error fits in with what domain you're in
domain in this case being edible
mushrooms uh be a little careful make
sure that you're looking at them
correctly
so we've looked at uh edible or not
edible we've looked at uh regression
model as far as uh the end values what's
going to be the cost and what our
predicted cost is so we can start
figuring out how much to charge these
people for their
insurance and so these really are the
fundamentals of data science when you
pull them together uh when I say data
science I'm talking about your machine
learning
code and hopefully you got a little bit
out of here again you can cont contct
our simply learn team and get a copy of
these files or get more information on
this if you want to become an AI expert
and gain handsome salary packages look
at the wide range of AIML courses by
simply learn in collaboration with top
universities across the globe by
enrolling in any of these certification
programs you will gain expertise in
skills like generative AI prompt
engineering chat GPT explainable AI
machine learning algorithms so
supervised and unsupervised learning
model training and optimization and
there's much more on the list with
hands-on experience in the tools like
chat GPT di python open CV and tens
oflow you will catch the eyes of top
recruiters so what are you waiting for
hurry up and enroll now an year of
experience is preferred to enroll in
these courses find the course Link in
the description
box a confusion Matrix represents a
table layout of the different outcomes
of prediction and results of a
classification problem and helps
visualize its
outcomes and so you see here we have our
uh uh simple chart predicted and actual
the confusion Matrix helps us identify
the correct predictions of a model for
different individual classes as well as
the errors so you'll see here that the
values predicted by our classifier are
along the rows this is what we're going
to guess it is or our our model is
guessing what this is based on its
training so we've already trained the
model to um guess whether it's spam or
not spam or whatever it is you're
working on and then the actual values of
our data set are along the
columns so this is the actual value it's
supposed to
be people who can speak English will be
classified as positives so because they
have a remember 01 do you speak English
yes no and you could extend this that
they might have do you speak uh French
do you speak whatever languages and so
you might have a whole lot of
classifiers that you would look at each
one of these people who cannot speak
English will be classified as negatives
so they'll be a zero so you know zero
ones the number of times are actual
positive values are equal to predicted
positive values gives us true positive
TP the number of times are actual
negative values are equal to predictive
negative values gives us true negative
TN the number of times our model wrongly
predicts negative values as
positives gives us a false positive
FP and you'll see when you're working
with these a lot you know memorizing
that it's false positive you can easily
figure out what that is and pretty soon
you're just looking at the FP or the TP
depending on what you're working on and
the number times our model wrongly
predicts positive values as negatives
gives us a false negative
FP now I'm going to do a quick step out
here let's say you're working in the
medical and we're talking about cancer
uh do you really want a bunch of false
negatives you want zero under false
negative uh so when we look at this
confusion Matrix if you have 5% false
positives and 5% false negatives it'd be
much better to even have 20% false
positives because they go in and test it
and zero false negatives the say might
be true if you're working on uh uh say
uh a car driving is this a safe place
for the car to go well you really don't
want any false positives you know yes
this is safe right over the cliff so
again when you're working on the project
or whatever it is you're working on this
chart suddenly has huge value uh we were
talking about spam email how many
important emails say from your banking
overdraft charge coming in that you want
to be uh a true a false negative you
don't want it to go in the spam folder
likewise you want to get as much of the
spam out of there but you don't want to
miss anything anything really
important confusion Matrix metrics are
performance measures which help us find
the accuracy of our classifier there are
four main metrics accuracy precision
recall and F1 score the F1 score is the
one I usually hear the most and accuracy
is usually what you put on your chart uh
when you're standing in front of the
shareholders how accurate is it people
understand
accuracy um F1 score is bit more on the
math side and so you got to be a little
careful when you're quoting F1 scores in
the when you're sitting there with all
the shareholders because a lot of them
will just glaze over so confusion Matrix
metrics are performance measures which
help us find the accuracy of our
classifier there are four main metrics
accuracy the accuracy is used to find
the portion of the correctly classified
values it tells us how often our
classifier is right it is the sum of all
True Values divided by the total values
and this makes sense uh again it's one
of those
things I don't want to F you know
depends on what you're looking for are
you looking for uh not to miss any spam
mails are you looking to drive down the
road and not run anybody over Precision
is used to calculate the model's ability
to classify positive values correctly it
answers the question when the model
predicts a positive value how often is
it right it is the true positive divided
by the total number of predicted
positive values again this one uh
depends on what project you're working
on whether this is what you're going to
be focusing on uh so recall it is used
to calculate the model's ability to
predict positive values how often does
the model actually predict the correct
positive values it is the true positives
divided by the total number of actual
positive values and then your F1 score
it is the Harmon IC mean of recall and
precision it is useful when you need to
take both precision and recall into
account consider the following two
confusion Matrix derived from two
different classifier to figure out which
one performs better we can find the
confusion Matrix for both of them and
you can see we're back to uh does it
classify whether they can speak English
or are non-speaker they speak something
they don't know the English language and
so we put these two uh uh confusion
matrixes out here we can go ahead and do
the math behind that we can look up the
accuracy that's a tpn plus TN over the
TF plus TN plus FP plus FN and so we get
an accuracy of
8125 and we have a Precision if you do
the Precision which is your TP truth
positive over TP plus
FP uh we get
891 and if we do the call we'll end up
with the 0 825 that's your TP over TP
plus FN and then of course your F1 score
which is 2 * Precision Time recall over
Precision plus
recall and we get the 0
857 and if we do that um with another
model let's say we had two different
models and we're trying to see which one
we want to use uh for whatever reason uh
we might go ahead and compute the same
things we have our accuracy our
precision and our recall and our F1
score and uh as we're looking at this we
might uh look at the accuracy because
that's really what we're interested in
is uh how many people are we able to
classify as being able to speak English
I really don't want to know if I you
know I I I I really don't want to know
if I if they're non-speakers um I'd
rather Miss 10 people speaking English
instead of 15 and so you can see from
these charts we probably go with the
first model because it does a better job
guessing who speaks English and has
higher accuracy because in this case
that is what we're looking
for so uh with that we'll go ahead and
pull up a demo so you can see what this
looks like in the python uh setup in in
the actual coding for this we'll go into
Anaconda Navigator if you're not
familiar with Anaconda uh it's a really
good tool to use as far as doing display
and demos and for quick development um
as a data scientist I just love the
package now if you're going to do
something heavier lifting uh there's
some limitations with anaconda and with
the setup but in general you can do just
about anything in here with your Python
and for this we'll go with Jupiter
notebook uh Jupiter lab is the same as
Jupiter notebook you'll see they now
have integration with uh py charm if you
work in py charm uh certainly there's a
lot of other Integrations that Anaconda
has and we've opened up um my simply
learned files I work on and create a new
file called confusion Matrix demo
and the first thing we want to note is
the data we're working with uh here I've
opened it up in a word pad or not pad or
whatever uh you can see it's got a row
of uh headers uh comma separated and
then all the data going down below and
then I saved this in the same file so I
don't have to remember what path I'm
working on uh of course if you have your
data separated and you're working with a
lot of data you probably want to put it
in a different folder or file depending
on what you're doing and the first thing
we want to do is go ahead and import our
tools uh we're going to use the pandas
that's our data frame if you haven't had
a chance to work with the data frame
please review Panda's data frame going
to Simply learn you can pull up the
panda data frame um tutorial on there
and then we're going to use uh the scit
framework which is all denoted as
sklearn and I can just pull this in you
can see here's the um
scikit-learn dorg with the stable
version that you can import into your
Python and from here we're going to use
the train test split for splitting our
data we're going to do some
pre-processing we're going to do use the
logistic regression model that's our
actual uh machine learning model we're
using and then what this C this
particular setup is about is we're going
to do the accuracy score the confusion
Matrix and the classifier report so let
me go ahead and run that and bring all
that information
in and just like we opened the file we
need to go ahead and load our data in
here uh so we're going to go ahead and
do our pandas read
CSV and then just because we're in
Jupiter notebook we can just put data to
read the data in here a lot of times
we'll actually let me just do this I
prefer to do the just the head of the
date or the top
part and you can see we have age sex um
I'm not sure what CP stands for test BPS
cholesterol uh so a lot of different
measurements if you were in this domain
you'd want to know what all these
different measurements mean I don't want
to focus on that too much because when
we're talking about data science a lot
of times you have no idea what the data
means if you've ever looked up the
breast cancer measurement it's just a
bunch of measurements and numbers uh
unless you're a doctor you're going to
have no idea what those measurements
mean but if it's your specialty in your
domain you better know them so we're
going to go ahead and create Y and it's
going to we're going to set it equal to
the Target uh so here's our Target value
here and it's either one or
zero so we have a classifier if you're
dealing with one zero true false what do
you have you have a
classifier and then our X is going to be
uh everything except for the Target uh
so we're going to go ahead and drop the
target axis equals one remember that's
columns versus uh the index or rows ax
is equal Z would would give you an error
but you would drop like row two and then
we'll go ahead and just print that out
so you can see what we're looking at and
uh here we have um Y data X data uh you
can see from the X data we have the X
head and we can go ahead and just do
print the Y head
data and run
that so this is all loading the data
that we've done so far uh if there's a
confusion in there go back and rewind
the tape and review it and then we need
to go ahead and split our data into our
XT train X test YT train y test and then
keep in mind you always want to split
the data before we do the scaler and the
reason is is that uh you want the scaler
on the training data uh to be set on the
training data data or fit to it but not
on the test data think of this as being
out in the field uh you're not it could
actually alter your results uh so it's
always important to do make sure
whatever you do to the training data or
whatever um fit you're doing is always
done on the training not on the test and
then we want to go ahead and scale the
data now we are working with um linear
regression model now I'll mention this
here in a minute when we get to the
actual model uh so some sometimes you
don't need to scale it when you're
working with linear regression models
it's not going to change your result as
much as say a neural network where it
has a huge
impact uh but we're going to go ahead
and take here's our XT train X test YT
train y test we create our scaler we go
ahead and scale uh the scale is going to
fit the X
train and then we're going to go ahead
and take our X train and transform it
and then we also need to take our X test
and transform it based on the scale on
here so that our X is now between that
nice - one to one and so this is all uh
our pre- dat setup and hopefully uh all
of that looks fairly familiar to you if
you've done a number of our other
classes and you're up to the setup on
here and then we want to go ahead and do
is create our model and we're going to
use the logistic regression model and
from the logistic regression model uh
we're going to go ahead and fit our X
train and Y train and then we'll run our
predicted value on here
and so let's go ahead and run that and
so now we are we actually have like our
X test and our prediction so if you
remember
from our Matrix we're looking for the
actual versus the prediction and how
those
compare and if I take this back up here
you're going to notice that we imported
the accuracy score the confusion Matrix
and the classification report uh and
there's of course our logistic
regression the model we're using for
this and I did mention I was going to
talk a little bit about scaler and the
regression
model the scaler on a lot of your
regression models uh your basic Mass
standard regression models and I'd have
to look it up for the logistic
regression model when you're using a
standard regression model you don't need
to scale the data uh it's already just
built in by the way the model
Works in most cases uh but if you're in
a neural network and there's a lot of
other different setups then you really
want to take this and fit that on
there and so we can go in and do the
accuracy uh and this is if you remember
correctly we were looking at the
accuracy with the english- speaking uh
so this is saying our accuracy as to
whether this person is I believe this is
the heart data
set um it's going to be accurate about
85% of the time as far as whether it's
going to predict the person's going to
have um a heart condition or the one as
it comes up with the zero1 on there
which would mean at this point that you
have an 85% uh being correct on telling
someone they're extremely high risk for
a heart attack kind of
thing and so we want to go ahead and uh
create our confusion Matrix and let me
just do
that of course the software does
everything for us so we'll go ahead and
run this and you can see right here um
here's our 25
uh prediction uh correct predictions
right
here and if you remember from our slide
I'll just bring this over so it's a nice
visual we have our true positive false
positive uh so we had 25 which were true
that it said hey this person's going to
be high risk at um heart and we had four
that were still high risk that it said
were false um so out of these 25 people
or out of these 29 people and that makes
sense cuz you have 085 out of 29 people
it was correct on 25 of them and so uh
here's our accuracy score we were just
looking at that our accuracy is your
true positive and your true negative
over all of them so how true is it there
was our accuracy um coming up here 085
and then we have our nice Matrix
generated from that uh and you can see
right here is a similar Matrix we had
going for from the slide and this starts
to this should start asking questions at
this point um so if you're in a board
meeting or you're working with this you
really want to start looking at this
data here and saying well is this good
enough is uh this number of people and
hopefully you'd have a much larger data
set it my is my confusion Matrix showing
for the true positive and uh false
positive is that acceptable for what
we're doing uh and of course if you're
going to put together uh whatever data
you're putting out you might want to
separate the uh true negative false
positive false negative true positive
and you can simply do that uh by doing
the confusion Matrix uh and then of
course the Ravel part lets you um set
that up so you can just split that right
up into a nice tupal and the final thing
we want to show you here in the coding
on this part is the confusion Matrix
metrix and so we can come in here and
just use the Matrix equals
classification report the Y test and the
predict and then we're going to take
that classification report and go ahead
and print that out and you can see here
it does a nice job uh giving you your
accuracy uh your micro average your
weighted average um you have your
Precision your recall your F1 score and
your support all in one window so you
can start looking at this data and
saying oh okay our precisions at
083 uh 087 for getting a a positive and
83 for the negative side for a zero and
we start talking about whether this is a
valid information or not to use and when
we're looking at a heart attack
prediction we're only looking at one
aspect what's the chances of this person
having a heart attack or not um you
might have something where we went back
to languages maybe you also want to know
whether they speak English or Hindi uh
or French and you can see right here
that we can now take our confusion
Matrix and just expand it as big as we
need to depend on how many different
classifiers we're working on what is a
decision tree let's go through a very
simple example before we dig in deep
decision tree is a tree shaped diagram
used to determine a course of action
each branch of the tree represents a
possible decision occurrence or reaction
let's start with a simple question how
do identify a random vegetable from a
shopping bag so we have this group of
vegetables in here and we can start off
by asking a simple question is it red
and if it's not then it's going to be
the purple fruit to the left probably an
eggplant if it's true it's going to be
one of the red fruits is a diameter
greater than two if false it's going to
be a what looks to be a red chile and if
it's true it's going to be a bell pepper
from the capsicum family so it's a
capsicum problems that decision tree can
solve so let's look at the two different
categories the decision tree can be used
on it can be used on the classification
the true false yes no and it can be used
on regression where we figure out what
the next value is in a series of numbers
or a group of data in classification the
classification tree will determine a set
of logical if then conditions to
classify problems for example
discriminating between three types of
flowers based on certain features in
regression a regression tree is used
when the target variable is numerical or
continuous in nature we fit the
regression model to the Target variable
using each of the independent variables
each split is made based on the sum of
squared error before we dig deeper into
to the mechanics of the decision tree
let's take a look at the advantages of
using a decision tree and we'll also
take a glimpse at the disadvantages the
first thing you'll notice is that it's
simple to understand interpret and
visualize it really shines here because
you can see exactly what's going on in a
decision tree little effort is required
for data preparation so you don't have
to do special scaling there's a lot of
things you don't have to worry about
when using a decision tree it can handle
both numerical and categorical data as
we discovered earlier nonlinear
parameters don't affect its performance
so even if the data doesn't fit an easy
curved graph you can still use it to
create an effective decision or
prediction if we're going to look at the
advantages of a decision tree we also
need to understand the disadvantages of
a decision tree the first disadvantage
is overfitting overfitting occurs when
the algorithm captures noise in the data
that means you're solving for one
specific instance instead of a general
solution for for all the data High
variant the model can get unstable due
to small variation in data low bias tree
a highly complicated decision tree tends
to have a low bias which makes it
difficult for the model to work with new
data decision tree important terms
before we dive in further we need to
look at some basic terms we need to have
some definitions to go with our decision
tree in the different parts we're going
to be using we'll start with entropy
entropy is a measure of Randomness or un
predictability in the data set for
example we have a group of animals in
this picture there's four different
kinds of animals and this data set is
considered to have a high entropy you
really can't pick out what kind of
animal it is based on looking at just
the four animals as a big clump of of uh
entities so as we start splitting it
into subgroups we come up with our
second definition which is Information
Gain Information Gain it is a measure of
decrease in entropy after the data set
is split so in this case based on the
color yellow we've split one group of
animals on one side as true and those
who aren't yellow as false as we
continue down the yellow side we split
base on the height true or false equals
10 and on the other side height is less
than 10 true or false and as you see as
we split it the entropy continues to be
less and less and less and so our
Information Gain is simply the entropy
E1 from the top and how it's changed to
E2 in the bottom and we'll look at the
deeper math although you really don't
need to know a huge amount of math when
you actually do the programming in
Python cuz they'll do it for you but
we'll look on the actual math of how
they compute entropy finally we went
under the different parts of our tree
and they call the leaf node Leaf node
carries the classification or the
decision so it's the final end at the
bottom the decision node has two or more
branches this is where we're breaking
the group up into different parts and
finally you have the root node the
topmost decision node is known as the
root
node how does a decision Vision tree
work wonder what kind of animals I'll
get the jungle today maybe you're the
hunter with a gun or if you're more into
photography you're a photographer with a
camera so let's look at this group of
animals and let's try to classify
different types of animals based on
their features using a decision tree so
the problem statement is to classify the
different types of animals based on
their features using a decision tree the
data set is looking quite messy and the
entropy is high in this case so let's
look at a training set or a training
data set and we're looking at color
we're looking at height and then we have
our different animals we have our
elephants our giraffes our monkeys and
our tigers and they're of different
colors and shapes let's see what that
looks like and how do we split the data
we have to frame the conditions that
split the data in such a way that the
Information Gain is the highest note
gain is the measure of decrease in
entropy after splitting so the formula
for entropy is the sum that's what this
symbol looks like that looks like kind
of like a e funky e of K where I equals
1 to k k would represent the number of
animal the different animals in there
where value or P value of I would be the
percentage of that animal times the log
base 2 of the same the percentage of
that animal let's try to calculate the
entropy for the current data set and
take a look at what that looks like and
don't be afraid of the math don't really
have to memorize this math just be aware
that it's there and this is what's going
on in the background and so we have
three giraffes two tigers one monkey two
elephants a total of eight animals
gathered and if we plug that into the
formula we get an entropy that equals 3
over 8 so we have three drafts a total
of eight times the log usually they use
base two on the log so log base 2 of 3
over 8 plus in this case let's say it's
the elephants 2 over 8 two elephants
over total of 8 * log base 2 2 over 8
plus one monkey over total of 8 log base
2 1 over8 and plus 2 over 8 of the
Tigers log base 2/ 8 and if we plug that
into our computer our calculator I
obviously can't do logs in my head we
get an inop equal to
.571 the program will actually calculate
the entropy of the data set similarly
after every split to calculate the gain
now we're not going to go through each
set one at a time to see what those
numbers are we just want you to be aware
that this is a Formula or the
mathematics behind it gain can be
calculated by finding the difference of
the subsequent entropy values after a
split now we will try to choose a
condition that gives us the highest gain
we will do that by splitting the data
using each condition and checking that
the gain we get out of them the
condition that gives us the highest gain
will be used to make the first split can
you guess what that first split will be
just by looking at this image as a human
it's probably pretty easy to split it
let's see if you're right if you guessed
the color yellow you're correct let's
say the condition that gives us the
maximum gain is yellow so we will split
the data based on the color yellow if
it's true that group of animals goes to
the left if it's false it goes to the
right the entropy after the splitting
has de decreased considerably however we
still need some splitting at both the
branches to attain an entropy value
equal to zero so we decide to split both
the nodes using height as a condition
since every Branch now contains single
label type we can say that entropy in
this case has reached the least value
and here you see we have the giraffes
the Tigers the monkey and the elephants
all separated into their own groups this
tree can now predict all the classes of
animals present in the data set with
100% accuracy that was easy use case
loan repayment prediction let's get into
my favorite part and open up some Python
and see what the programming code and
the scripting looks like in here we're
going to want to do a prediction and we
start with this individual here who's
requesting to find out how good his
customers are going to be whether
they're going to repay their loan or not
for his bank and from that we want to
generate a problem statement to predict
if a customer will repay load amount or
not and then we're going to be using the
decision tree algorithm in Python let's
see what that looks like and let's dive
into the code in our first few steps of
implementation we're going to start by
importing the necessary packages that we
need from Python and we're going to load
up our data and take a look at what the
data looks like so the first thing I
need is I need something to edit my
Python and run it in so let's flip on
over and here I'm using the Anaconda
Jupiter notebook now you can use any
python IDE you like to run it in but I
find the jupyter notebooks really nice
for doing things on the Fly and let's go
ahead and just paste that code in the
beginning and before we start let's talk
a little bit about what we're bringing
in and then we're going to do a couple
things in here we have to make a couple
changes as we go through this first part
of the import the first thing we bring
in is numpy as NP that's very standard
when we're dealing with mathematics
especially with uh very complicated
machine learning tools you almost always
see the numpy come in for your num your
number it's called number python it has
your mathematics in there in this case
we actually could take it out but
generally you'll need it for most of
your different things you work with and
then we're going to use pandas as PD
that's also a standard the pandas is a
data frame setup and you can liken this
to uh taking your basic data and storing
it in a way that looks like an Excel
spreadsheet so as we come back to this
when you see NP or PD those are very
standard uses you'll know that that's
the pandas and I'll show you a little
bit more when we explore the data in
just a minute then we're going to need
to split the data so I'm going to bring
in our train test and split and this is
coming from the sklearn package cross
validation in just a minute we're going
to change that and we'll go over that
too and then there's also the sk. tree
import decision tree classifier that's
the actual tool we're using remember I
told you don't be afraid of the
mathematics it's going to be done for
you well the decision tree classifier
has all that mathematics in there for
you so you don't have to figure it back
out again and then we have sklearn docs
for accuracy score we need to score our
our setup that's the whole reason we're
splitting it between the training and
testing data and finally we still need
the sklearn import tree and that's just
the basic tree function is needed for
the decision tree classifier and finally
we're going to load our data down here
and I'm going to run this and we're
going to get two things on here one
we're going to get an error and two
we're going to get a warning let's see
what that looks like so the first thing
we had is we have an error why is this
error here well it's looking at this it
says I need to read a file and when this
was written the person who wrote it this
is their path where they stored the file
so let's go ahead and fix
that and I'm going to put in here my
file path I'm just going to call it full
file name and you'll see it's on my C
drive and it's this very lengthy setup
on here where I stored the data 2. CSV
file don't worry too much about the full
path because on your computer it'll be
different the data. 2 CSV file was
generated by simply learn if you want a
copy of that you can comment ment down
below and request it here in the
YouTube and then if I'm going to give it
a name full file name I'm going to go
ahead and change it here to
full file name so let's go ahead and run
it now and see what
happens and we get a
warning when you're coding understanding
these different warnings and these
different errors that come up is
probably the hardest lesson to learn so
let's just go ahead and take a look at
this and use this as a uh opportunity to
understand what's going on here if you
read the warning it says the cross
validation is depreciated so it's a
warning on it's being removed and it's
going to be moved in favor of the model
selection so if we go up here we have
sklearn Doc crossvalidation and if you
research this and go to sklearn site
you'll find out that you can actually
just swap it right in there with model
selection and so when I come in here and
I run it again that removes a warning
what they've done is they've had two
different developers develop it in two
different branches and then they decided
to keep one of those and eventually get
rid of the other one that's all that is
and very easy and quick to
fix before we go any further I went
ahead and opened up the data from this
file remember the the data file we just
loaded on here the dataor 2. CSV let's
talk a little bit more about that and
see what that looks like both as a text
file because it's a comma separated
variable file and in a spreadsheet this
is what it looks like as a basic text
file you can see at the top they've
created a header and it's got 1 2 3 four
five columns and each column has data in
it and let me flip this over cuz we're
also going to look at this uh in an
actual spreadsheet so you can see what
that looks like and here I've opened it
up in the open Office Cal which is
pretty much the same as um X Excel and
zoomed in and you can see we've got our
columns and our rows of data little
easier to read in here we have a result
yes yes no we have initial payment last
payment credit score house number if we
scroll way
down we'll see that this occupies a,1
lines of code or lines of data with uh
the first one being a column and then
1,000 lines of
data now as a program grammar if you're
looking at a small amount of data I
usually start by pulling it up in
different sources so I can see what I'm
working
with but in larger data you won't have
that option it'll just be um too too
large so you need to either bring in a
small amount that you can look at it
like we're doing right now or we can
start looking at it through the python
code so let's go ahead and move on and
take the next couple steps to explore
the data using python let's go ahead and
see what it looks like in Python to
print the length and the shape of the
data so let's start by printing the
length of the database we can use a
simple Lind function from Python and
when I run this you'll see that it's a
th long and that's what we expected
there's a th lines of data in there if
you subtract the column head and this is
one of the nice things when we did the
uh balance data from the panda read CSV
you'll see that the header is row zero
so it automatically removes a
row and then shows the data separate
does a good job sorting that data out
for us and then we can use a different
function and let's take a look at that
and again we're going to utilize the
tools in
Panda and since the balance uncore data
was loaded as a panda data
frame we can do a shape on it and let's
go ahead and run the shape and see what
that looks
like what's nice about the shape is not
only does it give me the length of the
data we have a thousand lines it also
tells me there's five columns so we were
looking at the data we had five columns
of data and then let's take one more
step to explore the data using Python
and now that we've taken a look at the
length and the shape let's go ahead and
use the uh pandas module for head
another beautiful thing in the data set
that we can utilize so let's put that on
our sheet here and we have print data
set and balance data doad and this is a
panda's print statement of its own so it
has its own print feature in there and
then we went ahead and gave a label for
a print job here of data set just a
simple print statement and when we run
that and let's just take a closer look
at that let me zoom in
here there we
go pandas does such a wonderful job of
making this a very clean readable data
set so you can look at the data you can
look at the column headers you can have
it uh when you put it as a head it
prints the first five lines of the data
and we always start with zero so we have
five lines we have 0 1 2 3 4 instead of
1 2 3 4 5 that's a standard scripting
and programming set as you want to start
with the zero position and that is what
the data head does it pulls the first
five rows of data puts in a nice format
that you can look at and view very
powerful tool to view the data so
instead of having to flip and open up an
Excel spreadsheet or open Office Cal or
trying to look at a word doc where it's
all scrunched together and hard to read
you can now get a nice open view of what
you're working with we're working with a
shape of a th long five wide so we have
five columns and we do the full data
head you can actually see what this data
looks like the initial payment last
payment credit scores house number so
let's take this now that we've explored
the data and let's start digging into
the decision tree so in our next step
we're going to train and build our data
tree and to do that we need to First
separate the data out we're going to
separate into two groups so that we have
something to actually train the data
with and then then we have some data on
the side to test it to see how good our
model is remember with any of the
machine learning you always want to have
some kind of test set to to weigh it
against so you know how good your model
is when you distribute it let's go ahead
and break this code down and look at it
in pieces so first we have our X and
Y where do X and Y come from well X is
going to be our data and Y is going to
be the answer or the target you can look
at it source and Target in this case
we're using X and Y to denote the data
in and the data that we're actually
trying to guess what the answer is going
to be and so to separate it we can
simply put in x equals the balance of
the data. values the first brackets
means that we're going to select all the
lines in the database so it's all the
data and the second one says we're only
going to look at columns 1 through five
remember we always start with zero zero
is a yes or no and that's whether the
loan went default or not so we want to
start with one if we go back up here
that's the initial payment and it goes
all the way through the house
number well if we want to look at uh 1
through five we can do the same thing
for Y which is the answers and we're
going to set that just equal to the zero
row so it's just the zero row and then
it's all rows going in there so now
we've divided this into two different
data sets one of them with the data
going in and one with the
answers next we need to split the
data and here you'll see that we have it
split into four different parts the
first one is your X training your X test
your y train your y
test simply put we have X going in where
we're going to train it and we have to
know the answer to train it with and
then we have X test where we're going to
test that data and we have to know in
the end what the Y was supposed to be
and that's where this train t test split
comes in that we loaded earlier in the
modules this does it all for us and you
can see they set the test size equal to3
so it's roughly 30% will be used in the
test and then we use a random state so
it's completely random which rows it
takes out of there and then finally we
get to actually build our decision tree
and they've called it here clf entropy
that's the actual decision tree or
decision tree classifier and in here
they've added a couple variables which
we'll explore in just a minute and then
finally we need to fit the data to that
so we take our clf entropy that we
created and we fit the X train and since
we know the answers for XT train or the
Y train we go ahe and put those in and
let's go ahead and run this and what
most of these sklearn modules do is when
you set up the variable in this case
when we set the clf entropy equal
decision tree classifier it
automatically prints out what's in that
decision tree there's a lot of variables
you can play with in here and it's quite
beyond the scope of this tutorial to go
through all of these and how they work
but we're working on entropy that's one
of the options we've added that it's
completely a random state of 100 so 100%
And we have a max depth of three now the
max depth if you remember above when we
were doing the different graphs of
animals means it's only going to go down
three layers before it stops and then we
have minimal samples of leaves is five
so it's going to have at least five
leaves at the end so I'll have at least
three splits I have no more than three
layers and at least five end leaves with
the final result at the bottom now that
we've created our decision tree
classifier not only created it but
trained it let's go ahead and apply it
and see what that looks like so let's go
ahead and make a prediction and see what
that looks like we're going to paste our
predict code in here and before we run
it let's just take a quick look at
what's this doing here we have a
variable y predict that we're going to
do and we're going to use our variable
clf entropy that we created
and then you'll see do predict and
that's very common in the sklearn
modules that their different tools have
the predict when you're actually running
a prediction in this case we're going to
put our X test data in here now if you
delivered this for use an actual
commercial use and distributed it this
would be the new loans you're putting in
here to guess whether the person's going
to be uh pay them back or not in this
case so we need to test out the data and
to see how good our sample is how good
of our tree does at predicting the loan
payments and finally since Anaconda
Jupiter notebook is it works as a
command line for python we can simply
put the Y predict e in to print it I
could just as easily have put the
print and put brackets around y predict
e in to print it out we'll go ahead and
do that it doesn't matter which way you
do
it and you'll see right here that it
runs a prediction this is roughly 300 in
here remember it's 30% of a th so you
should have about 300 answers in here
and this tells you which each one of
those lines of our test went in there
and this is what our y predict came out
so let's move on to the next step where
we're going to take this data and try to
figure out just how good a model we have
so here we go since sklearn does all the
heavy lifting for you and all the math
we have a simple line of code to let us
know what the accuracy is and let's go
ahead and go through that and see what
that means and what that looks like
let's go ahead and paste this in in and
let me zoom in a little bit there we go
so you have a nice full picture and
we'll see here we're just going to do a
print accuracy is and then we do the
accuracy score and this was something we
imported um earlier if you remember at
the very beginning let me just scroll up
there real quick so you can see where
that's coming from that's coming from
here down here from sklearn do metrics
import accuracy score and you could
probably run a script make your own
script to do do this very easily how
accurate is it how many out of 300 do we
get right and so we put in our y test
that's the one we ran the predict on and
then we put in our y predict in that's
the answers we got and we're just going
to multiply that by 100 because this is
just going to give us an answer as a
decimal and we want to see it as a
percentage and let's run that and see
what it looks like and if you see here
we got an accuracy of 93.
66667 so when we look at the number of
loans and we look at how good our model
fit we can tell people it has about a
93.6 fitting to it so just a quick recap
on that we now have accuracy setup on
here and so we have created a model that
uses the decision tree algorithm to
predict whether a customer will repay
the loan or not the accuracy of the
model is about
94.6% the bank can now use this model to
decide whether it should approve the
loan request from a particular customer
or not and so this information is really
powerful we might not be able ble to as
individuals understand all these numbers
because they have thousands of numbers
that come in but you can see that this
is a smart decision for the bank to use
a tool like this to help them to predict
how good their uh profits going to be
off of the loan balances and how many
are going to default or not let us dig
deep into the theory of exactly how it
works and let's look at what is random
Forest random forest or random decision
Forest is a method that operates by
constructing multiple decision trees the
decision of the majority of the trees is
chosen by the random Forest as the final
decision and this uh we have some nice
Graphics here we have a decision tree
and they actually use a real tree to
denote the decision tree which I love
and given a random some kind of picture
of a fruit this decision tree decides
that the output is it's an apple and we
have a decision tree to where we have
that picture of the fruit goes in and
this one decides that it's a lemon and
the decision 3 tree gets another image
and it decides it's an apple and then
this all go together in what they call
the random forest and this random Forest
then looks at it and says okay I got two
votes for apple one vote for lemon the
majority is Apples so the final decision
is apples to understand how the random
Forest works we first need to dig a
little deeper and take a look at the
random forest and the actual decision
tree and how it builds that decision
tree and looking closer at how the
individual decision trees work we'll go
ahead and continue to use the fruit
example since we're talking about trees
and forests a decision tree is a tree
shaped diagram used to determine a
course of action each branch of the tree
represents a possible decision
occurrence or reaction so in here we
have a bowl of fruit and if you look at
that it looks like um they switch from
lemons to oranges so we have oranges
cherries and apples and the first
decision of the decision tree might be
is a diameter greater than or equal to
three and if it says false it knows that
they're cherries because everything else
is bigger than that so all the cherries
fall into that decision so we have all
that data we're training we can look at
that we know that that's what's going to
come up is the color orange well goes hm
orange or red well if it's true then it
comes out as the orange and if it's
false that leaves apples so in this
example it sorts out the fruit in the
bowl or the images of the fruit a
decision tree these are very important
terms to knowe because these are very
Central to understanding the decision
tree and when working with them the
first is entropy everything on the
decision tree and how it makes those
decision is based on entropy entropy is
a measure of Randomness or
unpredictability in the data set uh then
they also have Information Gain the leaf
node the decision node and the root node
we'll cover these other four terms as we
go down the tree but let's start with
entropy so starting with entropy we have
here um a high amount of Randomness what
that means is that whatever is coming
out of this decision if it was going to
guess based on this data it wouldn't be
able to tell you whether it's a lemon or
an apple it would just say it's a fruit
uh so the first thing we want to do is
we want to split this apart and we take
the initial data set we're going to S
create a data set one and a data set two
we just split it in two and if you look
at these new data sets after splitting
them the entropy of each of those sets
is much less so for the first one
whatever comes in there it's going to
sort that data and it's going to say
okay if this data goes this direction
it's probably an apple and if it goes
into the other direction it's probably a
lemon so that brings us up to
Information Gain it is the measure of
decrease in the entropy after the data
set is split what that means in here is
that we've gone from one set which has a
very high entropy to two lower sets of
entropy and we've added in the values of
E1 for the first one and E2 for the
second two which are much lower and so
that Information Gain Is incre increase
greatly in this example and so you can
find that the information grain simply
equals uh decision E1 minus E2 as we're
going down our list of uh definitions
we'll look at the leaf node and the leaf
node carries the classification or the
decision so we look down here to the
leaf node we finally get to our set one
or our set two when it comes down there
and it says okay this object's gone into
set one if it's gone into set one
it's going to be split by some means and
we'll either end up with apples on the
leaf node or a lemon on the leaf node
and on the right it either be an apple
or lemons those Leaf nodes or those
final decisions or
classifications uh that's the definition
of leaf node in here if we're going to
have a final Leaf where we make the
decision we should have a name for the
nodes above it and they call those
decision nodes a decision node decision
node has two or more branches and you
can can see here where we have the uh
five apples and one lemon and in the
other case the five lemons and one apple
they have to make a choice of which tree
It Goes Down based on some kind of
measurement or information given to the
tree and that brings us to our last
definition the root node the topmost
decision node is known as the root node
and this is where you have all of your
data and you have your first decision it
has to make or the first split in
information so Far We've looked at a
very general image um with the fruit
being split let's look and see exactly
what that means to split the data and
how do we make those decisions on there
uh let's go in there and find out how
does a decision tree work so let's try
to understand this and let's use a
simple example and we'll stay with the
fruit we have a bowl of fruit and so
let's create a problem statement and the
problem is we want to classify the
different types of fruits in the Bowl
based on different features the data set
in the bowl is looking quite messy and
the entropy is high in this case so if
this bow was our decision maker it would
know what choice to make it has so many
choices which one do you pick Apple
grapes or lemons and so we look in here
we're going to start with a dra a
training set so this is our data that
we're training our data with and we have
a number of options here we have the
color and under the color we have red
yellow purple uh we have a diameter uh
331 331 and we have a label Apple lemon
Grapes apple lemon grapes and how do we
split the data we have to frame the
conditions to split the data in such a
way that the Information Gain is the
highest it's very key to note that we're
looking for the best gain we don't want
to just start sorting out the smallest
piece in there we want to split it the
biggest way we can and so we measure
this decrease in entropy that's what
they call it entropy there's our entropy
after splitting and now we'll try to
choose a condition that gives gives us
the highest gain we will do that by
splitting the data using each condition
and checking the gain that we get out of
them the conditions that give us the
highest gain will be used to make the
first split so let's take a look at
these different conditions we have color
we have diameter and if we look
underneath that we have a couple
different values we have diameter equals
3 color equals yellow red diameter
equals 1 and when we look at that you'll
see over here we have 1 2 3 four threes
that's a pretty high selection so let's
say the condition gives us the maximum
gain of three so we have the most pieces
fall into that range so our first split
from our decision node is we split the
data based on the diameter is it greater
than or equal to three if it's not
that's false it goes into the grape bowl
and if it's true it goes into a bowl
fold of lemon and apples the interp
after splitting has decreased
considerably so now we can make two
decisions if you look at they're very
much less chaos going on there this node
has already attained an entropy value of
zero as you can see there's only one
kind of label left for this Branch so no
further splitting is required for this
node however this node on the right is
still requires a split to decrease the
entropy further so we split the right
node further based on color if you look
at this if I split it on color that
pretty much cuts it right down the
middle it's the only thing we have left
in our choices of color and diameter too
and if the color
is yellow it's going to go to the right
bowl and if it's false it's going to go
to the left Bowl so the entropy in this
case is now zero so now we have three
bowls with zero entropy there's only one
type of data in each one of those bowls
so we can predict a lemon with 100%
accuracy and we can predict the Apple
also with 100% accuracy along with our
grapes up there so we've looked at kind
of a basic tree in our forest but what
we really want to know is how does a
random Forest work as a whole so to
begin our um random Forest classifier
let's say we already have built three
trees and we're going to start with the
first tree that looks like this just
like we did in the example this tree
looks at the diameter if it's greater
than or equal to three it's true
otherwise it's false so one side goes to
the smaller diameter one side goes to
larger diameter and if the color is
orange it's going to go to the right
true we're using oranges now instead of
lemons and if it's red it's going to go
to the left false and we build a second
tree very similar but split differently
instead of the first one being split by
a diameter uh this one when they created
it if you look at that first Bowl it has
a lot of red objects so it says is the
color red because that's going to bring
our entropy down the fastest and so of
course if it's true it goes to the left
if it's false it goes to the right and
then it looks at the shape false or true
and so on and so on and tree three is
the diameter equal to one and it came up
with this because there's a lot of
cherries in this bowl so that would be
the biggest split on there is is the
diameter equal to one that's going to
drop the entropy the quickest and as you
can see it splits it into true if it
goes false and they've added another
category does it grow in the summer and
if it's false it goes off to the left if
it's true it goes off to the right let's
go ahead and bring these three trees so
you can see them all in one image so
this would be three completely different
trees categorizing a fruit and let's
take a fruit now let's try this and this
fruit if you look at it we've blackened
it out you can't see the color on it so
it's missing data remember one of the
things we talked about earlier is that a
random Forest works really good if
you're missing data if you're missing
pieces so this fruit has an image but
maybe a person had a black and white
camera when they took the picture and
we're going to take a look at this and
it's going to have um they put the color
in there so ignore the color down there
but the diameter equals three we find
out it grows in the summer equals yes
and the shape is a circle and if you go
to the right you can look at what one of
the decision trees did this is the third
one is a diameter greater than equal to
three is a color orange well it doesn't
really know on this one but it if you
look at the value it say true and it go
to the right tree 2 classifies it as
cherries is a color equal red is the
shape a circle true it is a circle so
this would look at it and say oh that's
a cherry and then we go to the other
classifier and it says is the diameter
equal one well that's false does it grow
in the summer true so it goes down and
looks at as oranges so how does this
random Forest work the first one says
it's an orange the second one said it
was a cherry and the third one says H
it's an orange and you can guess that if
you have two oranges and one says it's a
cherry uh when you add that all together
the majority of the vote says orange so
the answer is it's classified as an
orange even though we didn't know the
color and we're missing data on it I
don't know about you but I'm I'm getting
tired of fruit so let's switch and I did
promise you we'd start looking at a case
example and get into some python coding
today we're going to use the case the
iris flower analysis o this is the
exciting part as we roll up our sleeves
and actually look at some python coding
before we start the python coding we
need to go ahead and create a problem
statement wonder what species of Iris do
these flowers belong to let's try to
predict the species of the flowers using
machine learning in Python let's see how
it can be done so here we begin to go
ahead and Implement our python code and
you'll find that the first half of our
implementation is all about organizing
and exploring the data coming in let's
go ahead and take this first step which
is loading the different modules into
Python and let's go ahead and put that
in our favorite editor whatever your
favorite editor is in this case I'm
going to be using the Anaconda Jupiter
notebook which is one of my favorites
certainly there's notepad++ and E clips
and dozens of others or just even using
the python terminal window any of those
will work just fine to go ahead and
explore this python coding so here we go
let's go ahead and flip over to our
Jupiter notebook and I've already opened
up a new page for Python 3 code and I'm
just going to paste this right in there
and let's take a look and see what we're
bringing into our python the first thing
we're going to do is from the SK learn.
dat sets import load Iris now this isn't
the actual data this is just the module
that allows us to bring in the data the
load Iris and the iris is so popular
it's been around since 1936 when Ronald
fiser published a paper on it and
they're measuring the different parts of
the flower and based on those
measurements predicting what kind of
flower it is and then if we're going to
do a random Forest classifier we need to
go ahead and import a random forest
classifier from the sklearn module so SK
learn. Ensemble import random force
classifier and then we want to bring in
two more modules uh and these are
probably the most commonly used modules
in Python and data science with any of
the um other modules that we bring in
and one is going to be pandas we're
going to import pandas as PD PD is the
common term used for pandas and pandas
is basically creates a data format for
us where when you create a panda data
frame it looks like an Excel spreadsheet
and you'll see that in a minute when we
start digging deeper into the code panda
is just wonderful cuz it plays nice with
all the other modules in there and then
we have numpy which is our numbers
Python and the numbers python allows us
to do different mathematical sets on
here we'll see right off the bat we're
going to take our NP and we're going to
go ahead and Seed the randomness with it
with zero so np. random. seed is seeding
that is zero this code doesn't actually
show anything we're going to go ahead
and run it because I need to make sure I
have all those loaded and then let's
take a look at the next module on here
the next six slides including this one
are all about exploring the data
remember I told you half of this is
about looking at the data and getting it
all set so let's go ahead and take this
code right here the script and let's get
that over into our Jupiter notebook and
here we go we've gone ahead and uh run
the Imports and I'm going to paste the
code down
here and let's take a look and see
what's going on the first thing we're
doing is we're actually loading the iris
data and if you remember up here we
loaded the module that tells it how to
get the iris data now we're actually
assigning that data to the variable Iris
and then we're going to go ahead and use
the DF to Define data frame and that's
going to equal PD and if you remember
that's pandas as PD so that's our pandas
and Panda data frame and then we're
looking at Iris data and columns equals
Iris feature names and we're going to do
the DF head and let's run this so you
can understand what's going on
here the first thing you want to notice
is that our DF has created uh what looks
like an Excel spreadsheet and in this
Excel spreadsheet we have set the
columns so up on the top you can see the
four different columns and then we have
the data iris. dat down below it's a
little confusing without knowing where
this data is coming from so let's look
at the bigger picture and I'm going to
go print I'm just going to change this
for a moment and we're going to print
all of Iris and see what that looks like
so when I print all of irus I get this
long list of information
and you can scroll through here and see
all the different titles on there what's
important to notice is that first off
there's a brackets at the beginning so
this is a python
dictionary and in a python dictionary
you'll have a key or a label and this
label pulls up whatever information
comes after it so feature names which we
actually used over here under columns is
equal to an array of SE length seel
width pedal length pedal width these SE
are the different names they have for
the four different columns and if you
scroll down far enough you'll also see
data down here oh goodness it came up
right towards the top and uh data is
equal to the different data we're
looking
at now there's a lot of other things in
here like Target we're going to be
pulling that up in a minute and there's
also the names uh the target names which
is further down and we'll show you that
also in a minute let's go ahead and set
that back to the head and this is one of
the neat features of pandas and Panda
data frames is when you do dfad or the
panda datf frame. head it'll print the
first five lines of the data set in
there along with the headers if you have
them in this case we have the column
headers set to Iris features and in here
you'll see that we have 0 1 2 3 4 in
Python most arrays always start at zero
so when you look at the first 5 it's
going to be 0 1 2 3 4 not 1 2 3 4 5 so
now we've got our Iris data import it
into a data frame let's take a look at
the next piece of code in here and so in
this section here of the code we're
going to take a look at the Target and
let's go ahead and get this into our
notebook this piece of code so we can
discuss it a little bit more in detail
so here we are in our Jupiter notebook
I'm going to put the code in here and
before I run it I want to look at a
couple things going on so we have a DF
species and this is interesting cuz
right here you'll see where I have DF
species in brackets which is uh the key
code for creating another column and
here we have iris. Target now these are
both in the pandas setup on here so in
pandas we can do either one I could have
just as easily done Iris and then in
Brackets Target depending on what I'm
working on both are um acceptable let's
go ahead and run this code and see how
this changes and what we've done is
we've added the target from the iris
data set as another column on the
end now what species is this is what
we're trying to predict so we have our
data which tells us the answer for all
these different pieces and then we've
added a column with the answer that way
when we do our final setup we'll have
the ability to program our our neural
network to look for these this different
data and know what a Sosa is or a Vera
color which we'll see in just a minute
or virginica those are the three that
are in there and now we're going to add
one more column I know we're organizing
all this data over and over again it's
kind of fun there's a lot of ways to
organize it what's nice about putting
everything onto one data frame is I can
then do a print out and it shows me
exactly what I'm looking at and I'll
show you where you where that's
different where you can alter that and
do it slightly differently but let's go
ahead and put this into our script up to
De now and here we go we're going to put
that down here and we're going to run
that and let's talk a little bit about
what we're doing now we're exploring
data
and one of the challenges is knowing how
good your model is did your model work
and to do this we need to split the data
and we split it into two different parts
they usually call it the training and
the testing and so in here we're going
to go ahead and put that in our database
so you can see it clearly and we've set
it DF remember you can put brackets this
is creating another column is train so
we're going to use part of it for
training and this equals NP remember
that stands for numpy random. uniform so
we're generating a random number between
0 and one and we're going to do it for
each of the rows that's where the length
DF comes from so each row gets a
generated number and if it's less than
75 it's true and if it's greater than 75
it's false this means we're going to
take 75% of the data roughly because
there's a Randomness involved and we're
going to use that to train it and then
the other 25% we're going to hold off to
the side and use that to test it later
on so let's flip back on over and see
what the next step is so now that we've
labeled our database for which is
training and which is testing let's go
ahead and sort that into two different
variables train and test and let's take
this code and let's bring it into our
project and here we go let's paste it on
down here and before I run this let's
just take a quick look at what's going
on here is we have up above we created
remember there's our def. head which
prints our first five rows and we've
added a column is train at the end and
so we're going to take that we're going
to create two variables we're going to
create two new data frames one's called
train one's called test 75% in train 25%
in test and then to sort that out we're
going to do that by doing DF our main
original data frame with the iris data
in it and if DF is train equals true
it's going to go in the train and if DF
is train equals false it goes in the Tex
test and so when I run this we're going
to print out the number in each one
let's see what that looks like and
you'll see that it puts 118 in the
training module and it puts 32 in the
testing module which lets us know that
there was 150 lines of data in here so
if you went and looked at the original
data you can see that there's 150 lines
and that's roughly 75% in one and 25%
for us to test our model on afterward so
let's jump back to our code and see
where this goes in the next two steps we
want to do one more thing with our data
and that's make it readable to humans um
I don't know about you but I hate
looking at zeros and ones so let's start
with the features and let's go ahead and
take those and make those readable to
humans and let's put that in our
code let's see here we go paste it in
and you'll see here we've done a couple
very basic things we know that the
columns in our data frame again this is
a panda thing the DF
columns and we know the first four of
them 0 1 2 3 that'd be the first four
are going to be the features or the
titles of those columns and so when I
run this you'll see down here that it
creates an index sepa length sea width
pedal length and petal width and this
should be familiar because if you look
up here here's our column titles going
across and here's the first
four one thing I want you to notice here
is that when you're in a command line
whether it's jupyter notebook or you're
running command line in the uh terminal
window if you just put the name of it
it'll print it out this is the same as
doing
print
features and the Shand is you just put
features in here if you're actually
writing a code and saving the script and
running it by remote you really need to
put the print in there but for this when
I run it you'll see it gives me the same
thing but for this we want to go ahead
and we'll just leave it as features cuz
it doesn't really matter
and this is one of the fun thing about
Jupiter notebooks is I'm just building
the code as we go and then we need to go
ahead and create the labels for the
other part so let's take a look and see
what that for our final step in prepping
our data before we actually start
running the training and the testing is
we're going to go ahead and convert the
species on here into something the
computer understands so let's put this
code into our script and see where that
takes
us all right here we go we set y equal
to PD
factorize train species of zero so let's
break this down just a little bit we
have our pandas right here PD factorize
what is factorize doing I'm going to
come back to that in just a second let's
look at what train species is and why
we're looking at the group zero on there
and let's go up here and here is our
species remember this on that we created
this whole column here for species and
then it has has Sosa satsa Sosa Sosa and
if you scroll down enough you'd also see
virginica and Vera color we need to
convert that into something the computer
understands zeros and ones so the
trained species of zero because this is
in the format of a of an array of arrays
so you have to have the zero on the end
and then species is just that column
factorize goes in there and looks at the
fact that there's only three of them so
when I run this you'll see that y
generates an array that's equal to in
this case it's a training set and it's
zeros ones and twos representing the
three different kinds of flowers we have
so now we have something the computer
understands and we have a nice table
that we can read and understand and now
finally we get to actually start doing
the predicting so here we go uh we have
two lines of code oh my goodness that
was a lot of work to get to two lines of
code but there is a lot in these two
lines of code so let's take a look and
see what's going on here and put this
into our full script that we're running
and let's paste this in here and let's
take a look and see what this is we have
we're creating a variable CF and we're
going to set this equal to the random
forest classifier and we're passing two
variables in here and there's a lot of
variables you can play with as far as
these two are concerned they're very
standard in jobs all that does is to
prioritize it not something to really
worry about usually when you're doing
this on your own computer you do in jobs
equals 2 if you're working in a larger
or big data and you need to prioritize
it differently this is what that number
does is it changes your priorities and
how it's going to run across the system
and things like that and then the random
state is just how it starts zero is fine
for
here but uh let's go ahead and run
this we also have cf. fit train features
comma Y and before we run it let's talk
about this a little bit more clf
do fit so we're fitting we're training
it we are actually creating our random
Forest classifier right here this is a
code that does everything and we're
going to take our training set remember
we kept our test off to the side and
we're going to take our training set
with the features and then we're going
to go ahead and put that in and here's
our Target the Y so the Y is 0 1 and two
that we just created and the features is
the actual data going in that we put
into the training set let's go ahead and
run
that and this is kind of an interesting
thing because it printed out the random
force
classifier and everything around it and
so when you're running this in your
terminal window or in a script like this
this automatically treats us like just
like when we were up here and I typed in
y and I printed out y instead of print y
this does the same thing it treats this
as a variable and prints it out but if
you're actually running your code that
wouldn't be the case and what it's
printed out is it shows us all the
different variables we can change and if
we go down here you can actually see in
jobs equals 2 you can see the random
State equals zero those are the two that
we sent in there you would really have
to dig deep to find out all these
different meanings of all these
different settings on here some of them
are self-explanatory if you kind of
think about it a little bit like Max
features is auto so all the features
that we're putting in there is just
going to automatically take all four of
them whatever we send it it'll take some
of them might have so many features
because you're processing words there
might be like 1.4 million features in
there because you're doing legal
documents and that's how many different
words are in there at that point you
probably want to limit the maximum
features that you're going to process
and leaf nodes that's the end nodes
remember we had the fruit and we're
talking about the leaf nodes like I said
there's a lot in this we're looking at a
lot of stuff here so you might have uh
in this case there's probably only think
three leaf nodes maybe four you might
have thousands of leaf nodes at which
point you do need to put a cap on on
that and say okay you can only go so far
and then we're going to use all of our
resources on processing this and that
really is what most of these are about
is limiting the process and making sure
we don't uh overwhelm a system and
there's some other settings in here
again we're not going to go over all of
them warm start equals false warm start
is if you're programming it one piece at
a time externally since we're not we're
not going to have like we're not going
to continually to train this particular
Learning Tree and again like I said
there's a lot of things in here that
you'll want to look up more detail from
the
sklearn and if you're digging in deep
and running a major project on here for
today though all we need to do is fit or
train our features and our Target why so
now we have our training model what's
next if we're going to create a
model we now need to test it remember we
set aside the test feature test group
25% of the data so let's go ahead and
take this code and let's put it into our
uh script and see what that looks like
okay here we go and we're going to run
this and it's going to come out with a
bunch of zeros ones and twos which
represents the three type of flowers the
satsa the virginica and the Versa color
and what we're putting into our predict
is the test features and I always kind
of like to know what it is I am looking
at so real quick we're going to do
test features and remember features is
an
array of SEO length SEO width pedal
length pedal width so when we put it in
this way it actually loads all these
different columns that we loaded into
features so if we did just features let
me just do features in here so you can
see what features looks like this is
just playing with the with Panda's data
frames you'll see that it's an index so
when you put an index in like
this into test features into test it
then takes those columns and creates a
panda data frames from those columns and
in this case we're going to go ahead and
put those into our predict so we're
going to put each one of these lines of
data the 5.0 3.4
1.5.2 and we're going to put those in
and we're going to predict what our new
um Forest classifier is going to come up
with and this is what it predicts it
predicts uh 01
21222 and and uh again this is the
flower type satos of virginica and Versa
color so now that we've taken our test
features let's explore that let's see
exactly what that data means to us so
the first thing we can do with our
predicts is we can actually generate a
different prediction model when I say
different we're going to view it
differently it's not that the data
itself is different so let's take this
next piece of code and put it into our
script so we're pasting it in here and
you'll see that we're doing uh predict
and we've added underscore proba for
probability so there's our cf. predict
probability so we're we're running it
just like we run it up here but this
time with this we're going to get a
slightly different result and we're only
going to look at the first 10 so you'll
see down here instead of looking at all
of them uh which was uh what 27 you'll
see right down here that this generates
a much larger field on the probability
and let's take a look and see what that
looks like and what that means
so when we do the predict underscore
proba for probability it generates three
numbers so we had three leaf nodes at
the end and if you remember from all the
theory we did this is the predictors the
first one is predicting a one for satsa
it predicts a zero for virginica and it
predicts a zero for versacolor and so on
and so on and so on and let's um you
know what I'm going to change this just
a little bit let's look at
10 to 20 just because we
can and we start to get in a little
different of data and you'll see right
down here it gets to this one this line
right here and this line has 0 0.5
0.5 and so if we're going to vote and we
have two equal votes it's going to go
with the first one so it says uh satsa
gets zero votes virginica gets 0.5 votes
Versa color gets 0.5 votes but let's
just go with the virginica since these
two are equal and so on and so on down
the list you can see how they vary on
here so now we've looked at both how to
do a basic predict of the features and
we've looked at the predict probability
let's see what's next on here so now we
want to go ahead and start mapping names
for the plants we want to attach names
so that it makes a little more sense for
us and that's what we're going to do in
these next two steps we're going to
start by setting up our predictions and
mapping them to the name so let's see
what that looks like
and let's go ahead and paste that code
in here and run it and this goes along
with the next piece of code so we'll
skip through this quickly and then come
back to it a little bit so here's
iris. Target
names and uh if you remember correctly
this was the the names that we've been
talking about this whole time the satsa
virginica versacolor and then we're
going to go ahead and do the prediction
again we run we could have just set a
variable equal to this instead of
rerunning it each time but we're go
ahead and run it again see lf. predict
test features remember that Returns the
zeros the ones and the twos and then
we're going to set that equal to
predictions so this time we're actually
putting it in a variable and when I run
this it distributes and it comes out as
an array and the array is sosa Sosa
stosa Sosa Sosa we're only looking at
the first five we could actually do
let's do the first 25 just so we can see
a little bit more on there and you'll
see that it starts uh mapping it to all
the different flower types the Versa col
and the virginica in there and let's see
how this goes with the next one so let's
take a look at the top part of our
species in here and we'll take this code
and put it in our
script and let's put that down here and
paste it there we go and we'll go ahead
and run it and let's talk about both
these sections of code here and how they
go together the first one is our
predictions and I went ahead and did uh
predictions through 25 let's just do
five and so we have sitosis sitosis
ptosis sitosis that's what we're
predicting from our test model and then
we come down here and we look at test
species I remember I could have just
done test. species. head and you'll see
it says Sosa Sosa Sosa Sosa and they
match so the first one is what our
forest is doing and the second one is
what the actual data is now is we need
to combine these so that we can
understand what that means we need to
know how good our forest is how good it
is at predicting the features so that's
where we come up to the next step which
is lots of fun we're going to use a
single line of code to combine our
predictions and our actuals so we have a
nice chart to look at and let's go ahead
and put that in our script in our
Jupiter notebook here let's see let's go
ahead and paste that in and then I'm
going to because I'm on the Jupiter
notebook I can do a control minus you
can see the whole line
there there we go resize it and let's
take a look and see what going on here
we're going to create in pandas remember
PD stands for pandas and we're doing a
cross tab this function takes two sets
of data and creates a chart out of them
so when I run it you'll get a nice chart
down here and we have the predicted
species so across the top you'll see the
Sosa vers color vinica and the actual
species Sosa Versa color vergen and so
the way to read this chart and let's go
ahead and take a look on how to read
this chart here when you read read this
chart you have Sosa where they meet you
have verol where they meet and you have
virginica where they meet and they're
meeting where the actual and the
predicted agree so this is the number of
accurate predictions so in this case it
equals 30 if you had 13 + 5 + 12 you get
30 and then we notice here where it says
virginica but it was supposed to be
versacolor this is inaccurate so now we
have two two inaccurate predictions and
30 accurate predictions so we'll say
that the model accuracy is 93 that's
just 30 ID 32 and if we multiply by 100
we can say that it is 93% accurate so we
have a 93% accuracy with our model I did
want to add one more quick thing in here
on our scripting before we wrap it up so
let's flip back on over to my script in
here we're going to take this uh line of
code from up above I don't know if you
remember it but predicts equals the
iris. target underscore names so we're
going to map it to the names and we're
going to run the prediction and we ran
it on test features but you know we're
not just testing it we want to actually
deploy it so at this point I would go
ahead and change this and this is an
array of arrays this is really important
when you're running these to know that
so you need the double brackets and I
could actually create data maybe let's
let's just do two flowers so maybe I'm
processing more data coming in and we'll
put two flowers in here and and then uh
I actually want to see what the answer
is so let's go ahead and type in PRS and
print that out and when I run this
you'll see that I've now predicted two
flowers that maybe I measured in my
front yard as Versa color and Versa
color not surprising since I put the
same data in for each one this would be
the actual uh end product going out to
be used on data that you don't know the
answer
for so that's going to conclude our
scripting part of this this and let's
just go ahead and take a look at the key
takeaways with today's tutorial if you
want to become an AI expert and gain
handsome salary packages look at the
wide range of AIML courses by simply
learn in collaboration with top
universities across the globe by
enrolling in any of these certification
programs you will gain expertise in
skills like generative AI prompt
engineering chat GPT explainable AI
machine learning algorithms supervised
and unsupervised learning model training
and optimization and there's much more
on the list with hands-on experience in
the tools like Char GPT di python open
CV and tens oflow you will catch the
eyes of top recruiters so what are you
waiting for hurry up and enroll now an
year of experience is preferred to
enroll in these courses find the course
Link in the description Box by now we
all know machine learning models make
predictions by learning from the past
data available so we have our input
values our machine learning model Builds
on those inputs of what we already know
and then we use that to create a
predicted output is that a dog little
kid looking over there and watching the
black cat cross their path no dear you
can differentiate between a cat and a
dog based on their
characteristics cats cats have sharp
claws uses to climb smaller length of
ears meows and purs doesn't love to play
around dogs they have claws bigger
length of ears barks loves to run around
you usually don't see a cat running
around people although I do have a cat
that does that we're dogs do and we can
look at these we can say uh we can
evaluate the sharpness of the claws how
sharp are their claws and we can
evaluate the length of the ears and we
can usually sort out cats from dogs
based on even those two
characteristics now tell me if it is a
cat or a dog odd question usually little
kids know cats and dogs by now unless
they live a place where there's not many
cats or dogs so if we look at the
sharpness of the claws the length of the
ears and we can see that the cat has
smaller ears and sharper claws than the
other animals its features are more like
cats it must be a cat sharp claws length
of ears and it goes in the cat group
because KNN is based on feature
similarity we can do classification
using KNN classifier so we have our
input value the picture of the black cat
it goes into our trained model and it
predicts that this is a cat coming out
so what is KNN what is the KNN algorithm
K nearest neighbors is what that stands
for it's one of the simplest supervised
machine learning algorithms mostly used
for classification so we want to know is
this a dog or it's not a dog is it a cat
or not a cat it classifies a data point
based on how its neighbors are
classified KNN stores all available
cases and classifies new cases based on
a similarity measure and here we gone
from cats and dogs right into wine
another favorite of mine KNN stores all
available cases and classifies new cases
based on a similarity measure and here
you see we have a measurement of sulfur
dioxide versus the chloride level and
then the different wines they've tested
and where they fall on that graph based
on how much sulfur dioxide and how much
chloride K and KNN is a perimeter that
refers to the number of nearest
neighbors to include in the majority of
the voting process and so if we add a
new glass of wine there red or white we
want to know what the neighbors are in
this case we're going to put uh k equals
5 we'll talk about K in just a minute a
data point is classified by the majority
of votes from its five nearest neighbors
here the unknown point would be
classified as red since four out of five
neighbors are red so how do we choose K
how do we know k equals 5 I mean that's
was the value we put in there so we're
going to talk about it how do we choose
the factor k k andn algorithm is based
on feature similarity choosing the right
value of K is a process called parameter
tuning and is important for better
accuracy so at k equal 3 we can classify
we have a question mark in the middle as
either a as a square or not is it a
square or is it in this case a triangle
and so if we set k equals to three we're
going to look at the three nearest
neighbors we're going to say this is a
square and if we put k equals a 7 we
classify as a triangle depending on what
the other data is around and you can see
as the K changes depending on where that
point is that drastically changes your
answer and uh we jump here we go how do
we choose the factor of K you'll find
this in all machine learning choosing
these factors that's the face you get
it's like oh my gosh did I choose the
right K did I set it right my values in
whatever machine learning tool you're
looking at so that you don't have a huge
bias in One Direction or the other and
in terms of knnn the number of K if you
choose it too low the bias is based on
it's just too noisy it's it's right next
to a couple things and it's going to
pick those things and you might get a
skewed answer and if your K is too big
then it's going to take forever to
process so you're going to run into
processing issues and resource issues so
what we do the most common use and
there's other options for choosing K is
to use the square root of n so N is a
total number of values you have you take
the square root of it in most cases you
also if it's an even number so if you're
using uh like in this case squares and
triangles if it's even you want to make
your K value odd that helps it select
better so in other words you're not
going to have a balance between two
different factors that are equal so
usually take the square root of N and if
it's even you add one to it or subtract
one from it and that's where you get the
K value from that is the most common use
and it's pretty solid it works very well
when do we use KNN we can use KNN when
data is labeled so you need a label on
it we know we have a group of pictures
with dogs dogs cats cats data is Noise
free and so you can see here when we
have a class and we have like
underweight 140 23 three Hello Kitty
normal that's pretty confusing we have a
high variety of data coming in so it's
very noisy and that would cause an issue
data set is small so we're usually
working with smaller data sets where I
you might get into gig of data if it's
really clean doesn't have a lot of noise
because KNN is a lazy learner I.E it
doesn't learn a discriminative function
from the training set so it's very lazy
so if you have very complicated data and
you have a large amount of it you're not
going to use the KNN but it's really
great to get a place to start even with
large data you can sort out a small
sample and get an idea of what that
looks like using the KNN and also just
using for smaller data sets KNN works
really good how does a KNN algorithm
work consider a data set having two
variables height and centimeters and
weight in kilograms and each point is
classified as normal or underweight so
we can see right here we have two
variables you know true false they're
either normal or they're not they're
underweight on the basis of the given
data we have to classify the below set
as normal or underweight using KNN so if
we have new data coming in that says 57
kilg and 177 cm is that going to be
normal or underweight to find the
nearest neighbors we'll calculate the
ukian distance according to the ukian
distance formula the distance between
two points in the plane with the
coordinates XY and ab is given by
distance D equals the Square < t of x -
A 2 + y - b^ 2 and you can remember that
from the two edges of a triangle we're
Computing the third Edge since we know
the X side and the yide let's calculate
it to understand clearly so we have our
unknown point and we placed it there in
red and we have our other points where
the data is scattered around the
distance D1 is a square otk of 170 - 167
2 + 57 - 51 2 which is about 6.7 and
distance two is about 13 and distance
three is about 13.4 similarly we will
calculate the ukian distance of unknown
data point from all the points in the
data set and because we're dealing with
small amount of data that's not that
hard to do and it's actually pretty
quick for a computer and it's not a
really complicated Mass you can just see
how close is the data based on the ukian
distance hence we have calculated the
ukian distance of unknown data point
from all the points as showing where X1
and y1 equal 57 and 170 whose class we
have to classify find now we're looking
at that we're saying well here's the
ukian distance who's going to be their
closest neighbors now let's calculate
the nearest neighbor at k equals three
and we can see the three closest
neighbors puts them at normal and that's
pretty self-evident when you look at
this graph it's pretty easy to say okay
what you know we're just voting normal
normal normal three votes for normal
this is going to be a normal weight so
majority of neighbors are pointing
towards normal hence as per KNN
algorithm the class of 57170 should be
normal so recap of KNN in positive
integer K is specified along with a new
sample we select the K entries in our
database which are closest to the new
sample we find the most common
classification of these entries this is
the classification we give to the new
sample so as you can see it's pretty
straightforward we're just looking for
the closest things that match what we
got so let's take a look and see what
that looks like in a use case in Python
so let's dive in the predict diabetes
use case so use case predict diabetes
the object objective predict whether a
person will be diagnosed with diabetes
or not we have a data set of 768 people
who were or were not diagnosed with
diabetes and let's go ahead and open
that file and just take a look at that
data and this is in a simple spreadsh
sheeet format the data itself is comma
separated very common set of data and
it's also a very common way to get the
data and you can see here we have
columns a through I that's what 1 2 3 4
5 6 7 8 um eight columns colums with a
particular tribute and then the ninth
column which is the outcome is whether
they have diabetes as a data scientist
the first thing you should be looking at
is insulin well you know if someone has
insulin they have diabetes because
that's why they're taking it and that
could cause issue on some of the machine
learning packages but for a very basic
setup this works fine for uh doing the
KNN and the next thing you notice is it
didn't take very much to open it up um I
can scroll down to the bottom of the
data there's
768 it's pretty much a small data set
you know at 769 I can easily fit this
into my ram on my computer I can look at
it I can manipulate it and it's not
going to really tax just a regular
desktop computer you don't even need an
Enterprise version to run a lot of this
so let's start with importing all the
tools we need and before that of course
we need to discuss what IDE I'm using
certainly you can use any particular
editor for python but I like to use for
doing uh very basic visual stuff the
Anaconda which is great for doing demos
with the Jupiter notebook and just a
quick view of the Anaconda Navigator
which is the new release out there which
is really nice you can see under home I
can choose my application we're going to
be using python 36 I have a couple
different uh versions on this particular
machine if I go under environments I can
create a unique environment for each one
which is nice and there's even a little
button there where I can install
different packages so if I click on that
button and open the terminal I can then
use a simple pip install to install
different packages I'm working with
let's go ahead and go back back under
home and we're going to launch our
notebook and I've already you know kind
of like uh the old cooking shows I've
already prepared a lot of my stuff so we
don't have to wait for it to launch
because it takes a few minutes for it to
open up a browser window in this case
I'm going it's going to open up Chrome
because that's my default that I use and
since the script is pre-done you'll see
you have a number of windows open up at
the top the one we're working in and uh
since we're working on the KNN predict
whether a person will have diabetes or
not let's go and put that title in there
and I'm also going to go up here and and
click on Cell actually we want to go
ahead and first insert a cell below and
then I'm going to go back up to the top
cell and I'm going to change the cell
type to markdown that means this is not
going to run as python it's a markdown
language so if I run this first one it
comes up in nice big letters which is
kind of nice remind us what we're
working on and by now you should be
familiar with doing all of our Imports
we're going to import the pandas as PD
import numpy is NP pandas is the pandas
data frame and numpy is a number array
very powerful tools to use in here so we
have our Imports so we've brought in our
pandas our numpy our two general python
tools and then you can see over here we
have our train test split by now youed
should be familiar with splitting the
data we want to split part of it for
training our thing and then training our
particular model and then we want to go
ahead and test the remaining data just
see how good it is pre-processing a
standard scaler pre-processor so we
don't have a bias of really large
numbers remember in the data we had like
number pregnancies isn't going to get
very large where the amount of insulin
they take and get up to 256 so 256
versus six that will skew results so we
want to go ahead and change that so
they're all uniform between minus one
and one and then the actual tool this is
the K neighbors classifier we're going
to use and finally the last three are
three tools to test all about testing
our model how good is it let me just put
down test on there and we have our
confusion Matrix our F1 score and our
accuracy so we have our two general
python modules we're importing and then
we have our six module specific from the
S learn setup and then we do need to go
ahead and run this so these are actually
imported there we go and then move on to
the next step and so in this set we're
going to go ahead and load the database
we're going to use pandas remember
pandas is PD and we'll take a look at
the data in Python we looked at it in a
simple spreadsheet but usually I like to
also pull it up so that we can see what
we're doing so here's our data set
equals pd. read CSV that's a pandas
command and the diabetes folder I just
put in the same folder where my IPython
script is if you put in a different
folder you'd need the full link on there
we can also do a quick link of uh the
data set that is a simple python command
Len for length we might even let's go
ahead and print that we'll go print and
if you do it on its own line link. data
set in the jupyter notebook it'll
automatically print it but when you're
in most of your different setups you
want to do the print in front of there
and then we want to take a look at the
actual data set and since we're in
pandas we can simply do data set head
and again let's go ahead and add the
print in there if you put a bunch of
these in a row you know the data set one
head data set two head it only prints
out the last one so I usually always
like to keep the print statement in
there but because most projects only use
one data frame Panda data frame doing it
this way doesn't really matter the other
way works just fine and you can see when
we hit the Run button we have the 768
lines which we knew and we have our
pregnancies it's automatically given a
label on the left remember the head only
shows the first five lines so we have
zero through four and just a quick look
at the data you can see it matches what
we looked at before we have pregnancy
glucose blood pressure all the way to Ag
and then the outcome on the end and
we're going to do a couple things in
this next step we're going to create a
list of columns where we can't have zero
there's no such thing as zero skin
thickness or zero blood pressure zero
glucose uh any of those you'd be dead so
not a really good Factor if they don't
if they have a zero in there because
they didn't have the data and we'll take
a look at that because we're going to
start replacing that information with a
couple of different things and let's see
what that looks like so first we create
a nice list as you can see we have the
values talked about glucose blood
pressure skin thickness uh and this is a
nice way when you're working with
columns is to list the columns you need
to do some kind of transformation on uh
very common thing to do and then for
this particular setup we certainly could
use the there's some Panda tools that
will do a lot of this where we can
replace the na but we're going to go
ahead and do it as a data set column
equals data set column. repace this is
this is still pandas you can do a direct
there's also one that that you look for
your n a lot of different options in
here but the na numn an is what that
stands for is is non doesn't exist so
the first thing we're doing here is
we're replacing the zero with a numpy
none there's no data there that's what
that says that's what this is saying
right here so put the zero in and we're
going to replace zeros with no data so
if it's a zero that means the person's
well hopefully not dead hopefully they
just didn't get the data the next thing
we want to do is we're going to create
the mean which is the in integer from
the data set from the column do mean
where we skip Naas we can do that that
is a panda's command there the skip na
so we're going to figure out the mean of
that data set and then we're going to
take that data set column and we're
going to replace all the
npnn with the means why did we do that
and we could have actually just uh taken
this step and gone right down here and
just replaced zero and Skip anything
where except you could actually there's
a way to skip zeros and then just
replace all the zeros but in this case
we want to go ahead and do it this way
so you can see that we're switching this
to a non-existent value then we're going
to create the mean well this is the
average person so if we don't know what
it is if they did not get the data and
the data is missing one of the tricks is
you replace it with the average what is
the most common data for that this way
you can still use the rest of those
values to do your computation and it
kind of just brings that particular
value of those missing values out of the
equation let's go ahead and take this
and we'll go ahead and run it doesn't
actually do anything so we're still
preparing our data if you want to see
what that looks like we don't have
anything in the first few lines so it's
not going to show up but we certainly
could look at a row let's do that let's
go into our data set with printed data
set and let's pick in this case let's
just do glucose and if I run this this
is going to print all the different
glucose levels going down and we
thankfully don't see anything in here
that looks like missing data at least on
the ones it shows you can see it skipped
a bunch in the middle CU that's what it
does if you have too many lines in
Jupiter notebook it'll skip a few and
and go on to the next in a data set let
me go and remove this and we'll just
zero out that and of course before we do
any processing before proceeding any
further we need to split the data set
into our train and testing data that way
we have something to train it with and
something to test it on and you're going
to notice we did a little something here
with the panda database code there we go
my drawing tool we've added in this
right here off the data set and what
this says is that the first one in Panda
this is from the PD pandas it's going to
say within the data set we want to look
at the iocation and it is all rows
that's what that says so we're going to
keep all the rows but we're only looking
at 0er column 0 to 8 remember column 9
here it is right up here we print it in
in here as outcome well that's not part
of the training data that's part of the
answer yes it's column 9 but it's listed
as eight number eight so 0 to8 is nine
columns so uh eight is the value and
when you see it in here zero this is
actually 0 to seven it doesn't include
the last one and then we go down here to
Y which is our answer and we want just
the last one just column 8 and you can
do it this way with this particular
notation and then if you remember we
imported the train test split that's
part of the SK learn right there and we
simply put in our X and our y we're
going to do random State equals zero you
don't have to necessarily seed it that's
a seed number I think the default is one
when you seed it I'd have to look that
up and then the test size test size is
0.2 that simply means we're going to
take 20% of the data and put it aside so
that we can test it later that's all
that is is and again we're going to run
it not very exciting so far we haven't
had any print out other than to look at
the data but that is a lot of this is
prepping this data once you prep it the
actual lines of code are quick and easy
and we're almost there with the actual
runting of our KNN we need to go ahead
and do a scale the data if you remember
correctly we're fitting the data in a
standard scaler which means instead of
the data being from you know 5 to 303 in
one column and the next column is 1 to
six we're going to set that all so that
all the D is between minus1 and one
that's what that standard scaler does
keeps it standardized and we only want
to fit the scaler with the training set
but we want to make sure the testing set
is the X test going in is also
transformed so it's processing it the
same so here we go with our standard
scaler we're going to call it scor X for
the scaler and we're going to import the
standard scalar into this variable and
then our X train equals score x. fit
transform so we're creating the scaler
on the XT train variable and then our X
test we're also going to transform it so
we've trained and transform the X train
and then the X test isn't part of that
training it isn't part of that of
training the Transformer it just gets
transform that's all it does and again
we're going to go and run this and if
you look at this we've now gone through
these steps all three of them we've
taken care of replacing our zeros for
key columns that shouldn't be zero and
we replac that with the means of those
columns that way that they fit right in
with our data models we've come down
here and we split the data so now we
have our test data and our training data
and then we've taken and we've scaled
the data so all of our data going in now
no we don't tra we don't train the Y
part the Y train and Y test that never
has to be trained it's only the data
going in that's what we want to train in
there then Define the model using K
neighbors classifier and fit the the
train data in the model so we do all
that data prep and you can see down here
we're only going to have a couple lines
of code where we're actually building
our model and training it that's one of
the cool things about Python and how far
we've come it's such an exciting time to
be in machine learning because there's
so many automated tools let's see before
we do this let's do a quick length of
and let's do y we want let's just do
length of Y and we get 768 and if we
import math we do
math. square root let's do y train there
we go it's actually supposed to be X
train before we do this let's go ahead
and do import math and do math square
root length of Y test and when I run
that we get
12.49 I want to see show you where this
number comes from we're about to use 12
is an even number so if you know if
you're ever voting on things remember
the neighbors all vote don't want to
have an even number of neighbors voting
so we want to do something odd and let's
just take take one away we'll make it 11
let me delete this out of here that's
one of the reasons I love Jupiter
notebook cuz you can flip around and do
all kinds of things on the fly so we'll
go ahead and put in our classifier we're
creating our classifier now and it's
going to be the K neighbors classifier n
neighbors equal 11 remember we did 12
minus one for 11 so we have an odd
number of neighbors P equals 2 because
we're looking for is it are they
diabetic or not and we're using the
ukian metric there are other means of
measuring the distance you could do like
Square means value there's all kinds of
nature of this but the ukian is the most
common one and it works quite well it's
important to evaluate the model let's
use the confusion Matrix to do that and
we're going to use the confusion Matrix
wonderful tool and then we'll jump into
the F1 score and finally accuracy score
which is probably the most commonly used
quoted number when you go into a meeting
or something like that so let's go ahead
and paste that in there and we'll set
the cm equal to confusion Matrix y test
y predict so those are the two values
we're going to put in there and let me
go ahead and run that and print it out
and the way you interpret this is you
have the Y predicted which would be your
title up here we could do uh let's just
do p r d predicted across the top and
actual going down actual it's always
hard to to write in here actual that
means that this column here down the
middle that's the important column and
it means that our prediction said 94 and
prediction and the actual agreed on 94
and 32 this number here the 13 and the
15 those are what was wrong so you could
have like three different if you're
looking at this across three different
variables instead of just two you'd end
up with the third row down here and the
column going down the middle so in the
first case we have the the and I believe
the zero is a 94 people who don't have
diabetes the prediction said that 13 of
those people did have diabetes and were
high risk and the 32 that had diabetes
it had correct but our prediction said
another 15 out of that 15 it classified
as incorrect so you can see where that
classification comes in and how that
works on the confusion Matrix then we're
going to go ahead and print the F1 score
let me just run that and you see we get
a 69 in our F1 score the F1 takes into
account both sides of the balance of
false positives where if we go ahead and
just do the accuracy account and that's
what most people think of is it looks at
just how many we got right out of how
many we got wrong so a lot of people
when you're a data scientist and you're
talking to other data scientists they're
going to ask you what the F1 score the F
score is if you're talking to the
general public or the U decision makers
in the business they're going to ask
what the accuracy is and the accuracy is
always better than the F1 score but the
F1 score is more telling it let let us
know that there's more false positives
than we would like on here but 82% not
too bad for a quick flash look at
people's different statistics and
running an sklearn and running the knnn
the K nearest neighbor on it so we have
created a model using KNN which can
predict whether a person will have
diabetes or not or at the very least
whether they should go get a checkup and
have their glucose checked regularly or
not the print accurate score we got the
818 was pretty close to what we got and
find out why support Vector machine so
in this example last week my son and I
visited a fruit shop dad is that an
apple or a strawberry so the question
comes up what fruit did they just pick
up from the Fruit Stand after a couple
of seconds you can figure out that it
was a strawberry so let's take this
model a step further and let's uh why
not build a model which can predict an
unknown data and in this we're going to
be looking at some sweet strawberries or
crispy apples we wanted to be able to
label those two and decide what the
fruit is and we do that by having data
already put in so we already have a
bunch of strawberries we know our
strawberries and they're already labeled
as such we already have a bunch of
apples we know our apples and are
labeled as such then once we train our
model that model then can be given the
new data and the new data is this image
in this case you can see a question mark
on it and it comes through and goes it's
a strawberry in this case we're using
the support Vector Machine model svm is
a supervised learning method that looks
at data and sorts it into one of two
categories and in this case we're
sorting the strawberry into the
strawberry side at this point you should
be asking the question how does the
prediction work before we dig into an
example with numbers let's apply this to
our fruit scenario we have our support
Vector machine we've taken it and we've
taken labeled sample of data
strawberries and apples and we draw a
line down the middle between the two
groups this split now allows us to take
new data in this case an apple and a
strawberry and place them in the
appropriate group based on which side
the line they fall in and that way we
can predict the unknown as colorful and
tasty as the fruit example is let's take
a look at another example with some
numbers involved and we can take a
closer look at how the math Works in
this example we're going to be
classifying men and women and we're
going to start with a set of people with
a different height and a different
weight and to make this work we'll have
to have a sample data set of female
where we have their height and weight
174 65 17488 and so on and we'll need a
sample data set of the male they have a
height 179 90 180 to 80 and so on let's
go ahead and put this on a graph so have
a nice visual so you can see here we
have two groups based on the height
versus the weight and on the left side
we're going to have the women on the
right side we're going to have the men
now if we're going to create a
classifier let's add a new data point
and figure out if it's male or female so
before we can do that we need to split
our data first we can split our data by
choosing any of these lines in this case
we drawn two lines through the data in
the middle that separates the men from
the women but to predict the gender of a
new data point we should split the data
in the best possible way and we say the
best possible way because this line has
a maximum space that separates the two
classes here you can see there's a clear
split between the two different classes
and in this one there's not so much a
clear split this doesn't have the
maximum space that separates the two
that is why this line best splits the
data we don't want to just do this by
eyeballing it and before we go further
we need to add some technical terms to
this we can also say that the distance
between the points and the line should
be as far as possible in technical terms
we can say the distance between the
support vector and the hyperplane should
be as far as possible and this is where
the support vectors are the extreme
points in the data set and if you look
at this data set they have circled two
points which seem to be right on the
outskirts of the women and one on the
outskirts of the men and hyperplane has
a maximum distance to the support
vectors of any class now you'll see the
line down the middle and we call this
the hyperplane because when you're
dealing with multiple Dimensions it's
really not just a line but a plane of
intersections and you can see here where
the support vectors have been drawn in
dash lines the math behind this is very
simple we take D+ the shortest distance
to the closest positive point which
would be on the Min side and D minus is
the shortest distance to the closest
negative point which is on the women's
side the sum of D+ and D minus is called
the distance margin or the distance
between the two support vectors that are
shown in the dash lines and then by
finding the largest distance margin we
can get the optimal hyper plane once
we've created an optimal hyper plane we
can easily see which side the new data
fits in and based on the hyper plane we
can say the new data point belongs to
the male gender hopefully that's clear
how that works on a visual level as a
data scientist you should also be asking
what happens if the hyperplane is not
optimal if we if we select a hyperplane
having low margin then there is a high
chance of misclassification this
particular svm model the one we
discussed so far is also called referred
to as the
lsvm so far so clear but a question
should be coming up we have our sample
data set but instead of looking like
this what if it looked like this where
we have two sets of data but one of them
occurs in the middle of another set you
can see here where we have the blue and
the yellow and then blue again on the
other side of our data line in this data
set we can't use a hyper plane so when
you see data like this it's necessary to
move away from a 1D view of the data to
a two-dimensional view of the data and
for the transformation we use what's
called a kernel function the kernel
function will take the 1D input and
transfer it to a two-dimensional output
as you can see in this picture here the
1D when transferred to a two-dimensional
makes it very easy to draw a line
between the two data sets what if we
make it even more complicated how do we
perform an svm for this type of data set
here you can see we have a
two-dimensional data set where the data
is in the middle surrounded by the green
data on the outside in this case we're
going to segregate the two classes we
have our sample data set and if you draw
a line through it's obviously not an
optimal hyper plane in there so to do
that we need to transfer the 2D to a 3D
array and when you translate it into a
three-dimensional array using the kernel
you can see where you can place a
hyperplane right through it and easily
split the data before we start looking
at a programming example and dive into
the script let's look at the advantage
of the support Vector machine we'll
start with high dimensional input space
or sometimes referred to as the curse of
dimensionality we looked at earlier one
dimension two Dimension three dimension
when you get to a thousand dimensions a
lot of problems start occurring with
most algorithms that have to be adjusted
for the svm automatically does that in
high dimensional space one of the high
dimensional space one high dimensional
space that we work on is sparse document
vectors this is where we tokenize the
words in documents so we can run our
machine learning algorithms over them
I've seen ones get as high as 2.4
million different tokens that's a lot of
vectors to look at and finally we have
regularization parameter the
regularization parameter or Lambda is a
parameter that helps figure out whether
we're going to have a bias or
overfitting of the data whether it's
going to be overfitted to very specific
instance or it's going to be biased to a
high or low value with the svm it
naturally avoids the overfitting and
bias problems that we see in many other
algorithms these three advantages of the
support Vector machine make it a very
powerful tool to add to your repertoire
of machine learning tools now we did
promise you a used case study we're
actually going to dive into some Python
Programming and so we're going to go
into a problem statement and start off
with the zoo so in the zoo example we
have um family members going to the zoo
and we have the young child going dad is
that a group of crocodiles or alligators
well that's that's hard to differentiate
and zoos are a great place to start
looking at science and understanding how
things work especially as a young child
and so we can see the parents sitting
here thinking well what is the
difference between a crocodile and an
alligator well one crocodiles are larger
in size alligators are smaller in size
snout width the crocodiles have a narrow
snout and alligators have a wider snout
and of course in the modern day and age
the father sitner is thinking how can I
turn this into a lesson for my son and
he goes let a support Vector machine
segregate the two groups I don't know if
my dad ever told me that but that would
be funny now in this example we're not
going to use actual measurements and
data we're just using that for imagery
and that's very common in a lot of
machine learning algorithms and setting
them up but let's roll up our sleeves
and we'll talk about that more in just a
moment as we break into our python
script so here we arrive in our actual
coding and I'm going to move this into a
python editor and just a moment but
let's talk a little bit about what we're
going to cover first we're going to
cover in the code the setup how to
actually create our svm and you're going
to find that there's only two lines of
code that actually create it and the
rest of it is done so quick and fast
that it's all here in the first page and
we'll show you what that looks like as
far as our data because we're going to
create some data I talked about creating
data just a minute ago and so we'll get
into the creating data here and you'll
see this nice correction of our two
blobs and we'll go through that in just
a second and then the second part is
we're going to take this and we're going
to bump it up a notch we're going to
show you what it looks like behind the
scenes but let's start with actually
creating our setup I like to use the
Anaconda Jupiter notebook because it's
very easy to use but you can use any of
your favorite python editors or setups
and go in there but let's go ahead and
switch over there and see what that
looks like so here we are in the
Anaconda python notebook or anaconda
jupyter notebook with python we're using
Python 3 I believe this is 3.5 but it
should be work in any of your 3x
versions and uh you'd have to look at
the sklearn and make sure if you're
using a 2X version or an earlier version
let's go and put our code in there and
one of the things I like about the
Jupiter notebook is I go up to view and
I'm going to go ahead and toggle the
line numbers on to make it a little bit
easier to talk about and we can even
increase the size because this is edited
in in this case I'm using Google Chrome
Explorer and that's how it opens up for
the editor although anyone any like I
said any editor will work now the first
step is going to be our Imports and
we're going to import four different
parts the first two I want you to look
at are line one and line two are numpy
as NP and Matt plot library. pyplot as
PLT now these are very standardized
Imports when you're doing work the first
one is the numbers python we need that
because part of the platform we're using
uses that for the numpy array and I'll
talk about that in a minute so you can
understand why we want to use a numpy
array versus a standard python array and
normally it's pretty standard setup to
use NP for numpy the map plot library is
how we're going to view our data so this
has uh you do need the NP for the SK
learn module but the map plot library is
purely for our use for visualization and
so you really don't need that for the
svm but we're going to put it there so
you have a nice visual aid and we can
show you what it looks like that's
really important at the end when you
finish everything so you have a nice
display for everybody to look at and
then finally we're going to I'm going to
jump one ahead to line number four
that's the SK learn. dat sets. samples
generator import make blobs and I told
you that we were going to make up data
and this is a tool that's in the SK
learn to make up data I personally don't
want to go to the zoo get in trouble for
jumping over the fence and probably get
eaten by the crocodiles or alligators as
I work on measuring their snouts and
width and length instead we're just
going to make up some data and that's
what that make blobs is It's a Wonderful
tool if you're ready to test your your
uh setup and you're not sure about what
data you're going to put in there you
can create this blob and it makes it
real easy to use and finally we have our
actual svm the sklearn import svm on
line three so that covers all our
Imports we're going to create remember I
used the make blobs to create data and
we're going to create a capital x and a
lowercase y equals make blobs in samples
equals 40 so we're going to make 40
lines of data it's going to have two
centers with a random State equals 20 so
each each each group's going to have 20
different pieces of data in it and the
way that looks is that we'll have under
X um an XY plane so I have two numbers
under X and Y will be 0 or 1 that's the
two different centers so we have yes or
no in this case alligator or crocodile
that's what that represents and then I
told you that the actual SK learner the
svm is in two lines of code and we see
it right here with clf equal svm do SVC
kernel equals linear and I set c equal
to one although in this example since we
are not uh regularizing the data because
we want it to be very clear and easy to
see I went ahead you can set it to a
thousand a lot of times when you're not
doing that but for this thing linear
because it's a very simple linear
example we only have the two dimensions
and it'll be a nice linear hyper plane
will'll be a nice linear line instead of
a full plane so we're not dealing with a
huge amount of data and then all we have
to do is do cf. fit X comma Y and that's
it clf has been created and then we're
going to go ahead and display it and I'm
going to talk about this display here in
just a second but let me go ahead and
run this code and this is what we've
done is we've created two blobs you'll
see the blue on the side and then kind
of an orangish uh on the other side
that's our two sets of data they
represent one represents crocodiles and
one represents alligators and then we
have our measurements in this case we
have like the uh width and length of the
snout and I did say I was going to come
up here and talk just a little bit about
our plot and you'll see PLT that's what
we imported we're going to do a scatter
plot that means we're just putting dots
on there and then look at this notation
I have the capital x and then in
brackets I have a colon comma 0o that's
from numpy if you did that in a regular
array you'll get an error in a python
array you have to have that in a numpy
array it turns out that our make blobs
returns a numpy array and this notation
is great because what it means is the
first part is the colon means we're
going to do all the rows that's all the
data in our blob we created under
capital x and then the second part has a
comma zero we're only going to take the
first value and then if you notice we do
the same thing but we're going to take
the second value remember we always
start with zero and then one so we have
column zero and column one and you can
look at this as our XY plots the first
one is the X plot and the second one is
the Y plot so the first one is on the
bottom 0 2 4 6 8 and 10 and then the
second one X of the one is the 4 5 6 7 8
9 10 going up the left hand side s equal
30 is just the size of the dot so we can
see them instead real tiny dots and then
cmap equals plt.com paired and you'll
also see the C equals y That's the color
we're using two colors 01 and that's why
we get the nice blue blue and the two
different colors for the alligator and
the crocodile now you can see here that
we did this the actual fit was done in
two lines of code a lot of times
there'll be a third line where we
regularize the data we set it between
like minus one and one and we reshape it
but for this it's not necessary and it's
also kind of nice because you can
actually see what's going on and then if
we wanted to we wanted to actually run a
prediction let's take a look and see
what that looks like and to predict some
new data and we'll show this again as we
get towards the end of in deep you can
simply assign your new data in this case
I'm giving it a uh width and length 34
and a width and length 56 and note that
I put the data as a set of brackets and
then I have the brackets inside and the
reason I do that is because when we're
looking at data it's designed to process
a large amount of data coming in we
don't want to just process one line at a
time and so in this case I'm processing
two lines and then I'm just going to
print and you'll see cf. predict new
data so so the clf and the do predict
part is going to give us an answer and
let's see what that looks like and
you'll see 01 so predicted the first one
the 3 four is going to be on the one
side and the 56 is going to be on the
other side so one came out as a
alligator and one came out as a
crocodile now that's pretty short
explanation for the setup but really we
want to dig in and see what going on
behind the scenes and let's see what
that looks like so the next step is to
dig in deep and find out what's going on
behind the scenes and also put that in a
nice pretty graph we're going to spend
more work on this and we did actually
generating the original model and you'll
see here that we go through a few steps
and I'll move this over to our editor in
just a second we come in we create our
original data it's exactly identical to
the first part and I'll explain why we
redid that and show you how not to redo
that and then we're going to go in there
and add in those lines we're going to
see what those lines look like and how
to set those up and finally we're going
to plot all that on here and show it and
you'll get a nice graph with the what we
saw earlier when we were going through
the theory behind this where it shows
the support vectors and the hyper plane
and those are done where you can see the
support vectors as the dash lines and
the solid line which is the hyper plane
let's get that into our Jupiter notebook
before I scroll down to a new line I
want you to notice line 13 it has Plot
show and we're going to talk about that
here in just a second but let's scroll
down to a new line down here and I'm
going to paste that code in and you'll
see that the plot show has moved down
below let's scroll up a little bit and
if you look at the top here of our new
Section 1 2 3 and four is the same code
we had before and let's go back up here
and take a look at that what is naive
Bays let's start with a basic
introduction to the Bay theorem named
after Thomas baze from the 1700s who
first coined this in the western
literature naive baz classifier works on
the principle of conditional probability
as given by the Bas theorem before we
move ahead let us go through some of the
simple Concepts and the probability that
we will be using let us consider the
following example of tossing two coins
here we have two quarters and if we look
at all the different possibilities of
what they can come up as we get that
they can come up as head heads they come
up as head tail tail head and tell tell
when doing the math on probability we
usually denote probability as a p a
capital P so the probability of getting
two heads equals 1/4 you can see in our
data set we have two heads and this
occurs once out of the four Poss
possibilities and then the probability
of at least one tail occurs 3/4 of the
time you'll see on three of the coin
tosses we have tails in them and out of
four that's 3/4s and then the
probability of the second coin being
head given the first coin is tail is 1/2
and the probability of getting two heads
given the first coin is a head is 1/2
we'll demonstrate that in just a minute
and show you how that math works now
when we're doing it with two coins it's
easy to see but when you have something
more complex you can see where these Pro
these formulas really come in and work
so the Bas theorem gives us the
conditional probability of an event a
given another event B has occurred in
this case the first coin toss will be B
and the second coin toss a this could be
confusing because we've actually
reversed the order of them and go from B
to a instead of a to B you'll see this a
lot when you work in probabilities the
reason is we're looking for event a we
want to know what that is so we're going
to label that a since that's our focus
and then given another event B has
occurred in the Baye theorem as you can
see on the left the probability of a
occurring given B has occurred equals
the probability of B occurring given a
has occurred times the probability of a
over the probability of B this simple
formula can be moved around just like
any algebra formula and we could do the
probability of a after a given B times
probability of b equals the probability
of B given a times probability of a you
can easily move that around and multiply
it and divide it out let us apply base
theorem to our example here we have our
two quarters and we'll notice that the
first two probabilities of getting two
heads and at least one tail we compute
directly off the data so you can easily
see that we have one example HH out of
four 1/4 and we have three with tells in
them giving us three4 or three4
75% the second condition the second uh
set three and four we're going to
explore a little bit more in detail now
we stick to a simple example with two
coins because you can easily understand
the math the probability of throwing a
tail doesn't matter what comes before it
and the same with the head so still
going to be 50% or 1/2 but when that
come when that probability gets more
complicated let's say you have a D6 dice
or some other instance then this formula
really comes in handy but let's stick to
the simple example for now in this
sample space let a be the event that the
second coin is head and B be the event
that the first coin is Tails again we
reversed it because we want to know what
the second event is going to be so we're
going to be focusing on a and we write
that that out is the probability of a
given B and we know this from our
formula that that equals the probability
of B given a times the probability of a
over the probability of B and when we
plug that in we plug in the probability
of the first coin being Tails given the
second coin is heads and the probability
of the second coin being heads given the
first coin being over the probability of
the first coin being Tails when we plug
that data in and we have the probability
of the first coin being Tails given the
second coin is heads times the
probability of the second coin being
heads over the probability of the first
coin being tails you can see it's a
simple formula to calculate we have 1/2
* 1/2 over 1/2 or 1/2 = .5 or 1/4 so the
base theorem basically calculates the
conditional probability of the
occurrence of an event based on prior
knowledge of conditions that might be
related to the event we will explore
this in detail when we take up an
example of online shopping further in
this tutorial understanding naive bays
and machine learning like with any of
our other machine learning tools it's
important to understand where the naive
base fits in the hierarchy so under the
machine learning we have supervised
learning and there is other things like
unsupervised learning there's also
reward system This falls under the
supervised learning and then under the
supervised learning there's
classification there's also regression
but we're going to be in the
classification side and then under
classification is your naive Bay let's
go ahead and glance into where is naive
Bay used let's look at some of the used
scenarios for it as a classifier we use
it in face recognition is this Cindy or
is it not Cindy or whoever or it might
be used to identify parts of the face
that they then feed into another part of
the face recognition program this is the
eye this is the nose this is the mouth
weather prediction is it going to be
rainy or sunny medical recognition news
prediction it's also used in medical
diagnosis we might diagnose somebody as
either as high risk or not as high risk
for cancer or heart disease or other
ailments and news classification you
look at the Google news and it says well
is this political or is this world news
or a lot of that's all done with a naive
Bay understanding naive Bay classifier
now we already went through a basic
understanding with the coins and the two
heads and two tails and head tail tail
heads Etc we're going to do just a quick
review on that and remind you that the
naive Baye classifier is based on the
bay theorem which gives a conditional
probability of in event a given event B
and that's where the probability of a
given b equals the probability of B
given a Time probability of a over
probability of B remember this is an
algebraic function so we can move these
different entities around we can
multiply by the probability of B so it
goes to the left hand side and then we
could divide by the probability of a
given B and just as easily come up with
a new formula for the probability of B
to me staring at these algebraic
functions kind of gives me a slight
headache it's a lot better to see if we
can actually understand how this data
fits together together in a table and
let's go ahead and start applying it to
some actual data so you can see what
that looks like so we're going to start
with the shopping demo problem statement
and remember we're going to solve this
first in uh table form so you can see
what the math looks like and then we're
going to solve it in Python and in here
we want to predict whether the person
will purchase a product are they going
to buy or don't buy very important if
you're running a business you want to
know how to maximize your profits or at
least maximize the purchase of the
people coming into your store and we're
going to look at a specific combination
of different variables in this case
we're going to look at the day the
discount and the free delivery and you
can see here under the day we want to
know whether it's uh on the weekday you
know somebody's working they come in
after work or maybe they don't work
weekend you can see the bright colors
coming down there celebrating not being
in work or holiday and did we offer a
discount that day yes or no did we offer
free delivery that day yes or no and
from this we want to know whether the
person's going to buy based on these
traits so we can maximize them and find
out the best system for getting somebody
to come in and purchase our goods and
products from our store now having a
nice visual is great but we do need to
dig into the data so let's go ahead and
take a look at the data set we have a
small sample data set of 30 rows we're
showing you the first 15 of those rows
for this demo now the actual data file
you can request just type in below under
the comments on the YouTube video and
we'll send you some more information and
send you that file as you can see here
the file is very simple columns and rows
we have the day the discount the free
delivery and did the person purchase or
not and then we have under the day
whether it was a weekday a holiday was
it the weekend this is a pretty simple
set of data and long before computers
people used to look at this data and
calculate this all by hand so let's go
ahead and walk through this and see what
that looks like when we put that into
tables also note in today's world we're
not usually looking at three different
variables in 30 rows nowadays because
we're able to collect data so much we're
usually looking at 27 30 variables
across hundreds of rows the first thing
we want to do is we're going to take
this data and uh based on the data set
containing our three inputs Day discount
and free delivery we're going to go
ahead and populate that to frequency
tables for each attribute so we want to
know if they had a discount how many
people buy and did not buy uh did they
have a discount yes or no do we have a
free delivery yes or no on those days
how many people made a purchase how many
people didn't and the same with the
three days of the week was it a weekday
a weekend a holiday and did they buy yes
or no as we dig in deeper to the table
for our Baye theorem let the event buy
be a now remember when we looked at the
coins I said we really want to know what
the outcome is did the person buy or not
and that's usually event a is what
you're looking for and the independent
variables discount free delivery and day
BB so we'll call that probability of B
now let us calculate the likelihood
table for one of the variables let's
start with day which includes weekday
weekend and holiday and let us start by
summing all of our rows so we have the
uh weekday row and out of the weekdays
there's 99 plus 2 so is 11 weekdays
there's eight weekend days and 11
holidays wow it's a lot of holidays and
then we want to sum up the total number
of days so we're looking at a total of
30 days let's start pulling some
information from our chart and see where
that takes us and when we fill in the
chart on the right you can see that nine
out of 24 purchases are made on the
weekday 7 out of 24 purchases on the
weekend and eight out of 24 purchases on
a holiday and out of all the people
people come in 24 out of 30 purchase you
can also see how many people do not
purchase on the week dates two out of
six didn't purchase and so on and so on
we can also look at the totals and
you'll see on the right we put together
some of the formulas the probability of
making a purchase on the weekend comes
out 11 out of 30 so out of the 30 people
who came into the store throughout the
weekend weekday and holiday 11 of those
purchases were made on the weekday and
then you can also see the probability of
them not making a purchase and this is
done for doesn't matter which day of the
week so we call that probability of no
buy would be 6 over 30 or02 so there's a
20% chance that they're not going to
make a purchase no matter what day of
the week it is and finally we look at
the probability of B if a in this case
we're going to look at the probability
of the weekday and not buying two of the
no buys were done out of the weekend out
of the six people who did not make
purchases so when we look at that
probability of the week day without a
purchase is going to be 33 or 33% let's
take a look at this at different
probabilities and uh based on this
likelihood table let's go ahead and
calculate conditional probabilities as
below the first three we just did the
probability of making a purchase on the
weekday is 11 out of 30 or roughly 36 or
37% 367 the probability of not making a
purchase at all doesn't matter what day
of the week is roughly 0.2 or 20% and
the probability of a weekday no purchase
is roughly two out of six so two out of
six of our no purchases were made on the
weekday and then finally we take our P
of ab if you looked we've kept the
symbols up there we got P of probability
of B probability of a probability of B
if a we should remember that the
probability of a if B is equal to the
first one times the probability of no
per buys over the probability of the
weekday so we could calculate it both
off the uh table we created we can also
calculate this by the formula and we get
the 367 which equals or uh 33 * 2 over
367 which equals. 179 or roughly uh 17
to 18% and that'd be the probability of
no purchase done on the weekday and this
is important because we can look at this
and say as the probability of buying on
the weekday is more than the probability
of not buying on the weekday we can
conclude that customers will most likely
buy the product on a weekday now we've
kept our chart simple and we're only
looking at one aspect so you should be
able to look at the table and come up
with the same information or the same
conclusion that should be kind of
intuitive at this point next we can take
the same setup we have the frequency
tables of all three independent
variables now we can construct the
likelihood tables for all three of the
variables we're working with we can take
our day like we did before we have
weekday weekend and holiday we filled in
this table and then we can come in and
also do that for the discount yes or no
did they buy yes or no and we fill in
that full table so now we have our
probabilities for a discount and whether
the discount leads to a purchase or not
and the probability for free delivery
does that lead to a purchase or not and
this is where it starts getting really
exciting let us use these three
likelihood tables to calculate whether a
customer will purchase a product on a
specific combination of Day discount and
free delivery or not purchase here let
us take a combination of these factors
day equals holiday discount equals yes
free to delivery equals yes let's dig
deeper into the math and actually see
what this looks like and we're going to
start with looking for the probability
of them not purchasing on the following
combinations of days we are actually
looking for the probability of a equal
no buy no purchase and our probability
of B we're going to set equal to is it a
holiday did they get a discount yes and
was it a free delivery yes before we go
further let's look at the original
equation the probability of a if B
equals the probability of B given the
condition a and the probability times
probability of a over the probability of
B occurring now this is basic algebra so
we can multiply this information
together so when you see the probability
of a given B in this case the condition
is b c and d or the three different
variables we're looking at and when you
see the probability of B that would be
the conditions we're actually going to
multiply those three separate conditions
out probability of you'll see that just
second in the formula times the full
probability of a over the full
probability of B so here we are back to
this and we're going to have let a equal
no purchase and we're looking for the
probability of B on the condition a
where a sets for three different things
remember that equals the probability of
a given the condition B and in this case
we just multiply those three different
variables together so we have the
probability of the discount times the
probability of free delivery times the
probability is the day equal a holiday
those are our three variables of the
probability of a ifb and then that is
going to be multiplied by the
probability of them not making a
purchase and then we want to divide that
by the total probabilities and they're
multiplied together so we have the
probability of a discount the
probability of a free delivery and the
probability of it being on a holiday
when we plug those numbers in we see
that one out of six were no purchase on
a discounted day two out of six were a
no purchase on a free delivery day and
three out of six were a no purchase on a
holiday those are our three
probabilities of a if B multiplied out
and then that has to be multiplied by
the probability of a no purchase and
remember the prop probability of a no bu
is across all the data so that's where
we get the 6 out of 30 we divide that
out by the probability of each category
over the total number so we get the 20
out of 30 had a discount 23 out of 30
had a yes for free delivery and 11 out
of 30 were on a holiday we plug all
those numbers in we get
0178 so in our probability math we have
a178 if it's a no buy for a holiday a
discount and a free delivery let's turn
that around and see what that looks like
if we have a purchase I promise this is
the last page of math before we dig into
the python script so here we're
calculating the probability of the
purchase using the same math we did to
find out if they didn't buy buy now we
want to know if they did buy and again
we're going to go by the day equals a
holiday discount equals yes free
delivery equals yes and let a equal buy
now right about now you might be asking
why are we doing both calculations why
why would we want to know the no buys
and buys for the same data going in well
we're going to show you that in just a
moment but we have to have both of those
pieces of information so that we can
figure it out as a percentage as opposed
to a probability equation and we'll get
to that normalization here in just a
moment let's go ahead and walk through
this calculation and as you can see here
the probability of a on the condition of
b b being all three categories did we
have a discount with a purchase did we
have a free delivery with a purchase and
did we is a day equal to Holiday and
when we plug this all into that formula
and multiply it all out we get our
probability of a discount probability of
a free delivery probability of the day
being a holiday times the overall
probability of it being a purchase
divided by again mult multiplying the
three variables out the full probability
of there being a discount the full
probability of being a free delivery and
the full probability of there being a
day equal holiday and that's where we
get this 19 over 24 * 21 over 24 * 8
over 24 time the P of a 24 over 30
divided by the probability of the
discount the free delivery times the day
or 20 over 30 23 over 30 * 11 over 30
and that gives us our
986 so what are we going to do with
these two pieces of data we just
generated well let's go ahead and go
over them we have a probability of
purchase equals
986 we have a probability of no purchase
equals 178 so finally we have a
conditional probabilities of purchase on
this day let us take that we're going to
normalize it and we're going to take
these probabilities and turn them into
percentages this is simply done by
taking the sum of probabilities which
equals
98686 plus 178 and that equals the
1.64 if we divide each probability by
the sum we get the percentage and so the
likelihood of a purchase is 84.7 1% and
the likelihood of no purchase is
15.29% given these three different
variables so it's if it's on a holiday
if it's a with a discount and has free
delivery then there's an 84. 71% chance
that the customer is going to come in
and make a purchase hooray they
purchased our stuff we're making money
if you're owning a shop that's like is
the bottom line is you want to make some
money so you can keep your shop open and
have a living now I promised you that we
were going to be finishing up the math
here with a few pages so we're going to
move on and we're going to do two steps
the first step is I want you to
understand why you want to why you want
to use the naive Bays what are the
advantages of naive bays and then once
we understand those advantages we're
just look at that briefly then we're
going to dive in and do some python
coding advantages of naive Bay
classifier so let's take a look at the
six advantages of the naive Baye
classifier and we're going to walk
around this lovely wheel looks like an
origami folded paper the first one is
very simple and easy to implement
certainly you could walk through the
tables and do this by hand you got to be
a little careful because the notations
can get confusing you have all these
different probabilities and I certainly
mess those up as I put them on you know
is it on the top or the bottom you got
to really pay close attention to that
when you put it into python it's really
nice because you don't have to worry
about any of that you let the python
handle that the python module but
understanding it you can put it on a
table and you can easily see how it
works and it's a simple algebraic
function it needs less training data so
if you have smaller amounts of data this
is great powerful tool for that handles
both continuous and discrete data it's
highly scalable with number of
predictors and data points so as you can
see you just keep multiplying different
probabilities in there and you can cover
not just three different variables or
sets you can now expand this to even
more categories number five it's fast
you can can be used in realtime
predictions this is so important this is
why it's used in a lot of our
predictions on online shopping carts uh
referrals spam filters is because
there's no time delay as it has to go
through and figure out a neural network
or one of the other mini setups where
you're doing classification and
certainly there's a lot of other tools
out there in the machine learning that
can handle these but most of them are
not as fast as the naive bays and then
finally it's not sensitive to irrelevant
features so so it picks up on your
different probabilities and if you're
short on date on one probability you can
kind of it automatically adjust for that
those formulas are very automatic and so
you can still get a very solid
predictability even if you're missing
data or you have overlapping data for
two completely different areas we see
that a lot in doing census and studying
of people and habits where they might
have one study that covers one aspect
another one that overlaps and because
the two overlap they can then predict
the unknown for the group that they have
haven't done the second study on or vice
versa so it's very powerful in that it
is not sensitive to the irrelevant
features and in fact you can use it to
help predict features that aren't even
in there so now we're down to my
favorite part we're going to roll up our
sleeves and do some actual programming
we're going to do the use case text
classification now I would challenge you
to go back and send us a note on the
notes below underneath the video and
request a data for the shopping cart so
you can plug that into python code and
do that on your own time so you can walk
through it since we walk through all the
information on it but we're going to do
a python code doing text classification
very popular for doing the naive Bays so
we're going to use our new tool to
perform a text classification of news
headlines and classify news into
different topics for a News website as
you can see here we have a nice image of
the Google news and then related on the
right subgroups I'm not sure where they
actually pulled the actual data we're
going to use from it's one of the
standard sets but certainly this can be
used on any of our news headlines and
classification so let's see how it can
be done using the naive Bay classifier
now we're at my favorite part we're
actually going to write some python
script roll up our sleeves and we're
going to start by doing our Imports
these are very basic Imports including
our news group and we'll take a quick
glance at the Target names then we're
going to go ahead and start training our
data set and putting it together we'll
put together a nice graph because it's
always good to have a graph to show
what's going on and once we've trained
it we've shown you a graph of what's
going on then we're going to explore how
to use it and see what that looks like
now I'm going to open up my favorite
editor or inline editor for python you
don't have to use this you can use
whatever your editor that you like
whatever uh interface IDE you want this
just happens to be the Anaconda Jupiter
notebook and I'm going to paste that
first piece of code in here so we can
walk through it let's make it a little
bigger on the screen so you have a nice
view of what's going on uh and we're
using Python 3 in this case 3.5 so this
would work work in any of your 3x if you
have it set up correctly should also
work in a lot of the 2x you just have to
make sure all the the versions of the
modules match your python version and in
here you'll notice the first line is
your percentage met plot library in line
now three of these lines of code are all
about plotting the graph this one let's
The Notebook notes and this is the
inline setup that we want the graphs to
show up on this page without it in a
notebook like this which is an uh
explore interface it won't show up now a
lot of idees don't require that a lot of
them like on if I'm working on one of my
other setups it just has a pop up and
the graph pops up on there so you have a
that setup also but for this we want the
mat plot library in line and then we're
going to import numpy as NP that's
number python which has a lot of
different formulas in it that we use for
both of our SK learn module and we also
use it for any of the upper math
functions in Python and it's very common
to see that as is NP nump is NP the next
two lines are all about our graphing
remember I said three of these were
about graphing well we need our map plot
library. pyplot as PLT and you'll see
that PLT is a very common setup as is
the SNS and just like the NP and we're
going import caborn as SNS and we're
going to do the SNS do set now caborn
sits on top of pip plot and it just
makes a really nice heat map it's really
good for heat maps and if you're not
familiar with heat maps that just means
we give it a color scale the term comes
from the brighter red it is the hotter
it is in some form of data and you can
set it to whatever you want and we'll
see that later on so those you'll see
that those three lines of code here are
just importing the graph function so we
can graph it and as a data scientist you
always want to graph your data and have
some kind of visual it's really hard
just to shove numbers in front of people
and they look at it and it doesn't mean
anything and then from the SK learn. dat
sets we're going to import the fetch 20
news groups very common one for
analyzing tokenizing words and setting
them up and exploring how the words work
and how do you categorize different
things when you're dealing with
documents and then we set our data equal
to fetch 20 news groups so our data
variable will have the data in it and
we're going to go ahead and just print
the target names data. Target names and
let's see what that looks like and
you'll see here we have alt atheism comp
Graphics comp osms windows.
miscellaneous and it goes all the way
down to talk politics. miscellaneous
talk religion. miscellaneous these are
the categories they've already assigned
to this news group and it's called Fetch
20 because you'll see there's I believe
there's 20 different topics in here or
20 different categories as we scroll
down now we've gone through the 20
different categories and we're going to
go ahead and start defining all the
categories and set up our data so we're
actually getting here going to go ahead
and get it get the data all set up and
take a look at our data and let's move
this over to our Jupiter notebook and
let's see what this code does first
we're going to set our categories now if
you noticed up here I could have just as
easily set this equal to data. Target
names because it's the same thing but we
want to kind of spell it out for you so
you can see the different categories it
kind of makes it more visual so you can
see what your data is looking like in
the background once we've created the
categories we're going to open up a
train set so this training set of data
is going to go into fetch 20 news groups
and it's a subset in there called train
and categories equals categories so
we're pulling out those categories that
match and then if you have a train set
you should also have the testing set we
have test equals fetch 20 News Group
subset equals test and categories equals
categories let's go down one side so it
all fits on my screen there we go and
just so we can really see what's going
on let's see what happens when we print
out one part of that data so it creates
train and under train it creates train.
dat and we're just going to look at data
piece number five and let's go ahead and
run that and see what that looks like
and you can see when I print train. data
number five under train it prints out
one of the Articles this is article
number five you can go through and read
it on there and we can also go in here
and change this to test which should
look identical because it's splitting
the date up into different groups train
and test and we'll see test number five
is a a different article but another
article in here and maybe you're curious
and you want to see just how many
articles are in here we could do length
of train. data and if we run that you'll
see that the training data has
11,314 articles so we're not going to go
through all those articles that's a lot
of articles but um we can look at one of
them just so you can see what kind of
information is coming out of it and what
we're looking at and we'll just look at
number five for today and here we have
it rewarding the Second Amendment IDs
vtt line 58 lines 58 in article uh Etc
and you can scroll all the way down and
see all the different parts to there now
we've looked at at it and that's pretty
complicated when you look at one of
these articles to try to figure out how
do you weight this if you look down here
we have different words and maybe the
word from well from is probably in all
the Articles so it's not going to have a
lot of meaning as far as trying to
figure out whether this article fits one
of the categories or not so trying to
figure out which category it fits in
based on these words is where the
challenge comes in now that we've viewed
our data we're going to dive in and do
the actual predictions this is the
actual naive Bas and we're going to
throw another model at you another
module at you here in just a second we
can't go into too much detail but it
deals specifically working with words
and text and what they call tokenizing
those words so let's take this code and
let's uh skip on over to our Jupiter
notebook and walk through it and here we
are in our jupyter notebook let's paste
that in there and I can run this code
right off the bat it's not actually
going to display anything yet but it has
a lot going on in here so the top we had
the print module from the earlier one I
didn't know why that was in there so
we're going to start by importing our
necessary packages and from the sklearn
features extraction. text we're going to
import tfidf vectorizer I told you we're
going to throw a module at you we can't
go too much into the math behind this or
how it works you can look it up the
notation for the math is usually tf.idf
and that's just a way of weighing the
words and it weighs the words based on
how many times they're used in a
document how many times or how many
documents they're used in and it's a
well-used formula it's been around for a
while it's a little confusing to put
this in here uh but let's let it know
that it just goes in there and waits the
different words in the document for us
that way we don't have to wait and if
you put a weight on it if you remember I
was talking about that up here earlier
if these are all emails they probably
all have the word from in them from
probably has a very low weight it has
very little value in telling you what
this document's about same with words
like in in article in articles in cost
of un maybe cost might or or where words
like criminal weapons destruction these
might have a heavier weight because they
describe a little bit more what the
article's doing well how do you figure
out all those weights in the different
articles that's what this module does
that's what the tfidf vectorizer is
going to do for us and then we're going
to import our SK learn. naive Bas and
that's our multinomial NB multinomial
naive Bas pretty easy to understand that
where that comes from and then finally
we have the sky learn pipeline line
import make pipeline now the make
pipeline is just a cool piece of code
because we're going to take the
information we get from the tfidf
vectorizer and we're going to pump that
into the multinomial INB so a pipeline
is just a way of organizing how things
flow it's used commonly you probably
already guess what it is if you've done
any businesses they talk about the
celles pipeline if you're on a work crew
or project manager you have your
pipeline of information is going through
or your projects and what has to be done
in what order that's all this pipeline
is we're going to take the tfid
vectorizer and then we're going to push
that into the multinomial NB now we've
designated that as the variable model we
have our pipeline model and we're going
to take that model and this is just so
elegant this is done in just a couple
lines of code model.fit and we're going
to fit the data and first the train data
and then the train Target now the train
data has the different articles in it
you can see the one we were just looking
at and the train. target is what
category they already categorized that
that particular article as and what's
Happening Here is the train data is
going into the tfid vectorizer so when
you have one of these articles it goes
in there it weights all the words in
there so there's thousands of words with
different weights on them I remember
once running a model on this and I
literally had 2.4 million tokens go into
this so when you're dealing like large
document bases you can have a huge
number of different words it then takes
those words gives them a weight and then
based on that weight based on the words
and the weights and then puts that into
the multinomial INB and once we go into
our naive Baye we want to put the train
Target in there so the train data that's
been mapped to the tfid vectorizer is
now going through the multinomial in B
and then we're telling it well these are
the answers these are the answers to the
different documents so this document
that has all these words with these
different weights from the first part is
going to be whatever category it comes
out of maybe it's the um Talk show or
the article on religion miscellaneous
once we fit that model we can then take
labels and we're going to set that equal
to model. predict most of the sklearn
use the term. predict to let us know
that we've now trained the model and now
we want to get some answers and we're
going to put our test data in there
because our our test data is the stuff
we held off to the side we didn't train
it on there and we don't know what's
going to come up out of it and we just
want to find out how good our labels are
do they match what they should be now
I've already run this through there's no
actual output to it to show this is just
setting it all up this is just trending
our model creating the labels so we can
see how good it is and then we move on
to the next step to find out what
happened to do this we're going to go
ahead and create a confusion Matrix and
a heat map so the confusion mat Matrix
which is confusing just by its very name
it's basically going to ask how confused
is our answer did it get it correct or
did it Miss some things in there or have
some missed labels and then we're going
to put that on a heat map so we have
some nice colors to look at to see how
that plots out let's go ahead and take
this code and see how that uh take a
walk through it and see what that looks
like so back to our Jupiter notebook
going to put the code in there and let's
go ahead and run that code take it just
a moment and remember we had the inline
that way my graph shows up on the inline
here and let's walk through the code and
then we'll look at this and see what
that means so make it a little bit
bigger there we go no reason not to use
a whole screen too big so we have here
from sklearn metrics import confusion
Matrix and that's just going to generate
a set of data that says I the prediction
was such the actual truth was either
agreed with it or is something different
and it's going to add up those numbers
so we can take a look and just see how
well it worked and we're going to set a
variable mat equal to confusion Matrix
we have our test Target our test data
that was not part of the training very
important in data science we always keep
our test data separate otherwise it's
not a valid model if we can't properly
test it with new data and this is the
labels we created from that test data
these are the ones that we predict it's
going to be so we go in we create our SN
heat map the SNS is our caborn which
sits on top of the PIP plot
so when we create a SNS do heat map we
take our confusion Matrix and it's going
to be uh matt. T and do we have other
variables that go into the SNS do heat
map we're not going to go into detail
what all the variables mean The
annotation equals true that's what tells
it to put the numbers here so you have
the 166 the 1 the 00001 format d and c
bar equals false have to do with the uh
format if you take those out you'll see
that some things disappear and then the
tick labels and the Y tick labels those
are our Target names and you can see
right here that's the alt atheism comp
Graphics comp osms windows.
miscellaneous and then finally we have
our PLT dox label remember the SNS or
the cbor sits on top of our map plot
Library our PLT and so we want to just
tell it xlabel equals a true is is true
the labels are true and then the Y label
is prediction label so when we say a
true this is what it actually is and the
prediction is what we predicted and
let's look at this graph because that's
probably a little confusing the way we
rattled through it and what I'm going to
do is I'm going to go ahead and flip
back to the slides because they have a
black background they put in there that
helps it shine a little bit better so
you can see the graph a little bit
easier so in reading this graph what we
want to look at is how the color scheme
has come out and you'll see a line right
down the middle diagonally from upper
left to bottom right what that is is if
you look at the labels we have our
predicted label on the left and our true
label on the right those are the numbers
where the prediction and the true come
together and this is what we want to see
is we want to see those lit up that's
what that heat map does as you can see
that it did a good job of finding those
data and you'll notice that there's a
couple of red spots on there where it
missed you know it's a little confused
when we talk about talk religion
miscellaneous versus talk politics
miscellaneous social religion Christian
versus Alt atheism it mislabeled some of
those and those are very similar topics
so you could understand why it might
mislabel them but overall it did a
pretty good job if we're going to create
these models we want to go ahead and be
able to use them so let's see what that
looks like to do this let's go ahead and
create a definition a function to run
and we're going to call this function
let me just expand that just a notch
here there we go I like mining big
letters predict categor we want to
predict the category we're going to send
it s a string and then we're sending it
train equals train we have our training
model and then we had our pipeline model
equals model this way we don't have to
resend these variables each time the the
definition knows that because I said
train equals train and I put the equal
for model and then we're going to set
the prediction equal to the model.
predicts so it's going to send whatever
string we send to it it's going to push
that string through the pipeline the
model pipeline it's going to go through
and uh tokenize it and put it through
the TF IDF convert that into number and
weights for all the different documents
and words and then it'll put that
through our naive Bay and from it we'll
go ahead and get our prediction we're
going to predict what value it is and so
we're going to return train. Target
names predict of zero and remember that
the train. target names that's just
categories I could have just as easily
put uh categories in there. predict of
zero so we're taking the prediction
which is a number and we're converting
it to an actual category we're
converting it from um I don't know what
the actual numbers are let's say Z
equals alt atheism so we're going to
convert that zero to the word or one
maybe it equals comp Graphics so we're
going to convert number one into comp
Graphics that's all that is and then we
got to go ahead and and then we need to
go ahead and run this so I load that up
and then once I run that we can start
doing some predictions I'm going go
ahead and type in predict category and
let's just do predict category Jesus
Christ and it comes back and says it's
social religion Christian that's pretty
good now note I didn't put print on this
one of the nice things about the Jupiter
notebook editor and a lot of inline
editors is if you just put the name of
the variable out is returning the
variable train. Target names it'll
automatically print that for you in your
own IDE you might have to put in print
let's see where else we can take this
and maybe you're a space science buff so
how about sending load to
International
space
station and if we run that we get
science space or maybe you're a uh
automobile buff and let's do um oh they
were going to tell me Audi is better
than BMW but I'm going to do BMW is
better than an Audi so maybe you're car
buff and we run that and you'll see it
says recreational I'm assuming that's
what recc stands for Autos so I did a
pretty good job labeling that one how
about uh if we have something like a
caption running through there president
of India and if we run that it comes up
and says talk politics
miscellaneous so when we take our
definition or our function and we run
all these things through Kudos we made
it we were able to correctly classify
texts into different groups based on
which category they belong to using the
naive Bas classifier now we did throw in
the pipeline the TF IDF vectorizer we
threw in the graphs those are all things
that you don't necessarily have to know
to understand the naive Bay setup or
classifier but they're important to know
one of the main uses for the naive Bay
is with the tfidf tokenizer vectorizer
where it tokenizes the word and as
labels and we use the pipeline because
you need to push all that data through
and it makes it really easy and fast you
don't have to know those to understand
naive Bays but they certainly help for
understanding the industry in data
science and we can see they categorized
ER or naive Bas classifier we were able
to predict the category religion space
motorcycles Autos politics and properly
classify all these different things we
pushed into our prediction and our train
model so first we will import some major
libraries of python so here I will write
import pandas as
PD and
import numai as
NP then
import
cbor that's
SNS okay then
import
skarn dot model
selection
p
train
underscore test underscore
split before that I will
import mat plot
Li P
plot as
PLT okay
then I will write here from
skar dot m
Trix
import
accuracy
four then
from
Escalon do
Matrix
import
classification
report
and import R then import
string
okay then press enter so it is
saying here I have to write
from everything seems
good loading let's see
okay till then numai is a python Library
used for working with arrays it also has
function for working with the domain of
linear algebra and
matrices it is an open source project
and you can use it
freely number stand for numerical
python pandas so panda is a software
Library written for Python programming
language for data manipulation and
Analysis in particular it offers data
structure and operation for manipulating
numeric iCal tables and time
series then cbor an open source python
Library based on M plot Li is called
cbon it is utilized for data exploration
and data visualization with data frames
and the pandas Library cbone functions
with ease then matplot lip for Python
and its numerical extension numpy M plot
lip is a crossplatform for the data
visualization and graphical charting
package as a result it presents a strong
open source suitable for MLB the apis
for met plot lib allow programmers to
incorporate graphs into gii applications
then this train test split we may build
our training data and the test data with
the aid of Escalon train test split
function this is so because the original
data set often serves as both the
training data and the test data starting
with the single data set we divide it
into two data sets to obtain the
information needed to create a model
like hor and test accuracy score the
accuracy score is used to gge the
model's Effectiveness by calculating the
ratio of total true positive to Total
true negative across all the model
prediction this R regular expression the
functions in the model allow you to
determine whether a given text fits a
given regular expression or not which is
known as
R okay then string a collection of
letters words or other character is
called a string it is one of the basic
data structure that serves as the
foundation of manipulating data the S
Str class is a built-in string class in
Python because python strings are
immutable they cannot be modified after
they have been formed okay so now let's
import the data set we will be going to
import two data set one for the fake
news and one for the True News or you
can say not fake news okay so I will
write write here
efcore P equals
to PD
do read uncore
CSV or what can I say DF fake
okay tore
fake
okay then
fake do CSV you can download this data
set from the description box below then
data dot true equals to pd. read _
CSV sorry
CSC then fake news sorry true through.
CSV okay then press enter
so these are the two data set you can
download these data set from the
description box below so let's see the
board data set okay then I will write
here
dataor
fake do
head so this is the fake data okay
then data underscore
true
Dot and this is the two
data okay this is not fake so if you
want to see your top five rows of the
particular data set you can use head and
if you want to see the last five rows of
the data set you can use tail instead of
head
okay so let me give some space for the
better
visual so now we will insert column
class as a Target feature okay then I
will write here
data go
fake
cl equals
to0 then data underscore
true and
plus to 1
okay
then I will write here data underscore
fake Dot shap and data underscore
true do
shap okay then press
enter so the shape method return the
shape of an array the shape is a tle of
integers these number represent the
length of the corresponding array
dimension in other words a tle
containing the quantities of entries on
each axis is an array shape Dimension so
what's the meaning of
shape in the fake World in this data set
we have 2 3 481 rows and five columns
and in this data set true we have 21 417
rows and five column okay so these are
the rows column rows column for the
particular data
set so now let's move
and let's remove the last 10 rows for
the manual testing okay then I will
write here data uncore
fake let's go
manual
testings to dataor
fake do
tail for the last 10 rows I have to
write here 10
okay so for I in
range 2 3
4 8 1 sorry
0o comma 2
3
470 comma minus
one
okay
then DF underscore not DF
data underscore
fake dot
drop
one instead of one I can write here
I
comma is equal = to
0 in
place to
true then
data not
here data
underscore same I will write for I will
copy from
here and I will paste it here and I will
make the particular changes so here I
can write
true here I can write
true
okay then I have to change a
[Music]
number 2
1
416
write 21 4
6 minus
one
same so press
enter x equal
to0 since X maybe you mean 0 or of
this okay we will put here double
course I'm putting
this . drop i z in place
okay also write equals
to yeah
so okay AIS is not
defined now it's working
so let me
see now d the
underscore pi.
shape
okay and data dot
true data underscore
true dot
shape as you can
see 10 rows are deleted from each data
set
yeah so I will write here data
underscore fake underscore
manual
testing
class =
to0 and data
underscore true
underscore manual undor
testing CL equals
to
1
okay just ignore this
warning then let's
see data
underscore
bore
manual testing Dot
head as you can see we have this and
then data dot sorry underscore
truecore
manual
testing do head
10
this is this is the uh true data
set so here I will merge
data go
merge
to
[Music]
pd.
concat concat is used for the
concatination
data uncope
fake data
score
comma
XIs = to
zero then data uncore
merge do
head the top 10
rows
yeah as you can see the data is merged
here
okay first it will come for the fake
news and then with the for the True
News then let's merge true and fake data
frames
okay we did this
and let's Mery column then data do
merge dot columns or let's see the
columns it is not defined what data
underscore
merge these are the columns same title
Tex subject date class
okay
now let's remove those columns which are
not required for the further process so
here I will write data uncore
or equals to data underscore
merge
prop title we don't
need
then subject we don't need
then
one so let's check some null
values it's giving error
here of
this that's good then
data dot is null
some
Center so no null values okay then let's
do the random shuffling of the data
frames okay for that we have to write
here data equals to data do
sample one
then data okay
data dot
head okay now you can see here the
random shuffling is
done and one for the true data set and
zero for the fake news one
okay then let me write here data
dot reset
underscore
index
Place equals to
True data dot
drop comma X is = to
1 then comma in place
equals to
True
okay then let me see columns now data
dot
columns so here we have two columns only
rest we have deleted
okay let me see data dot
add
yeah everything seems
good let's proceed further and let's
create a function to process the text
okay for that I will write
[Music]
here
what okay you can use any
name text
and text equals
to text.
lower okay and
text to r dot for the
substring remove these
things uh from
the datas
okay so for that I'm writing
herea okay then text equals to R do
substring
comma comma
text okay then I have to write text
equals
to R do
substring
www
dot
s+
comma comma
text okay then text x equals
to i.
subring
then
comma
okay then text equals to R do
substring
then
percentage
s again percentage for R do s SK
function right here
string do
punctuation
okay
comma then comma then
text
right then text equals to R do substring
and
N
comma text equals to R
do
subring right
here and again
D then
again okay then
comma then again text here okay then at
the end I have to write here return text
so everything like uh this this type of
special character will be removed from
the data set okay let's run this let's
see yeah so here I will write DF sorry
not DF
data
data
then
text to
data okay dot
apply to the function name whatp what
opt
okay press enter yeah so now let's uh
Define the dependent and independent
variables okay x equals to
data
text and y equal
to
data
class okay then splitting training and
testing
data okay sorry so here I will write
xcore
train comma xcore
test uh then Yore
train comma Yore tests to train
underscore testore
split then X comma
y comma
test let's go size equals to
0.25 okay press
enter so now let's convert X to vectors
for that I have to write
here Z is
X so here I will write from
Escalon do
feature
extraction
dot text
import D
vectorizer
okay then
vectorization was to T
fid factorer
okay
then V
underscore
train equals
to
vectorization r a on
factorization dot
fit then
transform
xcore
train okay then X Vore test
equals
factorization dot
transform xcore test okay then press
enter
uh so now let's see our first model
logistic
regression so here I will write
from Escalon
dot linear underscore
model OKAY
import
logistic
regession then LR go
to
logistic
regression have to write here LR
dot
fit then XV
dot not DOT do train
comma X Vore test
okay press
enter xv.
train okay here I have to write y
train okay and press
enter will work so here I will write
prediction
underscore linear
regression
lr.
predict Vore
test okay let's see the accuracy
score for that I have to write L do
score then XV uncore
test comma
Yore
test okay
let's see the accuracy so here as you
can see accuracy is quite good
98% now let's
print the
classification
put Yore test
comma prediction of linear regression
okay so this is you can see Precision
score then F1 score then support value
accuracy okay so now we will de uh do
this same for the decision free gradient
boosting classifier random Forest
classifier okay then we will do model
testing then we will predict this
school okay so now for the decision tree
classification so for that I have to
import from SK
Kean dot
tree
import
decision
three
classifier okay then at the short form I
will write here I will copy it from
here
then
okay then I have to write the same as
this so I will copy it from
here
and
yeah just change linear
regression to SE three
classific
okay then I will write here
same
dtal to dt.
predict
Vore
test still loading it's it will take
time
okay till then let me write here for the
accuracy D
doore
3core test comma
y
okay let's wait
okay
r the accuracy so as you can see
accuracy is good than this linear
regression okay logistic
regression okay so let
me you the let me
predict
print so this is the accuracy score this
is the all the
report
yeah now let's move for the uh gradient
boosting
classifier okay for that I write from
Escalon dot
assemble
port radient
boosting
classifier
clf I will write here
GB equals to let me copy it from
here
okay I will give here random let goore
State equals to
zero wait wait wait wait so I will write
here GB dot
fit X Vore train comma Yore train okay
then press
enter here I will write
predict
GB to GB dot pit sorry
predit
s dot
test not doore
test till then it's loading so I will
write here uh for the score then I will
add GB do score
then Vore test
comma Yore test okay so let's wait it is
running this
part till then let me WR for the
printing
this
okay it's taking
time taking time still taking
time what if I will run
this it's not coming because of
this yeah it's done now so you can see
the
accuracies uh not good
than decision tree but yeah it is also
good
99.4 something okay
so now let's check for the last one
random
Forest first I will
do for the random Forest we have to
write from Escalon
dot
symbol
import
random
Forest
classifier
okay then here I will write
RF
to right I will copy it from
here
then random state
=
to
then RF dot
fit X Spore
train comma Yore
train okay then press
enter and
predict
underscore
RC
RF equals
to RF do
predict Vore test
okay till then I will write here it's
still loading it will take time so till
then I will write for the score score
accuracy
score X Vore test comma Yore test
test okay then I will write here till
then
print
classification
port and Yore
test
comma will take time little
bit
so uh it run the accuracy score is 99 it
is also
good so now I will write the code for
the model testing so I will get back to
you but after writing the code
so so I have made two functions one for
the output label and one for the manual
testing okay so it will predict the all
the from the all models from
the repeat so it will
predict the it the news is fake or not
from all the models okay so for that let
me write here
news to
string
what okay
then I will write here manual
underscore
testing
so here I will you can add any news from
the you can copy it from the Internet or
whatever from wherever you want so I'm
just copying from the internet okay from
the Google the news which is not fake
okay I'm adding which is not fake
because I already know I searched on
Google so I'm entering this so just run
it let's see what is
showing okay string input object is not
callable okay let me check this
first okay I have to give here s Str
only
yeah let's
check okay I have to add here again the
script yeah manual testing is not
defined let me see manual
testing okay I have to edit
something
it is just GB and it is just
RF GBC is not defined okay okay so what
I have to do I have to remot
this
this okay everything seems
sorted
now
as I said to you I just copied this news
from the internet I already know the
news is not fake so it is showing not a
fake news okay so now what I will do I
will
copy one fake news from the
internet and let's see it is detecting
it or not
okay so let me run
this and let me add the news for
this
so all the models are predicting right
it is a fake news or you can add your
own script like this is the fake news
okay I hope you guys understand till
here so I hope you guys must have
understand how to detect a fake news
using machine learning you can and that
you can copy any news from the internet
and you can check it is fake or not if
you want to become an AI expert and gain
handsome salary packages look at the
wide range of AIML courses by simply
learn in collaboration with top
universities across the globe by
enrolling in any of these certification
programs you will gain expertise in
skills like generative AI prompt
engineering chat GPT explainable AI
machine learning algorithms supervised
and unsupervised learning model training
and optimization and there's much more
on the list with hands-on experience in
the tools like chat GPT di python open
CV and tens oflow you will catch the
eyes of top recruiters so what are you
waiting for hurry up and enroll now an
year of experience is preferred to
enroll in these courses find the cost
Link in the description
Box open CV open source computer vision
library is an open Source computer
vision and machine learning software
Library it is written in C++ but has a
binding for various programming
languages such as python Java MLB open
CV was designed with the goal of
providing a common infrastructure for
computer vision applications and to
accelerate the use of machine learning
perception in commercial product open CV
is widely used in a variety of
Industries including robotics automative
and Healthcare it's supported by a large
community of developers researchers and
users who contribute to its development
and provide support to its users it is
supported by a large community of
developers researchers and users who
contribute to its development and
provide supports to its users so now
let's see what is object detection
object detection is a computer vision
technology that involves identifying and
localizing the object of interest within
an image or a video it is a challenging
task as it involves not only recognizing
the presence of an object but also
detecting its precise location and size
within the image or video object Det
detection algorithm typically used deep
learning techniques such as CNN to
analyze the image or video for
identifying the objects these algorithm
can also determine the boundaries of the
object by drawing a bounding box around
them so after understanding what is
object detection now let's move on to
the programming part so this is a kernel
python kernel here we will change this
name so here I will write object
detection
demo Okay so
so first we will import some major
Library like open CV so for that we will
write import CV2 and the next one is
import M plot
lib P
plot as PLT so why we are writing PLT
because we can't write again and again M
plot lip. pip plot okay it's a long one
so we can write a short form
PLT so yeah so let's run this so what is
open CV opencv is an open source
software library for computer vision and
machine learning the open CV full form
is open source computer vision Library
it was created to provide a shared
infrastructure for application for
computer vision and to speed up the use
of machine learning perception in
consumer products open CV as a BSD
license software make it simple for
companies to use and change the code
okay so there are some predefined
packages and libraries that make our
simple and open CV is one of them second
one is mat BL Li mat blot Li is a easy
to use and an amazing visualize library
in Python it is built on numpy array and
designed to work with broader CPI stack
and consist of several plots like line
bar scatter histogram and many others
okay so moving forward we will import
our file okay so here I will write
config file equals to this is our file
name SSD
_
mobilet
V3
large
Coco
202 14.
BB okay so you can find this file on the
description box
below Frozen model equals
to I explain you every single thing
inference
graph. PB okay so let me run it first
mobile net as a name applied the mobile
net model is designed to use in mobile
application and it's tflow first mobile
computer vision model mobile net use
depthwise separable convolutions it
significantly reduces the numbers of
parameter when compared to the network
with regular convolutions with the same
depth in the NS this result in the
lightweight of the deep neural network
so mobile net is a class of CNN that was
open source by Google and therefore this
gives us an excellent starting point for
training our classifiers that are
insanely small and insanely fast okay so
what is this large Coco this is a data
set Coco data set like with applications
such as object detection segmentation
and captioning the Coco data set is
widely understood by the
state-of-the-art of neural network it's
versatility and the multi-purpose scene
variation serve best to train a computer
vision model and Benchmark its
performance okay so what is coko the
common object in context is one of the
most popular large scale label images
data set available for public use it
represent a handful of object we
encounter on a daily basis and contains
image in notations in 80 categories I
will show you the categories I have with
over 1.5 million object instances okay
so modern day AI driven solution are
still not capable of producing absolute
accuracy and result which comes down to
the fact that Coco data set is a major
Benchmark for CV to train test and
polish refine models for faster scaling
of The annotation Pipeline on the top of
that the Coco data set is a supplement
to transfer learning where the data used
for one model serves at a starting point
for the another so what is frozen
insurence graph like freezing is the
process to identify and save all the
required graphs like weights and many
others in a single file that you can
usually use a typical tensorflow model
contains four files and this contains a
complete graph okay so forward let's
create one model here I will write model
to CV2 do
DNN
model
Chen
model and then config
file here I'm giving the parameters two
parameters like frozen model and config
file score here yeah run it first okay
there is error
return okay so error is CV2 DNN
Direction model return is
result then exception set
the question comes what is the meaning
of deduction model or DNN deduction
model so this class represent the high
level API for object detection networks
detection models allows to set
parameters for pre-processing input
image detection model creates net from
file and with train weights and config
sets a processing input runs forward
pass and return the result deductions
okay moving forward let's set the class
labels okay class
labels file
name to labels.
dxt I will put this file on the
description box below you can download
from there
open file
name
as
labels
District
so here I created one array of name
class labels so this is the file name
what I'm doing I'm putting this label
file into these class labels okay so
here if I will
print class
labels so these are the 80 categories in
the Coco data
set okay this person bicycle car
motorbike aerplane bus train these all
are the 0
categories I will put this file label.
txt in the description box below you can
download from there okay
fine so let's print the length of the
Coco data set or you can see class
labels
this 0 as you can see I have already
told
you the length will be
80 so here let's set this some model
input size scaling mean and all so I
will write here model dot set
put
size 320 comma
320
do
set
input
scale
1.0 SL
127.5 okay I will explain you don't
worry model do
set
set
input
mean
127.5 comma
127.5 comma
127.5 okay and then model do
set
we
B will be what is set input
size
okay so set input size is a size of new
frame a shape of the new blob less than
zero okay so this is the size of the new
frame the second one is set input scale
the set input scale is a scale factor of
the value for the frame or you can say
the parameter will be the multiplier of
the frame values or you can say
multiplier for the frame values okay so
input mean so it set the mean value for
the frame the frame in which the photo
will come the video will come or my
webcam will come so it set the mean
value for the frame or the for
parameters mean scalar with the mean
values which are subtracted from the
channels you can see and the last one is
set input swap RB so it set the flag
swap RB for the every frame we don't
have to put every time a single frame
for a particular image it will be set
the true for the all the images okay so
parameters will be swap RB flag which
indicates the swap first and the last
channels so moving forward we will P one
image I am
read.
jpg
do I am
show
so this is the size of 320 by 320 okay
so first thing is you can download this
the random picture from the Google I
took from Google itself so now what we
will do we will set the class
index the confidence
value
value the B box B box is the boundary
box which I will create for the
particular person cycle motorbike and
the car okay bals to
model the confidence
threshold threshold is used
for if my model will confirm it's the
particular image which is the texting is
correct it will print the name okay
okay so let me
print
print
class class index is coming 1 2 3
4 okay so one means
person two means Bicycle three means car
and four means motorbike this this is
the class index index for particular
label what I will do I will print the
boxes
font
scale = to 3 and the font equals to CB2
dot font
her
Lane
for
class index and the
confidence and the
boxes
and Dot
pattern
confid
that
box the boundary
box
okay then I will write here CV2 do
rectangle make the
rectangle set the
image and
boxes five comma 0 comma 0 this is the
color of the box and this will be the
thickness
okay then I will write CV2 dop
put
text image
then class
labels I will write class index minus
one because always index start with zero
that's why and the box is
Z comma
boxes
one
4 okay
F
comma
scale
font
scale
color
to this will be the text color 0 comma
255 comma
0 and the
thickness three
let me run
it h no error okay now PLT dot I am
sure then CV2 do
CBT
color
then
cvt2 dot
color col then BGR to
brg that is why we wrote swap RB equals
to true because every time we will
convert BGR to
brg
sorry GB RGB so we don't have to write
again and again it will convert all the
files into RGB okay run
it okay as you can see the motorbike is
coming bicycle is coming the person is
coming the car will car is coming okay
so detecting the
right for the image now we will do this
for the video and for the
webcam we are done with this image one
and then now I will write
here
okay so this is will do for the
video
for the video I will write here cap
equals to capture you can write any name
so CV2
do
video
capture so you can take any random video
I of this
pexels
George can comment
down
share the
link app dot is
open so here I will write
cap equals
to2 sorry sorry
CV2 dot
video
capture
zero and if
not cap do is
open
then and
raise
output
error
can't open the
video can't open the
video here everything will be the same
font
scale = to
3 okay font equals to C V2
dot
font
her okay so here I will write V
true comma
frame equals to cap Dot
read this is for the reading of the
file the same I will write class
index comma
confidence comma boundary
boxum model dot
detect flame and the
confidence
threshold to
0.55 okay everything is the same we did
before so here I will
print
class
index okay so here I will write
if
and of the class
index does not equals to zero
then what to perform is here I have to
write
for class
index comma
confidence comma
boxes
inip
CL
index.
flatten flatten is a layers okay
confides
flatten e
box and
if
class index is greater than equals to
80
then what to do then I will copy from
here okay the same thing I have to write
here so here I will write CV2 Dot
I am
sure this will be the return in the
frame object
detection by simply
learn and
frame so if CV2 dot vit
key
to
and
zero
FFX
to o
d q okay
then here I will write
break will be break when it get into two
the weight key will be two okay I will
tell you what is the weight
key here I will write cap Dot
release and CV2
dot
destroy
all
windows
okay so now let me
run let's see error there will be error
okay
see Python programming language
modules let me run it
again
Keys
okay video is
here the video is here as you can see
see bicycle the person the person the
bus car traffic light the person person
so our object detection for the video is
coming right okay person okay person
traffic light
[Music]
buz this is how you can do for the video
okay so now let's we will do for the uh
webcam
live so this is for the video
so if we want to do for the webcam we
okay so we need to just
change one one thing only we have to
change instead of giving the file we
have to write one here okay the rest
will be the
same got it so I have to just shut down
my webcam so let me shut down the webcam
and get back to
you
as you can
see this is a 320 by 320 box
so so this is coming right okay so I if
I will show this the mobile phone is
coming right now okay so this is how you
can do the correct object detection okay
so first we will open command promp to
write a command to open Jupiter notebook
so here we will write
Jupiter
notebook then press enter so it will
take some time
yeah so this is the landing page of
jupyter notebook and here we have to new
then Python
3 so this is how the Jupiter notebook UI
look likees so at first we will import
some major libraries of python which
help in creating a mass detection system
so here here I will write
import CV2 comma OS and press enter then
I will give the path then data path
equals to
C then
slash
users then
slash
SLP 093
75 then slash
desktop then
slash then
pH
mask
detection then slash data
set
okay so here it will be slash my bad
sorry and
slash okay it's look fine
so here I will write
categories equals to OS
do
list directory list
di then I will assign data uncore
path then I will create some
labels equals
to uh here I will
right I will write here
I
for I in
range then here I will give
length of
categories yeah
then press enter Then here I will write
label then take directory equals
to directory
then zip then
categories comma
labels so here I will write
print
then label
directory okay then I will
print
categories then I will print
labels okay then press enter
yeah so here CV V means capturing video
the CV2 function in open CV can read
video video capture by using pass zero
as a function parameter we can access
our webcam we may pass rtsp URL in the
function parameter to capture CCTV
footage which is quite helpful for video
analysis then OS this this OS module the
OS module in Python has function for
adding and deleting folders retrieving
their contents changing the directory
locating the current directory and more
before you can communicate with the
underlying operation system you must
import the OS
module and this OS list directory to
retrieve a list of all files and folders
in the given directory use Python os.
list directory method the list of files
or directory in the current working
directory will be returned if no direct
is is
specified then label the the TK inter
visit CL class called a label is used to
show text or an image the user views the
label they cannot be interact with it so
moving forward I will write code so just
stay with me after that I will explain
you line wise
okay so here I will
write
that image underscore
size = to
100 then I I will create two classes
data then
Target okay
arrays then I will write here for
category in
categories
folder
underscore
path equals to OS do
path then dot
join then
data underscore
path comma
category okay then
IMG mage names equals to os.
list directory then folder
path just stay with me I will explain
you line
wise okay so here I will write
for IMG
name n
IMG names
IMG
path equals to os.
paath
dot
join then I will write here
folder I
repeat I repeat so here we WR
folder underscore
path then IMG underscore
name so IMG equals to
CV2 do IM am
read then IMG uncore
paath so here I will write
try that gray equals to
CV2 dot
CVT then Capital C
color then IMG comma
CV2
dot then
color then underscore
bg2
RGB
okay so I will go for the gray
one CVT color
BGR
to
yeah so I will write here
resize okay not in capital letter
resize equals to
CV2 Dot
resize then
gray comma
IMG
size comma I am just
s why this two IMG size because like
length and uh like width and breadth
would be
sh like 100 so press enter so I will
write dat
dot
appen then
resized then
Target do
append then
label then
directory of
category
Okay so
so I will write here
accept
exception s
e so here I will
print
then
exception comma
e okay then press enter let's see there
should be no
error so it is loading let's
wait okay no
error so image size should be like 100
by 100 so that is why I wrote here 100
then image size 100 by 100 and I made
two arrays like for one for data then
one for targets
so this for this gray equal to CV2 do
CVT color image so converting the image
into gray scale okay then this line
resize equals to CV2 do resize gray IMG
so resizing the gray scale into like 100
by 100 since we need a fixed common size
for all the Imes in the data set
okay then this target a do upend label
DIC so appending the image like and the
label categories into the list list what
is list data set so like for the last
print exception e so if any exception
raised the exception will be printed
here and the pass to the next image
okay so moving forward let's import
numai and save this data and the target
okay so I will write here
import
numi as
NP
right then I will write here
data equals to NP do
array then again
data by
25.0 okay then again data equals to NP
dot re
shape then
data
comma
data dot
shape then
zero comma mg
size comma mg
size comma 1
okay then press enter Then for
Target let me put do this like this then
Target equals to NP do
array then
Target
okay
okay so here I will write
from gas Dot
jtil
import
NPS okay so here I will write new
underscore
Target equals to npor
utils dot
2 underscore
categories okay categorical I guess
categorical then
Target
okay then press
enter so here I will save np.
save this
data comma
data okay and np.
save this
Target comma new
Target
okay so press
enter
yeah so numai numai is a python Library
used for working with arrays it also has
a function for working with in the
domain of linear algebra and matrices it
is an open source project and you can
use it freely numai stand for numerical
Python and the scas a Python interface
for artificial neutral network is
provided by the open source software
package known as kasas the tensorflow
library interface is provided by the
kasas a number of backends were
supported by kasas up until version 2.3
including tensorflow Microsoft
cognitive so this is the part of the
data processing this all this part of
the data processing so let me write here
data pre-processing
preprocessing so the data pre-processing
part is done now we will create the
another file of this python for the
training CNN CNN means convocation
neural network okay test
data
comma train underscore
Target
comma
testore
Target equals to
train underscore
split then we will split on data and
Target comma test size should
be
0.1 okay so now I will give press enter
okay train is train split is not
defined
okay it is
train test
spit
yeah so now I will write
checkpoint equals to
model
checkpoint
then
in
model I repeat so here I will write
model comma
EPO then Z
3D do
model
okay so here I have write comma
monitor equals to Value
loss
underscore
loss
comma
bubos equals to Z comma save
best
only equals
to True comma
mode equals to mode should be
Auto
okay then I will press
enter Then here I will write
history equals to model
Dot
fit to
train underscore
data
comma okay train uncore
Target
comma APS equal to
20 comma call
backs call back equals
to
checkpoint comma
validation validation underscore
split equals to 0.2 the best ratio okay
so let me press enter okay there is one
error model. fit C
10x okay there should be a spelling
mistake e p o
c
PS to
20
do model it is
fine then
monitor to Value
loss then BOS = to
0 save best only equ true then
more okay
so here I have to write o maybe it will
so so it will take some time to go till
20 okay so it will download one by one
so we will wait for a
why so the model checkpoints are
completed so here I will write
print
then
model dot
evaluate okay here I will write test
underscore
data comma
testore
Target then press
enter okay so I hope you guys understand
till here if you have any question or
any query regarding any code just put as
in comments our team will shortly
provide you the correct solution okay so
moving forward we will do we will create
another file for detecting mask okay so
I will go here and then new
file
python so I will write here
detecting okay
yeah so I will import some libraries
here kasas do
models import
load
models we will
import
CV2 and we will
import
numai as
NP so press enter okay M by as
NP okay so
presenter so what I will do I will write
here model equals to load underscore
model
then okay caras model from gas cannot
import
name okay
so it is model only yeah so here I will
add model equals to load model
so I will write here
model
017 do
model
okay okay
my so here I will write face underscore
classifier equals to
CB2 Dot
cascate
casate
classifier so this is one file for
frontal phase default so you can find
this file on the description box below
so I will write
here our
casate
underscore
frontal
Spore
default
do
XML okay so
Source equals
to
CV2 dot
video
capture zero it will open our camera so
labels underscore
directory equals
to
zero
mask and one
for no
mask
okay so
color directory equals
to like no mask for red and MK for green
okay so WR
zero then Z 0 comma
2 55 comma
0 then again comma for one there should
be 0 comma 0 comma
251
okay so I hope you guys understand till
here so if you have any question or any
query regarding any code so just put as
in comments our team will shortly
provide you the correct
solution
so the code is
written so let me run this for this
first I have to like so first I have to
close my this person on screen
camera so the code is running
fine now it is showing no
mask now it is showing
Mas so I hope you guys understand till
here if you have any queries any
question regarding any quote just put as
a comment our team will shortly provide
you the correct solution so in today's
session we will discuss what image
classification is and moving ahead we
will discuss what CNN is and at the end
we will do a hands of lab demo of image
classification using
CNN so before we move on to the
programming part let's discuss what
image classification is and proceed
further for this them what is image
classification the process of
classifying an entire image is known as
image
classification images are anticipated to
have just one class per image models for
image classification taken an image as
input and produce a prediction of the
class to which the image belongs so we
can utilize image classification models
when we are not interested in individual
instance of items with position
information or their
shape so let's see what is
CNN machine learning includes
convolutional neural networks also known
as convents or CNN it is a subset of the
several artificial neural network models
that are employed for diverse purpose
and data sets a CNN is a particular type
of network design for deep learning
algorithm that is utilized for task like
image recognition and pixel data
processing and so more okay although
there are different kinds of neural
network in deep learning CNN are
preferred neural architect for
identifying and recognizing object
therefore they are rarely suited for
computer vision activities and
applications where accurate object
recognition is crucial such as facial
and self-driving automobile system so
moving ahead so dear Learners if you
want to upskill your AI and machine
learning skills so give yourself a
chance to Simply learn professional
certificate program in Ai and machine
learning which comes with the completion
certificate and in-depth knowledge of AI
and machine learning check this course
out details from the description box
below so now let's move to our
programming part of how to do image
classification using CNN if getting your
learning started is half the battle what
if you could do that for free visit
scaleup by simply learn click on the
link in the description to know more so
first we we will open a command prompt
to write a command to open jupyter
notebook so here we will write
jupter
notebook press
enter so this is the landing page of
jupyter notebook so here you can select
new python
file so this is how the coners look like
okay jupyter notebook cers look
like so first we will import some major
libraries of python which will help us
in like analyzing the data okay so in
this file we will classify small images
of C if 10 data set from tens oflow data
set there are total 10 classes as shown
below so we will use CNN for the
classification purpose okay so here I
will write
import sensor
flow as
TF okay
so from tensor
flow dot
Kus
import data
sets comma
layers
models okay so we will
import numi as
NP right and
import met plot
Li as
PLT right okay so here I will write P
plot as
PLT right so tensor flow this one this
so tensorflow is a free and open source
machine learning and artificial
intelligence software Library it can be
used for variety of applications but it
focuses on mainly deep neural network
training and the inference purpose okay
got it and this numpy numai is a python
Library used for working with arrays it
also has a function for working with the
domain of linear algebra and matrices it
is an OP source project and you can use
it freely numai stand for numerical
Python and this third one met plot lip
for Python and its numerical extension
numai M plot Li is a crossplatform data
visualization and graphical charting
package as a result it present a strong
open source substitute for matlb the API
application programming interfaces for M
plot Li allow programmers to incorporate
graphs into GUI
applications got
it so let's run
this let me change
image classification
using
CNN
okay so let's load the data set okay we
will load the data set from the
uh load data function so here I will
write X
underscore
train comma Yore
TR okay
and one for test
xcore test comma Yore
test okay then equals to data
set sets dot we are using C far 10 C4 10
f
10 dot
load Dot
underscore data
okay it will load our data so let's load
the
data data is loaded let's see
X score
test.
shape
okay yeah
so as you can see so we have
th000 rows and one more
X underscore
train.
ship let me run
this so here you can
see like we see like training
data like training images are
50,000 and the test images are 10,000
okay so this is for testing this is for
training and so moving ahead we will see
for the
Y
train do
shape
okay th000 and here we will see the
array so ycore
train five okay let's send this first
let me give some space
yeah so here Yore train is a 2d array
like for our classification having 1D
arrs are good enough so what we will do
we will convert this to now 1D array
this is 2D array we will convert into 1D
array okay for
that we'll write here Yore
train equals to Yore
train dot
reshape minus one
comma okay then y underscore
train
and again semicolon
5 let's run this okay so now this is 1D
array so Yore
test = to Yore
test
dot
reshape in minus
one
here I will
write
classes go there are some classes okay
in the data set like
airplane comma
automobile
comma
BD
comma cat
comma here
dog
Rog
Course
Truck
okay so these are the some
classes like airplane automobile bird
cat deer so it will be help in
classifying the images so let's plot
some images to see like what they what
they exactly are okay so we will what we
will do we will create one
function for that let me write
here def plot uncore sample
okay then X comma
y comma
index
okay then I will write here plot do
figure PLT do
figure then figure size should
be comma 2 equals
to p do
show image show to I am
show
X
index
PLT dot X
label
classes y index
okay so let's see some samples of the
images so I will write a
plot underscore
sample uh xcore
train comma Yore
train okay comma let's see the fifth
image okay then
enter should be
Capital so as you can see this is a car
so it is showing Auto mobile okay so
let's see once
more like plot underscore
sample xcore
train comma Yore
test comma we'll see the 10th
one okay it is not quite
visible so we'll go for the 11
okay okay I
am why it is showing wrong because I use
here test I have to use here train
instead of test then it will show I
think correct yeah you can see HS then
HS
then what about 2011
image see you can see so we will see
once more
500y frog okay it's not quite
visible yeah so you can see
here the proper shape
okay so what we will do now we will
normalize the image is to a number from
0 to one image has three channels like
RGB colors so and each value in the
channel can range from 0 to
255 hence to normalize in 0 to 1 range
we need to divide it by 255 okay
so now what we will do we will normalize
the
data so here what we'll do at xcore
train equals to xcore
train divided by 255
and0
okay and same for
testal to X
test divided
by
25.0 okay to range between 0 to
1 yeah we will build simple artificial
Nal Network for image classification
first okay so we will write here
annals to models dot
sequential okay then I will write here
layers
Dot
plattin inut
score shape equals
to 32 comma 32 comma
3
okay then again layers dot
10
3,000 comma
activation to
rlu got
it then again for the Thousand so I will
what I will do I will copy it and paste
it here okay so
th000 so uh let me add one more one more
layer dot
10s then let 10 comma
activation
to soft
Max
okay here I will okay let me give for
the better
visuals yeah
Ann andn do
compile
Optimizer okay to SGD
then
comma I will write here
loss equals
to
sparse
categorical cross
entropy
okay so here I will write comma then
Matrix
equals
to
accuracy
andn do
fitore
train comma Yore
train comma
EPO
okay so number I will give a box = to 5
right so it will take time to
run it's
running here there is some
[Music]
issue
train x equals
to but it is
saying in user code the file program
C engine
okay I will copy it
and P it
again run it
again okay now it's
working
okay so it will take time and then I
will get back to you
okay then
import numai you guys already know what
napai is
NP Yore prediction
to ann.
predict
xcore
test okay then
Yore prediction
underscore
classes equals
to NP
dot Argent
Max
element for
element
in Yore
prediction
print
classification
port comma
classification let's go
report then
Yore test comma
ycore
prediction
underscore
classes okay so let me run
this
this
Capital yeah it will take less
time now we will create a graph
okay like X text is like we have
thousand images so CL graph will be like
messed up still let's see so I write
here
import
cbor as
SNS
Okay Okay C
Bor yeah me give some
space here I will write PLT dot
biger size should
be
14 comma
7 okay then as in as we will create a
heat map for this and Yore
prediction not equals to
true then PLT do y
Lael this
truth
PLT do X
label
ition then PLT do title should
be
Fusion okay then PLT do
show
let me run
this
okay see why prediction has X
text it's still
running let's
wait
so now let's make CNN model okay to
train our images so for that I will
write CNN equals
[Music]
to
models
dot
sequential
okay we'll write paste yeah so let me
run
this
so this is our CNN model from which we
will train our
images and CNN like
compile then
optimize
Optimizer equals to
ADM
okay okay
comma right here then loss equals
to
sparse
categorical then
cross
entropy okay and comma I will write here
Matrix equals
to
accac run this
okay
Lo yeah
same goes for
[Music]
you okay
loss will I do I will rewrite
this
okay now let's check CNN model for the
10 epox
okay let's see the accuracy is
increasing or
not
CNN
train comma Yore
train
comma
box
it is
started it will take less time than the
previous one
okay
see
you want this whole code you can comment
down the same
okay after completing this I will get
back to
you so it is almost done like 27 seconds
with the 10
aox till then let me write cnn.
evaluate xcore test comma Yore test
okay
so with CNN the N5 epox accur was around
like 70% and which is a significant
improvement over Ann okay Ann we have
like just
49 okay and CNN are the best for image
classification and gives the superb
accuracy also computation is the much
less compared to simple Ann as Max
pooling reduces the image Dimension
while still uh preserving these uh
features okay so let me run
this take some
time till then I will write Yore
prediction equals to
CNN dot
predict then xcore
test okay
then I will write here Yore
prediction then colon
five let me run
this so you can see the accuracy and all
the array
okay
yeah so
let's classes equals to NP dot ARG
Max
element
or element in y
prediction okay then
Yore
classes then
these are the number of classes then
Yore
test is colum
five these are the array so it's
converted into array then now let's see
the it is predicting Right image or
wrong image okay by not with the
training data here we predict from the
training data y training X train okay
now we will predict from the test data
so here I will write
plot underscore
sample then xcore test comma Yore test
and you can write the random one so here
I will write
60 let's see so you can easily see here
this is H and it is predicting right
HS and let me okay plot _
sample then
xcore test comma Yore
test comma
100 okay PR
enter okay X is
capital yeah so you can see this is
deer okay so our model is predicting the
correct image okay then what we like
let's see it is predicting the right
class or not okay we made the classes
like random
classes okay where are these these so
let's see it is predicting right or
wrong okay for that I have to write
classes and Yore
classes it's like which number 60 okay
okay
60 60 is not defined because like number
of
classes okay okay so there are 1 2 3 4 5
6
7 okay 0 to 9 I can
choose so here what I will
do I will take small one I will
five okay this is frog
okay this frog right so I will take
instead of 60 here I will see
frog okay y class is not
defined Yore
classes is defined see okay there are
3s yeah so as you can see frog this is
frog so our classes is defining right so
here I will write it again like
60 it was H and let me write here
60 you can see the right prediction
okay
so this is what how you can you do image
classification using CNN I hope you guys
must have understood this concept like
how to do image classification using CNN
in today's session we will go through
what is movie recommendation system
after that we will see how movie
recommendation system work moving
forward we will see filtering strategies
for movie recommendation system and at
the end we will see handson lab demo
like how to create movie recommendation
system using python in detail we already
have data set with us we will perform
different function and Implement a movie
movie recommendation system using python
before we move on to the programming
part let discuss what is movie
recommendation system actually is and
proceed further for the
same so renting CDs and DVDs reading
local TV listenings watching film strip
projectors or recordings all of this is
a thing of the past today all of the
world biggest film collection have been
digitized and moved to the online
streaming services like Netflix HB or
YouTube this platform can now help us
with what is possibly the most difficult
task of all choosing a movie they have
been enhanced with AI powered
capabilities well that is no longer a
concern for you it is finally time for
machine learning to put its skills on
display in the modern cinematic
landscape to create Advanced predictive
system for True movie expert data
scientists are prepared to investigate
our behavioral patterns and those of
movies a movie recommendation system
also known as a movie recommender system
uses machine learning to predict or
filter user fil preferences based on
their prior decision and actions it is
an advanced filtration system that
anticipate the consumer in question
potential like selection for a domain
specific item a
movie so after seeing what is movie
recommendation system let's move forward
and see how movie recommendation system
actually work Works a movie
recommendation system fundamental idea
is a pretty straightforward every
recommended system primarily consists of
two components users and items user
receive more prediction from the system
and the actual movies are the products
filtering and predicting only the movies
that a matching user is most likely to
wish to see in the main objective of
movie recommendation system the user
information from the system database is
used by the ml algorithm for these
recommendations system based on
information from the past this data is
used to forecast the user in questions
behavior in the future data should be
handled by expert because it is so
crucial to ml projects including the
movie recommendation
system after seeing how movie
recommendation system works let's see
some filtration strategies for movie
recommendation
system to assist user in finding the
most elevant films movie recommend
system employ a variety of filtration
techniques and algorithm the content
based filtering and the collaborative
filtering system subcategories of the ml
algorithm used for the movie
recommendations are the most well-liked
ones filtering based on content or
content based filtering a method of
filtering movies in a movie
recommendation system that makes
advantage of the items data this
information which is taken from the just
one user is quite important in this this
case this technique uses an ml algorithm
to suggest movies that are comparable to
the user's past choices therefore the
information about the prior movie
choices and likes just one person is
used to generate similarity in content
based
filtering and the second one is
collaborative filtering as the name
implies this filtering techniques is
based on the interaction between the
relevant person and the other user for
the best outcomes the system some
contrast and compare these behaviors it
combines the film choices and users
patterns of several people like there
are two types of collaborative filtering
algorithm the first one is collaborative
filtering based on users the goal is to
find patterns in Target users and other
database users like movie preferences
and the second one is collaborative item
based filtering the fundamental idea
behind this is to find comparable
products products like movies that
Target users rate or interact with after
seeing the filtering strategies for
movie recommendation system so here is
one question for you guys I will give
you one minute for this you can comment
or you can give answer in chat section
so I can see if the answers given by you
are right or wrong I'm repeating again
so here is one question for you guys I
will give you 1 minute exactly 1 minute
for this you can comment or you can give
your answer in chat section so I can see
if the answers given by you are right or
wrong so the question is which is the
best language for machine learning
programming which is the best language
for machine learning the first one is
Java the second one is python the third
one is R language and the fourth one is
C++ so this is the question I'm
repeating again which is the best
language for machine
learning the option one is is Java
second python R language and C++ so I'm
starting timer of 1 minute just type
your answers in comment section or in a
chat section do let me know your answers
please I want that everyone should
participate in this and make this live
session
interesting so I'm starting
timer so your time starts
now guys do let me know your answers in
chat section or in you can comment
down 42 seconds to
go you can comment or you can give your
answer in the chat section so I can see
if the answers given by you are right or
wrong 30 seconds
more which is the best language for
machine learning
you can give your answer chat section or
you can
comment so I can see if the answ is
given by you are right or wrong guys
come on please I want that everyone
should participate in this and make this
live section
interesting come on guys 5 Second
more one yep time up so time is over we
will give reply those who gave correct
answer and those who didn't give correct
answer we will give you a reply with the
correct answer so now let's move to our
programming part to perform movie
recommendation system using
python so first we will open command
prom to write command to open jupyter
notebook so we will write
Jupiter notebook press
enter
so this is the landing page of jupyter
notebook and select here new new python
file so this is how jupyter notebook UI
look likees so at first we will import
some major libraries of python which
will help us in mathematical functioning
so so the first one is numpy import
numai as NP so nump is a python Library
used for working with arrays it also has
a function for working in the domain of
linear algebra and matrices it is an
open-source project and you can use it
freely numai stand for numerical
python so here NP NP is denoting numpy
so we will import the next
Library import pandas
as
PD this should be space
yep so pandas is a software Library
written for the Python programming
language for data manipulation and
Analysis in particular it offers data
structures and operation for
manipulating numerical table
and time series so so after importing
libraries we will move ahead and import
data set so for importing data set we
have to write like we have two data set
with us let me show you the data set we
have two data set with
us first one is
credits this
one and the second one one is movies
one one movies one so don't worry you
can get this data set link in the
description box so let me
write
here
credits go data frame equals to pd.
read _
CSV here you have to give your location
of the data
set
credits.
CSV the second one is
movies let's go data frame this is for
movie data frame pd.
read
yesp okay
so here you have to give the movie
location movie data set location
movies do
CSV okay everything seems good let me
run
this
yeah so
then wait here PD is for the pandas
Library read is used for reading the
data set from the machine and CSV is
used for the type of file which you want
to read after this let's write code to
see the data set so we will write here
credits unor
DF and we will run it so this is our so
this is our credit data set and next one
is movies
underscore so this is our movie data
set so this is how our both data set
look like so here you can see three
dots this one and here
also this one so we are unable to see
our full data set like what if we want
like full 4803
rows so at that case we can write
here PD do
set underscore option
okay and here we will write
display do
maxcore
columns
this and here we will write
none same for the rows PD do
set
option
display do
Max underscore
rows
none
okay
none let's run
this so now if we will write credits
uncore
DF it is running
yeah so now you can see your full data
set like full credit data set with 4803
rows or 4802 rows because there
is this is starting from zero that's why
so what about movies data
set okay it
running it is this is still running you
can see
here so here you can see a full data set
movie data set with 4802
rows or if you want to see only five top
rows so you can write here
credits DF do head so by this only five
top rows so from head you we can see our
five top rows and if we will use tail
instead of head we can see our last five
rows so here let me do with the movies
one DF dot
tail yeah so here you can see 4798 row
4799 till
4802 so with tail we can see our last
five rows so moving forward let's merge
the credit data set to movie data set
because if we will combine them the
confusion will be less and at the end it
will be better for us so we will write
here
movies
uncore equals to
movies underscore data frame and
credits underscore
DF
on okay T is a small I guess yeah yeah
perfect
title so let me do something something
like
this okay it is saying data frame object
is not
callable edits underscore
DF
movies
okay why because we haven't wrote merge
here now it will
work yeah perfect it's working so data
is merged let's see the number of rows
and column using share function so now
now we have only one so
moviescore DF do
shape okay let's run it 4809 and 23 23
but if you see here like in movies data
set so in movies data set here here yeah
here there are 20 and in here we are
four columns in Credit Data
set
so like so so you will be thinking
that's why not 24 why only 23 columns
that is why on title you can see here on
title the titles in both titles are same
you can see
here both titles are
same in
both okay let me show you like this okay
AAR Pirates of the Caribbean Spectra and
so on here you can see AAR P of the car
Spector and so
on so that is why on title return while
merging so title are same so moving
forward let's see our M data set so we
will write only
moviescore dataframe do
head
yeah so our data is merged you can see
the last cast cre movie ID yeah you can
see here movie ID title are the same
they must cost cre okay
guys let me give some
more for the better view
yeah so you can see all Columns of
credit data set are added to movies one
so moving forward let's see another
function like
movies uncore DF do
info
so by this yes you can see so by this
the data frame information is printed
via the info method the data includes
the total number of columns their labels
data kinds memory use rage index and the
number of cells in each columns like
non-null values note that info method
does indeed print the information so
here you can see the all the information
is
printed okay
like columns name and the all null
values count and data types to like this
int object
INT in which we are working most so
after this let's move forward and select
some main column in which you are
working most so we will write
moviescore DF equals to
moviescore
DF
comma and another one is
overview
why okay here we have to like this yeah
so here we have to write
joner Journal is complete and we will
work on what keywords keywords are the
main keywords okay keywords then to
cast then we
will we will do recommendation with the
crew as
well
okay by running this let me see our data
again with seven okay okay there is one
error okay this is movie movie idea Jon
yeah not in index okay got it got it why
not
joner it's joners I
guess what it is it's jonuts
that's
why it's jonuts
okay yeah so let's see our data again
with seven columns so I will write so I
will write here moviescore
DF do
head
[Music]
here so you can see only our we have
like movies ID same title overview
joners keywords cast and crew
only so here you you can see the
selected columns okay so let's move
forward and once again let's see the
info again like movies
_ TF
dot info is come with selected columns
and let's move forward and see how many
missing values are present in particular
column so like the columns is full we
can see here but we don't know about
that 4,803 values are filled or not so
what we will do it we will see how many
missing values are present there
so here we will write like
moviescore DF do is
null
sum not like
this yeah
perfect so the function data frame is
null this one is null do sum do sum
Returns the number of missing values in
the data set so here you can see
overview have three missing values so
what we can do is we can write
here movies underscore
DF
DF
dot drop
na
this equals to true
okay we will run it but sayings okay
document drop
a should true okay
fine doore drop and yeah it will not
create an issue so if you set in place
equals to true the drop any method will
modify your data frame directly that
means that if you set in place equals to
true the drop any will drop all missing
values from your original data set let's
move forward and see
movies underscore DF
dot
duplicated so the duplicate method
returns a series with true or false
values that describe which rows in the
data frame are duplicated or not use the
subset parameter to specify if any
column should not be considered when
looking for duplicates so here let me
run this first so here we are we are
getting zero why zero so if you are like
thinking there is no true or false
return return so if you write it without
sum like if I will write it without
sum okay let me copy from here yeah and
I will run this so here you can see
false false false in whole data set so
some will combine them and return you a
value at the end zero represent false
itself so we need do not need this so we
will write something else so moving
forward let me write first
moviescore DF do I location
Z dot
juts let me run
this so the iog function this one iog
function first let me do something like
this it will go up okay huh okay so from
iog function in Python is defined in the
pandas module that helps us to select a
specific row or column from the data set
using the iog method in Python we can
easily retrieve any particular value
from a row or column by using index
value so here zero position line is
coming which is here you can
see so name action like name action name
Adventure name fantasy this s
fiction this one science fiction yeah so
this is because I'm giving here Jor that
is why you can see here
see id28 name action name something
something something like that
okay so let's import something called
ASG abstract syntax tree so this is one
of the major Library import
as let me run this import is a class
defined is the EST module that is used
to express the import statement in
python in the form of abstract syntax
tree so so an abstract syntax tree or
just syntax tree is a tree
representation of the of the abstract
syntactic structure of text written in a
formal language each node of the tree
denotes a construct occurring in the
text so let's convert some like literal
to object and append them so here I will
write
def
convert obj object
like
l equals
to blank
array for I in St
dot
pit B
object okay like L do a
pen
return here we are converting is like
here here is a convert is a function and
L is a array to append by using for Loop
we can append them to name so let me
write code then I will explain like what
is happening so let me write full code
so
movies _
DF like
donuts okay equs to
movies
oners and then dot
apply
what so then movies underscore
DF then I will write
keywords to
movies underscore
DF
keywords _ DF
Dot so if we will see our old data set
in that like we will see old data set in
that Jer and this keyword this
keyword ID name and so on like ID that
name like this type of things so it will
create a mess in retrieving them so by
using as Library we just convert them to
the normal one like title so got it guys
so I hope hope you guys understand till
here if you have any question or any
query regarding any code or question
just put as in comment our team will
shortly provide you the correct solution
I preting again I hope you guys
understand till here if you have any
type of questions regarding any code or
something so just put as in comments our
team will shortly provide you the
correct solution so moving forward let's
convert C to and like same as previous
way like like this only so what we will
do here we will write def
convert here I'm
using so L equals
to counter equals
to0 St do
literal if
counter does
not equals to 3 then l
dot l
dot do append okay then here I have to
write
to
name
okay
counter equals to 1 if you want to write
like counter equals to counter plus one
it is like both the
same
else
take
turn
l so let me run this let me run
this okay so after this small code we
have to assign the values like we are
doing it for KAS now so
movies okay
moviescore DF then
cost to movies
underscore
DF
fast
Li
convert everything seems
good so let's see the changes are
visible or not so we will write here
moviescore DF do
head you can ignore this like this is
nothing so here you can see cast is also
same like keyword and joner so like
let's do for the cre to so like we are
converting you can say in a structured
way so let's do for the cre one to so
let me put
some
yeah so here we will add Dev like
fetch f is nothing like a function name
f
director
obj
okay colon L equals
to for i n s do
literal
obj
if
I
job equals to like we can say
director so then L do
append L do append
and app to the same
name
put
J okay capital
[Music]
L so we have to again write that movies
equals to
movies again
crew
Li
H just remember F director is a function
name let's see changes
so here I have to write movies then
press enter so yeah crew is all set so
changes are visible and data is looking
good so what we like if I want to see
the first movie ofar overview this like
this dot dot dot dot so I have to write
here like
movies
overview overview at Juro
place so yeah so this is a full overview
of AAR movie like let me read for this
in this 22nd century a paraplegic Marine
is dispatched and blah blah blah blah
and
civilization so moving forward that what
we want to recommend movie in the basis
of overview genre keyword cast so for
that we need a separate separate words
like in the then 22nd Century
along so if you will see in other
columns all the words are separated but
like here if you will see all the words
are separated like action is separated
Adventure is separated fantasy separated
so there is a long sentence in this this
one in the overview one there is a long
sentence so it's difficult to recommend
from this so what we are going to do is
well we will separate them like like
like other columns so here we will
write
movies
overview was to
movies then
overview and Dot
apply Li my bad sorry
then
Lambda colon x dot
split yeah error so let's see the
results so
movies yeah you can see in the set so
now you can see all the words are
separated by comma so now it will easy
to match overview to recommend a
particular movie so moving forward so
what we are going to do is if there is a
two word and space between there so we
are going to remove that space so here
you can see science fiction are separate
separate okay so we are going to like
remove this one this space so let's
write code for that so here I will write
like overview is already done so we will
do not do for that so let's write code
for the rest
columns so here I have to
write
movies
like
Jers two
movies and again
chuts
apply Lambda
X I do
replace
B to no
space uh it seems
good then I have to write for everywhere
so for I in
X it's 4 I INX 4 I
INX movies geners equals to movie J
Earth do apply Lambda X I do replace
this to this
for seems good so let's copy paste this
see one for keywords and one for cast
and crew
first
first that
you
test is same so let me run
this okay no error seems good so let's
see the changes so
movies
yeah so now you can see the science
fiction space is gone like here is
science fiction space is there now
science fiction space is
gone
y so like uh okay let's move forward and
create one new column name as tag and
put all the column data in a particular
column okay let me do first yeah let me
write
movies is a column of column name goes
to
movies then first we will put put
overview Plus
movies and
geners and
movies then what we have to give uh
keywords
keywords then again
movies
then
F me run
this okay so let's see the data frame uh
movies okay still coming so like there
is one problem I guess yeah no no no so
here you can see all the data is merged
in a under tag column yes you can see
like in the second century blah blah
blah
so I hope you guys understand till here
if you have any question or any query
regarding any code or anything just put
as in comment or our team will shortly
provide you the correct solution okay so
I'm repeating again I hope you guys
understand till here if you have any
question or any query regarding any code
or like any question just put as in
command our team will shortly provide
you the correct
solution so from here let's create a new
data
frame
okay so let me create new data frame
because now we don't need this overview
C crew joner like columns like this we
have already merge into a one one column
like tag column so we will write here
new
DF equals to
movies
okay so like movies we need movie
ID new data frame we need movie ID comma
we need
Title and we have everything in tag
column TX column so we need Tex
column okay run this let's see our new
data frame
yeah so this is how our new data look
likes with only three columns like movie
ID title and text so here you can see
array brackets like this this aray
brackets so we are going to uh remove
them so we will do what we will write
here we will write
new data
frame that
intax
equals to
new data
frame
TXS do
apply
Lambda
X then I will give
space dot
join samees good everything is okay okay
fine
yeah so let's see the results
so we will write a new underscore data
frame so you can see the brackets are
removed now let's see the data which is
present in the zeroth index in tag
column like this dot dot dot in the okay
for that we have to write
here uh a
new data
frame
tax we did it before like for the
overviewer so here we have to yeah so
this is the whole data in the first like
first index zeroth index so some part of
the overview like from
civilization and some are you can see
this action adventage fantasy science
fiction these are some like
joners
and uh like like here you can see like
these are some Crews
members
and yeah like Sam warington like there
are some cast crew okay so like moving
forward like what we are going to do is
just make them in lower case for the
better prediction like something in
capital something like smaller so let's
make it to lower case so newcore
DF
TX equals to
newcore
DF
TX do
apply colum X do
lower
okay so don't worry about this error
error let's see the result uh newcore DF
do yeah now all the data present at TAG
column is in a small case or you can say
lower case moving forward let's do some
feature extraction using count
vectorizer so let let me write first
from
skar dot feature
extraction feature extraction okay cool
do
text
import
count
vectorizer CV equals
to
count
riser
underscore
features
features equals to like
5,000
top
words equals to
English and this yeah so count
vectorizer okay there is
a I cannot import count V czer from
eSalon from
user okay okay sorry sorry my bad here V
is
capital okay guys so he V is capital
yeah so no error so count vectorizer is
a great tool provided by the psychic
learn library in Python it is used to
transform a given text into a vector on
the basis of the frequency count of each
word that occurs in the entire text
so let me write something CV
dot fit
underscore
transform
f
X like dot 2
array
shape
okay so the fit method like learns a
vocabulary dictionary of all tokens in
the Raw documents that is it creates a
dictionary of tokens by default the
tokens are words separated by spaces and
like punctuations so that Maps each
single token to a position in the output
Matrix so here CV is a like here CV is a
counter vectorizer so let's convert into
Vector to array so for this we have to
write
vectors vectors equals to CV do
fit okay then
transform X
so here we have to write
dot two
array yeah then we have to write
vector
it's
vectors yeah you can see Vector 2 array
all the
zeros okay so moving forward let's get
the feature Name by
writing okay so we will write
here lean Lan CV do get
feature feure
names feature names okay then
bracket yeah so okay we have like 500
feature names so I'm repeating I'm
saying again like I hope you guys
understand till here if you have any
question or any query regarding any code
whatever it is so just put as in
comments our team will shortly provide
you the correct solution okay guys like
any code or you are not understand
anything just put as in commment our
team will shortly provide you the
correct solution so let's import some
major Library which is
import
nltk so what is nltk the natural
language toolkit nltk is a platform used
for building python programs that work
with human language for applying in
statical natural language processing NLP
it contain text processing libraries for
tokenization parsing classification
stemming taging and semantic
reasoning so moving forward like from
let me write first from n
nltk do
stem.
pter
import
pter
PS equals
to
watering is a process for reducing a
word to its word stem that suffix and
prefix are to the roots of words known
as Alma steming is important in natural
language understanding nlu and natural
language processing
NLP so here like I have to write so here
I will write def
stem to
text y equals
to for I in text.
split
y dot
aen dot
stem
done what
join
y
so let's apply timing to a tag column so
here we will write new
DF
TX equals to
new
F
tag do
apply so let's import
cosine similarity from skar so we will
write from
skarn dot
matrices do
pairwise
import
cosine underscore
sity okay it seem good from Escalon
matrices Matrix sorry Matrix dot pair
wise it
is PIR
wise pair wise import coine similarity
okay let's run this no error so cosine
similarity measures the similarity
between two factors I repeat cosine
similarity measures the similarity
between two vectors of an inner product
space it is measured by the cosine of
the angle between two vectors and
determines whether two vectors are
pointing in roughly the same direction
it is often used to measure document
similarity in text analysis so here we
will
write
cosine underscore
similarity
vectors so these all are the vector
present let's see the cosine similarity
Vector column and row using space so
here I will write
cosine
[Music]
similarity
vectors dot
shape it's still running you can see
here yeah so 4806 rows and 4806 columns
are there let's assign variable
similarity to this okay so let
similarity we'll write like this sity so
cosine
similarity underscore
here why I did this because we don't
have to write always this long sentence
a long keyword you can
say so moving forward let's see first
colum like we will write
similarity yeah so this is our first
column uh so let's see the rows only so
we will write
here
larity
zero
dot
shape 4806 okay so let's move forward
and let's create some like sorted list
so for that
sorted
okay T
enumerate spelling is
fine okay one more like
similarity
similarity
zero then I have to write
reverse equals to
True comma key = to lamb
colon X not capital x
x have to write
one 1 colum
6 seems good yeah so the enumerate
function in Python converts a data
collection object into an enumerate
object enumerate returns an object that
contains a counter as a key for each
value with an object making item within
a collection easier to access so finally
let's create the recommend function to
check the movie recommendation
system
okay
so so let's write here
DF
recommend
X to
new
DF and the
TF
title equals to equals to
movie
X
zero
tenses
similarity and
movie okay so
movies list equals to sorted
numerate R
numerate
distances
reverse to
true
keyword sorry key only key equals to
Lambda colon
x
1 then col
six or I in
movies
list
F do
iog
I
zero Z dot title we because we need
Title only so yeah it seems good
okay so finally that like time is
came so finally the time is come to
check the recommendation of your
favorite movies so you can comment in
the chat section so I can check so let
me write first
recommend recommand for movie
Avatar okay
so here you can see the recommendation
system is working fine so Titan a. small
soldier Independence Day are the
recommendation for the movie ofar so let
me check for the another one
recommend like
for one of my favorite movie Iron
Man so here you can see Iron Man 2 Iron
Man 3 Avengers Age of Ultron Captain
America and the Avengers are the
recommendations for the movie Iron Man
so let me check for the one more
recommend
Liar
Liar Going on 30 and the last one is
last one I want to check is Captain
America Civil
War let me copy from here
only
yeah you can see the Captain America the
First Avenger the vter soldier Iron Man
3 Age of Ultron The
Avengers so I hope you guys understand
till here if you have any questions or
any query regarding any code or question
just put as in comments our team will
shortly provide you the correct solution
if you want to become an AI expert and
gain handsome salary packages look at
the wide range of AIML courses by simply
learn in collaboration with top
universities across the globe by
enrolling in any of these certification
programs you will gain expertise in
skills like generative AI prompt
engineering chat GPT explainable AI
machine learning algorithms supervised
and unsupervised learning model training
and optimization and there's much more
on the list with hands-on experience in
the tools like chat GPT di python open
CV and tens oflow you will catch the
eyes of top recruiters so what are you
waiting for hurry up and enroll now an
year of experience is preferred to
enroll in these courses find the course
Link in the description box interview
questions for machine learning now this
video will probably help you when you're
attending interviews for machine
learning positions and the attempt here
is to probably consolidate 30 most
commonly asked uh questions and to help
you in answering these questions we
tried our best to give you the best
possible answers but of course what is
more important here is rather than the
theoretical knowledge you need to kind
of add to the answers or supplement your
answers with your own experience so the
responses that we put here are a bit
more generic in nature so that if there
are some Concepts that you are not clear
this video will help you in kind of
getting those Concepts cleared up as
well but what is more important is that
you need to supplement these responses
with your own practical experience okay
so with that let's get started so one of
the first questions that you may face is
what are the different types of machine
learning now what is the best way to
respond to this there are three types of
machine learning if you read any
material you will always be told there
are three types of machine learning but
what is important is you would probably
better of emphasizing that there are
actually two main types of machine
learning which is supervised and
unsupervised and then there is the third
type which is reinforcement learn so
supervised learning is where you have
some historical data and then you feed
that data to your model to learn now you
need to be aware of a keyword that they
will be looking for which is labeled
data right so if you just say past data
or historical data the impact may not be
so much you need to emphasize on labeled
data so what is labeled data basically
let's say if you're trying to do train
your model for classification you need
to be aware of for your existing data
which class each of the observations
belong to right so that is what is
labeling so it is nothing but a fancy
name you must be already aware but just
make it a point to throw in that keyword
labeled so that will have the right
impact okay so that is what is
supervised learning when you have
existing labeled data which you then use
to train your model that is known as
supervised learning and unsupervised
learning is when you don't have this
labeled data so you have data it is not
labeled so the system has to figure out
a way to do some analysis on this okay
so that is unsupervised learning and you
can then add a few things like what what
are the ways of Performing uh supervised
learning and unsupervised learning or
what are some of the techniques so
supervised learning we we perform or we
do uh regression and classification and
unsupervised learning uh we do
clustering okay and clustering can be of
different types similarly regression can
be of different types but you don't have
to probably elaborate so much if they
are asking uh for uh just the different
types you can just mention these and
just at a very high level you can but if
they want you to elaborate give examples
then of course I think there is a
different question for that we will see
that later then the third so we have
supervised then we have unsupervised and
then reinforcement you need to provide a
little bit of information around that as
well because it is sometimes a little
difficult to come up with a good
definition for reinforcement learning so
you may have to little bit elaborate on
how reinforcement learning works right
so reinforcement learning works in in
such a way that it basically has two
parts to it one is the agent and the
environment and the agent basically is
working inside of this environment and
it is given a Target that it has to
achieve and uh every time it is moving
in the direction of the target so the
agent basically has to take some action
and every time it takes an action which
is moving uh the agent towards the
Target right towards a goal a Target is
nothing but a goal okay then it is
rewarded and every time it is going in a
direction where it is away from the goal
then it is punished so that is the way
you can a little bit explain and uh this
is used primarily or very very impactful
for teaching the system to learn games
and so on examples of this are basically
used in alphao you can throw that as an
example where alphao used reinforcement
learning to actually learn to play the
game of Go and finally it defeated the
co world champion all right this much of
information that would be good enough
okay then there could be a question on
overfitting uh so the question could be
what is overfitting and how can you
avoid it so what is overfitting so let's
first try to understand the concept
because sometimes overfitting may be a
little difficult to understand
overfitting is a situation where the
model has kind of memorized the data so
this is an equivalent of memorizing the
data so we can draw an analogy so that
it becomes easy to explain this now
let's say you're teaching a child about
some recognizing some fruits or
something like that okay and you're
teaching this child about recognizing
let's say three fruits apples oranges
and pineapples okay so this is a a small
child and for the first time you're
teaching the child to recognize fruits
then so what will happen so this is very
much like that is your training data set
so what you will do is you'll take a
basket of fruits which consists of
apples oranges and pineapples okay and
you take this basket to this child and
uh there may be let's say hundreds of
these fruits so you take this basket to
this child and keep showing each of this
fruit and then first time obviously the
child will not know what it is so you
show an apple and you say hey this is
Apple then you show maybe an orange and
say this is orange and so on and so for
and then again you keep repeating that
right so till the basket is over this is
basically how training Works in machine
learning also that's how training works
so till the basket is completed maybe
100 fruits you keep showing this child
and then in the process what has
happened the child has pretty much
memorized these so even before you
finish that basket right by the time you
are halfway through the child has leared
about recognizing the Apple orange and
pineapple now what will happen after
halfway through initially you remember
it made mistakes in recognizing but
halfway through now it has learned so
every time you show a fruit it will
exactly 100% accurately it will identify
it will say the child will say this is
an apple this is an orange and if you
show a pineapple it will say this is a
pineapple right so that means it has
kind of memorized this data now let's
say you bring another basket of fruits
and it will have a mix of maybe apples
which were already there in the previous
set but it will also have in addition to
Apple it will probably have a banana or
maybe another fruit like a jack fruit
right so this is an equivalent of your
test data set which the child has not
seen before some parts of it it probably
has seen like the apples it has seen but
this banana and jack fruit it has not
seen so then what will happen in the
first round which is an equivalent of
your training data set towards the end
it has 100% it was telling you what the
fruits are right Apple was accurately
recognized orange were was accurately
recognized and pineapples were
accurately recognized right so that is
like a 100% accuracy but now when you
get another a fresh set which were not a
part of the original one what will
happen all the apples maybe it will be
able to recognize correctly but all the
others like the jackf fruit or the
banana will not be recognized by the
child right so this is an analogy this
is an equivalent of overfitting so what
has happened during the training process
it is able to recognize or reach 100%
accuracy maybe very high accuracy okay
and we call that as very low loss right
so that is the technical term so the
loss is pretty much zero and accuracy is
pretty much 100% whereas when you use
testing there will be a huge error which
means the loss will be pretty high and
therefore the accuracy will be also low
okay this is known as overfitting this
is basically a process where training is
done training process is it goes very
well almost reaching 100% accuracy but
while testing it really drops down now
how can you avoid it so that is a
extension of this question there are
multiple ways of avoiding overfitting
there are techniques like what do you
call regularization that is the most
common technique that is used for
avoiding overfitting and within
regularization there can be a few other
subtypes like Dropout in case of neural
networks and a few other examples but I
think if you give example or if you give
regularization as the technique probably
that should be sufficient so so there
will be some questions where the
interviewer will try to test your
fundamentals and your knowledge and
depth of knowledge and so on and so
forth and then there will be some
questions which are more like trick
questions that will be more to stump you
okay then the next question is around
the methodology so when we are
performing machine learning training we
split the data into training and test
right so this question is around that so
the question is what is training set and
test set in machine learning model and
how is the split done so the question
can be like that so in machine learning
when we are trying to train the model so
we have a three-step process we train
the model and then we test the model and
then once we are satisfied with the test
only then we deploy the model so what
happens in the train and test is that
you remember the labeled data so let's
say you have thousand records with
labeling information now one way of
doing it is you use all the thousand
records for training and then maybe
right which means that you have exposed
all this thousand records during the
training process and then you take a
small set of the same data and then you
say okay I will test it with this okay
and then you probably what will happen
you may get some good results all right
but there is a flaw there what is the
flaw this is very similar to human
beings it is like you are showing this
model the entire data as a part of
training okay so obviously it has become
become familiar with the entire data so
when you're taking a part of that again
and you're saying that I want to test it
obviously you will get good results so
that is not a very accurate way of
testing so that is the reason what we do
is we have the label data of this
thousand records or whatever we set
aside before starting the training
process we set aside a portion of that
data and we call that test set and the
remaining we call as training set and we
use only this for training our model now
the training process remember is not
just about passing one round of this
data set so let's say now your training
set has 800 records it is not just one
time you pass this 800 records what you
normally do is you actually as a part of
the training you may pass this data
through the model multiple times so this
thousand records may go through the
model maybe 10 15 20 times till the
training is perfect till the accuracy is
high till the errors are minim minimized
okay now so which is fine which means
that your that is what is known as the
model has seen your data and gets
familiar with your data and now when you
bring your test data what will happen is
this is like some new data because that
is where the real test is now you have
trained the model and now you are
testing the model with some data which
is kind of new that is like a situation
like like a realistic situation because
when the model is deployed that is what
will happen it will receive some new
data not the data that it has already
seen right so this is a realistic test
so you put some new data so this data
which you have set aside is for the
model it is new and if it is able to
accurately predict the values that means
your training has worked okay the model
got drained properly but let's say while
you're testing this with this test data
you're getting lot of errors that means
you need to probably either change your
model or retrain with more data and
things like that now coming back to the
question of how do you split this what
should be the ratio there is there is no
fixed uh number again this is like
individual preferences some people split
it into 50/50 50% test and 50% training
Some people prefer to have a larger
amount for training and a smaller amount
for test so they can go by either 6040
or 7030 or some people even go with some
odd numbers like
6535 or 6333 and 33 which is like 1/3
and 2/3 so there is no fixed ru rule
that it has to be something the ratio
has to be this you can go by your
individual preferences all right then
you may have questions around uh data
handling data manipulation or what do
you call data management or Preparation
so these are all some questions around
that area there is again no one answer
one single good answer to this it really
varies from situation to situation and
depending on what exactly is the problem
what kind of data it is how critical it
is what kind of data is missing and what
is the type of corruption so there a
whole lot of things this is a very
generic question and therefore you need
to be little careful about responding to
this as well so probably have to
illustrate this again if you have
experience in doing this kind of work in
handling data you can illustrate with
example saying that I was on one project
where I received this kind of data these
were the columns where data was not
filled or these were the this many rows
where the data was missing that would be
in fact a perfect way to respond to this
question but if you don't have that
obviously you have to provide some good
answer I think it really depends on what
exactly the situation is and there are
multiple ways of handling the missing
data or corrupt data now let's take a
few examples now let's say you have data
where some values in some of the columns
are missing and you have pretty much
half of your data having these missing
values in terms of number of rows okay
okay that could be one situation another
situation could be that you have records
or data missing but when you do some
initial calculation how many records are
corrupt or how many rows or observations
as we call it has this missing data
let's assume it is very minimal like 10%
okay now between these two cases how do
you so let's assume that this is not a
mission critical situation and in order
to fix this 10% of the data the effort
that is required ired is much higher and
obviously effort means also time and
money right so it is not so Mission
critical and it is okay to let's say get
rid of these records so obviously one of
the easiest ways of handling the data
part or missing data is remove those
records or remove those observations
from your analysis so that is the
easiest way to do but then the downside
is as I said in as in the first case if
let's say 50% of your data is like that
because some column or the other is
missing so it is not like every in every
place in every Row the same column is
missing but you have in maybe 10% of the
records column one is missing and
another 10% column 2 is missing another
10% column 3 is missing and so on and so
forth so it adds up to maybe half of
your data set so you cannot completely
remove half of your data set then the
whole purpose is lost okay so then how
do you handle then you need to come up
with ways of filling up this data with
some meaningful value right that is one
way of handling so when we say meaning F
value what is that meaningful value
let's say for a particular column you
might want to take a mean value for that
column and fill whever the data is
missing fill up with that mean value so
that when you're doing the calculations
your analysis is not completely way off
so you have values which are not missing
first of all so your system will work
number two these values are not so
completely out of whack that your whole
analysis goes for a toss right there may
be situations where if the missing
values instead of putting mean maybe a
good idea to fill it up with the minimum
value or with a zero so or with a
maximum value again as I said there are
so many possibilities so there is no
like one correct answer for this you
need to basically talk around this and
illustrate with your experience as I
said that would be the best otherwise
this is how you need to handle this
question okay so then the next question
can be how can you choose a classifier
based on a training set data size so
again this is one of those questions uh
where you probably do not have like a
one siiz fitall answer first of all you
may not let's say decide your classifier
based on the training set size maybe not
the best way to decide the type of the
classifier and uh even if you have to
there are probably some thumb rules
which we can use but then again every
time so in my opinion the best way to
respond to this question is you need to
try out few classifiers irrespective of
the size of the data and you need to
then decide on your particular situation
which of these classifiers are the right
ones this is a very generic issue so you
will never be able to just by if
somebody defines a a problem to you and
somebody even if if they show the data
to you or tell you what is the data or
even the size of the data I don't think
there is a way to really say that yes
this is the classifier that will work
here no that's not the right way so you
need to still uh you know test it out
get the data try out a couple of
classifiers and then only you will be in
a position to decide which classifier to
use you try out multiple classifiers see
which one gives the best accuracy and
only then you can decide then you can
have a question around confusion Matrix
so the question can be explained
confusion Matrix right so confusion
Matrix I think the best way to explain
it is by taking an example and drawing
like a small diagram otherwise it can
really become tricky so my suggestion is
to take a piece of pen and paper and uh
explain it by drawing a small Matrix and
confusion Matrix is about to find out
this is used especially in
classification uh learning process and
when you get the results when the our
model predicts the results you compare
it with the actual value and try to find
out what is the accuracy okay so in this
case let's say this is an example of a
confusion Matrix and uh it is a binary
Matrix so you have the actual values
which is the labeled data right and
which is so you have how many yes and
how many no so you have that information
and you have the predicted values how
many yes and how many no right so the
total actual values the total yes is 12
+ 13 and they are shown here and the
actual value NOS are 9 + 3 12 okay so
that is what this information here is so
this is about the actual and this is
about the predicted similarly the
predicted values there are yes are 12 +
3 15 yeses and no are 1 + 9 10 NOS okay
so this is the way to look at this
confusion Matrix okay and U out of this
what is the meaning conveyed so there
are two or three things that needs to be
explained outright the first thing is
for a model to be accurate the values
across the diag should be high like in
this case right that is one number two
the total sum of these values is equal
to the total observations in the test
data set so in this case for example you
have 12 + 3 15 + 10 25 so that means we
have 25 observations in our test data
set okay so these are the two things you
need to First explain that the total sum
in this Matrix the numbers is equal to
the size of the test data set and the
diagonal values indicate the accuracy so
by just by looking at it you can
probably have a idea about is this an
accurate model is the model being
accurate if they're all spread out
equally in all these four boxes that
means probably the accuracy is not very
good okay now how do you calculate the
accuracy itself right how do you
calculate the accuracy itself so it is a
very simple mathematical calculation you
take some of the diagonals right so in
this case it is 9 + 12 21 and divide it
by the total so in this case what will
it be let's me uh take a pen so your
your diagonal values is equal to if I
say d is equal to 12 + 9 so that is 21
right and the total data set is equal to
right we just calculated it is 25 so
what is your accuracy it is 21 by your
accuracy is equal to 21 by 25 and this
turns out to be about 85% right so this
is 85% so that is our accuracy okay so
this is the way you need to explain draw
a diagram Give an example and maybe it
may be a good idea to be prepared with
an example so that it becomes easy for
you don't have to calculate those
numbers on the fly right so couple of uh
hints are that you take some numbers
which are with which add up to 100 that
is always a good idea so you don't have
to really do this complex calculations
so the total value will be 100 and then
diagonal values you divide once you find
the diagonal values that is equal to
your percentage okay all right so the
next question can be a related question
about false positive and false negative
so what is false positive and what is
false negative now once again the best
way to explain this is using a piece of
paper and Pen otherwise it will be
pretty difficult to to explain this so
we use the same example of the confusion
Matrix and uh we can explain that so A
confusion Matrix looks somewhat like
this and um when we just take yeah it
look somewhat like this and we continue
with the previous example where this is
the actual value this is the predicted
value and uh in the actual value we have
12 + 1 13 yeses and 3 + 9 12 Nos and the
predicted values there are 12 + 3 15
yeses and uh 1 + 9 10 NOS okay now this
particular case which is the false
positive what is a false positive first
of all the second word which is positive
okay is referring to the predicted value
so that means the system has predicted
it as a positive but the real value so
this is what the false comes from but
the real value is not positive okay that
is the way you should understand this
term false positive or even false
negative so false positive so positive
is what your system has predicted so
where is that system predicted this is
the one positive is what yes so you
basically consider this row okay now if
you consider this row so this is this is
all positive values this entire row is
positive values okay now the false
positive is the one which where the
value actual value is negative predicted
value is positive but the actual value
is negative so this is a false positive
right and here is a true positive so the
predicted value is positive and the
actual value is also positive okay I
hope this is making sense now let's take
a look at what is false negative false
negative so negative is the second term
that means that is the predicted value
that we need to look for so which are
the predicted negative values this row
corresponds to predicted negative values
all right so this row corresponds to
predicted negative values and what they
are asking for false so this is the row
for predicted negative values and the
actual value is this one right this is
predicted negative and the actual value
is also negative therefore this is a
true negative so the false negative is
this one predicted is negative but
actual is positive right so this is the
false negative so this is the way to
explain and this is the way to look at
false positive and false negative same
way there can be true positive and true
negative as well so again positive the
second term you will need to use to
identify the predicted row right so if
we say true positive positive we need to
take for the predicted part so predicted
positive is here okay and then the first
term is for the actual so true positive
so true in case of actual is yes right
so true positive is this one okay and
then in case of actual the negative now
we are talking about let's say true
negative true negative negative is this
one and the true comes from here so this
is true negative right nine is true
negative the actual value is also
negative and the predicted value Val is
also negative okay so that is the way
you need to explain this the terms false
positive false negative and true
positive true negative then uh you might
have a question like what are the steps
involved in the machine learning process
or what are the three steps in the
process of developing a machine learning
model right so it is around the
methodology that is applied so basically
the way you can probably answer in your
own words but the way the model develop
vment of the machine learning model
happens is like this so first of all you
try to understand the problem and try to
figure out whether it is a
classification problem or a regression
problem based on that you select a few
algorithms and then you start the
process of training these models okay so
you can either do that or you can after
due diligence you can probably decide
that there is one particular algorithm
that which is most suitable usually it
happens through trial and error process
but at some point you will decide that
okay this is the model we are going to
use okay so in that case we have the
model algorithm and the model decided
and then you need to do the process of
training the model and testing the model
and this is where if it is supervised
learning you split your data the label
data into training data set and test
data set and you use the training data
set to train your model and then you use
the test data set to check the accuracy
whether it is working fine or not so you
test the model before you actually put
it into production right so once you
test the model you're satisfied it's
working fine then you go to the next
level which is putting it for production
and then in production obviously new
data will come and uh the inference
happens so the model is readily
available and only thing that happens is
new data comes and the model predicts
the values whether it is regression or
classification now so this can be an
iterative process so it is not a
straightforward process where you do the
training do the testing and then you
move it to production now so during the
training and test process there may be a
situation where because of either
overfitting or or things like that the
test doesn't go through which means that
you need to put that back into the
training process so that can be a an
iterative process not only that even if
the training and test goes through
properly and you deploy the model in
production there can be a situation that
the data that actually comes the real
data that comes with that this model is
failing so in which case you may have to
once again go back to the drawing board
or initially it will be working fine but
over a period of time maybe due to the
change in the nature of the data once
again the accuracy will deteriorate so
that is again a recursive process so
once in a while you need to keep
checking whether the model is working
fine or not and if required you need to
tweak it and modify it and so on and so
forth so net net this is a continuous
process of um tweaking the model and
testing it and making sure it is up to
date then you might have question around
deep learning so because deep learning
is now associated with AI artificial
intelligence and so on so can be as
simple as what is deep learning so I
think the best way to respond to this
could be deep learning is a part of
machine learning and then then obviously
the the question would be then what is
the difference right so deep learning
you need to mention there are two key
parts that interviewer will be looking
for when you're defining deep learning
so first is of course deep learning
learning is a subset of machine learning
so machine learning is still the bigger
let's say uh scope and deep learning is
one one part of it so then what exactly
is the difference deep learning is
primarily when we are implementing these
our algorithms or when we are using
neural networks for doing our training
and classification and regression and
all that right so when we use neural
network then it is considered as deep
learning and the term deep comes from
the fact that you you can have several
layers of neural networks and these are
called Deep neural networks and
therefore the term deep you know deep
learning uh the other difference between
machine learning and deep learning which
the interviewer may be wanting to hear
is that in case of machine learning the
feature engineering is done manually
what do we mean by feature engineering
basically when we are trying to train
our model we have our training data
right so we have our training label data
and uh this data has several let's say
if it is a regular table it has several
columns now each of these columns
actually has information about a feature
right so if we are trying to predict the
height weight and so on and so forth so
these are all features of human beings
let's say we have sensus data and we
have all this so those are the features
now there may be probably 50 or 100 in
some cases there may be 100 such
features now all of them do not
contribute to our model right so so we
as a data scientist we have to decide
whether we should take all of them all
the features or we should throw away
some of them because again if we take
all of them number one of course your
accuracy will probably get affected but
also there is a computational part so if
you have so many features and then you
have so much data it becomes very tricky
so in case of machine learning we
manually take care of identifying the
features that do not contribute to the
learning process and thereby we
eliminate those features and so on right
so this is known as feature engineering
and in machine learning we do that
manually whereas in deep learning where
we use neural networks the model will
automatically determine which features
to use and which to not use and
therefore feature engineering is also
done automatically so this is a
explanation these are two key things
probably will add value to your response
all right so the next question is what
is the difference between or what are
the differences between machine learning
and deep learning so here this is a a
quick comparison table between machine
learning and deep learning and in
machine learning learning enables
machines to take decisions on their own
based on past data so here we are
talking primarily of supervised learning
and um it needs only a small amount of
data for training and then works well on
low end system so you don't need large
machines and most features need to be
identified in advance and manually coded
so basically the feat feure engineering
part is done manually and uh the problem
is divided into parts and solved
individually and then combined so that
is about the machine learning part in
deep learning deep learning basically
enables machines to take decisions with
the help of artificial neural network so
here in deep learning we use neuralink
so that is the key differentiator
between machine learning and deep
learning and usually deep learning
involves a large amount of data and
therefore the training also requires
usually the training process requires
high-end machines uh because it needs a
lot of computing power and the Machine
learning features are or the feature
engineering is done automatically so the
neural networks takes care of doing the
feature engineering as well and in case
of deep learning therefore it is said
that the problem is handled end to end
so this is a quick comparison between
machine learning and deep learning in
case you have that kind of a question
then you might get a question around the
uses of machine learning or some real
life applications of machine learning in
modern business the question may be
worded in different ways but the the
meaning is how exactly is machine
learning used or actually supervised
machine learning it could be a very
specific question around supervised
machine learning so this is like give
examples of supervised machine learning
use of supervised machine learning in
modern business so that could be the
next question so there are quite a few
examples or quite a few use cases if you
will for supervised information learning
the very common one is email spam
detection so you want to train your
application or your system to detect
between spam and non-spam so this is a
very common business application of a
supervised machine learning so how does
this work the way it works is that you
obviously have historical data of your
emails and they are categorized as spam
and not spam so that is what is the
labeled information and then you feed
this information or the all these emails
as an input to your model right and the
model will then get trained to detect
which of the emails are to detect which
is Spam and which is not spam so that is
a training process and this is
supervised machine learning because you
have labeled data you already have
emails which are tagged as spam or not
spam and then you use that to train your
model right so this is one example now
there are a few few industry specific
applications for supervised machine
learning one of the very common ones is
in healthcare Diagnostics in healthcare
Diagnostics you have these images and
you want to train models to detect
whether from a particular image whether
it can find out if the person is sick or
not whether a person has cancer or not
right so this is a very good example of
supervised machine learning here the way
it works works is that existing images
it could be x-ray images it be MRI or
any of these images are available and
they are tacked saying that okay this
x-ray image is defective or the person
has an illness or it could be cancer
whichever illness right so it is tack as
defective or clear or good image and
defective image something like that so
we come up with the binary or it could
be multiclass as well saying that this
is defective to 10% this is 25% and so
on but let's keep it simple you can give
an example of just a binary
classification that would be good enough
so you can say that in healthcare
Diagnostics using image we need to
detect whether a person is ill or
whether a person is having cancer or not
so here the way it works is you feed
labeled images and you allow the model
to learn from that so that when New
Image is fed it will be able to predict
whether this person is having that
illness or not having cancer or not
right so I think this would be a very
good example for supervised machine
learning in modern business all right
then we can have a question like so
we've been talking about supervised and
um unsupervised then so there can be a
question around semi-supervised machine
learning so what is semi-supervised
machine learning now semi-supervised
learning as the name suggests it falls
between supervised learning and
unsupervised learning but for all
practical purposes it is considered as a
part of supervised learning and the
reason this has come into existence is
that in supervised learning you need
labeled data so all your data for
training your model has to be labeled
now this is a big problem in many
Industries or in many under many
situations getting the labeled data is
not that easy because there's a lot of
effort in labeling this data let's take
an example of the diagnostic images we
can just let's say take X-ray images now
there are actually millions of x-ray
images available all over the world but
the problem is they are not labeled so
the images are there but whether it is
defective or whether it is good that
information is not available along with
it right in a form that it can be used
by a machine which means that somebody
has to take a look at these images and
usually it should be like a doctor and
uh then say that okay yes this image is
clean and this image is cancerous and so
on and so forth now that is a huge
effort by itself so this is where
semi-supervised learning comes into play
so what happens is there is a large
amount of data maybe a part of it is
labeled then we try some techniques to
label the remaining part of the data so
that we get completely labeled data and
then we train our model so I know this a
little long winding explanation but
unfortunately there is no uh quick and
easy definition for semisupervised
machine learning this is the only way
probably to explain this concept we may
have another question as um what are
unsupervised machine learning techniques
or what are some of the techniques used
for performing unsupervised machine
learning so it can be worded in
different ways so how do we answer this
question so unsupervised learning you
can say that there are two types
clustering and Association
and clustering is a technique where
similar objects are put together and
there are different ways of finding
similar objects so their characteristics
can be measured and if they have in most
of the characteristics if they are
similar then they can be put together
this is clustering then Association you
can I think the best way to explain
Association is with an example in case
of Association you try to find out how
the items are linked to each other so
for example
if somebody bought a maybe a laptop the
person has also purchased a mouse so
this is more in an e-commerce scenario
for example so you can give this as an
example so people who are buying laptops
are also buying the mouse so that means
there is an association between laptops
and mouse or maybe people who are buying
bread are also buying butter so that is
a Association that can be created so
this is unsupervised learning one of the
techniques okay all right then we have
very fundamental question what is the
difference between supervised and
unsupervised machine learning so machine
learning these are the two main types of
machine learning supervised and unised
and in case of supervised and again here
probably the keyword that the person may
be wanting to hear is labeled data now
very often people say yeah we have
historical data and if we run it it is
supervised and if we don't have
historical data yes but you may have
historical data but if it is not labeled
then you cannot use it for supervised
learning so it is it's very key to
understand that we put in that keyword
labeled okay so when we have labeled
data for training our model then we can
use supervised learning and if we do not
have labeled data then we use
unsupervised learning and there are
different algorithms available to
perform both of these types of uh
trainings so there can be another
question a little bit more theoretical
and conceptual in nature this is about
inductive machine learning and deductive
machine learning so the question can be
what is the difference between inductive
machine learning and deductive machine
learning or somewhat that manner so that
the exact phrase or exact question can
vary they can ask for examples and
things like that but that could be the
question so let's first understand what
is inductive and deductive training
inductive training is induced by
somebody and you can illust that with a
small example I think that always helps
so whenever you're doing some
explanation try as much as possible as I
said to give examples from your work
experience or give some analogies and
that will also help a lot in explaining
as well and for the interviewer also to
understand so here we'll take an example
or rather we will use an analogy so
inductive training is when we induce
some knowledge or the learning process
into a person without the person
actually EXP experiencing it okay what
can be an example so we can probably
tell the person or show a person a video
that fire can burn the F burn his finger
or fire can cause damage so what is
happening here this person has never
probably seen a fire or never seen
anything getting damaged by fire but
just because he has seen this video he
knows that okay fire is dangerous and if
fire can cause damage right so so this
is inductive learning compared to that
what is deductive learning so here you
draw conclusion or the person draws
conclusion out of experience so we will
stick to the analogy so compared to the
showing a video Let's assume a person is
allowed to play with fire right and then
he figures out that if he puts his
finger it's burning or if throws
something into the fire it burns so he
is learning through experience so this
is known as deductive learning okay so
you can have have applications or models
that can be trained using inductive
learning or deductive learning all right
I think uh probably that explanation
will be sufficient the next question is
are KNN and K means clustering similar
to one another or are they same right
because that the letter K is kind of
common between them okay so let us take
a little while to understand what these
two are one is KNN and another is KNN
KNN stands for K nearest neighbors and K
means of course is the clustering
mechanism now these two are completely
different except for the letter K being
common between them KNN is completely
different K means clustering is
completely different KNN is a
classification process and therefore it
it comes under supervised learning
whereas K means clustering is actually a
unsupervised okay when you have KN andn
when you want to implement KNN which is
basically basically K nearest neighbors
the value of K is a number so you can
say k is equal to 3 you want to
implement K and N with K is equal to
three so which means that it performs
the classification in such a way that
how does it perform the classification
so it will take three nearest objects
and that's why it's called nearest
neighbor so basically based on the
distance it will try to find out its
nearest objects that are let's say three
of the nearest objects and then it will
check whether the class they belong to
which class right so if all three belong
to one particular class obviously this
new object is also classified as that
particular class but it is possible that
they may be from two or three different
classes okay so let's say they are from
two classes and then if they are from
two classes now usually you take a odd
number you assign a odd number to so if
there are three of them and two of them
belong to one class and then one belongs
to another class so this new object is
assigned to the class to which the two
of them belong now now the value of K is
sometimes tricky whether should you use
three should you use five should you use
seven that can be tricky because the
ultimate classification can also vary so
it's possible that if you're taking K as
three the object is probably in one
particular class but if you take K is
equal to 5 maybe the object will belong
to a different class because when you're
taking three of them probably two of
them belong to class one and one belong
to class two whereas when you take five
of them it is possible that only two of
them belong to class one and three of
them belong to class two so which means
that this object will belong to class
two right so you see that so this the
class allocation can vary depending on
the value of K now K means on the other
hand is a clustering process and it is
unsupervised where what it does is the
system will basically identify how the
objects are how close the objects are
with respect to some of their features
okay and but the similarity of course is
the the letter K and in case of K means
also we specify its value and it could
be three or five or seven there is no
technical limit as such but it can be
any number of clusters that uh you can
create okay so based on the value that
you provide the system will create that
many clusters of similar objects so
there is a similarity to that extent
that K is a number in both the cases but
actually these two are completely
different processes we have what is
known as na Bas classifier and people
often get confused thinking that naive
base is the name of the person who found
this uh classifier or who developed this
classifier which is not 100% true base
is the name of the person b a y s is the
name of the person but naive is not the
name of the person right so naive is
basically an English word and that has
been added here because of the nature of
this particular classifier n based
classifier is a probability based
classifier and uh it makes some
assumptions that presence of one feature
of a class is not related to the
presence of any other feature of maybe
other classes right so which is not very
strong or not a very what do you say
accurate assumption because these
features can be related and so on but
even if you go with this assumption this
whole algorithm works very well even
with this assumption and uh that is the
good side of it but the term comes from
there so that is the explanation that
you can give then there can be question
around reinforcement learning it can be
paraphrased in multiple ways one could
be can you explain how a system can play
a game of chess using reinforcement
learning or it can be any game so the
best way to explain this is again to
talk a little bit about what
reinforcement learning is about and then
elaborate on that to explain the process
so first of all reinforcement learning
has an environment and an age agent and
the agent is basically performing some
actions in order to achieve a certain
goal and this goals can be anything
either if it is related to game then the
goal could be that you have to score
very high score high value High number
or it could be that your uh number of
lives should be as high as possible
don't lose lives so this could be some
of them more advanced examples could be
for driving in the automotive industry
self-driving cars they actually also
make use of reinforcement learning to
teach the car how to navigate through
the roads and so on and so forth that is
also another example now how does it
work so if the system is basically there
is an agent and environment and every
time the agent takes a step or performs
a task which is taking it towards the
goal the final goal let's say to
maximize the score or to minimize the
number of lives and so on or minimize
the deaths for example it is a reward
and every time it takes a step which
goes against that goal right contrary or
in the reverse Direction it is penalized
okay so it is like a carrot and stick
system now how do you use this to create
a game of chess so to create a system to
play a game of chess now the way this
works is and this could probably go back
to this alphago example where alphao
defeated a human Champion so the way it
works is in reinforcement learning the
system is allowed for example if in this
case we're talking about Chess so we
allow the system to first of all watch
playing a game of chess so it could be
with a human being or it could be the
system itself there are computer games
of Chess right so either this new
learning system has to watch that game
or watch a human being play the game
because this is reinforcement uh
learning is pretty much all visual so
when you're teaching the system to play
a game the system will not actually go
behind the scenes to understand the
logic of your software of this game or
anything like that it is just visually
watching the screen and then it learns
okay so reinforcement learning to a
large extent works on that so you need
to create a mechanism whereby your model
will be able to watch somebody playing
the game and then you allow the system
also to start playing the game so it
pretty much starts from scratch okay and
and as it moves forward it it it's right
at the beginning the system really knows
nothing about the game of chess okay so
initially it is a clean slate it just
starts by observing how you playing so
it will make some random moves and keep
losing badly but then what happens is
over a period of time so you need to now
allow the system or you need to play
with this system not just 1 2 3 four or
five times but hundreds of times
thousands of times maybe even hundreds
of thousands of times and that's exactly
how alpha go has done it played millions
of games between itself and the system
right so for the game of chess also you
need to do something like that you need
to allow the system to playes and uh
then learn on its own over a period of
repetition so I think you can probably
explain it to this much to this extent
and it should be uh sufficient now this
is another question which is again
somewhat similar but here the size is
not coming into picture so the question
is how will you know which machine
learning algorithm to choose for your
classification problem now this is not
only classification problem it could be
a regression problem I would like to
generalize this question so if somebody
asks you how will you choose how will
you know which algorithm to use the
simple answer is there is no way you can
decide exactly saying that this is the
algorithm I'm going to use in a variety
of situations there are some guidelines
like for example you will obviously
depending on the problem you can say
whether it is a classification problem
or a regression problem and then in that
sense you are kind of restricting
yourself to if it is a classification
problem there are you can only apply a
classification algorithm right to that
extent you can probably let's say limit
the number of algorithms but now within
the classification algorithms you have
decision trees you have SPM you have
logistic regression is it possible to
outright say yes so for this particular
problem since you have explained this
now this is the exact algorithm that you
can use that is not possible okay so we
have to try out a bunch of algorithms
see which one gives us the best
performance and best accuracy and then
decide to go with that particular
algorithm so in machine learning a lot
of it happens through trial and error
there is a no real possibility that
anybody can just by looking at the
problem or understanding the problem
tell you that okay in this particular
situation this is exactly the algorithm
that you should use then the questions
may be around application of machine
learning and this question is
specifically around how Amazon is able
to recommend other things to buy so this
is around recommendation engine how does
it work how does the recommendation
engine work so this is basically the
question is all about so the
recommendation engine again Works based
on various inputs that are provided
obviously something like uh know Amazon
a website or e-commerce site like Amazon
collects a lot of data around the
customer Behavior who is purchasing what
and if somebody is buying a particular
thing they're also buying something else
so this kind of Association right so
this is the UN super provis learning we
talked about they use this to associate
and Link or relate items and that is one
part of it so they kind of build
association between items saying that
somebody buying this is also buying this
that is one part of it then they also
profile the users right based on their
age their gender their geographic
location they will do some profiling and
then when somebody is logging in and
when somebody is shopping kind of the
mapping of these two things are done
they try to identify obviously if you
have logged in then they know who you
are and your information is available
like for example your age may be your
agenda and uh where you're located what
you purchased earlier right so all this
is taken and the recommendation engine
basically uses all this information and
comes up with recommendations for a
particular user so that is how the
recommendation engine work all right
then the question can be uh something
very basic like when will you go for
classification versus regression right
when do you do classification instead of
regression or when we you use
classification instead of regression now
yes so so this is basically going back
to the understanding understanding of
the basics of classification and
regression so classification is used
when you have to identify or categorize
things into discrete classes so the best
way to respond to this question is to
take up some examples and use it
otherwise it can become a little tricky
the question may sound very simple but
explaining it can sometimes be very
tricky in case of regression we use of
course there will be some keywords that
they will be looking for so just you
need to make sure you use those keywords
one is the discrete values and other is
the continuous values so for regression
if you are trying to find some
continuous values you use regression
whereas if you're trying to find some
discrete values you use classification
and then you need to illustrate what are
some of the examples so classification
is like let's say there are images and
you need to put them into classes like
cat dog elephant tiger something like
that so that is a classification problem
or it can be that is a multiclass
classification problem it could be
binary classification problem like for
example whether a customer will buy or
he will not buy that is a classification
binary classification it can be in the
weather forecast area now weather
forecast is again combination of
regression and classification because on
the one hand you want to predict whether
it's going to rain or not that's a
classification problem that's a binary
classification right whether it's going
to rain or not rain however you also
have to predict what is going to be the
temperature tomorrow right now
temperature is a continuous value you
can't answer the temperature in a yes or
no kind of a response right so what will
be the temperature tomorrow so you need
to give a number which can be like 20
30 or whatever right so that is where
you use regression one more example is
stock price prediction so that is where
again you will use regression so these
are the various examples so you need to
illustrate with examples and make sure
you include those keywords like discrete
and continuous so the next question is
more about a little bit of a design
related question to understand your
Concepts and things like that so it is
how will you design a spam filter so how
do you basically design or develop a
spam filter so I think the main thing
here is he is looking at probably
understanding your Concepts in terms of
uh what is the algorithm you will use or
what is your understanding about
difference between classification and
regression uh and things like that right
and the process of course the
methodology and the process so the best
way to go about responding to this is we
say that okay this is a classification
problem because we want to find out
whether an email is a spam or not spam
so that we can apply the filter
accordingly so first thing is to
identify what type of a problem it is so
we have identified that it is a
classification then the second step may
be to find out what kind of algorithm to
use now since this is a binary
classification problem logistic
regression is a very common very common
algorithm but however right as I said
earlier also we can never say that okay
for this particular problem this is
exactly the algorithm that we can use so
we can also probably try decision trees
or even support Vector missions for
example SPM so we will kind of list down
a few of these algorithms and we will
say okay we want to we would like to try
out these algorithms and then we go
about taking your historical data which
is the labeled data which are marked so
you will have a bunch of emails and uh
then you split that into training and
test data sets you use your training
data set to train your model that or
your algorithm that you have used rather
the model actually so and you actually
will have three models let's say you are
trying to test out three algorithms so
you will obviously have three models so
you need to try all three models and
test them out as well see which one
gives the best accuracy and then you
decide that you will go with that model
okay so training and test will be done
and then you zero in on one particular
model and then you say okay this is the
model will we use we will use and then
go ahead and Implement that or put that
in production so that is the way you
design a Spam F the next question is
about random Forest so what is random
Forest so this is a very straightforward
question however the response you need
to be again a little careful while we
all know what is random Forest
explaining this can sometimes be tricky
so one thing is random Forest is kind of
in one way it is an extension of
decision trees because it is basically
nothing but you have multiple decision
trees and uh trees will basically we
will use for doing if it is
classification mostly it is
classification you will use the the
trees for classification and then you
use voting for finding the the final
class so that is the underlyings but how
will you explain this how will you
respond to this so first thing obviously
we will say that random Forest is one of
of the algorithms and the more important
thing that you need to probably the
interviewer is is waiting to hear is
Ensemble learner right so this is one
type of Ensemble learner what is
Ensemble learner Ensemble learner is
like a combination of algorithm so it is
a learner which consists of more than
one algorithm or more than one maybe
models okay so in case of random Forest
the algorithm is the same but instead of
using one instance of it we use multiple
instances of it and we use so in a way
that is a a random Forest is an ensemble
learner there are other types of
Ensemble Learners where we have like we
use different algorithms itself so you
have one maybe logistic regression and a
decision tree combined together and so
on and so forth or there are other ways
like for example splitting the data in a
certain way and so on so that's all
about Ensemble we will not go into that
but random Forest itself I think the
interviewer will be happy to hear this
word Ensemble Learners and so then you
go and explain how the random Forest
works so if the random Forest is used
for classification then we use what is
known as a voting mechanism so basically
how does it work let's say your random
Forest consists of 100 trees okay and
each observation you pass through this
forest and each observation let's say it
is a classification problem binary
classification zero or one and you have
100 trees now if 90 trees say that it is
a zero and 10 of the trees say it is a
one you take the majority you may take a
vote and since 90 of them are saying
zero you classify this as zero then you
take the next observation and so on so
that is the way random Forest works for
classification if it is a regression
problem it's somewhat similar but only
thing is instead of what what we will do
is so in regression remember what
happens you actually calculate a value
right so for example you're using
regression to predict the temperature
and you have 100 trees and each tree
obviously will probably predict a
different value of the temperature they
may be close to each other but they may
not be exactly the same value so these
100 trees so how do you now find the
actual value the output for the entire
Forest right so you have outputs of
individual trees which are a part of
this Forest but then you need to find
the final output of the forest itself so
how do you do that so in case of
regression you take like an average of
the mean of all the 100 G right so this
is also a way of reducing the error so
maybe if you have only one tree and if
that one tree makes a error it is
basically 100% wrong or 100% right right
but if you have on the other hand if you
have a bunch of trees you are basically
mitigating that error or reducing that
error okay so that is the way random
Forest works so the next question is
considering the long list of machine
learning algorithms how will you decide
on which one to use so once again here
there is no way to outright say that
this is the algorithm that we will use
for a given data set this is a very good
question but then the response has to be
like again there will not be a one size
fits all so we need to first of all you
can probably shorten the list in terms
of by saying okay whether it is a
classification problem or it is a
regression problem to that extent you
can probably uh shorten the list because
you don't have to use all of them if it
is a classification problem you only can
pick from the classification algorithms
right so for example if it's a
classification you cannot use linear
regression algorithm there or if it is a
regression problem you cannot use svm or
maybe no you can use svm but maybe a
logistic regression right so to that
extent you can probably shorten the list
but still you will not be able to 100%
decide on saying that this is the exact
algorithm that I'm going to use so the
way to go about is you choose a few
algorithms based on what the problem is
you try out your data you train some
models of these algorithms check which
one gives you the lowest error or the
highest accuracy and based on that you
choose that particular algorithm okay
all right then there can be questions
around bias and variance so the question
can be what is bias and variance in
machine learning uh so you just need to
give out a definition for each of these
for example bias in machine learning it
occurs when the predicted values are far
away from the actual value so that is a
bias okay and whereas they are all all
the values are probably they are far off
but they are very near to each other
though the predicted values are close to
each other right while they are far off
from the actual value but they are close
to each other you see the difference so
that is bias and then the other part is
your variance now variance is when the
predicted values are all over the place
right so the variance is high that means
it may be close to the Target but it is
kind of very scattered so the point the
pred predicted values are not close to
each other right in case of buyers the
predicted values are close to each other
but they are not close to the Target but
here they may be close to the Target but
they may not be close to each other so
they are a little bit more scattered so
that is what in case of a variance okay
then the next question is about again
related to bias and variance what is the
tradeoff between bias and variance yes I
think this is a interesting question
because these two are heading in
different directions so for example
example if you try to minimize the bias
variance will keep going high and if you
try to minimize the variance bias will
keep going high and there is no way you
can minimize both of them so you need to
have a tradeoff saying that okay this is
the level at which I will have my bias
and this is the level at which I will
have variance so the trade-off is that
pretty much uh that you you decide what
is the level you will tolerate for your
buyers and what is the level you will
tolerate for variance and combination of
these two in such a way that your final
results are not way off and having a
tradeoff will ensure that the results
are consistent right so that is
basically the output is consistent and
which means that they are close to each
other and they're also accurate that
means they are as close to the Target as
possible right so if either of these is
high then one of them will go off the
track define precision and Recall now
again here I think uh it would be best
to uh draw a diagram and take a the
confusion Matrix and it is very simple
the definition is like a formula your
Precision is true positive by true
positive plus false positive and your
recall is true positive by true positive
plus false negative okay so that's you
can just show it in a mathematical way
that's pretty much uh you know that can
be shown that's the easiest way to
define so the next question
can be about decision tree what is
decision tree pruning and why is it so
basically decision trees are really
simple to implement and understand but
one of the drawbacks of decision trees
is that it can become highly complicated
as it grows right and the rules and the
conditions can become very complicated
and this can also lead to overfitting
which is basically that during training
you will get 100% accuracy but when
you're doing testing you'll get a lot of
Errors so that is the reason pruning
needs to be done so the purpose or the
reason for doing uh decision tree
pruning is to reduce overfitting or to
cut down on overfitting and what is
decision tree pruning it is basically
that you reduce the number of branches
because as you may be aware a tree
consists of the root node and then there
are several internal nodes and then you
have the leaf nodes now if there are too
many of these internal nodes that is
when you face the problem of overfitting
and pruning is the process of reducing
those internal nodes all right so the
next question can be what is logistic
regression uh so basically logistic
regression is um one of the techniques
used for performing classification
especially binary classification now
there is something special about
logistic regression and there are a
couple of things you need to be careful
about first of all the name is a little
confusing it is called logistic
regression but it is used for
classification so this can be sometimes
confusing so you need to probably
clarify that to the interviewer if if
it's really you know if it is required
and they can also ask this like a trick
question right so that is one part
second thing is the term logistic has
nothing to do with the usual Logistics
that we talk about but it is derived
from log so that the mathematical
derivation involves log and therefore
the name logistic regression so what is
logistic regression and how is it used
so logistic regression is used for
binary classification and the output of
a logistic regression is either a zero
or a one and it varies so it's basically
it calculates a probability between zero
and one and we can set a threshold that
can vary typically it is 0. five so any
value above 0.5 is considered as one and
if the probability is below 0.5 it is
considered as zero so that is the way we
calculate the probability or the system
calculates the probability and based on
the threshold it sets a value of zero or
one which is like a binary
classification zero or one okay then we
have a question around K nearest
neighbor algorithm so explain K nearest
neighbor algorithm so first of all what
is a k nearest neighbor algorithm this
is a classification algorithm so that
that is the first thing we need to
mention and we also need to mention that
the K is a number it is an integer and
this is variable and we can Define what
the value of K should be it can be 2 3 5
7 and usually it is an odd number so
that is something we need to mention
technically it can be even number also
but then typically it would be odd
number and we will see why that is okay
so based on that we need to classify
objects okay we need to classify objects
so again it will be very helpful to draw
a diagram you know if you're explaining
I think that will be the best way so
draw some diagram like this and let's
say we have three clusters or three
classes existing and now you want to
find for a new item that has come you
want to find out which class this
belongs to right so you go about as the
name suggests you go about finding the
nearest neighbors right the points which
are closest to this and how many of them
you will find that is what is defined by
K now let's say our initial value of K
was five okay so you will find the K the
five nearest data points so in this case
as it is Illustrated these are the five
nearest data points but then all five do
not belong to the same class or cluster
so there are one belonging to this
cluster one the second one belonging to
this cluster to three of them belonging
to this third cluster okay so how do you
decide that's exactly the reason we
should as much as possible try to assign
a odd number so that it becomes easier
to assign this so in this case you see
that the majority actually if there are
multiple classes then you go with the
majority so since three of these items
belong to this class we assign which is
basically the in in this case the green
or the tennis or the third cluster as I
was talking about right so we assign it
to this third class so in this case it
is uh that's how it is decided okay so K
nearest neighbor so first thing is to
identify the number of neighbors that
are mentioned as K so in this case it is
K is equal to five so we find the five
nearest points and then find out out of
these five which class has the maximum
number in that okay and and then the uh
new data point is assigned to that and
with that we have come to the end of
machine learning beginners to Advance
full course I hope you found it valuable
and entertaining please ask any
questions about the topics covered in
this video in the comment section below
our experts will assist you in
addressing your problems thank you for
watching stay safe and keep learning
staying ahead in your career requires
continuous learning and upscaling
whether you're a student aiming to learn
today's top skills or a working
professional looking to advance your
career we've got you covered explore our
impressive catalog of certification
programs in cuttingedge domains
including data science cloud computing
cyber security AI machine learning or
digital marketing designed in
collaboration with leading universities
and top corporations and delivered by
industry experts choose any of our
programs and set yourself on the path to
Career Success click the link in the
description to know
more hi there if you like this video sub
subscribe to the simply learn YouTube
channel and click here to watch similar
videos to nerd up and get certified
click
here